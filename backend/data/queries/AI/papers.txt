{
    title: Representation Learning: A Review and New Perspectives,
    abstract: The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.,
    publicationDate: 2012-06-24,
    authors: ['Yoshua Bengio', 'Aaron C. Courville', 'Pascal Vincent'],
    score: 206.79462682196944
},
{
    title: Learning Deep Architectures for AI,
    abstract: Theoretical results strongly suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one needs deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult optimization task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.,
    publicationDate: None,
    authors: ['Yoshua Bengio'],
    score: 195.67313898372976
},
{
    title: A Survey on Bias and Fairness in Machine Learning,
    abstract: With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.,
    publicationDate: 2019-08-23,
    authors: ['Ninareh Mehrabi', 'Fred Morstatter', 'N. Saxena', 'Kristina Lerman', 'A. Galstyan'],
    score: 194.33427610809076
},
{
    title: Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI),
    abstract: At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.,
    publicationDate: 2018-09-17,
    authors: ['Amina Adadi', 'M. Berrada'],
    score: 193.50176433584227
},
{
    title: Probabilistic reasoning in intelligent systems - networks of plausible inference,
    abstract: From the Publisher: 
Probabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertaintyand offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognitionin short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information. 
Probabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability.,
    publicationDate: None,
    authors: ['J. Pearl'],
    score: 192.88192658786176
},
{
    title: Efficient Processing of Deep Neural Networks: A Tutorial and Survey,
    abstract: Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances toward the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic codesigns, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the tradeoffs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.,
    publicationDate: 2017-03-27,
    authors: ['V. Sze', 'Yu-hsin Chen', 'Tien-Ju Yang', 'J. Emer'],
    score: 190.20987540524695
},
{
    title: Understanding computers and cognition,
    abstract: Winograd and Flores' `Understanding Computers and Cognition' proposes that the rationalist tradition in AI must be replaced by a hermeneutic approach. Associating the rationalist tradition with the goal of building a human mind, the authors propose that a hermeneutic approach must adopt the goal of constructing prostheses which magnify the human mind. This paper argues that what AI needs is not so much a hermeneutic approach as a better appreciation of biology and psychology. Understanding Computers and Cognition is a groundbreaking book that presents an important new approach to understanding what computers do and how their functioning is related to human language, thought and action.,
    publicationDate: 1987-09-01,
    authors: ['T. Winograd', 'F. Flores'],
    score: 189.031284876513
},
{
    title: Education in the Era of Generative Artificial Intelligence (AI): Understanding the Potential Benefits of ChatGPT in Promoting Teaching and Learning,
    abstract: Since its maiden release into the public domain on November 30, 2022, ChatGPT garnered more than one million subscribers within a week. The generative AI tool ⎼ChatGPT took the world by surprise with it sophisticated capacity to carry out remarkably complex tasks. The extraordinary abilities of ChatGPT to perform complex tasks within the field of education has caused mixed feelings among educators, as this advancement in AI seems to revolutionize existing educational praxis. This is an exploratory study that synthesizes recent extant literature to offer some potential benefits and drawbacks of ChatGPT in promoting teaching and learning. Benefits of ChatGPT include but are not limited to promotion of personalized and interactive learning, generating prompts for formative assessment activities that provide ongoing feedback to inform teaching and learning etc. The paper also highlights some inherent limitations in the ChatGPT such as generating wrong information, biases in data training, which may augment existing biases, privacy issues etc. The study offers recommendations on how ChatGPT could be leveraged to maximize teaching and learning. Policy makers, researchers, educators and technology experts could work together and start conversations on how these evolving generative AI tools could be used safely and constructively to improve education and support students’ learning.,
    publicationDate: 2023-12-31,
    authors: ['David Baidoo-Anu', 'Leticia Owusu Ansah'],
    score: 185.54222740625806
},
{
    title: From local explanations to global understanding with explainable AI for trees,
    abstract: None,
    publicationDate: 2020-01-01,
    authors: ['Scott M. Lundberg', 'G. Erion', 'Hugh Chen', 'A. DeGrave', 'J. Prutkin', 'B. Nair', 'R. Katz', 'J. Himmelfarb', 'N. Bansal', 'Su-In Lee'],
    score: 185.3518450488482
},
{
    title: Rough sets,
    abstract: Rough set theory, introduced by Zdzislaw Pawlak in the early 1980s [11, 12], is a new mathematical tool to deal with vagueness and uncertainty. This approach seems to be of fundamental importance to artificial intelligence (AI) and cognitive sciences, especially in the areas of machine learning, knowledge acquisition, decision analysis, knowledge discovery from databases, expert systems, decision support systems, inductive reasoning, and pattern recognition.,
    publicationDate: 1995-11-01,
    authors: ['Z. Pawlak', 'J. Grzymala-Busse', 'R. Słowiński', 'W. Ziarko'],
    score: 185.0617151974401
},
{
    title: Relational inductive biases, deep learning, and graph networks,
    abstract: Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. 
The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.,
    publicationDate: 2018-06-04,
    authors: ['P. Battaglia', 'Jessica B. Hamrick', 'V. Bapst', 'Alvaro Sanchez-Gonzalez', 'V. Zambaldi', 'Mateusz Malinowski', 'Andrea Tacchetti', 'David Raposo', 'Adam Santoro', 'Ryan Faulkner', 'Çaglar Gülçehre', 'H. F. Song', 'A. J. Ballard', 'J. Gilmer', 'George E. Dahl', 'Ashish Vaswani', 'Kelsey R. Allen', 'C. Nash', 'Victoria Langston', 'Chris Dyer', 'N. Heess', 'D. Wierstra', 'Pushmeet Kohli', 'M. Botvinick', 'O. Vinyals', 'Yujia Li', 'Razvan Pascanu'],
    score: 184.94475847695116
},
{
    title: A Survey of Large Language Models,
    abstract: Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.,
    publicationDate: 2023-03-31,
    authors: ['Wayne Xin Zhao', 'Kun Zhou', 'Junyi Li', 'Tianyi Tang', 'Xiaolei Wang', 'Yupeng Hou', 'Yingqian Min', 'Beichen Zhang', 'Junjie Zhang', 'Zican Dong', 'Yifan Du', 'Chen Yang', 'Yushuo Chen', 'Z. Chen', 'Jinhao Jiang', 'Ruiyang Ren', 'Yifan Li', 'Xinyu Tang', 'Zikang Liu', 'Peiyu Liu', 'J. Nie', 'Ji-rong Wen'],
    score: 184.01353689313123
},
{
    title: Can AI Help in Screening Viral and COVID-19 Pneumonia?,
    abstract: Coronavirus disease (COVID-19) is a pandemic disease, which has already caused thousands of causalities and infected several millions of people worldwide. Any technological tool enabling rapid screening of the COVID-19 infection with high accuracy can be crucially helpful to the healthcare professionals. The main clinical tool currently in use for the diagnosis of COVID-19 is the Reverse transcription polymerase chain reaction (RT-PCR), which is expensive, less-sensitive and requires specialized medical personnel. X-ray imaging is an easily accessible tool that can be an excellent alternative in the COVID-19 diagnosis. This research was taken to investigate the utility of artificial intelligence (AI) in the rapid and accurate detection of COVID-19 from chest X-ray images. The aim of this paper is to propose a robust technique for automatic detection of COVID-19 pneumonia from digital chest X-ray images applying pre-trained deep-learning algorithms while maximizing the detection accuracy. A public database was created by the authors combining several public databases and also by collecting images from recently published articles. The database contains a mixture of 423 COVID-19, 1485 viral pneumonia, and 1579 normal chest X-ray images. Transfer learning technique was used with the help of image augmentation to train and validate several pre-trained deep Convolutional Neural Networks (CNNs). The networks were trained to classify two different schemes: i) normal and COVID-19 pneumonia; ii) normal, viral and COVID-19 pneumonia with and without image augmentation. The classification accuracy, precision, sensitivity, and specificity for both the schemes were 99.7%, 99.7%, 99.7% and 99.55% and 97.9%, 97.95%, 97.9%, and 98.8%, respectively. The high accuracy of this computer-aided diagnostic tool can significantly improve the speed and accuracy of COVID-19 diagnosis. This would be extremely useful in this pandemic where disease burden and need for preventive measures are at odds with available resources.,
    publicationDate: 2020-03-29,
    authors: ['M. Chowdhury', 'Tawsifur Rahman', 'A. Khandakar', 'R. Mazhar', 'M. A. Kadir', 'Z. Mahbub', 'Khandakar R. Islam', 'Muhammad Salman Khan', 'A. Iqbal', 'N. Al-Emadi', 'M. Reaz'],
    score: 183.33094452845103
},
{
    title: Intelligent Clinical Documentation: Harnessing Generative AI for Patient-Centric Clinical Note Generation,
    abstract: Comprehensive clinical documentation is crucial for effective healthcare delivery, yet it poses a significant burden on healthcare professionals, leading to burnout, increased medical errors, and compromised patient safety. This paper explores the potential of generative AI (Artificial Intelligence) to streamline the clinical documentation process, specifically focusing on generating SOAP (Subjective, Objective, Assessment, Plan) and BIRP (Behavior, Intervention, Response, Plan) notes. We present a case study demonstrating the application of natural language processing (NLP) and automatic speech recognition (ASR) technologies to transcribe patient-clinician interactions, coupled with advanced prompting techniques to generate draft clinical notes using large language models (LLMs). The study highlights the benefits of this approach, including time savings, improved documentation quality, and enhanced patient-centered care. Additionally, we discuss ethical considerations, such as maintaining patient confidentiality and addressing model biases, underscoring the need for responsible deployment of generative AI in healthcare settings. The findings suggest that generative AI has the potential to revolutionize clinical documentation practices, alleviating administrative burdens and enabling healthcare professionals to focus more on direct patient care.,
    publicationDate: 2024-05-28,
    authors: ['Anjanava Biswas', 'Wrick Talukdar'],
    score: 182.6081979393303
},
{
    title: Artificial intelligence in healthcare: past, present and future,
    abstract: Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data and rapid progress of analytics techniques. We survey the current status of AI applications in healthcare and discuss its future. AI can be applied to various types of healthcare data (structured and unstructured). Popular AI techniques include machine learning methods for structured data, such as the classical support vector machine and neural network, and the modern deep learning, as well as natural language processing for unstructured data. Major disease areas that use AI tools include cancer, neurology and cardiology. We then review in more details the AI applications in stroke, in the three major areas of early detection and diagnosis, treatment, as well as outcome prediction and prognosis evaluation. We conclude with discussion about pioneer AI systems, such as IBM Watson, and hurdles for real-life deployment of AI.,
    publicationDate: 2017-06-21,
    authors: ['F. Jiang', 'Yong Jiang', 'Hui Zhi', 'Yi Dong', 'Hao Li', 'Sufeng Ma', 'Yilong Wang', 'Q. Dong', 'Haipeng Shen', 'Yongjun Wang'],
    score: 182.33667094233977
},
{
    title: Explainable AI: A Review of Machine Learning Interpretability Methods,
    abstract: Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.,
    publicationDate: 2020-12-25,
    authors: ['Pantelis Linardatos', 'Vasilis Papastefanopoulos', 'S. Kotsiantis'],
    score: 181.26370603621817
},
{
    title: Focus Groups,
    abstract: pub*ic rw•oitng ourdet tot the € oilbctnf of intormUta n ,t it•fitC tO V! ,af te•o p i oate. wcuddug ta ue for reuewimg *nsWUuicucm, a o•"iW l oa0 9 ts toerc, el. =aht~e ad maintainin~fg the data msodad. and conwe"~i ai t. 11 fl =tco= 71ia f nformuubom, Sd cominiots eowvgardiig Vnis bunkms eounate or m any Otat ma of tift Z O informo tion.tncludin g suggie tmons fotr rae"UIn th O..rceo. to Waskw ngton Hod ateirst SvmLt. oviectotat ' for mat uom i auaw ai dfand Itoon,.• 1 J12e1S .L Deovn HNghway. Sust 120. Arhngton. VA 222024302. and to the 0C* of M•nageft iudget ParicitftfloductiOn • 10.t2. Wa cOa. OC 20: i. AGENCY USE ONLY (Leave blanx) 2. REPORT DATE 3. REPORT TYPE AND DATES COVERED 1993, October Final Apr 93 Aug 93,
    publicationDate: None,
    authors: ['J. M. Savell'],
    score: 180.55345159027604
},
{
    title: Concrete Problems in AI Safety,
    abstract: Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.,
    publicationDate: 2016-06-21,
    authors: ['Dario Amodei', 'C. Olah', 'J. Steinhardt', 'P. Christiano', 'John Schulman', 'Dandelion Mané'],
    score: 180.07041384785964
},
{
    title: Review of Artificial Intelligence Techniques in Imaging Data Acquisition, Segmentation, and Diagnosis for COVID-19,
    abstract: The pandemic of coronavirus disease 2019 (COVID-19) is spreading all over the world. Medical imaging such as X-ray and computed tomography (CT) plays an essential role in the global fight against COVID-19, whereas the recently emerging artificial intelligence (AI) technologies further strengthen the power of the imaging tools and help medical specialists. We hereby review the rapid responses in the community of medical imaging (empowered by AI) toward COVID-19. For example, AI-empowered image acquisition can significantly help automate the scanning procedure and also reshape the workflow with minimal contact to patients, providing the best protection to the imaging technicians. Also, AI can improve work efficiency by accurate delineation of infections in X-ray and CT images, facilitating subsequent quantification. Moreover, the computer-aided platforms help radiologists make clinical decisions, i.e., for disease diagnosis, tracking, and prognosis. In this review paper, we thus cover the entire pipeline of medical imaging and analysis techniques involved with COVID-19, including image acquisition, segmentation, diagnosis, and follow-up. We particularly focus on the integration of AI with X-ray and CT, both of which are widely used in the frontline hospitals, in order to depict the latest progress of medical imaging and radiology fighting against COVID-19.,
    publicationDate: 2020-04-06,
    authors: ['F. Shi', 'Jun Wang', 'Jun Shi', 'Zi-xiang Wu', 'Qian Wang', 'Zhenyu Tang', 'Kelei He', 'Yinghuan Shi', 'D. Shen'],
    score: 180.01595814892835
},
{
    title: Introduction to linear optimization,
    abstract: p. 27, l. −11, replace “Schwartz” by “Schwarz” p. 69, l. −13: “ai∗x = bi” should be “ai∗x = bi∗” p. 126, l. 16, replace “inequality constraints” by “linear inequality constraints” p. 153, l. −8, replace aix 6= bi by aix 6= bi p. 163, Example 4.9, first line: replace “from” with “form” p. 165, l. 11, replace p′Ax ≥ 0 by p′Ax ≥ 0 p. 175, l. 1, replace “To this see” by “To see this” p. 203, l. 12: replace x ≥ 0 by x ≥ 0, xn+1 ≥ 0 p. 216, l. −6: replace “≤ c}” by “≤ c′}” p. 216, l. −3: replace c′ by (c1)′ p. 216, l. −2: replace c′ by (c2)′ p. 216, l. −1: right-hand side should be λ(c1)′ + (1− λ)(c2)′ p. 220, l. −12: replace “added to the pivot row” by “added to the zeroth row”,
    publicationDate: None,
    authors: ['D. Bertsimas', 'J. Tsitsiklis'],
    score: 179.65922980132726
},
{
    title: The Arcade Learning Environment: An Evaluation Platform for General Agents,
    abstract: In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.,
    publicationDate: 2012-07-19,
    authors: ['Marc G. Bellemare', 'Yavar Naddaf', 'J. Veness', 'Michael Bowling'],
    score: 179.55592360787142
},
{
    title: Artificial Intelligence in Education: A Review,
    abstract: The purpose of this study was to assess the impact of Artificial Intelligence (AI) on education. Premised on a narrative and framework for assessing AI identified from a preliminary analysis, the scope of the study was limited to the application and effects of AI in administration, instruction, and learning. A qualitative research approach, leveraging the use of literature review as a research design and approach was used and effectively facilitated the realization of the study purpose. Artificial intelligence is a field of study and the resulting innovations and developments that have culminated in computers, machines, and other artifacts having human-like intelligence characterized by cognitive abilities, learning, adaptability, and decision-making capabilities. The study ascertained that AI has extensively been adopted and used in education, particularly by education institutions, in different forms. AI initially took the form of computer and computer related technologies, transitioning to web-based and online intelligent education systems, and ultimately with the use of embedded computer systems, together with other technologies, the use of humanoid robots and web-based chatbots to perform instructors’ duties and functions independently or with instructors. Using these platforms, instructors have been able to perform different administrative functions, such as reviewing and grading students’ assignments more effectively and efficiently, and achieve higher quality in their teaching activities. On the other hand, because the systems leverage machine learning and adaptability, curriculum and content has been customized and personalized in line with students’ needs, which has fostered uptake and retention, thereby improving learners experience and overall quality of learning.,
    publicationDate: 2020-04-17,
    authors: ['Lijia Chen', 'Pingping Chen', 'Zhijian Lin'],
    score: 179.4958466092731
},
{
    title: Edge Intelligence: Paving the Last Mile of Artificial Intelligence With Edge Computing,
    abstract: With the breakthroughs in deep learning, the recent years have witnessed a booming of artificial intelligence (AI) applications and services, spanning from personal assistant to recommendation systems to video/audio surveillance. More recently, with the proliferation of mobile computing and Internet of Things (IoT), billions of mobile and IoT devices are connected to the Internet, generating zillions bytes of data at the network edge. Driving by this trend, there is an urgent need to push the AI frontiers to the network edge so as to fully unleash the potential of the edge big data. To meet this demand, edge computing, an emerging paradigm that pushes computing tasks and services from the network core to the network edge, has been widely recognized as a promising solution. The resulted new interdiscipline, edge AI or edge intelligence (EI), is beginning to receive a tremendous amount of interest. However, research on EI is still in its infancy stage, and a dedicated venue for exchanging the recent advances of EI is highly desired by both the computer system and AI communities. To this end, we conduct a comprehensive survey of the recent research efforts on EI. Specifically, we first review the background and motivation for AI running at the network edge. We then provide an overview of the overarching architectures, frameworks, and emerging key technologies for deep learning model toward training/inference at the network edge. Finally, we discuss future research opportunities on EI. We believe that this survey will elicit escalating attentions, stimulate fruitful discussions, and inspire further research ideas on EI.,
    publicationDate: 2019-05-24,
    authors: ['Zhi Zhou', 'Xu Chen', 'En Li', 'Liekang Zeng', 'Ke Luo', 'Junshan Zhang'],
    score: 178.8827308806987
},
{
    title: Guidelines for clinical trial protocols for interventions involving artificial intelligence: the SPIRIT-AI extension,
    abstract: The SPIRIT 2013 statement aims to improve the completeness of clinical trial protocol reporting by providing evidence-based recommendations for the minimum set of items to be addressed. This guidance has been instrumental in promoting transparent evaluation of new interventions. More recently, there has been a growing recognition that interventions involving artificial intelligence (AI) need to undergo rigorous, prospective evaluation to demonstrate their impact on health outcomes. The SPIRIT-AI (Standard Protocol Items: Recommendations for Interventional Trials–Artificial Intelligence) extension is a new reporting guideline for clinical trial protocols evaluating interventions with an AI component. It was developed in parallel with its companion statement for trial reports: CONSORT-AI (Consolidated Standards of Reporting Trials–Artificial Intelligence). Both guidelines were developed through a staged consensus process involving literature review and expert consultation to generate 26 candidate items, which were consulted upon by an international multi-stakeholder group in a two-stage Delphi survey (103 stakeholders), agreed upon in a consensus meeting (31 stakeholders) and refined through a checklist pilot (34 participants). The SPIRIT-AI extension includes 15 new items that were considered sufficiently important for clinical trial protocols of AI interventions. These new items should be routinely reported in addition to the core SPIRIT 2013 items. SPIRIT-AI recommends that investigators provide clear descriptions of the AI intervention, including instructions and skills required for use, the setting in which the AI intervention will be integrated, considerations for the handling of input and output data, the human–AI interaction and analysis of error cases. SPIRIT-AI will help promote transparency and completeness for clinical trial protocols for AI interventions. Its use will assist editors and peer reviewers, as well as the general readership, to understand, interpret and critically appraise the design and risk of bias for a planned clinical trial. The CONSORT-AI and SPIRIT-AI extensions improve the transparency of clinical trial design and trial protocol reporting for artificial intelligence interventions.,
    publicationDate: 2020-09-01,
    authors: ['S. Cruz Rivera', 'Xiaoxuan Liu', 'A. Chan', 'A. Denniston', 'M. Calvert', 'Ara Christopher Christopher David Hutan Jonathan J. La Darzi Holmes Yau Moher Ashrafian Deeks Ferrante di', 'A. Darzi', 'Christopher Holmes', 'Christopher Yau', 'D. Moher', 'H. Ashrafian', 'J. J. Deeks', 'Lavinia Ferrante di Ruffano', 'Livia Faes', 'P. Keane', 'Sebastian J. Vollmer', 'Aaron Y. Adrian Andre Andrew L. Maria Beatrice Cecilia S Lee Jonas Esteva Beam Panico Lee Haug Kelly Yau Mu', 'Aaron Y. Lee', 'Adrian Jonas', 'Andre Esteva', 'A. L. Beam', 'M. Panico', 'Cecilia S Lee', 'Charlotte Haug', 'Christopher J. Kelly', 'C. Mulrow', 'Cyrus Espinoza', 'John Fletcher', 'Dina Paltoo', 'Elaine Manna', 'G. Price', 'Gary S Collins', 'Hugh Harvey', 'James Matcham', 'João Monteiro', 'M. Elzarrad', 'Luke Oakden-Rayner', 'Melissa McCradden', 'Richard Savage', 'R. Golub', 'Rupa Sarkar', 'Samuel Rowley'],
    score: 178.6261376089943
},
{
    title: The Supplementary Material,
    abstract: 1. Formula Derivation, Proof and Clarification Derivation of Formula (12) First, we claim that P (Ai,hi) = 1 is true for ∀i ∈ [1, 2nv − 1]. It’s intuitive because a(i, hi) always points to the root node and any prediction always falls into the interval corresponding to it. Besides, we can easily point out that P (Ai,kAi,k+1 · · ·Ai,hi) = P (Ai,k) (1) It’s quite trivial because Ai,k+∆k (∆k ≥ 0) always occurs when Ai,k occurs. Afterwards, we try to expand the item P (τs ∈ τ i) and perform a formula simplification. P (τs ∈ τ i) = P (Ai,0) = P (Ai,0Ai,1 · · ·Ai,hi) (2) Therefore,,
    publicationDate: None,
    authors: ['Yunbo Zhang', 'Wenhao Yu', 'Greg Turk', 'A. Hyperparameters'],
    score: 178.0956593720838
},
{
    title: Habitat: A Platform for Embodied AI Research,
    abstract: We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents. These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments {train, test} x {Matterport3D, Gibson} for multiple sensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.,
    publicationDate: 2019-04-02,
    authors: ['M. Savva', 'Abhishek Kadian', 'Oleksandr Maksymets', 'Yili Zhao', 'Erik Wijmans', 'Bhavana Jain', 'Julian Straub', 'Jia Liu', 'V. Koltun', 'Jitendra Malik', 'Devi Parikh', 'Dhruv Batra'],
    score: 178.0710955106652
},
{
    title: The potential for artificial intelligence in healthcare,
    abstract: ABSTRACT The complexity and rise of data in healthcare means that artificial intelligence (AI) will increasingly be applied within the field. Several types of AI are already being employed by payers and providers of care, and life sciences companies. The key categories of applications involve diagnosis and treatment recommendations, patient engagement and adherence, and administrative activities. Although there are many instances in which AI can perform healthcare tasks as well or better than humans, implementation factors will prevent large-scale automation of healthcare professional jobs for a considerable period. Ethical issues in the application of AI to healthcare are also discussed.,
    publicationDate: 2019-06-01,
    authors: ['T. Davenport', 'R. Kalakota'],
    score: 178.0614505487976
},
{
    title: Relationship of MRI delayed contrast enhancement to irreversible injury, infarct age, and contractile function.,
    abstract: BACKGROUND
Contrast MRI enhancement patterns in several pathophysiologies resulting from ischemic myocardial injury are controversial or have not been investigated. We compared contrast enhancement in acute infarction (AI), after severe but reversible ischemic injury (RII), and in chronic infarction.


METHODS AND RESULTS
In dogs, a large coronary artery was occluded to study AI and/or chronic infarction (n = 18), and a second coronary artery was chronically instrumented with a reversible hydraulic occluder and Doppler flowmeter to study RII (n = 8). At 3 days after surgery, cine MRI revealed reduced wall thickening in AI (5+/-6% versus 33+/-6% in normal, P<0.001). In RII, wall thickening before, during, and after inflation of the occluder for 15 minutes was 35+/-5%, 1+/-8%, and 21+/-10% and Doppler flow was 19.8+/-5.3, 0.2+/-0.5, and 56.3+/-17.7 (peak hyperemia) cm/s, respectively, confirming occlusion, transient ischemia, and reperfusion. Gd-DTPA-enhanced MR images acquired 30 minutes after contrast revealed hyperenhancement of AI (294+/-96% of normal, P<0.001) but not of RII (98+/-6% of normal, P = NS). Eight weeks later, the chronically infarcted region again hyperenhanced (253+/-54% of normal, n = 8, P<0.001). High-resolution (0.5 x 0.5 x 0.5 mm) ex vivo MRI demonstrated that the spatial extent of hyperenhancement was the same as the spatial extent of myocyte necrosis with and without reperfusion at 1 day (R = 0.99, P<0.001) and 3 days (R = 0.99, P<0.001) and collagenous scar at 8 weeks (R = 0.97, P<0.001).


CONCLUSIONS
In the pathophysiologies investigated, contrast MRI distinguishes between reversible and irreversible ischemic injury independent of wall motion and infarct age.,
    publicationDate: 1999-11-09,
    authors: ['R. Kim', 'D. Fieno', 'T. Parrish', 'Kathleen E. Harris', 'Enn‐ling Chen', 'Orlando P. Simonetti', 'J. Bundy', 'J. Finn', 'F. Klocke', 'R. Judd'],
    score: 177.97781931281202
},
{
    title: Artificial Intelligence in Service,
    abstract: Artificial intelligence (AI) is increasingly reshaping service by performing various tasks, constituting a major source of innovation, yet threatening human jobs. We develop a theory of AI job replacement to address this double-edged impact. The theory specifies four intelligences required for service tasks—mechanical, analytical, intuitive, and empathetic—and lays out the way firms should decide between humans and machines for accomplishing those tasks. AI is developing in a predictable order, with mechanical mostly preceding analytical, analytical mostly preceding intuitive, and intuitive mostly preceding empathetic intelligence. The theory asserts that AI job replacement occurs fundamentally at the task level, rather than the job level, and for “lower” (easier for AI) intelligence tasks first. AI first replaces some of a service job’s tasks, a transition stage seen as augmentation, and then progresses to replace human labor entirely when it has the ability to take over all of a job’s tasks. The progression of AI task replacement from lower to higher intelligences results in predictable shifts over time in the relative importance of the intelligences for service employees. An important implication from our theory is that analytical skills will become less important, as AI takes over more analytical tasks, giving the “softer” intuitive and empathetic skills even more importance for service employees. Eventually, AI will be capable of performing even the intuitive and empathetic tasks, which enables innovative ways of human–machine integration for providing service but also results in a fundamental threat for human employment.,
    publicationDate: 2018-02-05,
    authors: ['Ming-Hui Huang', 'R. Rust'],
    score: 177.24022742231978
},
{
    title: Dota 2 with Large Scale Deep Reinforcement Learning,
    abstract: On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.,
    publicationDate: 2019-12-13,
    authors: ['Christopher Berner', 'Greg Brockman', 'Brooke Chan', 'Vicki Cheung', 'Przemyslaw Debiak', 'Christy Dennison', 'David Farhi', 'Quirin Fischer', 'Shariq Hashme', 'Christopher Hesse', 'R. Józefowicz', 'Scott Gray', 'Catherine Olsson', 'J. Pachocki', 'Michael Petrov', 'Henrique Pondé de Oliveira Pinto', 'Jonathan Raiman', 'Tim Salimans', 'Jeremy Schlatter', 'Jonas Schneider', 'Szymon Sidor', 'I. Sutskever', 'Jie Tang', 'Filip Wolski', 'Susan Zhang'],
    score: 176.40716200112922
},
{
    title: Constitutional AI: Harmlessness from AI Feedback,
    abstract: As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.,
    publicationDate: 2022-12-15,
    authors: ['Yuntao Bai', 'Saurav Kadavath', 'Sandipan Kundu', 'Amanda Askell', 'John Kernion', 'Andy Jones', 'A. Chen', 'Anna Goldie', 'Azalia Mirhoseini', 'C. McKinnon', 'Carol Chen', 'Catherine Olsson', 'C. Olah', 'Danny Hernandez', 'Dawn Drain', 'Deep Ganguli', 'Dustin Li', 'Eli Tran-Johnson', 'E. Perez', 'Jamie Kerr', 'J. Mueller', 'Jeff Ladish', 'J. Landau', 'Kamal Ndousse', 'Kamilė Lukošiūtė', 'Liane Lovitt', 'M. Sellitto', 'Nelson Elhage', 'Nicholas Schiefer', "Noem'i Mercado", 'Nova Dassarma', 'R. Lasenby', 'Robin Larson', 'Sam Ringer', 'Scott Johnston', 'Shauna Kravec', 'S. E. Showk', 'Stanislav Fort', 'Tamera Lanham', 'Timothy Telleen-Lawton', 'Tom Conerly', 'T. Henighan', 'Tristan Hume', 'Sam Bowman', 'Zac Hatfield-Dodds', 'Benjamin Mann', 'Dario Amodei', 'Nicholas Joseph', 'Sam McCandlish', 'Tom B. Brown', 'Jared Kaplan'],
    score: 176.3386473254127
},
{
    title: HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face,
    abstract: Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.,
    publicationDate: None,
    authors: ['Yongliang Shen', 'Kaitao Song', 'Xu Tan', 'Dongsheng Li', 'Weiming Lu', 'Y. Zhuang'],
    score: 175.52116867117962
},
{
    title: Towards Personalized Federated Learning,
    abstract: In parallel with the rapid adoption of artificial intelligence (AI) empowered by advances in AI research, there has been growing awareness and concerns of data privacy. Recent significant developments in the data regulation landscape have prompted a seismic shift in interest toward privacy-preserving AI. This has contributed to the popularity of Federated Learning (FL), the leading paradigm for the training of machine learning models on data silos in a privacy-preserving manner. In this survey, we explore the domain of personalized FL (PFL) to address the fundamental challenges of FL on heterogeneous data, a universal characteristic inherent in all real-world datasets. We analyze the key motivations for PFL and present a unique taxonomy of PFL techniques categorized according to the key challenges and personalization strategies in PFL. We highlight their key ideas, challenges, opportunities, and envision promising future trajectories of research toward a new PFL architectural design, realistic PFL benchmarking, and trustworthy PFL approaches.,
    publicationDate: 2021-03-01,
    authors: ['A. Tan', 'Han Yu', 'Li-zhen Cui', 'Qiang Yang'],
    score: 174.89566801213655
},
{
    title: Federated Learning for Internet of Things: A Comprehensive Survey,
    abstract: The Internet of Things (IoT) is penetrating many facets of our daily life with the proliferation of intelligent services and applications empowered by artificial intelligence (AI). Traditionally, AI techniques require centralized data collection and processing that may not be feasible in realistic application scenarios due to the high scalability of modern IoT networks and growing data privacy concerns. Federated Learning (FL) has emerged as a distributed collaborative AI approach that can enable many intelligent IoT applications, by allowing for AI training at distributed IoT devices without the need for data sharing. In this article, we provide a comprehensive survey of the emerging applications of FL in IoT networks, beginning from an introduction to the recent advances in FL and IoT to a discussion of their integration. Particularly, we explore and analyze the potential of FL for enabling a wide range of IoT services, including IoT data sharing, data offloading and caching, attack detection, localization, mobile crowdsensing, and IoT privacy and security. We then provide an extensive survey of the use of FL in various key IoT applications such as smart healthcare, smart transportation, Unmanned Aerial Vehicles (UAVs), smart cities, and smart industry. The important lessons learned from this review of the FL-IoT services and applications are also highlighted. We complete this survey by highlighting the current challenges and possible directions for future research in this booming area.,
    publicationDate: 2021-04-16,
    authors: ['Dinh C. Nguyen', 'Ming Ding', 'P. Pathirana', 'A. Seneviratne', 'Jun Li', 'F. I. H. Vincent Poor'],
    score: 174.4366614054058
},
{
    title: AI Feynman: A physics-inspired method for symbolic regression,
    abstract: Our physics-inspired algorithm for symbolic regression is able to discover complex physics equations from mere tables of numbers. A core challenge for both physics and artificial intelligence (AI) is symbolic regression: finding a symbolic expression that matches data from an unknown function. Although this problem is likely to be NP-hard in principle, functions of practical interest often exhibit symmetries, separability, compositionality, and other simplifying properties. In this spirit, we develop a recursive multidimensional symbolic regression algorithm that combines neural network fitting with a suite of physics-inspired techniques. We apply it to 100 equations from the Feynman Lectures on Physics, and it discovers all of them, while previous publicly available software cracks only 71; for a more difficult physics-based test set, we improve the state-of-the-art success rate from 15 to 90%.,
    publicationDate: 2019-05-27,
    authors: ['S. Udrescu', 'Max Tegmark'],
    score: 174.3410448592476
},
{
    title: Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI,
    abstract: None,
    publicationDate: 2019-10-22,
    authors: ['Alejandro Barredo Arrieta', 'Natalia Díaz Rodríguez', 'J. Ser', 'Adrien Bennetot', 'S. Tabik', 'A. Barbado', 'S. García', 'S. Gil-Lopez', 'D. Molina', 'Richard Benjamins', 'Raja Chatila', 'Francisco Herrera'],
    score: 174.08630052593577
},
{
    title: About the authors,
    abstract: None,
    publicationDate: None,
    authors: ['Jim Austin'],
    score: 173.9930893289706
},
{
    title: Artificial Intelligence,
    abstract: None,
    publicationDate: None,
    authors: ['Bart Verheij', 'M. Wiering'],
    score: 173.79579641386863
},
{
    title: Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI,
    abstract: The rapid spread of artificial intelligence (AI) systems has precipitated a rise in ethical and human rights-based frameworks intended to guide the development and use of these technologies. Despite the proliferation of these "AI principles," there has been little scholarly focus on understanding these efforts either individually or as contextualized within an expanding universe of principles with discernible trends.

To that end, this white paper and its associated data visualization compare the contents of thirty-six prominent AI principles documents side-by-side. This effort uncovered a growing consensus around eight key thematic trends: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and promotion of human values. Underlying this “normative core,” our analysis examined the forty-seven individual principles that make up the themes, detailing notable similarities and differences in interpretation found across the documents. In sharing these observations, it is our hope that policymakers, advocates, scholars, and others working to maximize the benefits and minimize the harms of AI will be better positioned to build on existing efforts and to push the fractured, global conversation on the future of AI toward consensus.,
    publicationDate: 2020-01-15,
    authors: ['Jessica Fjeld', 'Nele Achten', 'Hannah Hilligoss', 'Ádám Nagy', 'Madhulika Srikumar'],
    score: 173.77855824748832
},
{
    title: Quorum sensing in Escherichia coli, Salmonella typhimurium, and Vibrio harveyi: a new family of genes responsible for autoinducer production.,
    abstract: In bacteria, the regulation of gene expression in response to changes in cell density is called quorum sensing. Quorum-sensing bacteria produce, release, and respond to hormone-like molecules (autoinducers) that accumulate in the external environment as the cell population grows. In the marine bacterium Vibrio harveyi two parallel quorum-sensing systems exist, and each is composed of a sensor-autoinducer pair. V. harveyi reporter strains capable of detecting only autoinducer 1 (AI-1) or autoinducer 2 (AI-2) have been constructed and used to show that many species of bacteria, including Escherichia coli MG1655, E. coli O157:H7, Salmonella typhimurium 14028, and S. typhimurium LT2 produce autoinducers similar or identical to the V. harveyi system 2 autoinducer AI-2. However, the domesticated laboratory strain E. coli DH5alpha does not produce this signal molecule. Here we report the identification and analysis of the gene responsible for AI-2 production in V. harveyi, S. typhimurium, and E. coli. The genes, which we have named luxSV.h., luxSS.t., and luxSE.c. respectively, are highly homologous to one another but not to any other identified gene. E. coli DH5alpha can be complemented to AI-2 production by the introduction of the luxS gene from V. harveyi or E. coli O157:H7. Analysis of the E. coli DH5alpha luxSE.c. gene shows that it contains a frameshift mutation resulting in premature truncation of the LuxSE.c. protein. Our results indicate that the luxS genes define a new family of autoinducer-production genes.,
    publicationDate: 1999-02-16,
    authors: ['M. Surette', 'Melissa B. Miller', 'B. Bassler'],
    score: 173.69114230739763
},
{
    title: Artificial Intelligence In,
    abstract: It goes without saying that coronavirus (COVID-19) is an infectious disease and many countries are coping with its different variants. Owing to the limited medical facilities, vaccine and medical experts, need of the hour is to intelligently tackle its spread by making artificial intelligence (AI) based smart decisions for COVID-19 suspects who develop different symptoms and they are kept under observation and monitored to see the severity of the symptoms. The target of this study is to analyze COVID-19 suspects data and detect whether a suspect is a COVID-19 patient or not, and if yes, then to what extent, so that a suitable decision can be made. The decision can be categorized such that an infected person can be isolated or quarantined at home or at a facilitation center or the person can be sent to the hospital for the treatment. This target is achieved by designing a mathematical model of COVID-19 suspects in the form of a multi-criteria decision making (MCDM) model and a novel AI based technique is devised and implemented with the help of newly developed plithogenic distance and similarity measures in fuzzy environment. All findings are depicted graphically for a clear understanding and to provide an insight of the necessity and effectiveness of the proposed method. The concept and results of the proposed technique make it suitable for implementation in machine learning, deep learning, pattern recognition etc.,
    publicationDate: None,
    authors: ['Muhammad Rayees', 'Usman Ahmad', 'Afzal'],
    score: 173.58597997653902
},
{
    title: Microsoft Academic Graph: When experts are not enough,
    abstract: Abstract An ongoing project explores the extent to which artificial intelligence (AI), specifically in the areas of natural language processing and semantic reasoning, can be exploited to facilitate the studies of science by deploying software agents equipped with natural language understanding capabilities to read scholarly publications on the web. The knowledge extracted by these AI agents is organized into a heterogeneous graph, called Microsoft Academic Graph (MAG), where the nodes and the edges represent the entities engaging in scholarly communications and the relationships among them, respectively. The frequently updated data set and a few software tools central to the underlying AI components are distributed under an open data license for research and commercial applications. This paper describes the design, schema, and technical and business motivations behind MAG and elaborates how MAG can be used in analytics, search, and recommendation scenarios. How AI plays an important role in avoiding various biases and human induced errors in other data sets and how the technologies can be further improved in the future are also discussed.,
    publicationDate: 2020-01-23,
    authors: ['Kuansan Wang', 'Zhihong Shen', 'Chiyuan Huang', 'Chieh-Han Wu', 'Yuxiao Dong', 'Anshul Kanakia'],
    score: 173.1590011553698
},
{
    title: International evaluation of an AI system for breast cancer screening,
    abstract: None,
    publicationDate: 2020-01-01,
    authors: ['S. McKinney', 'M. Sieniek', 'Varun Godbole', 'Jonathan Godwin', 'Natasha Antropova', 'H. Ashrafian', 'T. Back', 'Mary Chesus', 'Greg C. Corrado', 'A. Darzi', 'M. Etemadi', 'Florencia Garcia-Vicente', 'F. Gilbert', 'M. Halling-Brown', 'D. Hassabis', 'Sunny Jansen', 'A. Karthikesalingam', 'Christopher J. Kelly', 'Dominic King', 'J. Ledsam', 'David S. Melnick', 'Hormuz Mostofi', 'L. Peng', 'J. Reicher', 'Bernardino Romera-Paredes', 'R. Sidebottom', 'Mustafa Suleyman', 'Daniel Tse', 'K. Young', 'J. Fauw', 'S. Shetty'],
    score: 173.09338243951544
},
{
    title: Fuzzy Multiple Attribute Decision Making - Methods and Applications,
    abstract: None,
    publicationDate: None,
    authors: ['Shu-Jen Chen', 'C. Hwang'],
    score: 173.08310134042455
},
{
    title: Superhuman AI for heads-up no-limit poker: Libratus beats top professionals,
    abstract: Libratus versus humans Pitting artificial intelligence (AI) against top human players demonstrates just how far AI has come. Brown and Sandholm built a poker-playing AI called Libratus that decisively beat four leading human professionals in the two-player variant of poker called heads-up no-limit Texas hold'em (HUNL). Over nearly 3 weeks, Libratus played 120,000 hands of HUNL against the human professionals, using a three-pronged approach that included precomputing an overall strategy, adapting the strategy to actual gameplay, and learning from its opponent. Science, this issue p. 418 An artificial intelligence program called Libratus played 120,000 hands of a two-player variant of poker and beat four leading human professionals. No-limit Texas hold’em is the most popular form of poker. Despite artificial intelligence (AI) successes in perfect-information games, the private information and massive game tree have made no-limit poker difficult to tackle. We present Libratus, an AI that, in a 120,000-hand competition, defeated four top human specialist professionals in heads-up no-limit Texas hold’em, the leading benchmark and long-standing challenge problem in imperfect-information game solving. Our game-theoretic approach features application-independent techniques: an algorithm for computing a blueprint for the overall strategy, an algorithm that fleshes out the details of the strategy for subgames that are reached during play, and a self-improver algorithm that fixes potential weaknesses that opponents have identified in the blueprint strategy.,
    publicationDate: 2018-01-26,
    authors: ['Noam Brown', 'T. Sandholm'],
    score: 172.91936876433826
},
{
    title: Chatting and cheating: Ensuring academic integrity in the era of ChatGPT,
    abstract: ABSTRACT The use of artificial intelligence in academia is a hot topic in the education field. ChatGPT is an AI tool that offers a range of benefits, including increased student engagement, collaboration, and accessibility. However, is also raises concerns regarding academic honesty and plagiarism. This paper examines the opportunities and challenges of using ChatGPT in higher education, and discusses the potential risks and rewards of these tools. The paper also considers the difficulties of detecting and preventing academic dishonesty, and suggests strategies that universities can adopt to ensure ethical and responsible use of these tools. These strategies include developing policies and procedures, providing training and support, and using various methods to detect and prevent cheating. The paper concludes that while the use of AI in higher education presents both opportunities and challenges, universities can effectively address these concerns by taking a proactive and ethical approach to the use of these tools.,
    publicationDate: 2023-03-13,
    authors: ['Debby R. E. Cotton', 'Peter A. Cotton', 'J. Shipway'],
    score: 172.62423214716387
},
{
    title: The Roadmap to 6G - AI Empowered Wireless Networks,
    abstract: The recent upsurge of diversified mobile applications, especially those supported by Artificial Intelligence (AI), is spurring heated discussions on the future evolution of wireless communications. While 5G is being deployed around the world, efforts from industry and academia have started to look beyond 5G and conceptualize 6G. We envision 6G to undergo an unprecedented transformation that will make it substantially different from the previous generations of wireless cellular systems. In particular, 6G will go beyond mobile Internet and will be required to support ubiquitous AI services from the core to the end devices of the network. Meanwhile, AI will play a critical role in designing and optimizing 6G architectures, protocols, and operations. In this article, we discuss potential technologies for 6G to enable mobile AI applications, as well as AI-enabled methodologies for 6G network design and optimization. Key trends in the evolution to 6G will also be discussed.,
    publicationDate: 2019-04-26,
    authors: ['K. Letaief', 'Wei Chen', 'Yuanming Shi', 'Jun Zhang', 'Y. Zhang'],
    score: 172.27228195857103
},
{
    title: A survey of deep learning techniques for autonomous driving,
    abstract: The last decade witnessed increasingly rapid progress in self‐driving vehicle technology, mainly backed up by advances in the area of deep learning and artificial intelligence (AI). The objective of this paper is to survey the current state‐of‐the‐art on deep learning technologies used in autonomous driving. We start by presenting AI‐based self‐driving architectures, convolutional and recurrent neural networks, as well as the deep reinforcement learning paradigm. These methodologies form a base for the surveyed driving scene perception, path planning, behavior arbitration, and motion control algorithms. We investigate both the modular perception‐planning‐action pipeline, where each module is built using deep learning methods, as well as End2End systems, which directly map sensory information to steering commands. Additionally, we tackle current challenges encountered in designing AI architectures for autonomous driving, such as their safety, training data sources, and computational hardware. The comparison presented in this survey helps gain insight into the strengths and limitations of deep learning and AI approaches for autonomous driving and assist with design choices.,
    publicationDate: 2019-10-17,
    authors: ['S. Grigorescu', 'Bogdan Trasnea', 'Tiberiu T. Cocias', 'G. Macesanu'],
    score: 172.15426404056183
},
{
    title: Bacteria–host communication: The language of hormones,
    abstract: The interbacterial communication system known as quorum sensing (QS) utilizes hormone-like compounds referred to as autoinducers to regulate bacterial gene expression. Enterohemorrhagic Escherichia coli (EHEC) serotype O157:H7 is the agent responsible for outbreaks of bloody diarrhea in several countries. We previously proposed that EHEC uses a QS regulatory system to “sense” that it is within the intestine and activate genes essential for intestinal colonization. The QS system used by EHEC is the LuxS/autoinducer 2 (AI-2) system extensively involved in interspecies communication. The autoinducer AI-2 is a furanosyl borate diester whose synthesis depends on the enzyme LuxS. Here we show that an EHEC luxS mutant, unable to produce the bacterial autoinducer, still responds to a eukaryotic cell signal to activate expression of its virulence genes. We have identified this signal as the hormone epinephrine and show that β- and α-adrenergic antagonists can block the bacterial response to this hormone. Furthermore, using purified and in vitro synthesized AI-2 we showed that AI-2 is not the autoinducer involved in the bacterial signaling. EHEC produces another, previously undescribed autoinducer (AI-3) whose synthesis depends on the presence of LuxS. These results imply a potential cross-communication between the luxS/AI-3 bacterial QS system and the epinephrine host signaling system. Given that eukaryotic cell-to-cell signaling typically occurs through hormones, and that bacterial cell-to-cell signaling occurs through QS, we speculate that QS might be a “language” by which bacteria and host cells communicate.,
    publicationDate: 2003-07-07,
    authors: ['V. Sperandio', 'A. Torres', 'B. Jarvis', 'J. Nataro', 'J. Kaper'],
    score: 172.10244040588265
},
{
    title: Precision Medicine, AI, and the Future of Personalized Health Care,
    abstract: The convergence of artificial intelligence (AI) and precision medicine promises to revolutionize health care. Precision medicine methods identify phenotypes of patients with less‐common responses to treatment or unique healthcare needs. AI leverages sophisticated computation and inference to generate insights, enables the system to reason and learn, and empowers clinician decision making through augmented intelligence. Recent literature suggests that translational research exploring this convergence will help solve the most difficult challenges facing precision medicine, especially those in which nongenomic and genomic determinants, combined with information from patient symptoms, clinical history, and lifestyles, will facilitate personalized diagnosis and prognostication.,
    publicationDate: 2020-09-22,
    authors: ['Kevin B. Johnson', 'Wei-Qi Wei', 'D. Weeraratne', 'M. Frisse', 'K. Misulis', 'K. Rhee', 'Juan Zhao', 'J. Snowdon'],
    score: 171.82797845010185
},
{
    title: Artificial intelligence in radiology,
    abstract: None,
    publicationDate: 2018-05-17,
    authors: ['A. Hosny', 'C. Parmar', 'John Quackenbush', 'L. Schwartz', 'H. Aerts'],
    score: 171.34293015922177
},
{
    title: Atlas of AI,
    abstract: ATLAS OF AI: Power, Politics, and the Planetary Costs of Artificial Intelligence by Kate Crawford. New Haven, CT: Yale University Press, 2021. 336 pages. Hardcover; $28.00. ISBN: 9780300209570. *Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence is Kate Crawford's analysis of the state of the AI industry. A central idea of her book is the importance of redefining Artificial Intelligence (AI). She states, "I've argued that there is much at stake in how we define AI, what its boundaries are, and who determines them: it shapes what can be seen and contested" (p. 217). *My own definition of AI goes something like this: I¬†imagine a future where I'm sitting in a cafe drinking coffee with my friends, but in this future, one of my friends is a robot, who like me is trying to make a living in this world. A future where humans and robots live in harmony. Crawford views this definition as mythological: "These mythologies are particularly strong in the field of artificial intelligence, where the belief that human intelligence can be formalized and reproduced by machines has been axiomatic since the mid-twentieth century" (p.¬†5). I do not know if my definition of artificial intelligence can come true, but I am enjoying the process of building, experimenting, and dreaming. *In her book, she asks me to consider that I may be unknowingly participating, as she states, in "a material product of colonialism, with its patterns of extraction, conflict, and environmental destruction" (p. 38). The book's subtitle illuminates the purpose of the book: specifically, the power, politics, and planetary costs of usurping artificial intelligence. Of course, this is not exactly Crawford's subtitle, and this is where I both agree and disagree with her. The book's subtitle is actually Power, Politics, and the Planetary Costs of Artificial Intelligence. In my opinion, AI is more the canary in the coal mine. We can use the canary to detect the poisonous gases, but we cannot blame the canary for the poisonous gas. It risks missing the point. Is AI itself to be feared? Should we no longer teach or learn AI? Or is this more about how we discern responsible use and direction for AI technology? *There is another author who speaks to similar issues. In Weapons of Math Destruction, Cathy O'Neil states it this way, "If we had been clear-headed, we all would have taken a step back at this point to figure out how math had been misused ... But instead ... new mathematical techniques were hotter than ever ... A computer program could speed through thousands of resumes or loan applications in a second or two and sort them into neat lists, with the most promising candidates on top" (p. 13). *Both Crawford and O'Neil point to human flaws that often lead to well-intentioned software developers creating code that results in unfair and discriminatory decisions. AI models encode unintended human biases that may not evaluate candidates as fairly as we would expect, yet there is a widespread notion that we can trust the algorithm. For example, the last time you registered an account on a website, did you click the checkbox confirming that "yes, I read the disclaimer" even though you did not? When we click "yes" we are accepting this disclaimer and placing trust in the software. Business owners place trust in software when they use it to make predictions. Engineers place trust in their algorithms when they write software without rigorous testing protocols. I¬†am just as guilty. *Crawford suggests that AI is often used in ways that are harmful. In the Atlas of AI we are given a tour of how technology is damaging our world: strip mining, labor injustice, the misuse of personal data, issues of state and power, to name a few of the concerns Crawford raises. The reality is that AI is built upon existing infrastructure. For example, Facebook, Instagram, YouTube, Amazon, TikTok have been collecting our information for profit even before AI became important to them. The data centers, CPU houses, and worldwide network infrastructure were already in place to meet consumer demand and geopolitics. But it is true that AI brings new technologies to the table, such as automated face recognition and decision tools to compare prospective employment applicants with diverse databases and employee monitoring tools that can make automatic recommendations. Governments, militaries, and intelligence agencies have taken notice. As invasion of privacy and social justice concerns emerge, Crawford calls us to consider these issues carefully. *Reading Crawford's words pricked my conscience, convicting me to reconsider my erroneous ways. For big tech to exist, to supply what we demand, it needs resources. She walks us through the many resources the technology industry needs to provide what we want, and AI is the "new kid on the block." This book is not about AI, per se; it is instead about the side effects of poor business/research practices, opportunist behavior, power politics, and how these behaviors not only exploit our planet but also unjustly affect marginalized people. The AI industry is simply a new example of this reality: data mining, low wages to lower costs, foreign workers with fewer rights, strip mining, relying on coal and oil for electricity (although some tech companies have made strides to improve sustainability). This sounds more like a parable about the sins of the tech industry than a critique about the dangers of AI. *Could the machine learning community, like the inventors of dynamite who wanted to simply help railroads excavate tunnels, be unintentionally causing harm? Should we, as a community, be on the lookout for these potential harms? Do we have a moral responsibility? Maybe the technology sector needs to look more inwardly to ensure that process efficiency and cost savings are not elevated as most important. *I did not agree with everything that Crawford classified as AI, but I do agree that as a community we are responsible for our actions. If there are injustices, then this should be important to us. In particular, as people of faith, we should heed the call of Micah 6:8 to act justly in this world, and this includes how we use AI. *Reviewed by Joseph Vybihal, Professor of Computer Science, McGill University, Montreal, PQ H3A 0G4.,
    publicationDate: 2021-03-22,
    authors: ['Kate Crawford'],
    score: 171.30162090368458
},
{
    title: Guidelines for Human-AI Interaction,
    abstract: Advances in artificial intelligence (AI) frame opportunities and challenges for user interface design. Principles for human-AI interaction have been discussed in the human-computer interaction community for over two decades, but more study and innovation are needed in light of advances in AI and the growing uses of AI technologies in human-facing applications. We propose 18 generally applicable design guidelines for human-AI interaction. These guidelines are validated through multiple rounds of evaluation including a user study with 49 design practitioners who tested the guidelines against 20 popular AI-infused products. The results verify the relevance of the guidelines over a spectrum of interaction scenarios and reveal gaps in our knowledge, highlighting opportunities for further research. Based on the evaluations, we believe the set of design guidelines can serve as a resource to practitioners working on the design of applications and features that harness AI technologies, and to researchers interested in the further development of human-AI interaction design principles.,
    publicationDate: 2019-05-02,
    authors: ['Saleema Amershi', 'Daniel S. Weld', 'Mihaela Vorvoreanu', 'Adam Fourney', 'Besmira Nushi', 'Penny Collisson', 'Jina Suh', 'Shamsi T. Iqbal', 'Paul N. Bennett', 'K. Quinn', 'J. Teevan', 'Ruth Kikin-Gil', 'E. Horvitz'],
    score: 170.63429257909615
},
{
    title: Mortality and apnea index in obstructive sleep apnea. Experience in 385 male patients.,
    abstract: Although obstructive sleep apnea (OSA) has been studied in detail for over a decade, the mortality of this disorder is unclear. We calculated cumulative survival in 385 male OSA patients. We found that those with an apnea index (AI) greater than 20 had a much greater mortality than those with AI = less than 20. The probability of cumulative eight-year survival was .96 +/- 0.02 (SE) for AI = less than 20 vs. 63 +/- 0.17 for AI greater than 20 (p less than .05). This difference in mortality related to AI was particularly true in the patients less than 50 years of age in whom mortality from other causes is not common. None of the patients treated with tracheostomy or nasal CPAP died. Eight of the patients treated with uvulopalatopharyngoplasty (UPPP) died and the cumulative survival of the UPPP-alone treated group was not different from the survival curve of untreated OSA patients with an apnea index of greater than 20. We conclude that OSA patients with an apnea index of greater than 20 have a greater mortality than those below 20 and that UPPP patients be restudied after therapy. If the latter patients are found not to have marked amelioration of their AI, then they should be treated by nasal CPAP or tracheostomy.,
    publicationDate: 1988-07-01,
    authors: ['Jiang He', 'M. Kryger', 'F. Zorick', 'William A. Conway', 'Thomas Roth'],
    score: 170.60061464865072
},
{
    title: In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning,
    abstract: Recently, along with the rapid development of mobile communication technology, edge computing theory and techniques have been attracting more and more attention from global researchers and engineers, which can significantly bridge the capacity of cloud and requirement of devices by the network edges, and thus can accelerate content delivery and improve the quality of mobile services. In order to bring more intelligence to edge systems, compared to traditional optimization methodology, and driven by the current deep learning techniques, we propose to integrate the Deep Reinforcement Learning techniques and Federated Learning framework with mobile edge systems, for optimizing mobile edge computing, caching and communication. And thus, we design the "In-Edge AI" framework in order to intelligently utilize the collaboration among devices and edge nodes to exchange the learning parameters for a better training and inference of the models, and thus to carry out dynamic system-level optimization and application-level enhancement while reducing the unnecessary system communication load. "In-Edge AI" is evaluated and proved to have near-optimal performance but relatively low overhead of learning, while the system is cognitive and adaptive to mobile communication systems. Finally, we discuss several related challenges and opportunities for unveili,
    publicationDate: 2018-09-19,
    authors: ['Xiaofei Wang', 'Yiwen Han', 'Chenyang Wang', 'Qiyang Zhao', 'Xu Chen', 'Min Chen'],
    score: 170.59813750739804
},
{
    title: Artificial intelligence in cancer imaging: Clinical challenges and applications,
    abstract: Judgement, as one of the core tenets of medicine, relies upon the integration of multilayered data with nuanced decision making. Cancer offers a unique context for medical decisions given not only its variegated forms with evolution of disease but also the need to take into account the individual condition of patients, their ability to receive treatment, and their responses to treatment. Challenges remain in the accurate detection, characterization, and monitoring of cancers despite improved technologies. Radiographic assessment of disease most commonly relies upon visual evaluations, the interpretations of which may be augmented by advanced computational analyses. In particular, artificial intelligence (AI) promises to make great strides in the qualitative interpretation of cancer imaging by expert clinicians, including volumetric delineation of tumors over time, extrapolation of the tumor genotype and biological course from its radiographic phenotype, prediction of clinical outcome, and assessment of the impact of disease and treatment on adjacent organs. AI may automate processes in the initial interpretation of images and shift the clinical workflow of radiographic detection, management decisions on whether or not to administer an intervention, and subsequent observation to a yet to be envisioned paradigm. Here, the authors review the current state of AI as applied to medical imaging of cancer and describe advances in 4 tumor types (lung, brain, breast, and prostate) to illustrate how common clinical problems are being addressed. Although most studies evaluating AI applications in oncology to date have not been vigorously validated for reproducibility and generalizability, the results do highlight increasingly concerted efforts in pushing AI technology to clinical use and to impact future directions in cancer care.,
    publicationDate: 2019-02-05,
    authors: ['W. Bi', 'A. Hosny', 'M. Schabath', 'M. Giger', 'Nicolai J. Birkbak', 'Alireza Mehrtash', 'Tavis Allison', 'O. Arnaout', 'C. Abbosh', 'I. Dunn', 'R. Mak', 'R. Tamimi', 'C. Tempany', 'C. Swanton', 'U. Hoffmann', 'L. Schwartz', 'R. Gillies', 'Raymond Y Huang', 'H. Aerts'],
    score: 170.23568630730844
},
{
    title: Ray: A Distributed Framework for Emerging AI Applications,
    abstract: The next generation of AI applications will continuously interact with the environment and learn from these interactions. These applications impose new and demanding systems requirements, both in terms of performance and flexibility. In this paper, we consider these requirements and present Ray---a distributed system to address them. Ray implements a dynamic task graph computation model that supports both the task-parallel and the actor programming models. To meet the performance requirements of AI applications, we propose an architecture that logically centralizes the system's control state using a sharded storage system and a novel bottom-up distributed scheduler. In our experiments, we demonstrate sub-millisecond remote task latencies and linear throughput scaling beyond 1.8 million tasks per second. We empirically validate that Ray speeds up challenging benchmarks and serves as both a natural and performant fit for an emerging class of reinforcement learning applications and algorithms.,
    publicationDate: 2017-12-16,
    authors: ['Philipp Moritz', 'Robert Nishihara', 'Stephanie Wang', 'Alexey Tumanov', 'Richard Liaw', 'Eric Liang', 'William Paul', 'Michael I. Jordan', 'Ion Stoica'],
    score: 170.2222152691332
},
{
    title: What is AI Literacy? Competencies and Design Considerations,
    abstract: Artificial intelligence (AI) is becoming increasingly integrated in user-facing technology, but public understanding of these technologies is often limited. There is a need for additional HCI research investigating a) what competencies users need in order to effectively interact with and critically evaluate AI and b) how to design learner-centered AI technologies that foster increased user understanding of AI. This paper takes a step towards realizing both of these goals by providing a concrete definition of AI literacy based on existing research. We synthesize a variety of interdisciplinary literature into a set of core competencies of AI literacy and suggest several design considerations to support AI developers and educators in creating learner-centered AI. These competencies and design considerations are organized in a conceptual framework thematically derived from the literature. This paper's contributions can be used to start a conversation about and guide future research on AI literacy within the HCI community.,
    publicationDate: 2020-04-21,
    authors: ['D. Long', 'Brian Magerko'],
    score: 170.21282018185323
},
{
    title: DARPA's Explainable Artificial Intelligence (XAI) Program,
    abstract: Dramatic success in machine learning has led to a new wave of AI applications (for example, transportation, security, medicine, finance, defense) that offer tremendous benefits but cannot explain their decisions and actions to human users. DARPA’s explainable artificial intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. Realizing this goal requires methods for learning more explainable models, designing effective explanation interfaces, and understanding the psychologic requirements for effective explanations. The XAI developer teams are addressing the first two challenges by creating ML techniques and developing principles, strategies, and human-computer interaction techniques for generating effective explanations. Another XAI team is addressing the third challenge by summarizing, extending, and applying psychologic theories of explanation to help the XAI evaluator define a suitable evaluation framework, which the developer teams will use to test their systems. The XAI teams completed the first of this 4-year program in May 2018. In a series of ongoing evaluations, the developer teams are assessing how well their XAM systems’ explanations improve user understanding, user trust, and user task performance.,
    publicationDate: 2019-06-24,
    authors: ['David Gunning', 'D. Aha'],
    score: 169.96393974472892
},
{
    title: Shaping the Future of Education: Exploring the Potential and Consequences of AI and ChatGPT in Educational Settings,
    abstract: Over the last decade, technological advancements, especially artificial intelligence (AI), have significantly transformed educational practices. Recently, the development and adoption of Generative Pre-trained Transformers (GPT), particularly OpenAI’s ChatGPT, has sparked considerable interest. The unprecedented capabilities of these models, such as generating humanlike text and facilitating automated conversations, have broad implications in various sectors, including education and health. Despite their immense potential, concerns regarding their widespread use and opacity have been raised within the scientific community. ChatGPT, the latest version of the GPT series, has displayed remarkable proficiency, passed the US bar law exam, and amassed over a million subscribers shortly after its launch. However, its impact on the education sector has elicited mixed reactions, with some educators heralding it as a progressive step and others raising alarms over its potential to reduce analytical skills and promote misconduct. This paper aims to delve into these discussions, exploring the potential and problems associated with applying advanced AI models in education. It builds on extant literature and contributes to understanding how these technologies reshape educational norms in the “new AI gold rush” era.,
    publicationDate: 2023-07-07,
    authors: ['Simone Grassini'],
    score: 169.68328647351683
},
{
    title: Software Engineering for Machine Learning: A Case Study,
    abstract: Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be "entangled" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.,
    publicationDate: 2019-05-01,
    authors: ['Saleema Amershi', 'Andrew Begel', 'C. Bird', 'R. Deline', 'H. Gall', 'Ece Kamar', 'Nachiappan Nagappan', 'Besmira Nushi', 'Thomas Zimmermann'],
    score: 169.56324443509376
},
{
    title: What are ontologies, and why do we need them?,
    abstract: This survey provides a conceptual introduction to ontologies and their role in information systems and AI. The authors also discuss how ontologies clarify the domain's structure of knowledge and enable knowledge sharing.,
    publicationDate: None,
    authors: ['B. Chandrasekaran', 'J. Josephson', 'Richard Benjamins'],
    score: 169.4012069753274
},
{
    title: Doctor AI: Predicting Clinical Events via Recurrent Neural Networks,
    abstract: Leveraging large historical data in electronic health record (EHR), we developed Doctor AI, a generic predictive model that covers observed medical conditions and medication uses. Doctor AI is a temporal model using recurrent neural networks (RNN) and was developed and applied to longitudinal time stamped EHR data from 260K patients over 8 years. Encounter records (e.g. diagnosis codes, medication codes or procedure codes) were input to RNN to predict (all) the diagnosis and medication categories for a subsequent visit. Doctor AI assesses the history of patients to make multilabel predictions (one label for each diagnosis or medication category). Based on separate blind test set evaluation, Doctor AI can perform differential diagnosis with up to 79% recall@30, significantly higher than several baselines. Moreover, we demonstrate great generalizability of Doctor AI by adapting the resulting models from one institution to another without losing substantial accuracy.,
    publicationDate: 2015-11-18,
    authors: ['E. Choi', 'M. T. Bahadori', 'A. Schuetz', 'W. Stewart', 'Jimeng Sun'],
    score: 169.26222152663877
},
{
    title: and Machine,
    abstract: This editorial introduces the first part of CEJEME’s Special Issue on Artificial Intelligence and Machine Learning in Educational Measurement. As AI and ML technologies revolutionize education, they offer new opportunities for personalized learning and innovative assessment practices. This issue highlights the transformative impact of AI and ML on educational measurement, addressing both their potential and the ethical challenges they pose. This issue includes four articles that explore the opportunities and ethical challenges of AI in educational measurement, automated text scoring in the age of Generative AI for the GPU-poor, a novel approach using autoencoders and BERT to detect compromised items in computerized testing, and the use of ML packages in R. The issue provides valuable insights into the future of educational measurement. A second part of this special issue will be available in spring 2025.,
    publicationDate: None,
    authors: ['Okan Bulut', 'Yi Zheng'],
    score: 169.26202209292808
},
{
    title: Green AI,
    abstract: The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2]. These computations have a surprisingly large carbon footprint [38]. Ironically, deep learning was inspired by the human brain, which is remarkably energy efficient. Moreover, the financial cost of the computations can make it difficult for academics, students, and researchers, in particular those from emerging economies, to engage in deep learning research. This position paper advocates a practical solution by making efficiency an evaluation criterion for research alongside accuracy and related measures. In addition, we propose reporting the financial cost or"price tag"of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. Our goal is to make AI both greener and more inclusive---enabling any inspired undergraduate with a laptop to write high-quality research papers. Green AI is an emerging focus at the Allen Institute for AI.,
    publicationDate: 2019-07-22,
    authors: ['Roy Schwartz', 'Jesse Dodge', 'Noah A. Smith', 'Oren Etzioni'],
    score: 169.00134538596072
},
{
    title: AiiA, an enzyme that inactivates the acylhomoserine lactone quorum-sensing signal and attenuates the virulence of Erwinia carotovora.,
    abstract: N-acylhomoserine lactones, known as autoinducers (AIs), are widely conserved signal molecules present in quorum-sensing systems of many gram-negative bacteria. AIs are involved in the regulation of diverse biological functions, including expression of pathogenic genes in the plant pathogens Pseudomonas solanacearum, several Erwinia species, and the human pathogen Pseudomonas aeruginosa. A bacterial isolate, Bacillus sp. 240B1, is capable of enzymatic inactivation of AIs. The gene (aiiA) for AI inactivation from Bacillus sp. 240B1 has been cloned and shown to encode a protein of 250 amino acids. Sequence alignment indicates that AiiA contains a "HXHXDH" zinc-binding motif that is conserved in several groups of metallohydrolases. Site-directed mutagenesis showed that conserved aspartate and most histidine residues are required for AiiA activity. Expression of aiiA in transformed Erwinia carotovora strain SCG1 significantly reduces the release of AI, decreases extracellular pectolytic enzyme activities, and attenuates pathogenicity on potato, eggplant, Chinese cabbage, carrot, celery, cauliflower, and tobacco. Our results indicate that the AI-inactivation approach represents a promising strategy for prevention of diseases in which virulence is regulated by AIs.,
    publicationDate: None,
    authors: ['Yi-hu Dong', 'Jin‐Ling Xu', 'Xianzhen Li', 'Lian-Hui Zhang'],
    score: 168.6887681801515
},
{
    title: Challenges and Opportunities,
    abstract: None,
    publicationDate: 1999-01-01,
    authors: ['Masa Sylvester Motadi'],
    score: 168.22495341522455
},
{
    title: Artificial intelligence faces reproducibility crisis.,
    abstract: The booming field of artificial intelligence (AI) is grappling with a replication crisis, much like the ones that have afflicted psychology, medicine, and other fields over the past decade. Just because algorithms are based on code doesn9t mean experiments are easily replicated. Far from it. Unpublished codes and a sensitivity to training conditions have made it difficult for AI researchers to reproduce many key results. That is leading to a new conscientiousness about research methods and publication protocols. Last week, at a meeting of the Association for the Advancement of Artificial Intelligence in New Orleans, Louisiana, reproducibility was on the agenda, with some teams diagnosing the problem—and one laying out tools to mitigate it.,
    publicationDate: 2018-02-16,
    authors: ['M. Hutson'],
    score: 168.18909143627278
},
{
    title: Causability and explainability of artificial intelligence in medicine,
    abstract: Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black‐box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use‐case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system,
    publicationDate: 2019-04-02,
    authors: ['Andreas Holzinger', 'G. Langs', 'H. Denk', 'K. Zatloukal', 'Heimo Müller'],
    score: 168.01961613455438
},
{
    title: Scaling learning algorithms towards AI,
    abstract: One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), reasoning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, with minimal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally limited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very inefficient in terms of required number of computational elements and examples. Second, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learning) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more abstract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence.,
    publicationDate: None,
    authors: ['Yoshua Bengio', 'Yann LeCun'],
    score: 167.72387916917796
},
{
    title: Decision-Theoretic Planning: Structural Assumptions and Computational Leverage,
    abstract: Planning under uncertainty is a central problem in the study of automated sequential decision making, and has been addressed by researchers in many different fields, including AI planning, decision analysis, operations research, control theory and economics. While the assumptions and perspectives adopted in these areas often differ in substantial ways, many planning problems of interest to researchers in these fields can be modeled as Markov decision processes (MDPs) and analyzed using the techniques of decision theory. This paper presents an overview and synthesis of MDP-related methods, showing how they provide a unifying framework for modeling many classes of planning problems studied in AI. It also describes structural properties of MDPs that, when exhibited by particular classes of problems, can be exploited in the construction of optimal or approximately optimal policies or plans. Planning problems commonly possess structure in the reward and value functions used to describe performance criteria, in the functions used to describe state transitions and observations, and in the relationships among features used to describe states, actions, rewards, and observations. Specialized representations, and algorithms employing these representations, can achieve computational leverage by exploiting these various forms of structure. Certain AI techniques -- in particular those based on the use of structured, intensional representations -- can be viewed in this way. This paper surveys several types of representations for both classical and decision-theoretic planning problems, and planning algorithms that exploit these representations in a number of different ways to ease the computational burden of constructing policies or plans. It focuses primarily on abstraction, aggregation and decomposition techniques based on AI-style representations.,
    publicationDate: 1999-07-01,
    authors: ['Craig Boutilier', 'T. Dean', 'S. Hanks'],
    score: 167.59787613729898
},
{
    title: Objaverse: A Universe of Annotated 3D Objects,
    abstract: Massive data corpora like WebText, Wikipedia, Conceptual Captions, WebImageText, and LAION have propelled recent dramatic progress in AI. Large neural models trained on such datasets produce impressive results and top many of today's benchmarks. A notable omisslion within this family of large-scale datasets is 3D data. Despite considerable interest and potential applications in 3D vision, datasets of high-fidelity 3D models continue to be mid-sized with limited diversity of object categories. Addressing this gap, we present Objaverse 1.0, a large dataset of objects with 800K + (and growing) 3D models with descriptive captions, tags, and animations. Objaverse improves upon present day 3D repositories in terms of scale, number of categories, and in the visual diversity of instances within a category. We demonstrate the large potential of Objaverse via four diverse applications: training generative 3D models, improving tail category segmentation on the LVIS benchmark, training open-vocabulary object-navigation models for Embodied AI, and creating a new benchmark for robustness analysis of vision models. Objaverse can open new directions for research and enable new applications across the field of AI.,
    publicationDate: 2022-12-15,
    authors: ['Matt Deitke', 'Dustin Schwenk', 'Jordi Salvador', 'Luca Weihs', 'Oscar Michel', 'Eli VanderBilt', 'Ludwig Schmidt', 'Kiana Ehsani', 'Aniruddha Kembhavi', 'Ali Farhadi'],
    score: 167.3608530175376
},
{
    title: Embodied Question Answering,
    abstract: We present a new AI task - Embodied Question Answering (EmbodiedQA) - where an agent is spawned at a random location in a 3D environment and asked a question ('What color is the car?'). In order to answer, the agent must first intelligently navigate to explore the environment, gather necessary visual information through first-person (egocentric) vision, and then answer the question ('orange'). EmbodiedQA requires a range of AI skills - language understanding, visual recognition, active perception, goal-driven navigation, commonsense reasoning, long-term memory, and grounding language into actions. In this work, we develop a dataset of questions and answers in House3D environments [1], evaluation metrics, and a hierarchical model trained with imitation and reinforcement learning.,
    publicationDate: 2017-11-30,
    authors: ['Abhishek Das', 'Samyak Datta', 'Georgia Gkioxari', 'Stefan Lee', 'Devi Parikh', 'Dhruv Batra'],
    score: 167.27547403904316
},
{
    title: Blockchain for AI: Review and Open Research Challenges,
    abstract: Recently, artificial intelligence (AI) and blockchain have become two of the most trending and disruptive technologies. Blockchain technology has the ability to automate payment in cryptocurrency and to provide access to a shared ledger of data, transactions, and logs in a decentralized, secure, and trusted manner. Also with smart contracts, blockchain has the ability to govern interactions among participants with no intermediary or a trusted third party. AI, on the other hand, offers intelligence and decision-making capabilities for machines similar to humans. In this paper, we present a detailed survey on blockchain applications for AI. We review the literature, tabulate, and summarize the emerging blockchain applications, platforms, and protocols specifically targeting AI area. We also identify and discuss open research challenges of utilizing blockchain technologies for AI.,
    publicationDate: None,
    authors: ['K. Salah', 'M. H. Rehman', 'Nishara Nizamuddin', 'Ala I. Al-Fuqaha'],
    score: 167.20188435751035
},
{
    title: “Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI,
    abstract: AI models are increasingly applied in high-stakes domains like health and conservation. Data quality carries an elevated significance in high-stakes AI due to its heightened downstream impact, impacting predictions like cancer detection, wildlife poaching, and loan allocations. Paradoxically, data is the most under-valued and de-glamorised aspect of AI. In this paper, we report on data practices in high-stakes AI, from interviews with 53 AI practitioners in India, East and West African countries, and USA. We define, identify, and present empirical evidence on Data Cascades—compounding events causing negative, downstream effects from data issues—triggered by conventional AI/ML practices that undervalue data quality. Data cascades are pervasive (92% prevalence), invisible, delayed, but often avoidable. We discuss HCI opportunities in designing and incentivizing data excellence as a first-class citizen of AI, resulting in safer and more robust systems for all.,
    publicationDate: 2021-05-06,
    authors: ['Nithya Sambasivan', 'Shivani Kapania', 'H. Highfill', 'Diana Akrong', 'Praveen K. Paritosh', 'Lora Aroyo'],
    score: 167.17764463312852
},
{
    title: From mass to structure: an aromaticity index for high‐resolution mass data of natural organic matter,
    abstract: Recent progress in Fourier transform ion cyclotron resonance mass spectrometry (FTICRMS) provided extensive molecular mass data for complex natural organic matter (NOM). Structural information can be deduced solely from the molecular masses for ions with extreme molecular element ratios, in particular low H/C ratios, which are abundant in thermally altered NOM (e.g. black carbon). In this communication we propose a general aromaticity index (AI) and two threshold values as unequivocal criteria for the existence of either aromatic (AI > 0.5) or condensed aromatic structures (AI >= 0.67) in NOM. AI can be calculated from molecular formulae which are derived from exact molecular masses of naturally occurring compounds containing C, H, O, N, S and P and is especially applicable for substances with aromatic cores and few alkylations. In order to test the validity of our model index, AI is applied to FTICRMS data of a NOM deep-water sample from the Weddell Sea (Antarctica), a fulvic acid standard and an artificial dataset of all theoretically possible molecular formulae. For graphical evaluation a ternary plot is suggested for four-dimensional data representation. The proposed aromaticity index is a step towards structural identification of NOM and the molecular identification of black carbon in the environment.,
    publicationDate: 2006-03-15,
    authors: ['B. Koch', 'T. Dittmar'],
    score: 166.9634824544452
},
{
    title: An Overview of Artificial Intelligence Applications for Power Electronics,
    abstract: This article gives an overview of the artificial intelligence (AI) applications for power electronic systems. The three distinctive life-cycle phases, design, control, and maintenance are correlated with one or more tasks to be addressed by AI, including optimization, classification, regression, and data structure exploration. The applications of four categories of AI are discussed, which are expert system, fuzzy logic, metaheuristic method, and machine learning. More than 500 publications have been reviewed to identify the common understandings, practical implementation challenges, and research opportunities in the application of AI for power electronics. This article is accompanied by an Excel file listing the relevant publications for statistical analytics.,
    publicationDate: 2020-06-07,
    authors: ['Shuai Zhao', 'F. Blaabjerg', 'Huai Wang'],
    score: 166.67507971054016
},
{
    title: Preparing Medical Imaging Data for Machine Learning.,
    abstract: Artificial intelligence (AI) continues to garner substantial interest in medical imaging. The potential applications are vast and include the entirety of the medical imaging life cycle from image creation to diagnosis to outcome prediction. The chief obstacles to development and clinical implementation of AI algorithms include availability of sufficiently large, curated, and representative training data that includes expert labeling (eg, annotations). Current supervised AI methods require a curation process for data to optimally train, validate, and test algorithms. Currently, most research groups and industry have limited data access based on small sample sizes from small geographic areas. In addition, the preparation of data is a costly and time-intensive process, the results of which are algorithms with limited utility and poor generalization. In this article, the authors describe fundamental steps for preparing medical imaging data in AI algorithm development, explain current limitations to data curation, and explore new approaches to address the problem of data availability.,
    publicationDate: 2020-02-18,
    authors: ['M. Willemink', 'W. A. Koszek', 'Cailin Hardell', 'Jie Wu', 'D. Fleischmann', 'H. Harvey', 'L. Folio', 'R. Summers', 'D. Rubin', 'M. Lungren'],
    score: 166.63810249702297
},
{
    title: Questioning the AI: Informing Design Practices for Explainable AI User Experiences,
    abstract: A surge of interest in explainable AI (XAI) has led to a vast collection of algorithmic work on the topic. While many recognize the necessity to incorporate explainability features in AI systems, how to address real-world user needs for understanding AI remains an open question. By interviewing 20 UX and design practitioners working on various AI products, we seek to identify gaps between the current XAI algorithmic work and practices to create explainable AI products. To do so, we develop an algorithm-informed XAI question bank in which user needs for explainability are represented as prototypical questions users might ask about the AI, and use it as a study probe. Our work contributes insights into the design space of XAI, informs efforts to support design practices in this space, and identifies opportunities for future XAI work. We also provide an extended XAI question bank and discuss how it can be used for creating user-centered XAI.,
    publicationDate: 2020-01-08,
    authors: ['Q. Liao', 'D. Gruen', 'Sarah Miller'],
    score: 166.59025556650147
},
{
    title: Empowering Things With Intelligence: A Survey of the Progress, Challenges, and Opportunities in Artificial Intelligence of Things,
    abstract: In the Internet-of-Things (IoT) era, billions of sensors and devices collect and process data from the environment, transmit them to cloud centers, and receive feedback via the Internet for connectivity and perception. However, transmitting massive amounts of heterogeneous data, perceiving complex environments from these data, and then making smart decisions in a timely manner are difficult. Artificial intelligence (AI), especially deep learning, is now a proven success in various areas, including computer vision, speech recognition, and natural language processing. AI introduced into the IoT heralds the era of AI of things (AIoT). This article presents a comprehensive survey on AIoT to show how AI can empower the IoT to make it faster, smarter, greener, and safer. Specifically, we briefly present the AIoT architecture in the context of cloud computing, fog computing, and edge computing. Then, we present progress in AI research for IoT from four perspectives: 1) perceiving; 2) learning; 3) reasoning; and 4) behaving. Next, we summarize some promising applications of AIoT that are likely to profoundly reshape our world. Finally, we highlight the challenges facing AIoT and some potential research opportunities.,
    publicationDate: 2020-11-17,
    authors: ['Jing Zhang', 'D. Tao'],
    score: 166.53222148787134
},
{
    title: Current Challenges and Future Opportunities for XAI in Machine Learning-Based Clinical Decision Support Systems: A Systematic Review,
    abstract: Machine Learning and Artificial Intelligence (AI) more broadly have great immediate and future potential for transforming almost all aspects of medicine. However, in many applications, even outside medicine, a lack of transparency in AI applications has become increasingly problematic. This is particularly pronounced where users need to interpret the output of AI systems. Explainable AI (XAI) provides a rationale that allows users to understand why a system has produced a given output. The output can then be interpreted within a given context. One area that is in great need of XAI is that of Clinical Decision Support Systems (CDSSs). These systems support medical practitioners in their clinic decision-making and in the absence of explainability may lead to issues of under or over-reliance. Providing explanations for how recommendations are arrived at will allow practitioners to make more nuanced, and in some cases, life-saving decisions. The need for XAI in CDSS, and the medical field in general, is amplified by the need for ethical and fair decision-making and the fact that AI trained with historical data can be a reinforcement agent of historical actions and biases that should be uncovered. We performed a systematic literature review of work to-date in the application of XAI in CDSS. Tabular data processing XAI-enabled systems are the most common, while XAI-enabled CDSS for text analysis are the least common in literature. There is more interest in developers for the provision of local explanations, while there was almost a balance between post-hoc and ante-hoc explanations, as well as between model-specific and model-agnostic techniques. Studies reported benefits of the use of XAI such as the fact that it could enhance decision confidence for clinicians, or generate the hypothesis about causality, which ultimately leads to increased trustworthiness and acceptability of the system and potential for its incorporation in the clinical workflow. However, we found an overall distinct lack of application of XAI in the context of CDSS and, in particular, a lack of user studies exploring the needs of clinicians. We propose some guidelines for the implementation of XAI in CDSS and explore some opportunities, challenges, and future research needs.,
    publicationDate: 2021-05-31,
    authors: ['A. Antoniadi', 'Yuhan Du', 'Yasmine Guendouz', 'Lan Wei', 'Claudia Mazo', 'Brett A. Becker', 'C. Mooney'],
    score: 166.52481493690658
},
{
    title: Multiagent Systems: A Modern Approach to Dis- tributed Artificial Intelligence A Review,
    abstract: dent on automated intelligent systems; at the same time, these systems have become more and more complicated. Society’s expectation regarding the capabilities and intelligence of such systems has also grown. We have become a more complicated society with more complicated problems. As the expectation of intelligent systems rises, we discover many more applications for AI. Additionally, as the difficulty level and computational requirements of such problems rise, there is a need to distribute the problem solving. Although the field of multiagent systems and distributed AI is relatively young, the importance and applicability of this technology for solving today’s problems continues to grow. As the title indicates, Multiagent Systems: A Modern Approach to Distributed Artificial Intelligence covers the design and development of multiagent and distributed AI systems. The purpose of this book is to provide a comprehensive overview of the field. It is an excellent collection of closely related papers that provides a wonderful introduction to multiagent systems and distributed AI. The book provides not only basic introductory information but also detailed discussions covering the important topics in the field, practical examples and applications, and a section dedicated to the relationship between multiagent systems and various other research areas. This book compiles the important concepts and methodologies required to develop a multiagent system in an understandable, and comprehensive, manner. Not only does the book focus on the known solutions and issues, it also discusses the open questions and dilemmas. The prologue begins by defining distributed AI as “the study, construc-,
    publicationDate: None,
    authors: ['J. Adams'],
    score: 166.41352268887132
},
{
    title: The Rise and Potential of Large Language Model Based Agents: A Survey,
    abstract: For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.,
    publicationDate: 2023-09-14,
    authors: ['Zhiheng Xi', 'Wenxiang Chen', 'Xin Guo', 'Wei He', 'Yiwen Ding', 'Boyang Hong', 'Ming Zhang', 'Junzhe Wang', 'Senjie Jin', 'Enyu Zhou', 'Rui Zheng', 'Xiaoran Fan', 'Xiao Wang', 'Limao Xiong', 'Qin Liu', 'Yuhao Zhou', 'Weiran Wang', 'Changhao Jiang', 'Yicheng Zou', 'Xiangyang Liu', 'Zhangyue Yin', 'Shihan Dou', 'Rongxiang Weng', 'Wensen Cheng', 'Qi Zhang', 'Wenjuan Qin', 'Yongyan Zheng', 'Xipeng Qiu', 'Xuanjing Huan', 'Tao Gui'],
    score: 166.3730353585808
},
{
    title: Federated Learning for Smart Healthcare: A Survey,
    abstract: Recent advances in communication technologies and the Internet-of-Medical-Things (IOMT) have transformed smart healthcare enabled by artificial intelligence (AI). Traditionally, AI techniques require centralized data collection and processing that may be infeasible in realistic healthcare scenarios due to the high scalability of modern healthcare networks and growing data privacy concerns. Federated Learning (FL), as an emerging distributed collaborative AI paradigm, is particularly attractive for smart healthcare, by coordinating multiple clients (e.g., hospitals) to perform AI training without sharing raw data. Accordingly, we provide a comprehensive survey on the use of FL in smart healthcare. First, we present the recent advances in FL, the motivations, and the requirements of using FL in smart healthcare. The recent FL designs for smart healthcare are then discussed, ranging from resource-aware FL, secure and privacy-aware FL to incentive FL and personalized FL. Subsequently, we provide a state-of-the-art review on the emerging applications of FL in key healthcare domains, including health data management, remote health monitoring, medical imaging, and COVID-19 detection. Several recent FL-based smart healthcare projects are analyzed, and the key lessons learned from the survey are also highlighted. Finally, we discuss interesting research challenges and possible directions for future FL research in smart healthcare.,
    publicationDate: 2021-11-16,
    authors: ['Dinh C. Nguyen', 'Viet Quoc Pham', 'P. Pathirana', 'Ming Ding', 'A. Seneviratne', 'Zihuai Lin', 'O. Dobre', 'W. Hwang'],
    score: 166.24235739547532
},
{
    title: Edge Intelligence: The Confluence of Edge Computing and Artificial Intelligence,
    abstract: Along with the rapid developments in communication technologies and the surge in the use of mobile devices, a brand-new computation paradigm, edge computing, is surging in popularity. Meanwhile, the artificial intelligence (AI) applications are thriving with the breakthroughs in deep learning and the many improvements in hardware architectures. Billions of data bytes, generated at the network edge, put massive demands on data processing and structural optimization. Thus, there exists a strong demand to integrate edge computing and AI, which gives birth to edge intelligence. In this article, we divide edge intelligence into AI for edge (intelligence-enabled edge computing) and AI on edge (artificial intelligence on edge). The former focuses on providing more optimal solutions to key problems in edge computing with the help of popular and effective AI technologies while the latter studies how to carry out the entire process of building AI models, i.e., model training and inference, on the edge. This article provides insights into this new interdisciplinary field from a broader perspective. It discusses the core concepts and the research roadmap, which should provide the necessary background for potential future research initiatives in edge intelligence.,
    publicationDate: 2019-09-02,
    authors: ['Shuiguang Deng', 'Hailiang Zhao', 'Jianwei Yin', 'S. Dustdar', 'Albert Y. Zomaya'],
    score: 166.0523859671966
},
{
    title: Introduction to AI Robotics,
    abstract: From the Publisher: 
This text covers all the material needed to understand the principles behind the AI approach to robotics and to program an artificially intelligent robot for applications involving sensing, navigation, planning, and uncertainty. Robin Murphy is extremely effective at combining theoretical and practical rigor with a light narrative touch. In the overview, for example, she touches upon anthropomorphic robots from classic films and science fiction stories before delving into the nuts and bolts of organizing intelligence in robots. 
Following the overview, Murphy contrasts AI and engineering approaches and discusses what she calls the three paradigms of AI robotics: hierarchical, reactive, and hybrid deliberative/reactive. Later chapters explore multiagent scenarios, navigation and path-planning for mobile robots, and the basics of computer vision and range sensing. Each chapter includes objectives, review questions, and exercises. Many chapters contain one or more case studies showing how the concepts were implemented on real robots. Murphy, who is well known for her classroom teaching, conveys the intellectual adventure of mastering complex theoretical and technical material.,
    publicationDate: 2000-11-06,
    authors: ['R. Murphy'],
    score: 165.971385416877
},
{
    title: Toward understanding the impact of artificial intelligence on labor,
    abstract: Rapid advances in artificial intelligence (AI) and automation technologies have the potential to significantly disrupt labor markets. While AI and automation can augment the productivity of some workers, they can replace the work done by others and will likely transform almost all occupations at least to some degree. Rising automation is happening in a period of growing economic inequality, raising fears of mass technological unemployment and a renewed call for policy efforts to address the consequences of technological change. In this paper we discuss the barriers that inhibit scientists from measuring the effects of AI and automation on the future of work. These barriers include the lack of high-quality data about the nature of work (e.g., the dynamic requirements of occupations), lack of empirically informed models of key microlevel processes (e.g., skill substitution and human–machine complementarity), and insufficient understanding of how cognitive technologies interact with broader economic dynamics and institutional mechanisms (e.g., urban migration and international trade policy). Overcoming these barriers requires improvements in the longitudinal and spatial resolution of data, as well as refinements to data on workplace skills. These improvements will enable multidisciplinary research to quantitatively monitor and predict the complex evolution of work in tandem with technological progress. Finally, given the fundamental uncertainty in predicting technological change, we recommend developing a decision framework that focuses on resilience to unexpected scenarios in addition to general equilibrium behavior.,
    publicationDate: 2019-03-25,
    authors: ['M. Frank', 'David Autor', 'James E. Bessen', 'Erik Brynjolfsson', 'M. Cebrián', 'D. Deming', 'M. Feldman', 'Matthew Groh', 'J. Lobo', 'E. Moro', 'Dashun Wang', 'Hyejin Youn', 'Iyad Rahwan'],
    score: 165.85176019842936
},
{
    title: Bias in data‐driven artificial intelligence systems—An introductory survey,
    abstract: Artificial Intelligence (AI)‐based systems are widely employed nowadays to make decisions that have far‐reaching impact on individuals and society. Their decisions might affect everyone, everywhere, and anytime, entailing concerns about potential human rights issues. Therefore, it is necessary to move beyond traditional AI algorithms optimized for predictive performance and embed ethical and legal principles in their design, training, and deployment to ensure social good while still benefiting from the huge potential of the AI technology. The goal of this survey is to provide a broad multidisciplinary overview of the area of bias in AI systems, focusing on technical challenges and solutions as well as to suggest new research directions towards approaches well‐grounded in a legal frame. In this survey, we focus on data‐driven AI, as a large part of AI is powered nowadays by (big) data and powerful machine learning algorithms. If otherwise not specified, we use the general term bias to describe problems related to the gathering or processing of data that might result in prejudiced decisions on the bases of demographic features such as race, sex, and so forth.,
    publicationDate: 2020-02-03,
    authors: ['Eirini Ntoutsi', 'P. Fafalios', 'U. Gadiraju', 'Vasileios Iosifidis', 'W. Nejdl', 'Maria-Esther Vidal', 'S. Ruggieri', 'F. Turini', 'S. Papadopoulos', 'Emmanouil Krasanakis', 'I. Kompatsiaris', 'K. Kinder-Kurlanda', 'Claudia Wagner', 'F. Karimi', 'Miriam Fernández', 'Harith Alani', 'Bettina Berendt', 'Tina Kruegel', 'C. Heinze', 'Klaus Broelemann', 'Gjergji Kasneci', 'T. Tiropanis', 'Steffen Staab'],
    score: 165.72724026109148
},
{
    title: Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making,
    abstract: Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.,
    publicationDate: 2020-01-07,
    authors: ['Yunfeng Zhang', 'Q. Liao', 'R. Bellamy'],
    score: 165.59979684365518
},
{
    title: Reproductive management of lactating dairy cows using synchronization of ovulation.,
    abstract: Lactating dairy cows have poor reproductive efficiency because of low fertility and low rates of estrus detection. To eliminate the dependence on detection of estrus, we have recently developed a timed artificial insemination (AI) protocol that synchronized the time of ovulation using GnRH and PGF2 alpha. The effectiveness of this protocol as a management tool was compared with standard reproductive management. Lactating dairy cows (n = 333) from three herds were randomly assigned at parturition to two groups. Control cows were managed according to the typical reproductive strategy of the farm that relied on detection of estrus, the a.m.-p.m. breeding rule, and periodic use of PGF2 alpha. Treated cows had timed AI after synchronization of ovulation with GnRH and PGF2 alpha. For both groups, the voluntary waiting period was 50 d postpartum. Pregnancy diagnosis was performed by ultrasound between 32 and 38 d post-AI. Nonpregnant cows were inseminated again using the original treatment until diagnosed as pregnant or until culled from the herd. Median days to first AI (54 vs. 83) and days open (99 vs. 118) were lower for treated cows than for control cows, respectively. Pregnancy rates for the first AI were similar (37% vs. 39%) for the two groups even though treated cows were bred at an earlier time postpartum. More treated cows than control cows were pregnant at 60 d (37% vs. 5%) and at 100 d (53% vs. 35%) after calving. Thus, this protocol allowed effective management of AI in lactating dairy cows without the need for estrus detection.,
    publicationDate: 1997-02-01,
    authors: ['J. R. Pursley', 'M. Kosorok', 'M. Wiltbank'],
    score: 165.57417770847786
},
{
    title: AI in health and medicine,
    abstract: None,
    publicationDate: 2022-01-01,
    authors: ['P. Rajpurkar', 'E. Chen', 'Oishi Banerjee', 'E. Topol'],
    score: 165.4495936755958
},
{
    title: A Brief History of Artificial Intelligence: On the Past, Present, and Future of Artificial Intelligence,
    abstract: This introduction to this special issue discusses artificial intelligence (AI), commonly defined as “a system’s ability to interpret external data correctly, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.” It summarizes seven articles published in this special issue that present a wide variety of perspectives on AI, authored by several of the world’s leading experts and specialists in AI. It concludes by offering a comprehensive outlook on the future of AI, drawing on micro-, meso-, and macro-perspectives.,
    publicationDate: 2019-07-17,
    authors: ['M. Haenlein', 'A. Kaplan'],
    score: 165.3830747195778
},
{
    title: Resistance To Medical Artificial Intelligence,
    abstract: 
 Artificial intelligence (AI) is revolutionizing healthcare, but little is known about consumer receptivity to AI in medicine. Consumers are reluctant to utilize healthcare provided by AI in real and hypothetical choices, separate and joint evaluations. Consumers are less likely to utilize healthcare (study 1), exhibit lower reservation prices for healthcare (study 2), are less sensitive to differences in provider performance (studies 3A–3C), and derive negative utility if a provider is automated rather than human (study 4). Uniqueness neglect, a concern that AI providers are less able than human providers to account for consumers’ unique characteristics and circumstances, drives consumer resistance to medical AI. Indeed, resistance to medical AI is stronger for consumers who perceive themselves to be more unique (study 5). Uniqueness neglect mediates resistance to medical AI (study 6), and is eliminated when AI provides care (a) that is framed as personalized (study 7), (b) to consumers other than the self (study 8), or (c) that only supports, rather than replaces, a decision made by a human healthcare provider (study 9). These findings make contributions to the psychology of automation and medical decision making, and suggest interventions to increase consumer acceptance of AI in medicine.,
    publicationDate: 2019-04-21,
    authors: ['Chiara Longoni', 'Andrea Bonezzi', 'Carey K. Morewedge'],
    score: 165.38125613759942
},
{
    title: Generative AI at Work,
    abstract: We study the staggered introduction of a generative AI-based conversational assistant using data from 5,172 customer support agents. Access to AI assistance increases worker productivity, as measured by issues resolved per hour, by 15\% on average, with substantial heterogeneity across workers. Less experienced and lower-skilled workers improve both the speed and quality of their output while the most experienced and highest-skilled workers see small gains in speed and small declines in quality. We also find evidence that AI assistance facilitates worker learning and improves English fluency, particularly among international agents. While AI systems improve with more training data, we find that the gains from AI adoption are largest for relatively rare problems, where human agents have less baseline training and experience. Finally, we provide evidence that AI assistance improves the experience of work along two key dimensions: customers are more polite and less likely to ask to speak to a manager.,
    publicationDate: 2023-04-01,
    authors: ['Erik Brynjolfsson', 'Danielle Li', 'Lindsey Raymond'],
    score: 165.3553918148609
},
{
    title: Engineering Education in the Era of ChatGPT: Promise and Pitfalls of Generative AI for Education,
    abstract: Engineering education is constantly evolving to keep up with the latest technological developments and meet the changing needs of the engineering industry. One promising development in this field is the use of generative artificial intelligence technology, such as the ChatGPT conversational agent. ChatGPT has the potential to offer personalized and effective learning experiences by providing students with customized feedback and explanations, as well as creating realistic virtual simulations for hands-on learning. However, it is important to also consider the limitations of this technology. ChatGPT and other generative AI systems are only as good as their training data and may perpetuate biases or even generate and spread misinformation. Additionally, the use of generative AI in education raises ethical concerns such as the potential for unethical or dishonest use by students and the potential unemployment of humans who are made redundant by technology. While the current state of generative AI technology represented by ChatGPT is impressive but flawed, it is only a preview of what is to come. It is important for engineering educators to understand the implications of this technology and study how to adapt the engineering education ecosystem to ensure that the next generation of engineers can take advantage of the benefits offered by generative AI while minimizing any negative consequences.,
    publicationDate: 2023-05-01,
    authors: ['Junaid Qadir'],
    score: 165.18130912909862
},
{
    title: Technologies,
    abstract: : With the continuous developments of information technology, advanced computer technology and information technology have been promoted and used in the ﬁeld of music. As one of the products of the rapid development of information technology, Artiﬁcial Intelligence (AI) involves many interdisciplinary subjects, adding new elements to music education. By analyzing the advantages of AI in music education, this paper systematically summarizes the application of AI in music education and discusses the development prospects of AI in music education. With the aid of AI, the combination of intelligent technology and on-site teaching solves the lack of individuation in the traditional mode and enhances students’ interest in learning,
    publicationDate: 2018-07-27,
    authors: ['Octavian Iordache'],
    score: 165.09946951651483
},
{
    title: A Structure-Based Drug Discovery Paradigm,
    abstract: Structure-based drug design is becoming an essential tool for faster and more cost-efficient lead discovery relative to the traditional method. Genomic, proteomic, and structural studies have provided hundreds of new targets and opportunities for future drug discovery. This situation poses a major problem: the necessity to handle the “big data” generated by combinatorial chemistry. Artificial intelligence (AI) and deep learning play a pivotal role in the analysis and systemization of larger data sets by statistical machine learning methods. Advanced AI-based sophisticated machine learning tools have a significant impact on the drug discovery process including medicinal chemistry. In this review, we focus on the currently available methods and algorithms for structure-based drug design including virtual screening and de novo drug design, with a special emphasis on AI- and deep-learning-based methods used for drug discovery.,
    publicationDate: 2019-06-01,
    authors: ['Maria Batool', 'Bilal Ahmad', 'Sangdun Choi'],
    score: 165.09529739402598
},
{
    title: Human-level play in the game of Diplomacy by combining language models with strategic reasoning,
    abstract: Despite much progress in training artificial intelligence (AI) systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players’ beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10% of participants who played more than one game. Description AI masters Diplomacy The game Diplomacy has been a major challenge for artificial intelligence (AI). Unlike other competitive games that AI has recently mastered, such as chess, Go, and poker, Diplomacy cannot be solved purely through self-play; it requires the development of an agent to understand other players’ motivations and perspectives and to use natural language to negotiate complex shared plans. The Meta Fundamental AI Research Diplomacy Team (FAIR) et al. developed an agent that is able to play the full natural language form of the game and demonstrates performance well above the human average in an online Diplomacy league. The present work has far-reaching implications for the development of cooperative AI and language models for communication with people, even when interactions involve a mixture of aligned and competing interests. —YS Artificial intelligence demonstrates human-level performance in the strategic board game Diplomacy.,
    publicationDate: 2022-11-22,
    authors: ['A. Bakhtin', 'Noam Brown', 'Emily Dinan', 'Gabriele Farina', 'Colin Flaherty', 'Daniel Fried', 'Andrew Goff', 'Jonathan Gray', 'Hengyuan Hu', 'Athul Paul Jacob', 'Mojtaba Komeili', 'Karthik Konath', 'Minae Kwon', 'Adam Lerer', 'Mike Lewis', 'Alexander H. Miller', 'S. Mitts', 'Adithya Renduchintala', 'Stephen Roller', 'Dirk Rowe', 'Weiyan Shi', 'Joe Spisak', 'Alexander Wei', 'David J. Wu', 'Hugh Zhang', 'Markus Zijlstra'],
    score: 165.0482138447078
},
{
    title: Pregnancy rates per artificial insemination for cows and heifers inseminated at a synchronized ovulation or synchronized estrus.,
    abstract: Two synchronization protocols were tested for lactating dairy cows and heifers. Nulliparous dairy heifers (13 to 23 mo; n = 155) and primiparous and multiparous dairy cows (60 to 289 d postpartum; n = 310) were assigned randomly to two treatments. Controls received 25 mg of PGF2 alpha and were artificially inseminated according to the a.m.-p.m. rule following detected estrus. All controls that were not detected in estrus were injected with 25 mg of PGF2 alpha at 14-d intervals until artificial insemination (AI) at a detected estrus or until timed AI at 72 to 80 h after a third sequential injection of PGF2 alpha. Treated cows and heifers received a protocol that used GnRH and PGF2 alpha to synchronize ovulation (Ovsynch). Cows and heifers that were treated with Ovsynch were injected i.m. with 100 micrograms of GnRH at a random stage of the estrous cycle. Seven days later, cows and heifers in this group received 25 mg of PGF2 alpha followed by a second injection of 100 micrograms of GnRH 30 to 36 h later. Subsequently, the treated cows and heifers received AI 16 to 20 h after the second injection of GnRH. Pregnancy rates per AI were similar (38.9% vs. 37.8%) for control cows and cows treated with the Ovsynch protocol, respectively. However, pregnancy rate per AI was greater for control heifers (74.4%) than for heifers treated with Ovsynch (35.1%). Evaluation of serum progesterone concentrations at each hormonal injection indicated that the first injection of GnRH synchronized luteal function of lactating dairy cows but not of heifers. In summary, one fixed-time AI at a synchronized ovulation provided similar pregnancy rates per AI as did AI following the a.m-p.m. rule after estrus had been induced by PGF2 alpha in lactating cows, but the fixed-time AI was not effective for heifers because of the lack of synchronization.,
    publicationDate: 1997-02-01,
    authors: ['J. R. Pursley', 'M. Wiltbank', 'J. Stevenson', 'J. Ottobre', 'H. Garverick', 'Elizabeth L. Anderson'],
    score: 164.99919442209534
},
{
    title: Principles of Artificial Intelligence,
    abstract: None,
    publicationDate: 1980-01-23,
    authors: ['N. Nilsson'],
    score: 164.2334509494503
},
{
    title: Edge Artificial Intelligence for 6G: Vision, Enabling Technologies, and Applications,
    abstract: The thriving of artificial intelligence (AI) applications is driving the further evolution of wireless networks. It has been envisioned that 6G will be transformative and will revolutionize the evolution of wireless from “connected things” to “connected intelligence”. However, state-of-the-art deep learning and big data analytics based AI systems require tremendous computation and communication resources, causing significant latency, energy consumption, network congestion, and privacy leakage in both of the training and inference processes. By embedding model training and inference capabilities into the network edge, edge AI stands out as a disruptive technology for 6G to seamlessly integrate sensing, communication, computation, and intelligence, thereby improving the efficiency, effectiveness, privacy, and security of 6G networks. In this paper, we shall provide our vision for scalable and trustworthy edge AI systems with integrated design of wireless communication strategies and decentralized machine learning models. New design principles of wireless networks, service-driven resource allocation optimization methods, as well as a holistic end-to-end system architecture to support edge AI will be described. Standardization, software and hardware platforms, and application scenarios are also discussed to facilitate the industrialization and commercialization of edge AI systems.,
    publicationDate: 2021-11-24,
    authors: ['K. Letaief', 'Yuanming Shi', 'Jianmin Lu', 'Jianhua Lu'],
    score: 164.16603672669459
},
{
    title: The LuxS family of bacterial autoinducers: biosynthesis of a novel quorum‐sensing signal molecule,
    abstract: Many bacteria control gene expression in response to cell population density, and this phenomenon is called quorum sensing. In Gram‐negative bacteria, quorum sensing typically involves the production, release and detection of acylated homoserine lactone signalling molecules called autoinducers. Vibrio harveyi, a Gram‐negative bioluminescent marine bacterium, regulates light production in response to two distinct autoinducers (AI‐1 and AI‐2). AI‐1 is a homoserine lactone. The structure of AI‐2 is not known. We have suggested previously that V. harveyi uses AI‐1 for intraspecies communication and AI‐2 for interspecies communication. Consistent with this idea, we have shown that many species of Gram‐negative and Gram‐positive bacteria produce AI‐2 and, in every case, production of AI‐2 is dependent on the function encoded by the luxS gene. We show here that LuxS is the AI‐2 synthase and that AI‐2 is produced from S‐adenosylmethionine in three enzymatic steps. The substrate for LuxS is S‐ribosylhomocysteine, which is cleaved to form two products, one of which is homocysteine, and the other is AI‐2. In this report, we also provide evidence that the biosynthetic pathway and biochemical intermediates in AI‐2 biosynthesis are identical in Escherichia coli, Salmonella typhimurium, V. harveyi, Vibrio cholerae and Enterococcus faecalis. This result suggests that, unlike quorum sensing via the family of related homoserine lactone autoinducers, AI‐2 is a unique, ‘universal’ signal that could be used by a variety of bacteria for communication among and between species.,
    publicationDate: 2001-07-01,
    authors: ['S. Schauder', 'K. Shokat', 'M. Surette', 'B. Bassler'],
    score: 163.79525774771116
},
{
    title: Intelligent 5G: When Cellular Networks Meet Artificial Intelligence,
    abstract: 5G cellular networks are assumed to be the key enabler and infrastructure provider in the ICT industry, by offering a variety of services with diverse requirements. The standardization of 5G cellular networks is being expedited, which also implies more of the candidate technologies will be adopted. Therefore, it is worthwhile to provide insight into the candidate techniques as a whole and examine the design philosophy behind them. In this article, we try to highlight one of the most fundamental features among the revolutionary techniques in the 5G era, i.e., there emerges initial intelligence in nearly every important aspect of cellular networks, including radio resource management, mobility management, service provisioning management, and so on. However, faced with ever-increasingly complicated configuration issues and blossoming new service requirements, it is still insufficient for 5G cellular networks if it lacks complete AI functionalities. Hence, we further introduce fundamental concepts in AI and discuss the relationship between AI and the candidate techniques in 5G cellular networks. Specifically, we highlight the opportunities and challenges to exploit AI to achieve intelligent 5G networks, and demonstrate the effectiveness of AI to manage and orchestrate cellular network resources. We envision that AI-empowered 5G cellular networks will make the acclaimed ICT enabler a reality.,
    publicationDate: 2017-03-27,
    authors: ['Rongpeng Li', 'Zhifeng Zhao', 'Xuan Zhou', 'Guoru Ding', 'Yan Chen', 'Zhongyao Wang', 'Honggang Zhang'],
    score: 163.54416098737184
},
{
    title: Artificial Intelligence for Remote Sensing Data Analysis: A review of challenges and opportunities,
    abstract: Artificial intelligence (AI) plays a growing role in remote sensing (RS). Applications of AI, particularly machine learning algorithms, range from initial image processing to high-level data understanding and knowledge discovery. AI techniques have emerged as a powerful strategy for analyzing RS data and led to remarkable breakthroughs in all RS fields. Given this period of breathtaking evolution, this work aims to provide a comprehensive review of the recent achievements of AI algorithms and applications in RS data analysis. The review includes more than 270 research papers, covering the following major aspects of AI innovation for RS: machine learning, computational intelligence, AI explicability, data mining, natural language processing (NLP), and AI security. We conclude this review by identifying promising directions for future research.,
    publicationDate: 2022-06-01,
    authors: ['Lefei Zhang', 'Liangpei Zhang'],
    score: 163.52516755641645
},
{
    title: Artificial Intelligence in Advanced Manufacturing: Current Status and Future Outlook,
    abstract: 
 Today’s manufacturing systems are becoming increasingly complex, dynamic, and connected. The factory operations face challenges of highly nonlinear and stochastic activity due to the countless uncertainties and interdependencies that exist. Recent developments in artificial intelligence (AI), especially Machine Learning (ML) have shown great potential to transform the manufacturing domain through advanced analytics tools for processing the vast amounts of manufacturing data generated, known as Big Data. The focus of this paper is threefold: (1) review the state-of-the-art applications of AI to representative manufacturing problems, (2) provide a systematic view for analyzing data and process dependencies at multiple levels that AI must comprehend, and (3) identify challenges and opportunities to not only further leverage AI for manufacturing, but also influence the future development of AI to better meet the needs of manufacturing. To satisfy these objectives, the paper adopts the hierarchical organization widely practiced in manufacturing plants in examining the interdependencies from the overall system level to the more detailed granular level of incoming material process streams. In doing so, the paper considers a wide range of topics from throughput and quality, supervisory control in human–robotic collaboration, process monitoring, diagnosis, and prognosis, finally to advances in materials engineering to achieve desired material property in process modeling and control.,
    publicationDate: 2020-08-13,
    authors: ['J. Arinez', 'Q. Chang', 'R. Gao', 'Chengyi Xu', 'Jianjing Zhang'],
    score: 163.52516755641645
},
{
    title: Collaborating With ChatGPT: Considering the Implications of Generative Artificial Intelligence for Journalism and Media Education,
    abstract: Generative artificial intelligence (AI) is ushering in an era of potential transformation of journalism and media content. This essay considers one notable generative AI platform called ChatGPT made available to the public in 2022 for free use. ChatGPT allows users to enter text prompts and rapidly generates text responses drawn from its knowledge acquired via machine learning in engagement with the internet. This essay is coauthored by a human journalism and media professor in collaboration with ChatGPT. The essay demonstrates the capacity and limitations of ChatGPT and offers reflections on the implications of generative AI for journalism and media education.,
    publicationDate: 2023-01-07,
    authors: ['J. Pavlik'],
    score: 163.33864402107054
},
{
    title: AI2-THOR: An Interactive 3D Environment for Visual AI,
    abstract: We introduce The House Of inteRactions (THOR), a framework for visual AI research, available at this http URL AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks. AI2-THOR enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of AI2-THOR is to facilitate building visually intelligent models and push the research forward in this domain.,
    publicationDate: 2017-12-14,
    authors: ['Eric Kolve', 'Roozbeh Mottaghi', 'Winson Han', 'Eli VanderBilt', 'Luca Weihs', 'Alvaro Herrasti', 'Matt Deitke', 'Kiana Ehsani', 'Daniel Gordon', 'Yuke Zhu', 'Aniruddha Kembhavi', 'A. Gupta', 'Ali Farhadi'],
    score: 163.32858689348046
},
{
    title: ChatGPT: five priorities for research,
    abstract: None,
    publicationDate: 2023-02-01,
    authors: ['E. A. V. van Dis', 'J. Bollen', 'W. Zuidema', 'R. van Rooij', 'C. Bockting'],
    score: 163.2365620649677
},
{
    title: Notes on Formalizing Context,
    abstract: These notes discuss formalizing contexts as first class objects. The basic relation is ist(c,p). It asserts that the proposition p is true in the context c. The most important formulas relate the propositions true in different contexts. Introducing contexts as formal objects will permit axiomatizations in limited contexts to be expanded to transcend the original limitations. This seems necessary to provide AI programs using logic with certain capabilities that human fact representation and human reasoning possess. Fully implementing transcendence seems to require further extensions to mathematical logic, i.e. beyond the nonmonotonic inference methods first invented in AI and now studied as a new domain of logic. Various notations are considered, but. these notes are tentative in not, proposing a single language with all the desired capabilities.,
    publicationDate: 1993-08-28,
    authors: ['J. McCarthy'],
    score: 163.19033706690658
},
{
    title: Epistemic logic for AI and computer science,
    abstract: From the Publisher: 
Epistemic logic has grown from its philosophical beginnings to find diverse applications in computer science as a means of reasoning about the knowledge and belief of agents. This book, based on courses taught at universities and summer schools, provides a broad introduction to the subject; many exercises are included as well as their solutions. After presenting the necessary apparatus from mathematics and logic, the authors consider applications in the areas of common knowledge, distributed knowledge, explicit and implicit belief.,
    publicationDate: 1995-11-01,
    authors: ['J. Meyer', 'W. Hoek'],
    score: 163.07209735651506
},
{
    title: Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance,
    abstract: Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations. However, prior studies observed improvements from explanations only when the AI, alone, outperformed both the human and the best team. Can explanations help lead to complementary performance, where team accuracy is higher than either the human or the AI working solo? We conduct mixed-method user studies on three datasets, where an AI with accuracy comparable to humans helps participants solve a task (explaining itself in some conditions). While we observed complementary improvements from AI augmentation, they were not increased by explanations. Rather, explanations increased the chance that humans will accept the AI’s recommendation, regardless of its correctness. Our result poses new challenges for human-centered AI: Can we develop explanatory approaches that encourage appropriate trust in AI, and therefore help generate (or improve) complementary performance?,
    publicationDate: 2020-06-26,
    authors: ['Gagan Bansal', 'Tongshuang Sherry Wu', 'Joyce Zhou', 'Raymond Fok', 'Besmira Nushi', 'Ece Kamar', 'Marco Tulio Ribeiro', 'Daniel S. Weld'],
    score: 163.06836643853035
},
{
    title: From Artificial Intelligence to Explainable Artificial Intelligence in Industry 4.0: A Survey on What, How, and Where,
    abstract: Nowadays, Industry 4.0 can be considered a reality, a paradigm integrating modern technologies and innovations. Artificial intelligence (AI) can be considered the leading component of the industrial transformation enabling intelligent machines to execute tasks autonomously such as self-monitoring, interpretation, diagnosis, and analysis. AI-based methodologies (especially machine learning and deep learning support manufacturers and industries in predicting their maintenance needs and reducing downtime. Explainable artificial intelligence (XAI) studies and designs approaches, algorithms and tools producing human-understandable explanations of AI-based systems information and decisions. This article presents a comprehensive survey of AI and XAI-based methods adopted in the Industry 4.0 scenario. First, we briefly discuss different technologies enabling Industry 4.0. Then, we present an in-depth investigation of the main methods used in the literature: we also provide the details of what, how, why, and where these methods have been applied for Industry 4.0. Furthermore, we illustrate the opportunities and challenges that elicit future research directions toward responsible or human-centric AI and XAI systems, essential for adopting high-stakes industry applications.,
    publicationDate: 2022-08-01,
    authors: ['Imran Ahmed', 'Gwanggil Jeon', 'F. Piccialli'],
    score: 163.03177563065594
},
{
    title: Artificial Intelligence in Dentistry: Chances and Challenges,
    abstract: The term “artificial intelligence” (AI) refers to the idea of machines being capable of performing human tasks. A subdomain of AI is machine learning (ML), which “learns” intrinsic statistical patterns in data to eventually cast predictions on unseen data. Deep learning is a ML technique using multi-layer mathematical operations for learning and inferring on complex data like imagery. This succinct narrative review describes the application, limitations and possible future of AI-based dental diagnostics, treatment planning, and conduct, for example, image analysis, prediction making, record keeping, as well as dental research and discovery. AI-based applications will streamline care, relieving the dental workforce from laborious routine tasks, increasing health at lower costs for a broader population, and eventually facilitate personalized, predictive, preventive, and participatory dentistry. However, AI solutions have not by large entered routine dental practice, mainly due to 1) limited data availability, accessibility, structure, and comprehensiveness, 2) lacking methodological rigor and standards in their development, 3) and practical questions around the value and usefulness of these solutions, but also ethics and responsibility. Any AI application in dentistry should demonstrate tangible value by, for example, improving access to and quality of care, increasing efficiency and safety of services, empowering and enabling patients, supporting medical research, or increasing sustainability. Individual privacy, rights, and autonomy need to be put front and center; a shift from centralized to distributed/federated learning may address this while improving scalability and robustness. Lastly, trustworthiness into, and generalizability of, dental AI solutions need to be guaranteed; the implementation of continuous human oversight and standards grounded in evidence-based dentistry should be expected. Methods to visualize, interpret, and explain the logic behind AI solutions will contribute (“explainable AI”). Dental education will need to accompany the introduction of clinical AI solutions by fostering digital literacy in the future dental workforce.,
    publicationDate: 2020-04-21,
    authors: ['F. Schwendicke', 'W. Samek', 'J. Krois'],
    score: 163.00763761064036
},
{
    title: Engaged to a Robot? The Role of AI in Service,
    abstract: This article develops a strategic framework for using artificial intelligence (AI) to engage customers for different service benefits. This framework lays out guidelines of how to use different AIs to engage customers based on considerations of nature of service task, service offering, service strategy, and service process. AI develops from mechanical, to thinking, and to feeling. As AI advances to a higher intelligence level, more human service employees and human intelligence (HI) at the intelligence levels lower than that level should be used less. Thus, at the current level of AI development, mechanical service should be performed mostly by mechanical AI, thinking service by both thinking AI and HI, and feeling service mostly by HI. Mechanical AI should be used for standardization when service is routine and transactional, for cost leadership, and mostly at the service delivery stage. Thinking AI should be used for personalization when service is data-rich and utilitarian, for quality leadership, and mostly at the service creation stage. Feeling AI should be used for relationalization when service is relational and high touch, for relationship leadership, and mostly at the service interaction stage. We illustrate various AI applications for the three major AI benefits, providing managerial guidelines for service providers to leverage the advantages of AI as well as future research implications for service researchers to investigate AI in service from modeling, consumer, and policy perspectives.,
    publicationDate: 2020-02-21,
    authors: ['Ming-Hui Huang', 'R. Rust'],
    score: 162.9466619169178
},
{
    title: A survey on adrenal incidentaloma in Italy. Study Group on Adrenal Tumors of the Italian Society of Endocrinology.,
    abstract: The aim of this study was to perform a national survey on occasionally discovered adrenal masses [adrenal incidentalomas (AI)] under the auspices of the Italian Society of Endocrinology. This multicentric and retrospective evaluation of patients with AI includes 1096 cases collected in 26 centers between 1980 and 1995. Relevant information was obtained by means of a specifically tailored questionnaire. Of the 1096 forms received, 1004 were retained for final analysis. Patients were 420 males and 584 females, aged between 15-86 yr (median, 58 yr). Mass size (computed tomography measurement) ranged from 0.5-25 cm (median, 3.0 cm). Hormonal work-up demonstrated that 85% of the masses were nonhypersecretory, 9.2% were defined as subclinical Cushing's syndrome, 4.2% were pheochromocytomas, and 1.6% were aldosteronomas. Adrenalectomy was performed in 380 patients with removal of 198 cortical adenomas (52%), 47 cortical carcinomas (12%), 42 pheochromocytomas (11%), and other less frequent tumor types. Patients with carcinoma were significantly younger than patients with adenoma (median, 46; range, 17-84; vs. 57, 16-83 yr; P = 0.05). Adenomas were significantly smaller than carcinomas (3.5, 1-15 vs. 7.5, 2.6-25 cm; P < 0.001), and a cut-off at 4.0 cm had the highest sensitivity (93%) in differentiating between benign and malignant tumors. Hormonal work-up of patients with subclinical Cushing's syndrome showed low baseline ACTH in 79%, cortisol unsuppressibility after 1 mg dexamethasone in 73%, above normal urinary free cortisol in 75%, disturbed cortisol rhythm in 43%, and blunted ACTH response to CRH in 55%. Only 43% of patients with pheochromocytoma were hypertensive, and 86% showed elevated urinary catecholamines. All patients with aldosteronoma were hypertensive and had suppressed upright PRA. These results indicate that mass size is the most reliable variable in separating benign from malignant AI. Adrenalectomy should be recommended for AI greater than 4.0 cm because of the increased risk of malignancy, especially in young patients. Endocrine evaluation should be performed in all patients to identify silent states of hormone excess.,
    publicationDate: 2000-01-30,
    authors: ['F. Mantero', 'M. Terzolo', 'G. Arnaldi', 'G. Osella', 'A. M. Masini', 'Anna Alı̀', 'M. Giovagnetti', 'G. Opocher', 'A. Angeli'],
    score: 162.86271093818084
},
{
    title: The global landscape of AI ethics guidelines,
    abstract: None,
    publicationDate: 2019-09-01,
    authors: ['Anna Jobin', 'M. Ienca', 'E. Vayena'],
    score: 162.83751607630484
},
{
    title: Artificial Intelligence in Surgery: Promises and Perils,
    abstract: Objective: The aim of this review was to summarize major topics in artificial intelligence (AI), including their applications and limitations in surgery. This paper reviews the key capabilities of AI to help surgeons understand and critically evaluate new AI applications and to contribute to new developments. Summary Background Data: AI is composed of various subfields that each provide potential solutions to clinical problems. Each of the core subfields of AI reviewed in this piece has also been used in other industries such as the autonomous car, social networks, and deep learning computers. Methods: A review of AI papers across computer science, statistics, and medical sources was conducted to identify key concepts and techniques within AI that are driving innovation across industries, including surgery. Limitations and challenges of working with AI were also reviewed. Results: Four main subfields of AI were defined: (1) machine learning, (2) artificial neural networks, (3) natural language processing, and (4) computer vision. Their current and future applications to surgical practice were introduced, including big data analytics and clinical decision support systems. The implications of AI for surgeons and the role of surgeons in advancing the technology to optimize clinical effectiveness were discussed. Conclusions: Surgeons are well positioned to help integrate AI into modern practice. Surgeons should partner with data scientists to capture data across phases of care and to provide clinical context, for AI has the potential to revolutionize the way surgery is taught and practiced with the promise of a future optimized for the highest quality patient care.,
    publicationDate: 2018-07-01,
    authors: ['D. Hashimoto', 'G. Rosman', 'D. Rus', 'O. Meireles'],
    score: 162.78720931910593
},
{
    title: The practical implementation of artificial intelligence technologies in medicine,
    abstract: None,
    publicationDate: 2019-01-01,
    authors: ['J. He', 'Sally L. Baxter', 'Jie Xu', 'Jiming Xu', 'Xingtao Zhou', 'Kang Zhang'],
    score: 162.73528167827894
},
{
    title: Designing Theory-Driven User-Centric Explainable AI,
    abstract: From healthcare to criminal justice, artificial intelligence (AI) is increasingly supporting high-consequence human decisions. This has spurred the field of explainable AI (XAI). This paper seeks to strengthen empirical application-specific investigations of XAI by exploring theoretical underpinnings of human decision making, drawing from the fields of philosophy and psychology. In this paper, we propose a conceptual framework for building human-centered, decision-theory-driven XAI based on an extensive review across these fields. Drawing on this framework, we identify pathways along which human cognitive patterns drives needs for building XAI and how XAI can mitigate common cognitive biases. We then put this framework into practice by designing and implementing an explainable clinical diagnostic tool for intensive care phenotyping and conducting a co-design exercise with clinicians. Thereafter, we draw insights into how this framework bridges algorithm-generated explanations and human decision-making theories. Finally, we discuss implications for XAI design and development.,
    publicationDate: 2019-05-02,
    authors: ['Danding Wang', 'Qian Yang', 'Ashraf Abdul', 'Brian Y. Lim'],
    score: 162.72069036308795
},
{
    title: Planning chemical syntheses with deep neural networks and symbolic AI,
    abstract: None,
    publicationDate: 2017-08-14,
    authors: ['Marwin H. S. Segler', 'M. Preuss', 'M. Waller'],
    score: 162.540250250059
},
{
    title: Encyclopedia of artificial intelligence, vols. 1 and 2 (2nd ed.),
    abstract: From the Publisher: 
Encompasses the variable approaches to Artificial Intelligence in 267 articles written by 205 experts. It is a comprehensive, up-to-date reference work that defines the discipline and brings together the core of knowledge from all the AI fields. Clarifies and corrects misinterpretations and provides a proper understanding of Artificial Intelligence.,
    publicationDate: 1992-03-01,
    authors: ['S. Shapiro'],
    score: 162.4592002651382
},
{
    title: Trustworthy Artificial Intelligence: A Review,
    abstract: Artificial intelligence (AI) and algorithmic decision making are having a profound impact on our daily lives. These systems are vastly used in different high-stakes applications like healthcare, business, government, education, and justice, moving us toward a more algorithmic society. However, despite so many advantages of these systems, they sometimes directly or indirectly cause harm to the users and society. Therefore, it has become essential to make these systems safe, reliable, and trustworthy. Several requirements, such as fairness, explainability, accountability, reliability, and acceptance, have been proposed in this direction to make these systems trustworthy. This survey analyzes all of these different requirements through the lens of the literature. It provides an overview of different approaches that can help mitigate AI risks and increase trust and acceptance of the systems by utilizing the users and society. It also discusses existing strategies for validating and verifying these systems and the current standardization efforts for trustworthy AI. Finally, we present a holistic view of the recent advancements in trustworthy AI to help the interested researchers grasp the crucial facets of the topic efficiently and offer possible future research directions.,
    publicationDate: 2022-01-18,
    authors: ['Davinder Kaur', 'Suleyman Uslu', 'Kaley J. Rittichier', 'A. Durresi'],
    score: 162.3835266081592
},
{
    title: Metrics for Explainable AI: Challenges and Prospects,
    abstract: The question addressed in this paper is: If we present to a user an AI system that explains how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? In other words, how do we know that an explanainable AI system (XAI) is any good? Our focus is on the key concepts of measurement. We discuss specific methods for evaluating: (1) the goodness of explanations, (2) whether users are satisfied by explanations, (3) how well users understand the AI systems, (4) how curiosity motivates the search for explanations, (5) whether the user's trust and reliance on the AI are appropriate, and finally, (6) how the human-XAI work system performs. The recommendations we present derive from our integration of extensive research literatures and our own psychometric evaluations.,
    publicationDate: 2018-12-11,
    authors: ['R. Hoffman', 'Shane T. Mueller', 'Gary Klein', 'Jordan Litman'],
    score: 162.3608530175376
},
{
    title: Why Johnny Can’t Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts,
    abstract: Pre-trained large language models (“LLMs”) like GPT-3 can engage in fluent, multi-turn instruction-taking out-of-the-box, making them attractive materials for designing natural language interactions. Using natural language to steer LLM outputs (“prompting”) has emerged as an important design technique potentially accessible to non-AI-experts. Crafting effective prompts can be challenging, however, and prompt-based interactions are brittle. Here, we explore whether non-AI-experts can successfully engage in “end-user prompt engineering” using a design probe—a prototype LLM-based chatbot design tool supporting development and systematic evaluation of prompting strategies. Ultimately, our probe participants explored prompt designs opportunistically, not systematically, and struggled in ways echoing end-user programming systems and interactive machine learning systems. Expectations stemming from human-to-human instructional experiences, and a tendency to overgeneralize, were barriers to effective prompt design. These findings have implications for non-AI-expert-facing LLM-based tool design and for improving LLM-and-prompt literacy among programmers and the public, and present opportunities for further research.,
    publicationDate: 2023-04-19,
    authors: ['J.D. Zamfirescu-Pereira', 'Richmond Y. Wong', 'Bjoern Hartmann', 'Qian Yang'],
    score: 162.35468478378334
},
{
    title: Appreciative Inquiry: A Positive Revolution in Change,
    abstract: Written by the originators and leaders of the Appreciative Inquiry (AI) movement itself, this short, practical guide offers an approach to organizational change based on the possibility of a more desirable future, experience with the whole system, and activities that signal "something different is happening this time." That difference systematically taps the potential of human beings to make themselves, their organizations, and their communities more adaptive and more effective. AI, a theory of collaborative change, erases the winner/loser paradigm in favor of coordinated actions and closer relationships that lead to solutions at once simpler and more effective.,
    publicationDate: 2005-10-10,
    authors: ['David L. Cooperrider', 'D. Whitney'],
    score: 162.31923035370494
},
{
    title: The role of artificial intelligence in achieving the Sustainable Development Goals,
    abstract: None,
    publicationDate: 2019-04-30,
    authors: ['R. Vinuesa', 'Hossein Azizpour', 'Iolanda Leite', 'Madeline Balaam', 'Virginia Dignum', 'S. Domisch', 'Anna Felländer', 'S. Langhans', 'Max Tegmark', 'F. F. Nerini'],
    score: 162.24875254198162
},
{
    title: Human Trust in Artificial Intelligence: Review of Empirical Research,
    abstract: Artificial intelligence (AI) characterizes a new generation of technologies capable of interacting with the environment and aiming to simulate human intelligence. The success of integrating AI into...,
    publicationDate: 2020-07-01,
    authors: ['Ella Glikson', 'A. Woolley'],
    score: 162.21814345889936
},
{
    title: Gender differences in negative affect and well-being: the case for emotional intensity.,
    abstract: Affect intensity (AI) may reconcile 2 seemingly paradoxical findings: Women report more negative affect than men but equal happiness as men. AI describes people's varying response intensity to identical emotional stimuli. A college sample of 66 women and 34 men was assessed on both positive and negative affect using 4 measurement methods: self-report, peer report, daily report, and memory performance. A principal-components analysis revealed an affect balance component and an AI component. Multimeasure affect balance and AI scores were created, and t tests were computed that showed women to be as happy as and more intense than men. Gender accounted for less than 1% of the variance in happiness but over 13% in AI. Thus, depression findings of more negative affect in women do not conflict with well-being findings of equal happiness across gender. Generally, women's more intense positive emotions balance their higher negative affect.,
    publicationDate: 1991-09-01,
    authors: ['Frank Fujita', 'E. Diener', 'Ed Sandvik'],
    score: 162.1851764126622
},
{
    title: Artificial intelligence applications in the development of autonomous vehicles: a survey,
    abstract: The advancement of artificial intelligence ( AI ) has truly stimulated the development and deployment of autonomous vehicles ( AVs ) in the transportation industry. Fueled by big data from various sensing devices and advanced computing resources, AI has become an essential component of AVs for perceiving the surrounding environment and making appropriate decision in motion. To achieve goal of full automation ( i.e., self-driving ( , it is important to know how AI works in AV systems. Existing research have made great efforts in investigating different aspects of applying AI in AV development. However, few studies have offered the research community a thorough examination of current practices in implementing AI in AVs. Thus, this paper aims to shorten the gap by providing a comprehensive survey of key studies in this research avenue. Specifically, it intends to analyze their use of AIs in supporting the primary applications in AVs: 1) perception; 2) localization and mapping; and 3) decision making. It investigates the current practices to understand how AI can be used and what are the challenges and issues associated with their implementation. Based on the exploration of current practices and technology advances, this paper further provides insights into potential opportunities regarding the use of AI in conjunction with other emerging technologies: 1) high definition maps, big data, and high performance computing; 2) augmented reality( AR ) / virtual reality ( VR ) enhanced simulation platform; and 3) 5G communication for connected AVs. This paper is expected to offer a quick reference for researchers interested in understanding the use of AI in AV research.,
    publicationDate: 2020-02-27,
    authors: ['Yifang Ma', 'Zhenyu Wang', 'Hong Yang', 'Lin Yang'],
    score: 162.04858446218788
},
{
    title: Communication-Efficient Edge AI: Algorithms and Systems,
    abstract: Artificial intelligence (AI) has achieved remarkable breakthroughs in a wide range of fields, ranging from speech processing, image classification to drug discovery. This is driven by the explosive growth of data, advances in machine learning (especially deep learning), and the easy access to powerful computing resources. Particularly, the wide scale deployment of edge devices (e.g., IoT devices) generates an unprecedented scale of data, which provides the opportunity to derive accurate models and develop various intelligent applications at the network edge. However, such enormous data cannot all be sent to the cloud for processing, due to the varying channel quality, traffic congestion and/or privacy concerns, and the enormous energy consumption. By pushing inference and training processes of AI models to edge nodes, edge AI has emerged as a promising alternative. AI at the edge requires close cooperation among edge devices, such as smart phones and smart vehicles, and edge servers at the wireless access points and base stations, which however result in heavy communication overheads. In this paper, we present a comprehensive survey of the recent developments in various techniques for overcoming these communication challenges. Specifically, we first identify key communication challenges in edge AI systems. We then introduce communication-efficient techniques, from both algorithmic and system perspectives for training and inference tasks at the network edge. Potential future research directions are also highlighted.,
    publicationDate: 2020-02-22,
    authors: ['Yuanming Shi', 'Kai Yang', 'Tao Jiang', 'Jun Zhang', 'K. Letaief'],
    score: 161.95149674460362
},
{
    title: What do we need to build explainable AI systems for the medical domain?,
    abstract: Artificial intelligence (AI) generally and machine learning (ML) specifically demonstrate impressive practical success in many different application domains, e.g. in autonomous driving, speech recognition, or recommender systems. Deep learning approaches, trained on extremely large data sets or using reinforcement learning methods have even exceeded human performance in visual tasks, particularly on playing games such as Atari, or mastering the game of Go. Even in the medical domain there are remarkable results. The central problem of such models is that they are regarded as black-box models and even if we understand the underlying mathematical principles, they lack an explicit declarative knowledge representation, hence have difficulty in generating the underlying explanatory structures. This calls for systems enabling to make decisions transparent, understandable and explainable. A huge motivation for our approach are rising legal and privacy aspects. The new European General Data Protection Regulation entering into force on May 25th 2018, will make black-box approaches difficult to use in business. This does not imply a ban on automatic learning approaches or an obligation to explain everything all the time, however, there must be a possibility to make the results re-traceable on demand. In this paper we outline some of our research topics in the context of the relatively new area of explainable-AI with a focus on the application in medicine, which is a very special domain. This is due to the fact that medical professionals are working mostly with distributed heterogeneous and complex sources of data. In this paper we concentrate on three sources: images, *omics data and text. We argue that research in explainable-AI would generally help to facilitate the implementation of AI/ML in the medical domain, and specifically help to facilitate transparency and trust.,
    publicationDate: 2017-12-28,
    authors: ['Andreas Holzinger', 'Chris Biemann', 'C. Pattichis', 'D. Kell'],
    score: 161.85154483358247
},
{
    title: Can we open the black box of AI?,
    abstract: None,
    publicationDate: 2016-10-05,
    authors: ['D. Castelvecchi'],
    score: 161.58679194221907
},
{
    title: Explaining Explanations in AI,
    abstract: Recent work on interpretability in machine learning and AI has focused on the building of simplified models that approximate the true criteria used to make decisions. These models are a useful pedagogical device for teaching trained professionals how to predict what decisions will be made by the complex system, and most importantly how the system might break. However, when considering any such model it's important to remember Box's maxim that "All models are wrong but some are useful." We focus on the distinction between these models and explanations in philosophy and sociology. These models can be understood as a "do it yourself kit" for explanations, allowing a practitioner to directly answer "what if questions" or generate contrastive explanations without external assistance. Although a valuable ability, giving these models as explanations appears more difficult than necessary, and other forms of explanation may not have the same trade-offs. We contrast the different schools of thought on what makes an explanation, and suggest that machine learning might benefit from viewing the problem more broadly.,
    publicationDate: 2018-11-04,
    authors: ['B. Mittelstadt', 'Chris Russell', 'Sandra Wachter'],
    score: 161.44579217058708
},
{
    title: Aligning AI With Shared Human Values,
    abstract: We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete understanding of basic ethical knowledge. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.,
    publicationDate: 2020-08-05,
    authors: ['Dan Hendrycks', 'Collin Burns', 'Steven Basart', 'Andrew Critch', 'J. Li', 'D. Song', 'J. Steinhardt'],
    score: 161.43736843648338
},
{
    title: Hypothalamic-pituitary-adrenal activity in aged, cognitively impaired and cognitively unimpaired rats,
    abstract: There is a tendency for increased hypothalamic-pituitary-adrenal (HPA) activity with age in the rat, and the resulting elevations in circulating glucocorticoid levels have been implicated in the occurrence of hippocampal pathology and memory deficits. In the experiments reported here, we examined whether HPA dysfunction is selectively associated with cognitive impairments in a population of aged rats. Fifty-eight 23-27-month-old male Long-Evans rats were screened for spatial memory impairments using the Morris swim maze, and 2 groups of aged animals were selected; aged, cognitively impaired (AI) animals whose performance was significantly different (greater than 2 SD) from that of 6-month-old controls and aged, cognitively unimpaired (AU) animals whose performance was comparable to that of the young controls (a difference of less than 0.5 SD). Twenty-eight percent of the animals tested were designated as AI and 20% as AU. Histological analysis of a subset of these animals showed that, while both AU and AI animals showed neuron loss in the pyramidal cell fields of the hippocampus, the loss was significantly greater in the AI animals. The AI animals showed clear evidence of increased HPA activity. Thus, basal ACTH and corticosterone levels were significantly higher in the AI animals compared with both AU animals and young controls, especially during the dark phase of the cycle. The AI, AU, and young animals exhibited comparable corticosterone levels during a 20-min immobilization stress; however, following the termination of the stressor, corticosterone levels in AI animals were significantly elevated compared with both AU animals and controls.(ABSTRACT TRUNCATED AT 250 WORDS),
    publicationDate: 1990-10-01,
    authors: ['Amalia M Lssa', 'Wayne Rowe', 'Serge Gauthier', 'Michael J Meaneyl', 'M. J. Meaney'],
    score: 161.30162090368458
},
{
    title: Governing artificial intelligence: ethical, legal and technical opportunities and challenges,
    abstract: This paper is the introduction to the special issue entitled: ‘Governing artificial intelligence: ethical, legal and technical opportunities and challenges'. Artificial intelligence (AI) increasingly permeates every aspect of our society, from the critical, like urban infrastructure, law enforcement, banking, healthcare and humanitarian aid, to the mundane like dating. AI, including embodied AI in robotics and techniques like machine learning, can improve economic, social welfare and the exercise of human rights. Owing to the proliferation of AI in high-risk areas, the pressure is mounting to design and govern AI to be accountable, fair and transparent. How can this be achieved and through which frameworks? This is one of the central questions addressed in this special issue, in which eight authors present in-depth analyses of the ethical, legal-regulatory and technical challenges posed by developing governance regimes for AI systems. It also gives a brief overview of recent developments in AI governance, how much of the agenda for defining AI regulation, ethical frameworks and technical approaches is set, as well as providing some concrete suggestions to further the debate on AI governance. This article is part of the theme issue ‘Governing artificial intelligence: ethical, legal, and technical opportunities and challenges’.,
    publicationDate: 2018-10-15,
    authors: ['Corinne Cath'],
    score: 161.14504781714226
},
{
    title: Generating Project Networks,
    abstract: Procedures for optimization and resource allocation in Operations Research first require a project network for the task to be specified. The specification of a project network is at present done in an intuitive way. AI work in plan formation has developed formalisms for specifying primitive activities, and recent work by Sacerdoti (1975a) has developed a planner able to generate a plan as a partially ordered network of actions. The "planning: a joint AI/OR approach" project at Edinburgh has extended such work and provided a hierarchic planner which can aid in the generation of project networks. This paper describes the planner (NONLIN) and the Task Formalism (TF) used to hierarchically specify a domain.,
    publicationDate: 1977-08-22,
    authors: ['A. Tate'],
    score: 161.07228741893937
},
{
    title: When Will AI Exceed Human Performance? Evidence from AI Experts,
    abstract: Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military. To adapt public policy, we need to better anticipate these advances. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50% chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI.,
    publicationDate: 2017-05-24,
    authors: ['Katja Grace', 'J. Salvatier', 'A. Dafoe', 'Baobao Zhang', 'Owain Evans'],
    score: 160.9539448282422
},
{
    title: Actions and Events in Interval Temporal Logic,
    abstract: We present a representation of events and action based on interval temporal logic that is significantly more expressive and more natural than most previous AI approaches. The representation is motivated by work in natural language semantics and discourse, temporal logic, and AI planning and plan recognition. The formal basis of the representation is presented in detail, from the axiomatization of time periods to the relationship between actions and events and their effects. The power of the representation is illustrated by applying it to the axiomatization and solution of several standard problems from the AI literature on action and change. An approach to the frame problem based on explanation closure is shown to be both powerful and natural when combined with our representational framework. We also discuss features of the logic that are beyond the scope of many traditional representations, and describe our approach to difficult problems such as external events and simultaneous actions.,
    publicationDate: 1994-07-01,
    authors: ['James F. Allen', 'G. Ferguson'],
    score: 160.94736105734214
},
{
    title: Trustworthy AI: From Principles to Practices,
    abstract: The rapid development of Artificial Intelligence (AI) technology has enabled the deployment of various systems based on it. However, many current AI systems are found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection. These shortcomings degrade user experience and erode people’s trust in all AI systems. In this review, we provide AI practitioners with a comprehensive guide for building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, and accountability. To unify currently available but fragmented approaches toward trustworthy AI, we organize them in a systematic approach that considers the entire lifecycle of AI systems, ranging from data acquisition to model development, to system development and deployment, finally to continuous monitoring and governance. In this framework, we offer concrete action items for practitioners and societal stakeholders (e.g., researchers, engineers, and regulators) to improve AI trustworthiness. Finally, we identify key opportunities and challenges for the future development of trustworthy AI systems, where we identify the need for a paradigm shift toward comprehensively trustworthy AI systems.,
    publicationDate: 2021-10-04,
    authors: ['Bo Li', 'Peng Qi', 'Bo Liu', 'Shuai Di', 'Jingen Liu', 'Jiquan Pei', 'Jinfeng Yi', 'Bowen Zhou'],
    score: 160.9444072020392
},
{
    title: Consumers and Artificial Intelligence: An Experiential Perspective,
    abstract: Artificial intelligence (AI) helps companies offer important benefits to consumers, such as health monitoring with wearable devices, advice with recommender systems, peace of mind with smart household products, and convenience with voice-activated virtual assistants. However, although AI can be seen as a neutral tool to be evaluated on efficiency and accuracy, this approach does not consider the social and individual challenges that can occur when AI is deployed. This research aims to bridge these two perspectives: on one side, the authors acknowledge the value that embedding AI technology into products and services can provide to consumers. On the other side, the authors build on and integrate sociological and psychological scholarship to examine some of the costs consumers experience in their interactions with AI. In doing so, the authors identify four types of consumer experiences with AI: (1) data capture, (2) classification, (3) delegation, and (4) social. This approach allows the authors to discuss policy and managerial avenues to address the ways in which consumers may fail to experience value in organizations’ investments into AI and to lay out an agenda for future research.,
    publicationDate: 2020-10-16,
    authors: ['S. Puntoni', 'R. W. Reczek', 'M. Giesler', 'Simona Botti'],
    score: 160.92185378392026
},
{
    title: Art and the science of generative AI,
    abstract: Understanding shifts in creative work will help guide AI’s impact on the media ecosystem The capabilities of a new class of tools, colloquially known as generative artificial intelligence (AI), is a topic of much debate. One prominent application thus far is the production of high-quality artistic media for visual arts, concept art, music, and literature, as well as video and animation. For example, diffusion models can synthesize high-quality images (1), and large language models (LLMs) can produce sensible-sounding and impressive prose and verse in a wide range of contexts (2). The generative capabilities of these tools are likely to fundamentally alter the creative processes by which creators formulate ideas and put them into production. As creativity is reimagined, so too may be many sectors of society. Understanding the impact of generative AI—and making policy decisions around it—requires new interdisciplinary scientific inquiry into culture, economics, law, algorithms, and the interaction of technology and creativity.,
    publicationDate: 2023-06-07,
    authors: ['Ziv Epstein', 'Aaron Hertzmann', 'L. Herman', 'Robert Mahari', 'M. Frank', 'Matthew Groh', 'Hope Schroeder', 'Amy Smith', 'Memo Akten', 'Jessica Fjeld', 'H. Farid', 'Neil Leach', 'A. Pentland', 'Olga Russakovsky'],
    score: 160.6984603031069
},
{
    title: Artificial intelligence in oncology,
    abstract: Artificial intelligence (AI) has contributed substantially to the resolution of a variety of biomedical problems, including cancer, over the past decade. Deep learning, a subfield of AI that is highly flexible and supports automatic feature extraction, is increasingly being applied in various areas of both basic and clinical cancer research. In this review, we describe numerous recent examples of the application of AI in oncology, including cases in which deep learning has efficiently solved problems that were previously thought to be unsolvable, and we address obstacles that must be overcome before such application can become more widespread. We also highlight resources and datasets that can help harness the power of AI for cancer research. The development of innovative approaches to and applications of AI will yield important insights in oncology in the coming decade.,
    publicationDate: 2020-03-04,
    authors: ['Hideyuki Shimizu', 'K. Nakayama'],
    score: 160.6984603031069
},
{
    title: Recent developments of induction motor drives fault diagnosis using AI techniques,
    abstract: The paper presents a review, of the developments in the field of diagnosis of electrical machines and drives based on artificial intelligence (AI). This review covers the application of expert systems, neural networks and fuzzy logic systems that can be integrated into each other and also with more traditional techniques to overcome specific problems. Usually a diagnostic procedure starts from a fault tree developed on the basis of the physical behaviour of the electrical system under consideration. In this phase the knowledge of well tested models able to simulate the electrical machine in different fault renditions is fundamental to obtain the patterns characterising the faults. Then the fault tree navigation performed by an expert system inference engine leads to the choice of suitable diagnostic indexes, referred to a particular fault, and relevant to build an input dataset for specific AI (neural networks, fuzzy logic or neuro-fuzzy) systems. The discussed methodologies, that play a general role in the diagnostic field, are applied to an induction machine, utilising as input signals the instantaneous voltages and currents. In addition, the supply converter is also considered to also incorporate in the diagnostic procedure the most typical failures of power electronic components. A brief description of the various techniques is provided, to highlight the advantages and the validity limits of using AI technologies. Some application examples are discussed and areas for future research are also indicated.,
    publicationDate: 1998-08-31,
    authors: ['F. Filippetti', 'G. Franceschini', 'C. Tassoni', 'P. Vas'],
    score: 160.67602213729373
},
{
    title: Overview of artificial intelligence in medicine,
    abstract: Background: Artificial intelligence (AI) is the term used to describe the use of computers and technology to simulate intelligent behavior and critical thinking comparable to a human being. John McCarthy first described the term AI in 1956 as the science and engineering of making intelligent machines. Objective: This descriptive article gives a broad overview of AI in medicine, dealing with the terms and concepts as well as the current and future applications of AI. It aims to develop knowledge and familiarity of AI among primary care physicians. Materials and Methods: PubMed and Google searches were performed using the key words 'artificial intelligence'. Further references were obtained by cross-referencing the key articles. Results: Recent advances in AI technology and its current applications in the field of medicine have been discussed in detail. Conclusions: AI promises to change the practice of medicine in hitherto unknown ways, but many of its practical applications are still in their infancy and need to be explored and developed better. Medical professionals also need to understand and acclimatize themselves with these advances for better healthcare delivery to the masses.,
    publicationDate: 2019-07-01,
    authors: ['Amisha', 'Paras Malik', 'Monika Pathania', 'V. Rathaur'],
    score: 160.6509042184794
},
{
    title: Algorithms for Constraint-Satisfaction Problems: A Survey,
    abstract: A large number of problems in AI and other areas of computer science can be viewed as special cases of the constraint-satisfaction problem. Some examples are machine vision, belief maintenance, scheduling, temporal reasoning, graph problems, floor plan design, the planning of genetic experiments, and the satisfiability problem. A number of different approaches have been developed for solving these problems. Some of them use constraint propagation to simplify the original problem. Others use backtracking to directly search for possible solutions. Some are a combination of these two techniques. This article overviews many of these approaches in a tutorial fashion.,
    publicationDate: 1992-04-01,
    authors: ['Vipin Kumar'],
    score: 160.59490524793114
},
{
    title: Protocol for development of a reporting guideline (TRIPOD-AI) and risk of bias tool (PROBAST-AI) for diagnostic and prognostic prediction model studies based on artificial intelligence,
    abstract: Introduction The Transparent Reporting of a multivariable prediction model of Individual Prognosis Or Diagnosis (TRIPOD) statement and the Prediction model Risk Of Bias ASsessment Tool (PROBAST) were both published to improve the reporting and critical appraisal of prediction model studies for diagnosis and prognosis. This paper describes the processes and methods that will be used to develop an extension to the TRIPOD statement (TRIPOD-artificial intelligence, AI) and the PROBAST (PROBAST-AI) tool for prediction model studies that applied machine learning techniques. Methods and analysis TRIPOD-AI and PROBAST-AI will be developed following published guidance from the EQUATOR Network, and will comprise five stages. Stage 1 will comprise two systematic reviews (across all medical fields and specifically in oncology) to examine the quality of reporting in published machine-learning-based prediction model studies. In stage 2, we will consult a diverse group of key stakeholders using a Delphi process to identify items to be considered for inclusion in TRIPOD-AI and PROBAST-AI. Stage 3 will be virtual consensus meetings to consolidate and prioritise key items to be included in TRIPOD-AI and PROBAST-AI. Stage 4 will involve developing the TRIPOD-AI checklist and the PROBAST-AI tool, and writing the accompanying explanation and elaboration papers. In the final stage, stage 5, we will disseminate TRIPOD-AI and PROBAST-AI via journals, conferences, blogs, websites (including TRIPOD, PROBAST and EQUATOR Network) and social media. TRIPOD-AI will provide researchers working on prediction model studies based on machine learning with a reporting guideline that can help them report key details that readers need to evaluate the study quality and interpret its findings, potentially reducing research waste. We anticipate PROBAST-AI will help researchers, clinicians, systematic reviewers and policymakers critically appraise the design, conduct and analysis of machine learning based prediction model studies, with a robust standardised tool for bias evaluation. Ethics and dissemination Ethical approval has been granted by the Central University Research Ethics Committee, University of Oxford on 10-December-2020 (R73034/RE001). Findings from this study will be disseminated through peer-review publications. PROSPERO registration number CRD42019140361 and CRD42019161764.,
    publicationDate: 2021-07-01,
    authors: ['G. Collins', 'P. Dhiman', 'Constanza L. Andaur Navarro', 'Jie Ma', 'L. Hooft', 'J. Reitsma', 'P. Logullo', 'Andrew Beam', 'L. Peng', 'B. Van calster', 'M. van Smeden', 'R. Riley', 'K. Moons'],
    score: 160.53222148787134
},
{
    title: How AI can be a force for good,
    abstract: An ethical framework will help to harness the potential of AI while keeping humans in control Artificial intelligence (AI) is not just a new technology that requires regulation. It is a powerful force that is reshaping daily practices, personal and professional interactions, and environments. For the well-being of humanity it is crucial that this power is used as a force of good. Ethics plays a key role in this process by ensuring that regulations of AI harness its potential while mitigating its risks.,
    publicationDate: 2018-08-24,
    authors: ['M. Taddeo', 'L. Floridi'],
    score: 160.53222148787134
},
{
    title: Artificial intelligence, bias and clinical safety,
    abstract: In medicine, artificial intelligence (AI) research is becoming increasingly focused on applying machine learning (ML) techniques to complex problems, and so allowing computers to make predictions from large amounts of patient data, by learning their own associations.1 Estimates of the impact of AI on the wider economy globally vary wildly, with a recent report suggesting a 14% effect on global gross domestic product by 2030, half of which coming from productivity improvements.2 These predictions create political appetite for the rapid development of the AI industry,3 and healthcare is a priority area where this technology has yet to be exploited.2 3 The digital health revolution described by Duggal et al 4 is already in full swing with the potential to ‘disrupt’ healthcare. Health AI research has demonstrated some impressive results,5–10 but its clinical value has not yet been realised, hindered partly by a lack of a clear understanding of how to quantify benefit or ensure patient safety, and increasing concerns about the ethical and medico-legal impact.11 

This analysis is written with the dual aim of helping clinical safety professionals to critically appraise current medical AI research from a quality and safety perspective, and supporting research and development in AI by highlighting some of the clinical safety questions that must be considered if medical application of these exciting technologies is to be successful.

Clinical decision support systems (DSS) are in widespread use in medicine and have had most impact providing guidance on the safe prescription of medicines,12 guideline adherence, simple risk screening13 or prognostic scoring.14 These systems use predefined rules, which have predictable behaviour and are usually shown to reduce clinical error,12 although sometimes inadvertently introduce safety issues themselves.15 16 Rules-based systems have also been developed to address diagnostic uncertainty17–19 …,
    publicationDate: 2019-01-12,
    authors: ['R. Challen', 'J. Denny', 'M. Pitt', 'Luke Gompels', 'Tom Edwards', 'K. Tsaneva-Atanasova'],
    score: 160.5228077952574
},
{
    title: Effect of time of artificial insemination on pregnancy rates, calving rates, pregnancy loss, and gender ratio after synchronization of ovulation in lactating dairy cows.,
    abstract: In order to assess the optimal time of artificial insemination (AI) in relation to ovulation, lactating dairy cows (n = 732) from herds with rolling herd averages of 9980 to 11,800 kg from three milkings per day were randomly assigned to five groups by stage of lactation and parity. Ovulation was synchronized by administration of GnRH followed 7 d later with PGF2 alpha followed 2 d later with a second treatment with GnRH. Cows were inseminated at 0, 8, 16, 24, or 32 h after the second injection of GnRH (ovulation occurs between 24 and 32 h after GnRH). Pregnancy diagnoses were performed by ultrasound at 25 to 35 d post-AI. Pregnancy rates per AI were similar for the groups inseminated at 0, 8, 16, and 24 h and lower for the group inseminated at 32 h. A significant quadratic effect of treatment suggests that the middle time periods (8, 16, and 24 h) may produce the greatest pregnancy rate per AI. However, the group inseminated at 0 h had lowest pregnancy loss, and the group inseminated at 32 h tended to have the greatest pregnancy loss compared with that of the other groups. The calving rate was similar between the groups inseminated at 0, 8, 16, and 24 h and lower in the group inseminated at 32 h. The time of AI also appeared to affect gender of calf: cows bred at 0 and 32 h having a higher percentage of female offspring. In conclusion, there appears to be substantial flexibility in the time of AI after the second injection of GnRH, and lower reproductive rates were observed only when AI was after the time of ovulation.,
    publicationDate: 1998-08-01,
    authors: ['J. R. Pursley', 'R. Silcox', 'M. Wiltbank'],
    score: 160.49629332698203
},
