[
  {
    "paperId": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22",
    "url": "https://www.semanticscholar.org/paper/58ed1fbaabe027345f7bb3a6312d41c5aac63e22",
    "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
    "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "citationCount": 4275,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2020-05-22",
    "authors": [
      {
        "authorId": "145222654",
        "name": "Patrick Lewis"
      },
      {
        "authorId": "3439053",
        "name": "Ethan Perez"
      },
      {
        "authorId": "1716179427",
        "name": "Aleksandara Piktus"
      },
      {
        "authorId": "40052301",
        "name": "F. Petroni"
      },
      {
        "authorId": "2067091563",
        "name": "Vladimir Karpukhin"
      },
      {
        "authorId": "39589154",
        "name": "Naman Goyal"
      },
      {
        "authorId": "103131985",
        "name": "Heinrich Kuttler"
      },
      {
        "authorId": "35084211",
        "name": "M. Lewis"
      },
      {
        "authorId": "144105277",
        "name": "Wen-tau Yih"
      },
      {
        "authorId": "2620211",
        "name": "Tim Rocktäschel"
      },
      {
        "authorId": "48662861",
        "name": "Sebastian Riedel"
      },
      {
        "authorId": "1743722",
        "name": "Douwe Kiela"
      }
    ],
    "source": "semantic_scholar",
    "score": 202.41159908217404
  },
  {
    "paperId": "a81a09b2a4ce36ae5c847fc4e3558c523d301179",
    "url": "https://www.semanticscholar.org/paper/a81a09b2a4ce36ae5c847fc4e3558c523d301179",
    "title": "Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks",
    "abstract": "Retrieval-augmented generation models have shown state-of-the-art performance across many knowledge-intensive NLP tasks such as open-domain question answering and fact verification. These models are trained to generate a final output given retrieved passages that can be irrelevant to an input query, leading to learning spurious cues or memorization. This work introduces a method to incorporate evidentiality of passages—whether a passage contains correct evidence to support the output—into training the generator. We introduce a multi-task learning framework to jointly generate the final output and predict the evidentiality of each passage. Furthermore, we introduce a new task-agnostic method for obtaining high-quality silver evidentiality labels, addressing the issues of gold evidentiality labels being unavailable in most domains. Our experiments on five datasets across three knowledge-intensive tasks show that our new evidentiality-guided generator significantly outperforms its direct counterpart on all of them, and advances the state of the art on three of them. Our analysis shows that multi-task learning and silver evidentiality mining play key roles. Our code is available at https://github.com/AkariAsai/evidentiality_qa",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2021,
    "citationCount": 45,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2022.naacl-main.162.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-12-16",
    "authors": [
      {
        "authorId": "35584853",
        "name": "Akari Asai"
      },
      {
        "authorId": "40642935",
        "name": "Matt Gardner"
      },
      {
        "authorId": "2548384",
        "name": "Hannaneh Hajishirzi"
      }
    ],
    "source": "semantic_scholar",
    "score": 127.42962094733642
  },
  {
    "paperId": "b565394952065c37345fb75fb66e84709b6402a3",
    "url": "https://www.semanticscholar.org/paper/b565394952065c37345fb75fb66e84709b6402a3",
    "title": "Domain-Specific Retrieval-Augmented Generation Using Vector Stores, Knowledge Graphs, and Tensor Factorization",
    "abstract": "Large Language Models (LLMs) are pre-trained on large-scale corpora and excel in numerous general natural language processing (NLP) tasks, such as question answering (QA). Despite their advanced language capabilities, when it comes to domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations, knowledge cut-offs, and lack of knowledge attributions. Additionally, fine tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and time consuming process. The retrieval-augmented generation (RAG) process has recently emerged as a method capable of optimization of LLM responses, by referencing them to a predetermined ontology. It was shown that using a Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into account relevant sub-graphs that preserve the information in a structured manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM framework, that integrates RAG with KG and a vector store (VS) that store factual domain specific information. Importantly, to avoid hallucinations in the KG, we build these highly domain-specific KGs and VSs without the use of LLMs, but via NLP, data mining, and nonnegative tensor factorization with automatic model selection. Pairing our RAG with a domain-specific: (i) KG (containing structured information), and (ii) VS (containing unstructured information) enables the development of domain-specific chat-bots that attribute the source of information, mitigate hallucinations, lessen the need for fine-tuning, and excel in highly domain-specific question answering tasks. We pair SMART-SLIC with chain-of-thought prompting agents. The framework is designed to be generalizable to adapt to any specific or specialized domain. In this paper, we demonstrate the question answering capabilities of our framework on a corpus of scientific publications on malware analysis and anomaly detection.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-03",
    "authors": [
      {
        "authorId": "2237801926",
        "name": "Ryan Barron"
      },
      {
        "authorId": "2324053974",
        "name": "Ves Grantcharov"
      },
      {
        "authorId": "2290016888",
        "name": "Selma Wanna"
      },
      {
        "authorId": "1712221229",
        "name": "M. Eren"
      },
      {
        "authorId": "2267890820",
        "name": "Manish Bhattarai"
      },
      {
        "authorId": "1917400194",
        "name": "N. Solovyev"
      },
      {
        "authorId": "2324053187",
        "name": "George Tompkins"
      },
      {
        "authorId": "2237800893",
        "name": "Charles Nicholas"
      },
      {
        "authorId": "2237799040",
        "name": "Kim Ø. Rasmussen"
      },
      {
        "authorId": "2127879703",
        "name": "Cynthia Matuszek"
      },
      {
        "authorId": "2025666",
        "name": "B. Alexandrov"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "f99fac868db0cfa8906ec4094f403c1e3c296481",
    "url": "https://www.semanticscholar.org/paper/f99fac868db0cfa8906ec4094f403c1e3c296481",
    "title": "A Quick, trustworthy spectral knowledge Q&A system leveraging retrieval-augmented generation on LLM",
    "abstract": "Large Language Model (LLM) has demonstrated significant success in a range of natural language processing (NLP) tasks within general domain. The emergence of LLM has introduced innovative methodologies across diverse fields, including the natural sciences. Researchers aim to implement automated, concurrent process driven by LLM to supplant conventional manual, repetitive and labor-intensive work. In the domain of spectral analysis and detection, it is imperative for researchers to autonomously acquire pertinent knowledge across various research objects, which encompasses the spectroscopic techniques and the chemometric methods that are employed in experiments and analysis. Paradoxically, despite the recognition of spectroscopic detection as an effective analytical method, the fundamental process of knowledge retrieval remains both time-intensive and repetitive. In response to this challenge, we first introduced the Spectral Detection and Analysis Based Paper(SDAAP) dataset, which is the first open-source textual knowledge dataset for spectral analysis and detection and contains annotated literature data as well as corresponding knowledge instruction data. Subsequently, we also designed an automated Q\\&A framework based on the SDAAP dataset, which can retrieve relevant knowledge and generate high-quality responses by extracting entities in the input as retrieval parameters. It is worth noting that: within this framework, LLM is only used as a tool to provide generalizability, while RAG technique is used to accurately capture the source of the knowledge.This approach not only improves the quality of the generated responses, but also ensures the traceability of the knowledge. Experimental results show that our framework generates responses with more reliable expertise compared to the baseline.",
    "venue": "",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-08-21",
    "authors": [
      {
        "authorId": "2302210536",
        "name": "Jiheng Liang"
      },
      {
        "authorId": "2112220957",
        "name": "Ziru Yu"
      },
      {
        "authorId": "2316587337",
        "name": "Zujie Xie"
      },
      {
        "authorId": "2316670665",
        "name": "Xiangyang Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "fcefbb496545d2174270f27183cefa14f2830892",
    "url": "https://www.semanticscholar.org/paper/fcefbb496545d2174270f27183cefa14f2830892",
    "title": "Retrieval-augmented Generation across Heterogeneous Knowledge",
    "abstract": "Retrieval-augmented generation (RAG) methods have been receiving increasing attention from the NLP community and achieved state-of-the-art performance on many NLP downstream tasks. Compared with conventional pre-trained generation models, RAG methods have remarkable advantages such as easy knowledge acquisition, strong scalability, and low training cost. Although existing RAG models have been applied to various knowledge-intensive NLP tasks, such as open-domain QA and dialogue systems, most of the work has focused on retrieving unstructured text documents from Wikipedia. In this paper, I first elaborate on the current obstacles to retrieving knowledge from a single-source homogeneous corpus. Then, I demonstrate evidence from both existing literature and my experiments, and provide multiple solutions on retrieval-augmented generation methods across heterogeneous knowledge.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2022,
    "citationCount": 28,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2022.naacl-srw.7.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "38767143",
        "name": "W. Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 120.5094374497971
  },
  {
    "paperId": "1b7492a4b813052146300fa8c03325c4ad7a0a25",
    "url": "https://www.semanticscholar.org/paper/1b7492a4b813052146300fa8c03325c4ad7a0a25",
    "title": "RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing",
    "abstract": "Large Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge. To mitigate these, recent methodologies have integrated information retrieved from external resources with LLMs, substantially enhancing their performance across NLP tasks. This survey paper addresses the absence of a comprehensive overview on Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an in-depth examination of their paradigm, evolution, taxonomy, and applications. The paper discusses the essential components of RALMs, including Retrievers, Language Models, and Augmentations, and how their interactions lead to diverse model structures and applications. RALMs demonstrate utility in a spectrum of tasks, from translation and dialogue systems to knowledge-intensive applications. The survey includes several evaluation methods of RALMs, emphasizing the importance of robustness, accuracy, and relevance in their assessment. It also acknowledges the limitations of RALMs, particularly in retrieval quality and computational efficiency, offering directions for future research. In conclusion, this survey aims to offer a structured insight into RALMs, their potential, and the avenues for their future development in NLP. The paper is supplemented with a Github Repository containing the surveyed works and resources for further study: https://github.com/2471023025/RALM_Survey.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-04-30",
    "authors": [
      {
        "authorId": "2299263700",
        "name": "Yucheng Hu"
      },
      {
        "authorId": "2220675861",
        "name": "Yuxing Lu"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "a8b66565cdb2b8c90556bb98a7fc58ac679c2cec",
    "url": "https://www.semanticscholar.org/paper/a8b66565cdb2b8c90556bb98a7fc58ac679c2cec",
    "title": "Accelerating Retrieval-Augmented Language Model Serving with Speculation",
    "abstract": "Retrieval-augmented language models (RaLM) have demonstrated the potential to solve knowledge-intensive natural language processing (NLP) tasks by combining a non-parametric knowledge base with a parametric language model. Instead of fine-tuning a fully parametric model, RaLM excels at its low-cost adaptation to the latest data and better source attribution mechanisms. Among various RaLM approaches, iterative RaLM delivers a better generation quality due to a more frequent interaction between the retriever and the language model. Despite the benefits, iterative RaLM usually encounters high overheads due to the frequent retrieval step. To this end, we propose RaLMSpec, a speculation-inspired framework that provides generic speed-up over iterative RaLM while preserving the same model outputs through speculative retrieval and batched verification. By further incorporating prefetching, optimal speculation stride scheduler, and asynchronous verification, RaLMSpec can automatically exploit the acceleration potential to the fullest. For naive iterative RaLM serving, extensive evaluations over three language models on four downstream QA datasets demonstrate that RaLMSpec can achieve a speed-up ratio of 1.75-2.39x, 1.04-1.39x, and 1.31-1.77x when the retriever is an exact dense retriever, approximate dense retriever, and sparse retriever respectively compared with the baseline. For KNN-LM serving, RaLMSpec can achieve a speed-up ratio up to 7.59x and 2.45x when the retriever is an exact dense retriever and approximate dense retriever, respectively, compared with the baseline.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-25",
    "authors": [
      {
        "authorId": "2185953295",
        "name": "Zhihao Zhang"
      },
      {
        "authorId": "2231613073",
        "name": "Alan Zhu"
      },
      {
        "authorId": "2231664587",
        "name": "Lijie Yang"
      },
      {
        "authorId": "2281208130",
        "name": "Yihua Xu"
      },
      {
        "authorId": "2281325214",
        "name": "Lanting Li"
      },
      {
        "authorId": "2623548",
        "name": "P. Phothilimthana"
      },
      {
        "authorId": "2271058444",
        "name": "Zhihao Jia"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "edeb9ec83948942b6e2b4f88ce380ff148bfbad9",
    "url": "https://www.semanticscholar.org/paper/edeb9ec83948942b6e2b4f88ce380ff148bfbad9",
    "title": "REAL: A Retrieval-Augmented Entity Linking Approach for Biomedical Concept Recognition",
    "abstract": "Large Language Models (LLMs) offer an appealing alternative to training dedicated models for many Natural Language Processing (NLP) tasks. However, outdated knowledge and hallucination issues can be major obstacles in their application in knowledge-intensive biomedical scenarios. In this study, we consider the task of biomedical concept recognition (CR) from unstructured scientific literature and explore the use of Retrieval Augmented Generation (RAG) to improve accuracy and reliability of the LLM-based biomedical CR. Our approach, named REAL (Retrieval Augmented Entity Linking), combines the generative capabilities of LLMs with curated knowledge bases to automatically annotate natural language texts with concepts from bio-ontologies. By applying REAL to benchmark corpora on phenotype concept recognition, we show its effectiveness in improving LLM-based CR performance. This research highlights the potential of combining LLMs with external knowledge sources to advance biomedical text processing.",
    "venue": "Workshop on Biomedical Natural Language Processing",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2315312730",
        "name": "Darya Shlyk"
      },
      {
        "authorId": "1778643",
        "name": "T. Groza"
      },
      {
        "authorId": "2312764542",
        "name": "Marco Mesiti"
      },
      {
        "authorId": "2315307503",
        "name": "Stefano Montanelli"
      },
      {
        "authorId": "2221100434",
        "name": "Emanuele Cavalleri"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "ed99a2572fb5f4240aa6068e3bf274832e831306",
    "url": "https://www.semanticscholar.org/paper/ed99a2572fb5f4240aa6068e3bf274832e831306",
    "title": "Recitation-Augmented Language Models",
    "abstract": "We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrieval-augmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE first recites one or several relevant passages from LLMs' own memory via sampling, and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of \\method~on four pre-trained models (PaLM, UL2, OPT, and Codex) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA). Our code is available at\"https://github.com/Edward-Sun/RECITE\".",
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "citationCount": 56,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2210.01296",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-10-04",
    "authors": [
      {
        "authorId": "48064856",
        "name": "Zhiqing Sun"
      },
      {
        "authorId": "1524732527",
        "name": "Xuezhi Wang"
      },
      {
        "authorId": "144447820",
        "name": "Yi Tay"
      },
      {
        "authorId": "46286308",
        "name": "Yiming Yang"
      },
      {
        "authorId": "65855107",
        "name": "Denny Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 130.64576901751826
  },
  {
    "paperId": "eace69307070ea0db779e345362eb349117f510b",
    "url": "https://www.semanticscholar.org/paper/eace69307070ea0db779e345362eb349117f510b",
    "title": "Knowledge Ply Chat",
    "abstract": "Despite their ability to store information and excel at many NLP tasks with fine-tuning, large language models tend to have issues about accurately accessing and altering knowledge, which leads to performance gaps in knowledge-intensive tasks compared to domain-specific architectures. Additionally, these models face problems when it comes to having transparent decision-making processes or updating their world knowledge. To mitigate these limitations, we propose a Retrieval Augmented Generation (RAG) system by improving the Mistral7B model specifically for RAG tasks. The novel training technique includes Parameter-Efficient Fine-Tuning (PEFT) which enables efficient adaptation of large pre- trained models on-the-fly according to task-specific requirements while reducing computational costs. In addition, this system combines pre-trained embedding models that use pre-trained cross-encoders for effective retrieval and reranking of information. This RAG system will thus leverage these state-of-the-art methodologies towards achieving top performances in a range of NLP tasks such as question answering and summarization.",
    "venue": "International Journal of Innovative Science and Research Technology",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-12",
    "authors": [
      {
        "authorId": "2296531954",
        "name": "M. K. Satya Varma"
      },
      {
        "authorId": "2296687704",
        "name": "Koteswara Rao"
      },
      {
        "authorId": "2296470233",
        "name": "Sai Ganesh"
      },
      {
        "authorId": "2296531757",
        "name": "Venkat Sai Koushik"
      },
      {
        "authorId": "2296531760",
        "name": "Rama Krishnam Raju"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.39720770839918
  },
  {
    "paperId": "c9f9c547fe6d7c66af399d516c879e651518e586",
    "url": "https://www.semanticscholar.org/paper/c9f9c547fe6d7c66af399d516c879e651518e586",
    "title": "TongGu: Mastering Classical Chinese Understanding with Knowledge-Grounded Large Language Models",
    "abstract": "Classical Chinese is a gateway to the rich heritage and wisdom of ancient China, yet its complexities pose formidable comprehension barriers for most modern people without specialized knowledge. While Large Language Models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), they struggle with Classical Chinese Understanding (CCU), especially in data-demanding and knowledge-intensive tasks. In response to this dilemma, we propose \\textbf{TongGu} (mean understanding ancient and modern), the first CCU-specific LLM, underpinned by three core contributions. First, we construct a two-stage instruction-tuning dataset ACCN-INS derived from rich classical Chinese corpora, aiming to unlock the full CCU potential of LLMs. Second, we propose Redundancy-Aware Tuning (RAT) to prevent catastrophic forgetting, enabling TongGu to acquire new capabilities while preserving its foundational knowledge. Third, we present a CCU Retrieval-Augmented Generation (CCU-RAG) technique to reduce hallucinations based on knowledge-grounding. Extensive experiments across 24 diverse CCU tasks validate TongGu's superior ability, underscoring the effectiveness of RAT and CCU-RAG. The model and dataset are available at \\url{https://github.com/SCUT-DLVCLab/TongGu-LLM}.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-04",
    "authors": [
      {
        "authorId": "2257099757",
        "name": "Jiahuan Cao"
      },
      {
        "authorId": "50705920",
        "name": "Dezhi Peng"
      },
      {
        "authorId": "2292351664",
        "name": "Peirong Zhang"
      },
      {
        "authorId": "2215687341",
        "name": "Yongxin Shi"
      },
      {
        "authorId": "2287973950",
        "name": "Yang Liu"
      },
      {
        "authorId": "2053138829",
        "name": "Kai Ding"
      },
      {
        "authorId": "2262197299",
        "name": "Lianwen Jin"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "03cfdde24c6b9837ad8933cb535a2c4a7c0fd971",
    "url": "https://www.semanticscholar.org/paper/03cfdde24c6b9837ad8933cb535a2c4a7c0fd971",
    "title": "HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild",
    "abstract": "Hallucinations pose a significant challenge to the reliability of large language models (LLMs) in critical domains. Recent benchmarks designed to assess LLM hallucinations within conventional NLP tasks, such as knowledge-intensive question answering (QA) and summarization, are insufficient for capturing the complexities of user-LLM interactions in dynamic, real-world settings. To address this gap, we introduce HaluEval-Wild, the first benchmark specifically designed to evaluate LLM hallucinations in the wild. We meticulously collect challenging (adversarially filtered by Alpaca) user queries from ShareGPT, an existing real-world user-LLM interaction datasets, to evaluate the hallucination rates of various LLMs. Upon analyzing the collected queries, we categorize them into five distinct types, which enables a fine-grained analysis of the types of hallucinations LLMs exhibit, and synthesize the reference answers with the powerful GPT-4 model and retrieval-augmented generation (RAG). Our benchmark offers a novel approach towards enhancing our comprehension of and improving LLM reliability in scenarios reflective of real-world interactions. Our benchmark is available at https://github.com/HaluEval-Wild/HaluEval-Wild.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-07",
    "authors": [
      {
        "authorId": "2290238327",
        "name": "Zhiying Zhu"
      },
      {
        "authorId": "48064856",
        "name": "Zhiqing Sun"
      },
      {
        "authorId": "2257099254",
        "name": "Yiming Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "168fdce74327ab2e1001799c02c810f07df84bdd",
    "url": "https://www.semanticscholar.org/paper/168fdce74327ab2e1001799c02c810f07df84bdd",
    "title": "Optimizing RAG with Hybrid Search and Contextual Chunking",
    "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state- of-the-art results when fine-tuned on down- stream NLP tasks. However, their ability to access and precisely manip- ulate knowledge is still limited, and hence on knowledge- intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre- trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. Retrieval-Augmented Generation (RAG) is a prevalent ap- proach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q&A(Question-Answering) systems. However, RAG accu- racy becomes increasingly challenging as the corpus of docu- ments scales up,with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose different ways to optimize the re- treivals, Reciprocal Rank Fusion, Reranking and dynamic chunking schemes.",
    "venue": "Journal of Engineering and Applied Sciences Technology",
    "year": 2023,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-08-31",
    "authors": [
      {
        "authorId": "2318564284",
        "name": "Ashish Bansal"
      }
    ],
    "source": "semantic_scholar",
    "score": 80
  },
  {
    "paperId": "f7fd184eaa573205dff97d86c836f3038143e87a",
    "url": "https://www.semanticscholar.org/paper/f7fd184eaa573205dff97d86c836f3038143e87a",
    "title": "An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks",
    "abstract": "Access to external knowledge is essential for many natural language processing tasks, such as question answering and dialogue. Existing methods often rely on a parametric model that stores knowledge in its parameters, or use a retrieval-augmented model that has access to an external knowledge source. Parametric and retrieval-augmented models have complementary strengths in terms of computational efficiency and predictive accuracy. To combine the strength of both approaches, we propose the Efficient Memory-Augmented Transformer (EMAT) – it encodes external knowledge into a key-value memory and exploits the fast maximum inner product search for memory querying. We also introduce pre-training tasks that allow EMAT to encode informative key-value representations, and to learn an implicit strategy to integrate multiple memory slots into the transformer. Experiments on various knowledge-intensive tasks such as question answering and dialogue datasets show that, simply augmenting parametric models (T5-base) using our method produces more accurate results (e.g., 25.8 → 44.3 EM on NQ) while retaining a high throughput (e.g., 1000 queries/s on NQ). Compared to retrieval-augmented models, EMAT runs substantially faster across the board and produces more accurate results on WoW and ELI5.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "citationCount": 35,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2210.16773",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-10-30",
    "authors": [
      {
        "authorId": null,
        "name": "Yuxiang Wu"
      },
      {
        "authorId": "2155474139",
        "name": "Yu Zhao"
      },
      {
        "authorId": "33968873",
        "name": "Baotian Hu"
      },
      {
        "authorId": "3051815",
        "name": "Pasquale Minervini"
      },
      {
        "authorId": "1918552",
        "name": "Pontus Stenetorp"
      },
      {
        "authorId": "48662861",
        "name": "Sebastian Riedel"
      }
    ],
    "source": "semantic_scholar",
    "score": 123.75278407684165
  },
  {
    "paperId": "46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5",
    "url": "https://www.semanticscholar.org/paper/46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5",
    "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
    "abstract": "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 933,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-12-18",
    "authors": [
      {
        "authorId": "2280046531",
        "name": "Yunfan Gao"
      },
      {
        "authorId": "2275320371",
        "name": "Yun Xiong"
      },
      {
        "authorId": "2275341478",
        "name": "Xinyu Gao"
      },
      {
        "authorId": "2275191447",
        "name": "Kangxiang Jia"
      },
      {
        "authorId": "2275530552",
        "name": "Jinliu Pan"
      },
      {
        "authorId": "2275171009",
        "name": "Yuxi Bi"
      },
      {
        "authorId": "2276187454",
        "name": "Yi Dai"
      },
      {
        "authorId": "2275540959",
        "name": "Jiawei Sun"
      },
      {
        "authorId": "2258800561",
        "name": "Qianyu Guo"
      },
      {
        "authorId": "2291409458",
        "name": "Meng Wang"
      },
      {
        "authorId": "2256769434",
        "name": "Haofen Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 172.59214657343264
  },
  {
    "paperId": "38fcc3667a907d6c94267c674aad114aae68441e",
    "url": "https://www.semanticscholar.org/paper/38fcc3667a907d6c94267c674aad114aae68441e",
    "title": "xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token",
    "abstract": "This paper introduces xRAG, an innovative context compression method tailored for retrieval-augmented generation. xRAG reinterprets document embeddings in dense retrieval--traditionally used solely for retrieval--as features from the retrieval modality. By employing a modality fusion methodology, xRAG seamlessly integrates these embeddings into the language model representation space, effectively eliminating the need for their textual counterparts and achieving an extreme compression rate. In xRAG, the only trainable component is the modality bridge, while both the retriever and the language model remain frozen. This design choice allows for the reuse of offline-constructed document embeddings and preserves the plug-and-play nature of retrieval augmentation. Experimental results demonstrate that xRAG achieves an average improvement of over 10% across six knowledge-intensive tasks, adaptable to various language model backbones, ranging from a dense 7B model to an 8x7B Mixture of Experts configuration. xRAG not only significantly outperforms previous context compression methods but also matches the performance of uncompressed models on several datasets, while reducing overall FLOPs by a factor of 3.53. Our work pioneers new directions in retrieval-augmented generation from the perspective of multimodality fusion, and we hope it lays the foundation for future efficient and scalable retrieval-augmented systems",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-22",
    "authors": [
      {
        "authorId": "2193630544",
        "name": "Xin Cheng"
      },
      {
        "authorId": "2193104542",
        "name": "Xun Wang"
      },
      {
        "authorId": "2284863493",
        "name": "Xingxing Zhang"
      },
      {
        "authorId": "50251691",
        "name": "Tao Ge"
      },
      {
        "authorId": "2263708536",
        "name": "Si-Qing Chen"
      },
      {
        "authorId": "2290016262",
        "name": "Furu Wei"
      },
      {
        "authorId": "2302812779",
        "name": "Huishuai Zhang"
      },
      {
        "authorId": "2302798554",
        "name": "Dongyan Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.49820016084324
  },
  {
    "paperId": "403ff3523a949743290d4ce7b770da28ef3e1953",
    "url": "https://www.semanticscholar.org/paper/403ff3523a949743290d4ce7b770da28ef3e1953",
    "title": "Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering",
    "abstract": "Multi-Hop Question Answering (MHQA) tasks present a significant challenge for large language models (LLMs) due to the intensive knowledge required. Current solutions, like Retrieval-Augmented Generation, typically retrieve potential documents from an external corpus to read an answer. However, the performance of this retrieve-then-read paradigm is constrained by the retriever and the inevitable noise in the retrieved documents. To mitigate these challenges, we introduce a novel generate-then-ground (GenGround) framework, synergizing the parametric knowledge of LLMs and external documents to solve a multi-hop question. GenGround empowers LLMs to alternate two phases until the final answer is derived: (1) formulate a simpler, single-hop question and directly generate the answer; (2) ground the question-answer pair in retrieved documents, amending any wrong predictions in the answer. We also propose an instructional grounding distillation method to generalize our method into smaller models. Extensive experiments conducted on four datasets illustrate the superiority of our method.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-21",
    "authors": [
      {
        "authorId": "2195381022",
        "name": "Zhengliang Shi"
      },
      {
        "authorId": "2307994549",
        "name": "Shuo Zhang"
      },
      {
        "authorId": "2153198380",
        "name": "Weiwei Sun"
      },
      {
        "authorId": "2268683617",
        "name": "Shen Gao"
      },
      {
        "authorId": "1749477",
        "name": "Pengjie Ren"
      },
      {
        "authorId": "1721165",
        "name": "Zhumin Chen"
      },
      {
        "authorId": "2260895127",
        "name": "Zhaochun Ren"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "50ea1f3387e0e1a5f45722fa9d657982c78c2ce8",
    "url": "https://www.semanticscholar.org/paper/50ea1f3387e0e1a5f45722fa9d657982c78c2ce8",
    "title": "CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs for Legal Question Answering",
    "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Model (LLM) output by providing prior knowledge as context to input. This is beneficial for knowledge-intensive and expert reliant tasks, including legal question-answering, which require evidence to validate generated text outputs. We highlight that Case-Based Reasoning (CBR) presents key opportunities to structure retrieval as part of the RAG process in an LLM. We introduce CBR-RAG, where CBR cycle's initial retrieval stage, its indexing vocabulary, and similarity knowledge containers are used to enhance LLM queries with contextually relevant cases. This integration augments the original LLM query, providing a richer prompt. We present an evaluation of CBR-RAG, and examine different representations (i.e. general and domain-specific embeddings) and methods of comparison (i.e. inter, intra and hybrid similarity) on the task of legal question-answering. Our results indicate that the context provided by CBR's case reuse enforces similarity between relevant components of the questions and the evidence base leading to significant improvements in the quality of generated answers.",
    "venue": "International Conference on Case-Based Reasoning",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-04",
    "authors": [
      {
        "authorId": "1784639",
        "name": "N. Wiratunga"
      },
      {
        "authorId": "66680254",
        "name": "Ramitha Abeyratne"
      },
      {
        "authorId": "2288593555",
        "name": "Lasal Jayawardena"
      },
      {
        "authorId": "143999525",
        "name": "Kyle Martin"
      },
      {
        "authorId": "144004005",
        "name": "Stewart Massie"
      },
      {
        "authorId": "1399085102",
        "name": "Ikechukwu Nkisi-Orji"
      },
      {
        "authorId": "2837941",
        "name": "R. Weerasinghe"
      },
      {
        "authorId": "2633228",
        "name": "A. Liret"
      },
      {
        "authorId": "2217276579",
        "name": "Bruno Fleisch"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "03532123ccffae8d411264320e8a5ae2b6eddea0",
    "url": "https://www.semanticscholar.org/paper/03532123ccffae8d411264320e8a5ae2b6eddea0",
    "title": "Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP",
    "abstract": "Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple\"retrieve-then-read\"pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37-120%, 8-39%, and 80-290% relative gains against the vanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively. We release DSP at https://github.com/stanfordnlp/dsp",
    "venue": "arXiv.org",
    "year": 2022,
    "citationCount": 218,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2212.14024",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-12-28",
    "authors": [
      {
        "authorId": "144112155",
        "name": "O. Khattab"
      },
      {
        "authorId": "50818255",
        "name": "Keshav Santhanam"
      },
      {
        "authorId": "32551341",
        "name": "Xiang Lisa Li"
      },
      {
        "authorId": "145385471",
        "name": "David Leo Wright Hall"
      },
      {
        "authorId": "145419642",
        "name": "Percy Liang"
      },
      {
        "authorId": "144922861",
        "name": "Christopher Potts"
      },
      {
        "authorId": "143834867",
        "name": "M. Zaharia"
      }
    ],
    "source": "semantic_scholar",
    "score": 150.8360759472475
  },
  {
    "paperId": "918fb17504fe62438e40c3340669ea53c202be04",
    "url": "https://www.semanticscholar.org/paper/918fb17504fe62438e40c3340669ea53c202be04",
    "title": "DR-RAG: Applying Dynamic Document Relevance to Retrieval-Augmented Generation for Question-Answering",
    "abstract": "Retrieval-Augmented Generation (RAG) has recently demonstrated the performance of Large Language Models (LLMs) in the knowledge-intensive tasks such as Question-Answering (QA). RAG expands the query context by incorporating external knowledge bases to enhance the response accuracy. However, it would be inefficient to access LLMs multiple times for each query and unreliable to retrieve all the relevant documents by a single query. We have found that even though there is low relevance between some critical documents and query, it is possible to retrieve the remaining documents by combining parts of the documents with the query. To mine the relevance, a two-stage retrieval framework called Dynamic-Relevant Retrieval-Augmented Generation (DR-RAG) is proposed to improve document retrieval recall and the accuracy of answers while maintaining efficiency. Additionally, a compact classifier is applied to two different selection strategies to determine the contribution of the retrieved documents to answering the query and retrieve the relatively relevant documents. Meanwhile, DR-RAG call the LLMs only once, which significantly improves the efficiency of the experiment. The experimental results on multi-hop QA datasets show that DR-RAG can significantly improve the accuracy of the answers and achieve new progress in QA systems.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-11",
    "authors": [
      {
        "authorId": "2305682244",
        "name": "Zijian Hei"
      },
      {
        "authorId": "2305742427",
        "name": "Weiling Liu"
      },
      {
        "authorId": "46223131",
        "name": "Wenjie Ou"
      },
      {
        "authorId": "2305681770",
        "name": "Juyi Qiao"
      },
      {
        "authorId": "2305663892",
        "name": "Junming Jiao"
      },
      {
        "authorId": "2305665203",
        "name": "Guowen Song"
      },
      {
        "authorId": "2306947084",
        "name": "Ting Tian"
      },
      {
        "authorId": "2306954967",
        "name": "Yi Lin"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "915539c51252649cf7d09e25dea3af199235ebed",
    "url": "https://www.semanticscholar.org/paper/915539c51252649cf7d09e25dea3af199235ebed",
    "title": "Adaptive Contrastive Decoding in Retrieval-Augmented Generation for Handling Noisy Contexts",
    "abstract": "When using large language models (LLMs) in knowledge-intensive tasks, such as open-domain question answering, external context can bridge the gap between external knowledge and the LLMs' parametric knowledge. Recent research has been developed to amplify contextual knowledge over the parametric knowledge of LLMs with contrastive decoding approaches. While these approaches could yield truthful responses when relevant context is provided, they are prone to vulnerabilities when faced with noisy contexts. We extend the scope of previous studies to encompass noisy contexts and propose adaptive contrastive decoding (ACD) to leverage contextual influence effectively. ACD demonstrates improvements in open-domain question answering tasks compared to baselines, especially in robustness by remaining undistracted by noisy contexts in retrieval-augmented generation.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-08-02",
    "authors": [
      {
        "authorId": "2189657321",
        "name": "Youna Kim"
      },
      {
        "authorId": "2166352356",
        "name": "Hyuhng Joon Kim"
      },
      {
        "authorId": "2284731913",
        "name": "Cheonbok Park"
      },
      {
        "authorId": "2188345697",
        "name": "Choonghyun Park"
      },
      {
        "authorId": "2259114784",
        "name": "Hyunsoo Cho"
      },
      {
        "authorId": "2109166096",
        "name": "Junyeob Kim"
      },
      {
        "authorId": "31760501",
        "name": "Kang Min Yoo"
      },
      {
        "authorId": "2261393515",
        "name": "Sang-goo Lee"
      },
      {
        "authorId": "5041757",
        "name": "Taeuk Kim"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "4df2b1e7d54fe5ad81dc2ed6774b93ef7891b3c8",
    "url": "https://www.semanticscholar.org/paper/4df2b1e7d54fe5ad81dc2ed6774b93ef7891b3c8",
    "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems",
    "abstract": "Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. We make our code and datasets publicly available on Github.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 66,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-16",
    "authors": [
      {
        "authorId": "2127522115",
        "name": "Jon Saad-Falcon"
      },
      {
        "authorId": "144112155",
        "name": "O. Khattab"
      },
      {
        "authorId": "2254255092",
        "name": "Christopher Potts"
      },
      {
        "authorId": "2253469012",
        "name": "Matei Zaharia"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.07038929086448
  },
  {
    "paperId": "e2050c0aa8aa27235c0708b5a5ff741dfd11e2a9",
    "url": "https://www.semanticscholar.org/paper/e2050c0aa8aa27235c0708b5a5ff741dfd11e2a9",
    "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
    "abstract": "\n Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate “hallucinated” content. However, evaluating RAG systems is a challenge. Most benchmarks focus primarily on question answering applications, neglecting other potential scenarios where RAG could be beneficial. Accordingly, in the experiments, these benchmarks often assess only the LLM components of the RAG pipeline or the retriever in knowledge-intensive scenarios, overlooking the impact of external knowledge base construction and the retrieval component on the entire RAG pipeline in non-knowledge-intensive scenarios. To address these issues, this paper constructs a large-scale and more comprehensive benchmark, and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we refer to the CRUD actions that describe interactions between users and knowledge bases, and also categorize the range of RAG applications into four distinct types–Create, Read, Update, and Delete (CRUD). “Create” refers to scenarios requiring the generation of original, varied content. “Read” involves responding to intricate questions in knowledge-intensive situations. “Update” focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. “Delete” pertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories, we have developed different datasets to evaluate the performance of RAG systems. We also analyze the effects of various components of the RAG system, such as the retriever, context length, knowledge base construction, and LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios\n \n 1\n \n .\n",
    "venue": "ACM Transactions on Information Systems",
    "year": 2024,
    "citationCount": 17,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-30",
    "authors": [
      {
        "authorId": "2187857206",
        "name": "Yuanjie Lyu"
      },
      {
        "authorId": "2268429641",
        "name": "Zhiyu Li"
      },
      {
        "authorId": "2268393907",
        "name": "Simin Niu"
      },
      {
        "authorId": "2268399953",
        "name": "Feiyu Xiong"
      },
      {
        "authorId": "2268400606",
        "name": "Bo Tang"
      },
      {
        "authorId": "2117833477",
        "name": "Wenjin Wang"
      },
      {
        "authorId": "2282083454",
        "name": "Hao Wu"
      },
      {
        "authorId": "2304320758",
        "name": "Huan Liu"
      },
      {
        "authorId": "2277237058",
        "name": "Tong Xu"
      },
      {
        "authorId": "2265580543",
        "name": "Enhong Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.35557636844247
  },
  {
    "paperId": "7848d4b4e6ba0897a85cebb6467e94eb0b60d583",
    "url": "https://www.semanticscholar.org/paper/7848d4b4e6ba0897a85cebb6467e94eb0b60d583",
    "title": "Learning to Filter Context for Retrieval-Augmented Generation",
    "abstract": "On-the-fly retrieval of relevant knowledge has proven an essential element of reliable systems for tasks such as open-domain question answering and fact verification. However, because retrieval systems are not perfect, generation models are required to generate outputs given partially or entirely irrelevant passages. This can cause over- or under-reliance on context, and result in problems in the generated output such as hallucinations. To alleviate these problems, we propose FILCO, a method that improves the quality of the context provided to the generator by (1) identifying useful context based on lexical and information-theoretic approaches, and (2) training context filtering models that can filter retrieved contexts at test time. We experiment on six knowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our method outperforms existing approaches on extractive question answering (QA), complex multi-hop and long-form QA, fact verification, and dialog generation tasks. FILCO effectively improves the quality of context, whether or not it supports the canonical output.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 44,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-14",
    "authors": [
      {
        "authorId": "1390877035",
        "name": "Zhiruo Wang"
      },
      {
        "authorId": "2266466286",
        "name": "Jun Araki"
      },
      {
        "authorId": "2669515",
        "name": "Zhengbao Jiang"
      },
      {
        "authorId": "3405393",
        "name": "Md. Rizwan Parvez"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ],
    "source": "semantic_scholar",
    "score": 127.0999373465548
  },
  {
    "paperId": "73c7426559030bf7ef1ec525f99223b670a76fe6",
    "url": "https://www.semanticscholar.org/paper/73c7426559030bf7ef1ec525f99223b670a76fe6",
    "title": "Embedding-Informed Adaptive Retrieval-Augmented Generation of Large Language Models",
    "abstract": "Retrieval-augmented large language models (LLMs) have been remarkably competent in various NLP tasks. However, it was observed by previous works that retrieval is not always helpful, especially when the LLM is already knowledgeable on the query to answer. Motivated by this, Adaptive Retrieval-Augmented Generation (ARAG) studies retrieving only when the knowledge asked by the query is absent in the LLM. Previous works of ARAG either require accessing the pre-training corpus or prompting with additional model inferences. Aiming to avoid such drawbacks, we propose to determine whether the model is knowledgeable on a query via inspecting the (contextualized) pre-trained token embeddings of LLMs. We hypothesize that such embeddings capture rich information on the model's intrinsic knowledge base, which enables an efficient way of judging the necessity to retrieve from an external corpus. Extensive experiments demonstrate our ARAG approach's superior performance across various benchmarks.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-04",
    "authors": [
      {
        "authorId": "2284770018",
        "name": "Chengkai Huang"
      },
      {
        "authorId": "2282516443",
        "name": "Yu Xia"
      },
      {
        "authorId": "2298344454",
        "name": "Rui Wang"
      },
      {
        "authorId": "2284680446",
        "name": "Kaige Xie"
      },
      {
        "authorId": "2284723016",
        "name": "Tong Yu"
      },
      {
        "authorId": "2284680226",
        "name": "Julian McAuley"
      },
      {
        "authorId": "2284707764",
        "name": "Lina Yao"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "29528d8cb030a65f62a35b1237f1f5483077ad0a",
    "url": "https://www.semanticscholar.org/paper/29528d8cb030a65f62a35b1237f1f5483077ad0a",
    "title": "Inference Scaling for Long-Context Retrieval Augmented Generation",
    "abstract": "The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring strategies beyond simply increasing the quantity of knowledge. We focus on two inference scaling strategies: in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-06",
    "authors": [
      {
        "authorId": "2028213158",
        "name": "Zhenrui Yue"
      },
      {
        "authorId": "39371343",
        "name": "Honglei Zhuang"
      },
      {
        "authorId": "2324782053",
        "name": "Aijun Bai"
      },
      {
        "authorId": "2261281337",
        "name": "Kai Hui"
      },
      {
        "authorId": "1886219",
        "name": "R. Jagerman"
      },
      {
        "authorId": "2324910979",
        "name": "Hansi Zeng"
      },
      {
        "authorId": "2099586642",
        "name": "Zhen Qin"
      },
      {
        "authorId": "2325158655",
        "name": "Dong Wang"
      },
      {
        "authorId": "2261356664",
        "name": "Xuanhui Wang"
      },
      {
        "authorId": "1815447",
        "name": "Michael Bendersky"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "5c538b02e876fd39f8a60907a792556f73fb74b0",
    "url": "https://www.semanticscholar.org/paper/5c538b02e876fd39f8a60907a792556f73fb74b0",
    "title": "Customized Retrieval Augmented Generation and Benchmarking for EDA Tool Documentation QA",
    "abstract": "Retrieval augmented generation (RAG) enhances the accuracy and reliability of generative AI models by sourcing factual information from external databases, which is extensively employed in document-grounded question-answering (QA) tasks. Off-the-shelf RAG flows are well pretrained on general-purpose documents, yet they encounter significant challenges when being applied to knowledge-intensive vertical domains, such as electronic design automation (EDA). This paper addresses such issue by proposing a customized RAG framework along with three domain-specific techniques for EDA tool documentation QA, including a contrastive learning scheme for text embedding model fine-tuning, a reranker distilled from proprietary LLM, and a generative LLM fine-tuned with high-quality domain corpus. Furthermore, we have developed and released a documentation QA evaluation benchmark, ORD-QA, for OpenROAD, an advanced RTL-to-GDSII design platform. Experimental results demonstrate that our proposed RAG flow and techniques have achieved superior performance on ORD-QA as well as on a commercial tool, compared with state-of-the-arts. The ORD-QA benchmark and the training dataset for our customized RAG flow are open-source at https://github.com/lesliepy99/RAG-EDA.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-22",
    "authors": [
      {
        "authorId": "2220363044",
        "name": "Yuan Pu"
      },
      {
        "authorId": "8822971",
        "name": "Zhuolun He"
      },
      {
        "authorId": "2312323764",
        "name": "Tairu Qiu"
      },
      {
        "authorId": "2275801674",
        "name": "Haoyuan Wu"
      },
      {
        "authorId": "2257320153",
        "name": "Bei Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "43d35e1c866bbcfa3c573d69df217c35fcec27b2",
    "url": "https://www.semanticscholar.org/paper/43d35e1c866bbcfa3c573d69df217c35fcec27b2",
    "title": "Similarity is Not All You Need: Endowing Retrieval Augmented Generation with Multi Layered Thoughts",
    "abstract": "In recent years, large language models (LLMs) have made remarkable achievements in various domains. However, the untimeliness and cost of knowledge updates coupled with hallucination issues of LLMs have curtailed their applications in knowledge intensive tasks, where retrieval augmented generation (RAG) can be of help. Nevertheless, existing retrieval augmented models typically use similarity as a bridge between queries and documents and follow a retrieve then read procedure. In this work, we argue that similarity is not always the panacea and totally relying on similarity would sometimes degrade the performance of retrieval augmented generation. To this end, we propose MetRag, a Multi layEred Thoughts enhanced Retrieval Augmented Generation framework. To begin with, beyond existing similarity oriented thought, we embrace a small scale utility model that draws supervision from an LLM for utility oriented thought and further come up with a smarter model by comprehensively combining the similarity and utility oriented thoughts. Furthermore, given the fact that the retrieved document set tends to be huge and using them in isolation makes it difficult to capture the commonalities and characteristics among them, we propose to make an LLM as a task adaptive summarizer to endow retrieval augmented generation with compactness-oriented thought. Finally, with multi layered thoughts from the precedent stages, an LLM is called for knowledge augmented generation. Extensive experiments on knowledge-intensive tasks have demonstrated the superiority of MetRag.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-30",
    "authors": [
      {
        "authorId": "2223763670",
        "name": "Chunjing Gan"
      },
      {
        "authorId": "2215590272",
        "name": "Dan Yang"
      },
      {
        "authorId": "2279160677",
        "name": "Binbin Hu"
      },
      {
        "authorId": "2257028770",
        "name": "Hanxiao Zhang"
      },
      {
        "authorId": "2258340246",
        "name": "Siyuan Li"
      },
      {
        "authorId": "2284032340",
        "name": "Ziqi Liu"
      },
      {
        "authorId": "2180240771",
        "name": "Yue Shen"
      },
      {
        "authorId": "2256994202",
        "name": "Lin Ju"
      },
      {
        "authorId": "2266809812",
        "name": "Zhiqiang Zhang"
      },
      {
        "authorId": "2283257801",
        "name": "Jinjie Gu"
      },
      {
        "authorId": "2303947668",
        "name": "Lei Liang"
      },
      {
        "authorId": "2272742832",
        "name": "Jun Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "a546fa44c110c33a3280f31090f96f5b886ac44f",
    "url": "https://www.semanticscholar.org/paper/a546fa44c110c33a3280f31090f96f5b886ac44f",
    "title": "Mindful-RAG: A Study of Points of Failure in Retrieval Augmented Generation",
    "abstract": "Large Language Models (LLMs) excel at generating coherent text but often struggle with knowledge-intensive queries, particularly in domain-specific and factual question-answering tasks. Retrieval-augmented generation (RAG) systems have emerged as a promising solution by integrating external knowledge sources, such as structured knowledge graphs (KGs). While KG-based RAG approaches have demonstrated value, current state-of-the-art solutions frequently fall short, failing to deliver accurate and reliable answers even when the necessary factual knowledge is available. In this paper, we present a critical analysis of failure points in existing KG-based RAG methods, identifying eight key areas of concern, including misinterpretation of question context, incorrect relation mapping, and ineffective ambiguity resolution. We argue that these failures primarily stem from design limitations in current KG-RAG systems, such as inadequate attention to discerning user intent and insufficient alignment of retrieved knowledge with the contextual demands of the query. Based on this analysis, we propose a new approach for KG-RAG systems, termed Mindful-RAG, which re-engineers the retrieval process to be more intent-driven and contextually aware. By enhancing reasoning capabilities, improving constraint identification, and addressing the structural limitations of knowledge graphs, we aim to improve the reliability and effectiveness of KG-RAG systems. To validate this approach, we developed a proof-of-concept by integrating the principles of Mindful-RAG into an existing KG-RAG system. The Mindful-RAG approach seeks to deliver more robust, accurate, and contextually aligned AI-driven knowledge retrieval systems, with potential applications in critical domains such as healthcare, legal, research, and scientific discovery, where precision and reliability are paramount.",
    "venue": "2024 2nd International Conference on Foundation and Large Language Models (FLLM)",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2407.12216",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-16",
    "authors": [
      {
        "authorId": "145363324",
        "name": "Garima Agrawal"
      },
      {
        "authorId": "40899329",
        "name": "Tharindu Kumarage"
      },
      {
        "authorId": "2185743354",
        "name": "Zeyad Alghamdi"
      },
      {
        "authorId": "2146398603",
        "name": "Huanmin Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "b5b200d405b099a88a838f0428aaa817dcb61714",
    "url": "https://www.semanticscholar.org/paper/b5b200d405b099a88a838f0428aaa817dcb61714",
    "title": "MBA-RAG: a Bandit Approach for Adaptive Retrieval-Augmented Generation through Question Complexity",
    "abstract": "Retrieval Augmented Generation (RAG) has proven to be highly effective in boosting the generative performance of language model in knowledge-intensive tasks. However, existing RAG framework either indiscriminately perform retrieval or rely on rigid single-class classifiers to select retrieval methods, leading to inefficiencies and suboptimal performance across queries of varying complexity. To address these challenges, we propose a reinforcement learning-based framework that dynamically selects the most suitable retrieval strategy based on query complexity. % our solution Our approach leverages a multi-armed bandit algorithm, which treats each retrieval method as a distinct ``arm'' and adapts the selection process by balancing exploration and exploitation. Additionally, we introduce a dynamic reward function that balances accuracy and efficiency, penalizing methods that require more retrieval steps, even if they lead to a correct result. Our method achieves new state of the art results on multiple single-hop and multi-hop datasets while reducing retrieval costs. Our code are available at https://github.com/FUTUREEEEEE/MBA .",
    "venue": "International Conference on Computational Linguistics",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-12-02",
    "authors": [
      {
        "authorId": "2333469226",
        "name": "Xiaqiang Tang"
      },
      {
        "authorId": "2282245320",
        "name": "Q. Gao"
      },
      {
        "authorId": "2333929144",
        "name": "Jian Li"
      },
      {
        "authorId": "2333355466",
        "name": "Nan Du"
      },
      {
        "authorId": "2261359496",
        "name": "Qi Li"
      },
      {
        "authorId": "2261347599",
        "name": "Sihong Xie"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "88dc871460a3a03699dc0a8ca248542a5d39f41e",
    "url": "https://www.semanticscholar.org/paper/88dc871460a3a03699dc0a8ca248542a5d39f41e",
    "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
    "abstract": "This paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing its evolution from foundational concepts to the current state of the art. RAG combines retrieval mechanisms with generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs. The study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated to handle knowledge-intensive tasks. A detailed review of the significant technological advancements in RAG is provided, including key innovations in retrieval-augmented language models and applications across various domains such as question-answering, summarization, and knowledge-based tasks. Recent research breakthroughs are discussed, highlighting novel methods for improving retrieval efficiency. Furthermore, the paper examines ongoing challenges such as scalability, bias, and ethical concerns in deployment. Future research directions are proposed, focusing on improving the robustness of RAG models, expanding the scope of application of RAG models, and addressing societal implications. This survey aims to serve as a foundational resource for researchers and practitioners in understanding the potential of RAG and its trajectory in natural language processing.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-10-03",
    "authors": [
      {
        "authorId": "2311997786",
        "name": "Shailja Gupta"
      },
      {
        "authorId": "2311893279",
        "name": "Rajesh Ranjan"
      },
      {
        "authorId": "2321535962",
        "name": "Surya Narayan Singh"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "c00980a88effd94814bb7650ccaf6ab6c184d249",
    "url": "https://www.semanticscholar.org/paper/c00980a88effd94814bb7650ccaf6ab6c184d249",
    "title": "Adapting to Non-Stationary Environments: Multi-Armed Bandit Enhanced Retrieval-Augmented Generation on Knowledge Graphs",
    "abstract": "Despite the superior performance of Large language models on many NLP tasks, they still face significant limitations in memorizing extensive world knowledge. Recent studies have demonstrated that leveraging the Retrieval-Augmented Generation (RAG) framework, combined with Knowledge Graphs that encapsulate extensive factual data in a structured format, robustly enhances the reasoning capabilities of LLMs. However, deploying such systems in real-world scenarios presents challenges: the continuous evolution of non-stationary environments may lead to performance degradation and user satisfaction requires a careful balance of performance and responsiveness. To address these challenges, we introduce a Multi-objective Multi-Armed Bandit enhanced RAG framework, supported by multiple retrieval methods with diverse capabilities under rich and evolving retrieval contexts in practice. Within this framework, each retrieval method is treated as a distinct ``arm''. The system utilizes real-time user feedback to adapt to dynamic environments, by selecting the appropriate retrieval method based on input queries and the historical multi-objective performance of each arm. Extensive experiments conducted on two benchmark KGQA datasets demonstrate that our method significantly outperforms baseline methods in non-stationary settings while achieving state-of-the-art performance in stationary environments. Code and data are available at https://github.com/FUTUREEEEEE/Dynamic-RAG.git",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-12-10",
    "authors": [
      {
        "authorId": "2333469226",
        "name": "Xiaqiang Tang"
      },
      {
        "authorId": "2333929144",
        "name": "Jian Li"
      },
      {
        "authorId": "2333355466",
        "name": "Nan Du"
      },
      {
        "authorId": "2261347599",
        "name": "Sihong Xie"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "41eef50bd07609252593cc0912608e9456b8f6e4",
    "url": "https://www.semanticscholar.org/paper/41eef50bd07609252593cc0912608e9456b8f6e4",
    "title": "Empowering Large Language Model Reasoning : Hybridizing Layered Retrieval Augmented Generation and Knowledge Graph Synthesis",
    "abstract": ": Retrieval Augmented Generation has improved LLM question answering significantly. However, this mechanism still produces hallucinations and structural incoherence in knowledge-intensive tasks. Additionally, many existing techniques neither holistically leverage multiple properties of text nor integrate diverse prompting and agenting frameworks. To address these limitations, this paper proposes a novel methodology that extracts and utilizes unstructured and structured properties of text to construct layered RAG pipelines designed to enhance complex LLM reasoning. Our approach synthesizes three distinct RAG methodologies, each specialized in various aspects: textual entity knowledge graph extraction (Textual Entity RAG); community summary and entity generation (Microsoft GraphRAG), and structural link navigation (MetaWiki RAG). By cumulatively layering these techniques along with advanced prompting and agentic evaluation, we aim to capture a more comprehensive context, enabling the model to generate well-structured responses that reflect all relevant attributes of the text. The proposed framework not only enhances existing RAG mechanisms but also demonstrates the effective integration of knowledge graphs. Additionally, it showcases the application of this framework to advanced answer generation using Wikipedia, with extensions to similar knowledge networks. This novel approach offers a robust solution for social recommender systems and other practical applications, delivering holistic outcomes by synthesizing diverse RAG techniques.",
    "venue": "International Journal of High School Research",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-12-31",
    "authors": [
      {
        "authorId": "2342410931",
        "name": "Vedanth Aggarwal"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "1bf253a3428b4a4b077907be3e1ac03e5c56ffd3",
    "url": "https://www.semanticscholar.org/paper/1bf253a3428b4a4b077907be3e1ac03e5c56ffd3",
    "title": "Enhancing Vector based Retrieval Augmented Generation with Contextual Knowledge Graph Construction",
    "abstract": "The proliferation of unstructured text data necessitates efficient information retrieval systems. Traditional vector-based Retrieval Augmented Generation (RAG) models often fail to capture complex relationships and contextual nuances, limiting effectiveness in knowledge-intensive tasks. We introduce Contextual Knowledge Graph Construction (CKGC), a novel approach enhancing vector-based RAG by dynamically building a knowledge graph that reflects inherent data structures and connections.CKGC leverages text chunking, large language models (LLMs), and ontology mapping. By segmenting text and using LLMs to identify key entities and relationships, CKGC constructs a contextualized knowledge graph enriching information representation. This bridges the gap between semantic similarity and deeper contextual understanding, enabling more accurate and nuanced retrieval.Experiments on 2,000 lease agreements demonstrate that CKGC significantly improves vector-based RAG in information retrieval and question answering tasks, with substantial gains in Mean Reciprocal Rank (MRR) and Top-k Accuracy. CKGC’s adaptability across domains positions it as a valuable tool for enhancing performance and understanding of complex textual data. Our findings underscore CKGC’s transformative potential in unlocking insights from vast text corpora, paving the way for more intelligent and context-aware information retrieval systems.",
    "venue": "2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2024-11-29",
    "authors": [
      {
        "authorId": "2341385191",
        "name": "Sagar Mankari"
      },
      {
        "authorId": "2341384250",
        "name": "Abhishek Sanghavi"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "79422235e53d3a070fa343f8c82efa29d5351945",
    "url": "https://www.semanticscholar.org/paper/79422235e53d3a070fa343f8c82efa29d5351945",
    "title": "RETRIEVAL-AUGMENTED GENERATION USING DOMAIN-SPECIFIC TEXT: A PILOT STUDY",
    "abstract": "The natural language processing (NLP) field has witnessed remarkable advancements with the advent of large language models (LLMs) like GPT, Gemini, Claude, etc. These models are trained on vast amounts of text data, allowing them to generate human-like responses for various tasks. However, despite their impressive capabilities, LLMs have limitations in their ability to incorporate and reason over external knowledge that is not in their training data. This limitation of LLMs is particularly evident in the case of specific domain knowledge. This situation has given rise to the concept of retrieval augmented generation (RAG), an approach that combines the generative power of LLMs with the ability to retrieve and integrate relevant information from external knowledge sources. This research attempts to use RAG as a module in an application designed to answer questions concerning a specific domain, namely social philosophy/philosophy of management, using a published book from the respective domain as an external source. The paper analyzes the mentioned application output, draws conclusions, and traces future directions to improve the accuracy of the output.",
    "venue": "Journal of Engineering Science",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-15",
    "authors": [
      {
        "authorId": "134837237",
        "name": "Victor Iapăscurtă"
      },
      {
        "authorId": "2322594403",
        "name": "Sergey Kronin"
      },
      {
        "authorId": "2215153731",
        "name": "Ion Fiodorov"
      }
    ],
    "source": "semantic_scholar",
    "score": 80
  },
  {
    "paperId": "e2f09ea5a19fe9c4044e0a4696d68ed8a6e1f675",
    "url": "https://www.semanticscholar.org/paper/e2f09ea5a19fe9c4044e0a4696d68ed8a6e1f675",
    "title": "Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination",
    "abstract": "While ongoing advancements in Large Language Models have demonstrated remarkable success across various NLP tasks, Retrieval Augmented Generation Model stands out to be highly effective on downstream applications like Question Answering. Recently, RAG-end2end model further optimized the architecture and achieved notable performance improvements on domain adaptation. However, the effectiveness of these RAG-based architectures remains relatively unexplored when fine-tuned on specialized domains such as customer service for building a reliable conversational AI system. Furthermore, a critical challenge persists in reducing the occurrence of hallucinations while maintaining high domain-specific accuracy. In this paper, we investigated the performance of diverse RAG and RAG-like architectures through domain adaptation and evaluated their ability to generate accurate and relevant response grounded in the contextual knowledge base. To facilitate the evaluation of the models, we constructed a novel dataset HotelConvQA, sourced from wide range of hotel-related conversations and fine-tuned all the models on our domain specific dataset. We also addressed a critical research gap on determining the impact of domain adaptation on reducing hallucinations across different RAG architectures, an aspect that was not properly measured in prior work. Our evaluation shows positive results in all metrics by employing domain adaptation, demonstrating strong performance on QA tasks and providing insights into their efficacy in reducing hallucinations. Our findings clearly indicate that domain adaptation not only enhances the models' performance on QA tasks but also significantly reduces hallucination across all evaluated RAG architectures.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-23",
    "authors": [
      {
        "authorId": "2327211339",
        "name": "Salman Rakin"
      },
      {
        "authorId": "2327211291",
        "name": "Md. A.R. Shibly"
      },
      {
        "authorId": "2327211491",
        "name": "Zahin M. Hossain"
      },
      {
        "authorId": "2327258807",
        "name": "Zeeshan Khan"
      },
      {
        "authorId": "2327212434",
        "name": "Md. Mostofa Akbar"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "6a1b2470b7a03a204ad6927d753a8a1193f02f8a",
    "url": "https://www.semanticscholar.org/paper/6a1b2470b7a03a204ad6927d753a8a1193f02f8a",
    "title": "Document Embeddings Enhance Biomedical Retrieval-Augmented Generation",
    "abstract": "Large language models (LLMs) perform well in many NLP tasks but frequently generate inaccurate information in the biomedical domain, due to hallucination issues. Retrieval-Augmented Generation (RAG) has been introduced to address this issue by integrating external knowledge, enhancing the factual accuracy of outputs. However, naive RAG encounters challenges in effectively utilizing retrieved content, particularly in specialized domains like biomedicine. LLMs often struggle to integrate retrieved content as irrelevant information can interfere with the model’s judgment. Even if relevant documents are retrieved, the model may be unable to accurately comprehend and utilize the domain-specific features due to its inherent knowledge limitations. To overcome these limitations, we propose Document Embeddings Enhanced Biomedical RAG (DEEB-RAG), a framework that incorporates document embeddings along with the original retrieved text. DEEB-RAG uses MedCPT to generate document embeddings and these embeddings are then aligned with the LLM’s semantic space using a two-stage training process on a simple projector. Experimental results on biomedical QA datasets show that DEEB-RAG improves accuracy, with an average performance increase of 2.3% over naive RAG. This demonstrates DEEB-RAG’s ability to mitigate the challenges of utilizing complex biomedical information, thereby enhancing the reliability and effectiveness of LLMs in biomedical domain.",
    "venue": "IEEE International Conference on Bioinformatics and Biomedicine",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-12-03",
    "authors": [
      {
        "authorId": "2330558113",
        "name": "Yongle Kong"
      },
      {
        "authorId": "151472911",
        "name": "Zhihao Yang"
      },
      {
        "authorId": "145965453",
        "name": "Ling Luo"
      },
      {
        "authorId": "12130245",
        "name": "Zeyuan Ding"
      },
      {
        "authorId": "145131943",
        "name": "Lei Wang"
      },
      {
        "authorId": "2279996835",
        "name": "Wei Liu"
      },
      {
        "authorId": "2108505046",
        "name": "Yin Zhang"
      },
      {
        "authorId": "2279974261",
        "name": "Bo Xu"
      },
      {
        "authorId": "49605993",
        "name": "Jian Wang"
      },
      {
        "authorId": "2143552718",
        "name": "Yuanyuan Sun"
      },
      {
        "authorId": "3009486",
        "name": "Zhehuan Zhao"
      },
      {
        "authorId": "2278407005",
        "name": "Hongfei Lin"
      }
    ],
    "source": "semantic_scholar",
    "score": 76
  },
  {
    "paperId": "80b516bdcb598009a06bbf364055f58f87c442b6",
    "url": "https://www.semanticscholar.org/paper/80b516bdcb598009a06bbf364055f58f87c442b6",
    "title": "Experiments with Large Language Models on Retrieval-Augmented Generation for Closed-Source Simulation Software",
    "abstract": "Large Language Models (LLMs) are increasingly helpful in text generation, even writing code in programming languages based on user prompts written in natural language. They are even applied to generate simulation models for multibody systems from natural language. Research results suggest that LLMs surpass the mere replication of existing code examples, where some LLMs have been trained on an open-source multibody simulation code. However, for closed-source simulation software, such results are not to be expected as their ideas and concepts might differ from other publicly available ones. LLMs can hallucinate for knowledge-intensive tasks, such as model creation, which can lead to wrong responses. This is especially the case for the LLM unknown closed-source simulation software. The same applies to other internal knowledge kept private to protect intellectual property or data privacy. The Retrieval-Augmented Generation (RAG) approach might yield a solution for these knowledge-intensive tasks. This paper explores the application of RAG to closed-source simulation software and presents first experiments. After a brief introduction to LLMs, the RAG approach, and the simulation method applied by the close-source simulation software, several examples are provided to test LLMs' knowledge of the simulation software and the creation of simulation models using two RAG systems. The examples show promising results indicating the benefits of applying RAG systems to closed-source simulation software, helping to access their knowledge. Nevertheless, they also reveal gaps in the applied information and open questions for further research.",
    "venue": "",
    "year": 2025,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2025-02-06",
    "authors": [
      {
        "authorId": "2274761775",
        "name": "Andreas Baumann"
      },
      {
        "authorId": "2344091628",
        "name": "Peter Eberhard"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "c311a89de3f869ec97973ad68c7a267cbfd23971",
    "url": "https://www.semanticscholar.org/paper/c311a89de3f869ec97973ad68c7a267cbfd23971",
    "title": "DuetRAG: Collaborative Retrieval-Augmented Generation",
    "abstract": "Retrieval-Augmented Generation (RAG) methods augment the input of Large Language Models (LLMs) with relevant retrieved passages, reducing factual errors in knowledge-intensive tasks. However, contemporary RAG approaches suffer from irrelevant knowledge retrieval issues in complex domain questions (e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to low-quality generations. To address this issue, we propose a novel Collaborative Retrieval-Augmented Generation framework, DuetRAG. Our bootstrapping philosophy is to simultaneously integrate the domain fintuning and RAG models to improve the knowledge retrieval quality, thereby enhancing generation quality. Finally, we demonstrate DuetRAG' s matches with expert human researchers on HotPot QA.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-12",
    "authors": [
      {
        "authorId": "2302798653",
        "name": "Dian Jiao"
      },
      {
        "authorId": "2303434387",
        "name": "Li Cai"
      },
      {
        "authorId": "2303044665",
        "name": "Jingsheng Huang"
      },
      {
        "authorId": "2108125912",
        "name": "Wenqiao Zhang"
      },
      {
        "authorId": "2118071462",
        "name": "Siliang Tang"
      },
      {
        "authorId": "2253660817",
        "name": "Yueting Zhuang"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "df1768e8e4012637f5ce796f0e7144047cf3a52a",
    "url": "https://www.semanticscholar.org/paper/df1768e8e4012637f5ce796f0e7144047cf3a52a",
    "title": "Evaluating Self-Generated Documents for Enhancing Retrieval-Augmented Generation with Large Language Models",
    "abstract": "The integration of documents generated by LLMs themselves (Self-Docs) alongside retrieved documents has emerged as a promising strategy for retrieval-augmented generation systems. However, previous research primarily focuses on optimizing the use of Self-Docs, with their inherent properties remaining underexplored. To bridge this gap, we first investigate the overall effectiveness of Self-Docs, identifying key factors that shape their contribution to RAG performance (RQ1). Building on these insights, we develop a taxonomy grounded in Systemic Functional Linguistics to compare the influence of various Self-Docs categories (RQ2) and explore strategies for combining them with external sources (RQ3). Our findings reveal which types of Self-Docs are most beneficial and offer practical guidelines for leveraging them to achieve significant improvements in knowledge-intensive question answering tasks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-17",
    "authors": [
      {
        "authorId": "2322455383",
        "name": "Jiatao Li"
      },
      {
        "authorId": "1486281414",
        "name": "Xinyu Hu"
      },
      {
        "authorId": "2165272941",
        "name": "Xunjian Yin"
      },
      {
        "authorId": "2257016300",
        "name": "Xiaojun Wan"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "fa49021ce42229385f39bf5e1a2dcff48c2e2157",
    "url": "https://www.semanticscholar.org/paper/fa49021ce42229385f39bf5e1a2dcff48c2e2157",
    "title": "Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation",
    "abstract": "Retrieval-augmented language models (RALMs) have shown strong performance and wide applicability in knowledge-intensive tasks. However, there are significant trustworthiness concerns as RALMs are prone to generating unfaithful outputs, including baseless information or contradictions with the retrieved context. This paper proposes SynCheck, a lightweight monitor that leverages fine-grained decoding dynamics including sequence likelihood, uncertainty quantification, context influence, and semantic alignment to synchronously detect unfaithful sentences. By integrating efficiently measurable and complementary signals, SynCheck enables accurate and immediate feedback and intervention. Experiments show that SynCheck significantly outperforms existing faithfulness detection baselines, achieving over 0.85 AUROC across a suite of six long-form retrieval-augmented generation tasks. Leveraging SynCheck, we further introduce FOD, a faithfulness-oriented decoding algorithm guided by beam search for long-form retrieval-augmented generation. Empirical results demonstrate that FOD outperforms traditional strategies such as abstention, reranking, or contrastive decoding significantly in terms of faithfulness, achieving over 10% improvement across six datasets.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-19",
    "authors": [
      {
        "authorId": "2107535909",
        "name": "Di Wu"
      },
      {
        "authorId": "2308241925",
        "name": "Jia-Chen Gu"
      },
      {
        "authorId": "2281946748",
        "name": "Fan Yin"
      },
      {
        "authorId": "2256996328",
        "name": "Nanyun Peng"
      },
      {
        "authorId": "2257127887",
        "name": "Kai-Wei Chang"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "52008bf2a58054e38a7b630dcf021e1c9bf8d109",
    "url": "https://www.semanticscholar.org/paper/52008bf2a58054e38a7b630dcf021e1c9bf8d109",
    "title": "Adapting Large Language Models for Biomedicine though Retrieval-Augmented Generation with Documents Scoring",
    "abstract": "By integrating the generative capabilities of Large Language Models (LLMs) with external biomedical knowledge repositories, Retrieval-Augmented Generation (RAG) offers significant potential for addressing knowledge-intensive biomedical tasks, enhancing the precision and effectiveness of clinical decision-making. However, the standard RAG approach fails to fully leverage the information within the retrieved documents, which can lead to incorrect response generation. While advanced RAG approaches empower LLMs to understand the correlation between queries and documents, they require costly annotation. In this work, we propose a strategy to help LLMs better leverage the information within retrieved documents in RAG system while reducing data acquisition costs. We fine-tuned an LLM on a dataset comprising queries, retrieved documents, and their relevance scores generated by a pre-trained biomedical re-ranker to adapt the model for answering questions from reference documents. The fine-tuned model can independently score retrieved documents before answering the question, thereby more effectively utilizing the information contained within retrieved documents. Experimental results shows that we have improved the model’s accuracy on two biomedical benchmarks while reduced the costs of data acquisition.",
    "venue": "IEEE International Conference on Bioinformatics and Biomedicine",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-12-03",
    "authors": [
      {
        "authorId": "2339515370",
        "name": "Yinkui Huang"
      },
      {
        "authorId": "2240059911",
        "name": "Tianrun Gao"
      },
      {
        "authorId": "2339549165",
        "name": "Jiangjiang Zhang"
      },
      {
        "authorId": "2110997991",
        "name": "Xiaohong Liu"
      },
      {
        "authorId": "2197687830",
        "name": "Guangyu Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 76
  },
  {
    "paperId": "e447849a529b61dfb5a8a88b2a1551dbfa3ef3e9",
    "url": "https://www.semanticscholar.org/paper/e447849a529b61dfb5a8a88b2a1551dbfa3ef3e9",
    "title": "E2E-AFG: An End-to-End Model with Adaptive Filtering for Retrieval-Augmented Generation",
    "abstract": "Retrieval-augmented generation methods often neglect the quality of content retrieved from external knowledge bases, resulting in irrelevant information or potential misinformation that negatively affects the generation results of large language models. In this paper, we propose an end-to-end model with adaptive filtering for retrieval-augmented generation (E2E-AFG), which integrates answer existence judgment and text generation into a single end-to-end framework. This enables the model to focus more effectively on relevant content while reducing the influence of irrelevant information and generating accurate answers. We evaluate E2E-AFG on six representative knowledge-intensive language datasets, and the results show that it consistently outperforms baseline models across all tasks, demonstrating the effectiveness and robustness of the proposed approach.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-11-01",
    "authors": [
      {
        "authorId": "2329620771",
        "name": "Yun Jiang"
      },
      {
        "authorId": "2329138908",
        "name": "Zilong Xie"
      },
      {
        "authorId": "2329056472",
        "name": "Wei Zhang"
      },
      {
        "authorId": "2317037782",
        "name": "Yun Fang"
      },
      {
        "authorId": "2218801242",
        "name": "Shuai Pan"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "006168b63d1d4358bce840b55b76c1cacfb35121",
    "url": "https://www.semanticscholar.org/paper/006168b63d1d4358bce840b55b76c1cacfb35121",
    "title": "Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering",
    "abstract": "Retrieval-augmented generation (RAG) has emerged as a promising approach to enhance the performance of large language models (LLMs) in knowledge-intensive tasks such as those from medical domain. However, the sensitive nature of the medical domain necessitates a completely accurate and trustworthy system. While existing RAG benchmarks primarily focus on the standard retrieve-answer setting, they overlook many practical scenarios that measure crucial aspects of a reliable medical system. This paper addresses this gap by providing a comprehensive evaluation framework for medical question-answering (QA) systems in a RAG setting for these situations, including sufficiency, integration, and robustness. We introduce Medical Retrieval-Augmented Generation Benchmark (MedRGB) that provides various supplementary elements to four medical QA datasets for testing LLMs' ability to handle these specific scenarios. Utilizing MedRGB, we conduct extensive evaluations of both state-of-the-art commercial LLMs and open-source models across multiple retrieval conditions. Our experimental results reveals current models' limited ability to handle noise and misinformation in the retrieved documents. We further analyze the LLMs' reasoning processes to provides valuable insights and future directions for developing RAG systems in this critical medical domain.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-11-14",
    "authors": [
      {
        "authorId": "1692755523",
        "name": "Nghia Trung Ngo"
      },
      {
        "authorId": "2186540882",
        "name": "Chien Van Nguyen"
      },
      {
        "authorId": "2285778564",
        "name": "Franck Dernoncourt"
      },
      {
        "authorId": "2293544012",
        "name": "Thien Huu Nguyen"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "ee93275c87733f941f0aa027faab6a22f51f9283",
    "url": "https://www.semanticscholar.org/paper/ee93275c87733f941f0aa027faab6a22f51f9283",
    "title": "Survey on Speech Recognition and Retrieval-Augmented Generation",
    "abstract": "Automatic speech recognition\n(ASR) and retrieval-augmented generation (RAG)\nsystems have seen remarkable progress in handling\nmultilingualism, noise robustness, real-time\ntranscription, and knowledge-intensive tasks. The\nsurvey reviews 12 key papers that contribute to\nadvancements in ASR and RAG, covering\napproaches like end-to-end multilingual models,\nnoise-reduction techniques, and real-time speech\nprocessing. It also examines RAG systems that\nenhance generative models by integrating retrieval\nmechanisms for improved accuracy in tasks like\nquestion answering and summarization. By\ncategorizing the papers into themes, this survey\nhighlights key methodologies, compares their\nperformance, and identifies future directions for\nimproving ASR and RAG technologies in handling\nreal-world challenges.\n",
    "venue": "International Journal of Advances in Engineering and Management",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-12-01",
    "authors": [
      {
        "authorId": "2335764335",
        "name": "Edwin Alex Shaji"
      },
      {
        "authorId": "2335764992",
        "name": "Jerishab M Jerishab M"
      },
      {
        "authorId": "2335826692",
        "name": "Leya Thomas"
      },
      {
        "authorId": "2335764784",
        "name": "M. V. Prabhu"
      },
      {
        "authorId": "2335765211",
        "name": "Asst.Prof Chinchu M Pillai"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "08d8263ef29a9bd7cc63c5bdd43a1948ebf1cbd8",
    "url": "https://www.semanticscholar.org/paper/08d8263ef29a9bd7cc63c5bdd43a1948ebf1cbd8",
    "title": "Audiobox TTA-RAG: Improving Zero-Shot and Few-Shot Text-To-Audio with Retrieval-Augmented Generation",
    "abstract": "Current leading Text-To-Audio (TTA) generation models suffer from degraded performance on zero-shot and few-shot settings. It is often challenging to generate high-quality audio for audio events that are unseen or uncommon in the training set. Inspired by the success of Retrieval-Augmented Generation (RAG) in Large Language Model (LLM)-based knowledge-intensive tasks, we extend the TTA process with additional conditioning contexts. We propose Audiobox TTA-RAG, a novel retrieval-augmented TTA approach based on Audiobox, a conditional flow-matching audio generation model. Unlike the vanilla Audiobox TTA solution which generates audio conditioned on text, we augmented the conditioning input with retrieved audio samples that provide additional acoustic information to generate the target audio. Our retrieval method does not require the external database to have labeled audio, offering more practical use cases. To evaluate our proposed method, we curated test sets in zero-shot and few-shot settings. Our empirical results show that the proposed model can effectively leverage the retrieved audio samples and significantly improve zero-shot and few-shot TTA performance, with large margins on multiple evaluation metrics, while maintaining the ability to generate semantically aligned audio for the in-domain setting. In addition, we investigate the effect of different retrieval methods and data sources.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-11-07",
    "authors": [
      {
        "authorId": "2330162392",
        "name": "Mu Yang"
      },
      {
        "authorId": "2261676061",
        "name": "Bowen Shi"
      },
      {
        "authorId": "2261636821",
        "name": "Matt Le"
      },
      {
        "authorId": "2265493884",
        "name": "Wei-Ning Hsu"
      },
      {
        "authorId": "2894428",
        "name": "Andros Tjandra"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "422834b1229b9451ba9930b245a22eae1a825b5a",
    "url": "https://www.semanticscholar.org/paper/422834b1229b9451ba9930b245a22eae1a825b5a",
    "title": "A Hybrid Retrieval Approach for Advancing Retrieval-Augmented Generation Systems",
    "abstract": "Retrieval-Augmented Generation (RAG) has become a promising solution for utilizing Large Language Models (LLMs) in domain-intensive question-answering tasks. The performance of RAG is greatly influenced by the retriever component, which typically relies on semantic similarity between the text embeddings of the query and the passages to identify the relevant context. However, text embedding models may only capture the semantic meaning of individual passages, potentially neglecting global relationships between them. To address this limitation, we propose a hybrid retrieval method that integrates embeddings encoded from textual and knowledge graph information. Although in this paper, the knowledge graphs describe the passage relationships in a health-tech industry use case, the hybrid embedding solution is designed to be generic. Furthermore, the proposed retrieval approach aims to offer straight-forward implementation without requiring complex joint learning processes. Our results on custom test sets demonstrate significantly enhanced accuracy and ranking of the retriever, thus, supporting the LLM-based reader component in generating more accurate responses.",
    "venue": "International Conference on Natural Language and Speech Processing",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2329167883",
        "name": "Nguyen Nam Doan"
      },
      {
        "authorId": "2701180",
        "name": "Aki Härmä"
      },
      {
        "authorId": "2329167833",
        "name": "Remzi Celebi"
      },
      {
        "authorId": "2329167809",
        "name": "Valeria Gottardo"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "1148782ef3582e18d32c0e354f8e88b8d7e71cc9",
    "url": "https://www.semanticscholar.org/paper/1148782ef3582e18d32c0e354f8e88b8d7e71cc9",
    "title": "Retrieval Augmented Generation with Rich Answer Encoding",
    "abstract": "Knowledge-intensive generation tasks like generative question answering require models to retrieve appropriate passages from external knowledge sources to support answer generation. The generation quality relies heavily on the retrieved passages, which serve as contextual information. State-of-the-art Retrieval Augmented Generation models with marginalized output dominate this area but focus too much on label-relevant passages, rather than question-relevant passages and answers. This work addresses this issue by incorporating rich answer encoding through Dense Knowledge Similarity (DKS) and Retriever as Answer Classifier (RAC). We demonstrate the advantages of our proposed approach in open domain question answering (MSMARCO) and conversation (Wizard of Wikipedia) datasets, reporting both generation and retrieval metrics. In the MSMARCO development set, our best model achieves 12.1% relative improvement 1 on Recall@1 and 4.5% relative improvement on BLEU-4 compared to the baseline model. In the KILT-WoW leaderboard, our best model achieves 8.9% relative improvement on R-Precision and 13.3% relative improvement on KILT-RL compared to the baseline model. Our codes and models are available at https://github.com/hwy9855/rag-ae .",
    "venue": "International Joint Conference on Natural Language Processing",
    "year": 2023,
    "citationCount": 13,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.ijcnlp-main.65.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1896870270",
        "name": "Wenyu Huang"
      },
      {
        "authorId": "1747893",
        "name": "Mirella Lapata"
      },
      {
        "authorId": "7631872",
        "name": "P. Vougiouklis"
      },
      {
        "authorId": "3451396",
        "name": "Nikos Papasarantopoulos"
      },
      {
        "authorId": "2280059593",
        "name": "Jeff Z. Pan"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "5fdb4ca353809326ae5f5296a448c40ccbdc0b4b",
    "url": "https://www.semanticscholar.org/paper/5fdb4ca353809326ae5f5296a448c40ccbdc0b4b",
    "title": "Retrieval-Augmented Generation with Quantized Large Language Models: A Comparative Analysis",
    "abstract": "Large language models have demonstrated emergent intelligence and ability to handle a wide array of tasks. However, the reliability of these models in terms of factual accuracy and timely knowledge acquisition remains a challenge. Researchers explore the implementation of retrieval-augmented generation methods, aiming to enhance the authenticity and specificity in knowledge-intensive tasks. This paper discusses the practical application in industrial settings, particularly in assisting design personnel with navigating complex standards and quality manuals. Utilizing an open-source model with 6 billion parameters, the study employs quantization technology for local deployment, addressing computational challenges. The retrieval-augmented generation framework is analyzed, emphasizing the integration of document parsing, vector databases, and text embedding models. Experimental results compare models at different quantization levels, revealing trade-offs between response time, model size, and performance metrics. The findings suggest that 4-bit integer quantization is optimal for standard document retrieval and question-answering tasks, highlighting practical considerations for CPU inference. The paper concludes with insights into hyper-parameter tuning, model comparisons, and future optimizations for enhanced performance in edge device deployments of large language models.",
    "venue": "IoTAAI",
    "year": 2023,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2023-11-24",
    "authors": [
      {
        "authorId": "2300128478",
        "name": "Shanglin Yang"
      },
      {
        "authorId": "2300045418",
        "name": "Jialin Zhu"
      },
      {
        "authorId": "2300129153",
        "name": "Jialin Wang"
      },
      {
        "authorId": "2241951660",
        "name": "Xiaohan Xu"
      },
      {
        "authorId": "2300003699",
        "name": "Zihang Shao"
      },
      {
        "authorId": "2300244859",
        "name": "Liwei Yao"
      },
      {
        "authorId": "2112670948",
        "name": "Benchang Zheng"
      },
      {
        "authorId": "2299984697",
        "name": "Hu Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "b85cd7fbb981b31719c081c1ef28eb43bf360f1a",
    "url": "https://www.semanticscholar.org/paper/b85cd7fbb981b31719c081c1ef28eb43bf360f1a",
    "title": "SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model",
    "abstract": "The indexing-retrieval-generation paradigm of retrieval-augmented generation (RAG) has been highly successful in solving knowledge-intensive tasks by integrating external knowledge into large language models (LLMs). However, the incorporation of external and unverified knowledge increases the vulnerability of LLMs because attackers can perform attack tasks by manipulating knowledge. In this paper, we introduce a benchmark named SafeRAG designed to evaluate the RAG security. First, we classify attack tasks into silver noise, inter-context conflict, soft ad, and white Denial-of-Service. Next, we construct RAG security evaluation dataset (i.e., SafeRAG dataset) primarily manually for each task. We then utilize the SafeRAG dataset to simulate various attack scenarios that RAG may encounter. Experiments conducted on 14 representative RAG components demonstrate that RAG exhibits significant vulnerability to all attack tasks and even the most apparent attack task can easily bypass existing retrievers, filters, or advanced LLMs, resulting in the degradation of RAG service quality. Code is available at: https://github.com/IAAR-Shanghai/SafeRAG.",
    "venue": "",
    "year": 2025,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2025-01-28",
    "authors": [
      {
        "authorId": "2268495952",
        "name": "Xun Liang"
      },
      {
        "authorId": "2268393907",
        "name": "Simin Niu"
      },
      {
        "authorId": "2268429641",
        "name": "Zhiyu Li"
      },
      {
        "authorId": "2107969558",
        "name": "Sensen Zhang"
      },
      {
        "authorId": "2284861141",
        "name": "Hanyu Wang"
      },
      {
        "authorId": "2268399953",
        "name": "Feiyu Xiong"
      },
      {
        "authorId": "2343598730",
        "name": "Jason Zhaoxin Fan"
      },
      {
        "authorId": null,
        "name": "Bo Tang"
      },
      {
        "authorId": "2268434524",
        "name": "Shichao Song"
      },
      {
        "authorId": "2292726337",
        "name": "Mengwei Wang"
      },
      {
        "authorId": "2303425635",
        "name": "Jiawei Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "a3e55c874fa220fc42eb2ee43acfe24c7a309ffe",
    "url": "https://www.semanticscholar.org/paper/a3e55c874fa220fc42eb2ee43acfe24c7a309ffe",
    "title": "RAG-WM: An Efficient Black-Box Watermarking Approach for Retrieval-Augmented Generation of Large Language Models",
    "abstract": "In recent years, tremendous success has been witnessed in Retrieval-Augmented Generation (RAG), widely used to enhance Large Language Models (LLMs) in domain-specific, knowledge-intensive, and privacy-sensitive tasks. However, attackers may steal those valuable RAGs and deploy or commercialize them, making it essential to detect Intellectual Property (IP) infringement. Most existing ownership protection solutions, such as watermarks, are designed for relational databases and texts. They cannot be directly applied to RAGs because relational database watermarks require white-box access to detect IP infringement, which is unrealistic for the knowledge base in RAGs. Meanwhile, post-processing by the adversary's deployed LLMs typically destructs text watermark information. To address those problems, we propose a novel black-box\"knowledge watermark\"approach, named RAG-WM, to detect IP infringement of RAGs. RAG-WM uses a multi-LLM interaction framework, comprising a Watermark Generator, Shadow LLM&RAG, and Watermark Discriminator, to create watermark texts based on watermark entity-relationship tuples and inject them into the target RAG. We evaluate RAG-WM across three domain-specific and two privacy-sensitive tasks on four benchmark LLMs. Experimental results show that RAG-WM effectively detects the stolen RAGs in various deployed LLMs. Furthermore, RAG-WM is robust against paraphrasing, unrelated content removal, knowledge insertion, and knowledge expansion attacks. Lastly, RAG-WM can also evade watermark detection approaches, highlighting its promising application in detecting IP infringement of RAG systems.",
    "venue": "",
    "year": 2025,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2025-01-09",
    "authors": [
      {
        "authorId": "117426360",
        "name": "Peizhuo Lv"
      },
      {
        "authorId": "2311080521",
        "name": "Mengjie Sun"
      },
      {
        "authorId": "2339536239",
        "name": "Hao Wang"
      },
      {
        "authorId": "2321412683",
        "name": "Xiaofeng Wang"
      },
      {
        "authorId": "2289793253",
        "name": "Shengzhi Zhang"
      },
      {
        "authorId": "2339413010",
        "name": "Yuxuan Chen"
      },
      {
        "authorId": "2339419269",
        "name": "Kai Chen"
      },
      {
        "authorId": "2339421604",
        "name": "Limin Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "3ddc8365ab4f048a4b8e6c4c3c7a99777c9dcf13",
    "url": "https://www.semanticscholar.org/paper/3ddc8365ab4f048a4b8e6c4c3c7a99777c9dcf13",
    "title": "KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation",
    "abstract": "Academic researchers face challenges keeping up with exponentially growing published findings in their field. Performing comprehensive literature reviews to synthesize knowledge is time-consuming and labor-intensive using manual approaches. Recent advances in artificial intelligence provide promising solutions, yet many require coding expertise, limiting accessibility. KNIMEZoBot represents an innovative integration of Zotero, OpenAI, and the KNIME visual programming platform to automate literature review tasks for users with no coding experience. By leveraging KNIME's intuitive graphical interface, researchers can create workflows to search their Zotero libraries and utilize OpenAI models to extract key information without coding. Users simply provide API keys and configure settings through a user-friendly interface in a locally stored copy of the workflow. KNIMEZoBot then allows asking natural language questions via a chatbot and retrieves relevant passages from papers to generate synthesized answers. This system has significant potential to expedite literature reviews for researchers unfamiliar with coding by automating retrieval and analysis of publications in personal Zotero libraries. KNIMEZoBot demonstrates how thoughtfully designed AI tools can expand accessibility and accelerate knowledge building across diverse research domains.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-11-07",
    "authors": [
      {
        "authorId": "1708230331",
        "name": "S. Alshammari"
      },
      {
        "authorId": "2283455329",
        "name": "Lama Basalelah"
      },
      {
        "authorId": "2265647714",
        "name": "Walaa Abu Rukbah"
      },
      {
        "authorId": "2265646904",
        "name": "Ali Alsuhibani"
      },
      {
        "authorId": "6997832",
        "name": "D. Wijesinghe"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "e15a11e33bd115612a7713f4af4329b6a9bb2a1d",
    "url": "https://www.semanticscholar.org/paper/e15a11e33bd115612a7713f4af4329b6a9bb2a1d",
    "title": "GeneRAG: Enhancing Large Language Models with Gene-Related Task by Retrieval-Augmented Generation",
    "abstract": "Large Language Models (LLMs) like GPT-4 have revolutionized natural language processing and are used in gene analysis, but their gene knowledge is incomplete. Fine-tuning LLMs with external data is costly and resource-intensive. Retrieval-Augmented Generation (RAG) integrates relevant external information dynamically. We introduce GeneRAG, a frame-work that enhances LLMs’ gene-related capabilities using RAG and the Maximal Marginal Relevance (MMR) algorithm. Evaluations with datasets from the National Center for Biotechnology Information (NCBI) show that GeneRAG outperforms GPT-3.5 and GPT-4, with a 39% improvement in answering gene questions, a 43% performance increase in cell type annotation, and a 0.25 decrease in error rates for gene interaction prediction. These results highlight GeneRAG’s potential to bridge a critical gap in LLM capabilities for more effective applications in genetics.",
    "venue": "bioRxiv",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "https://www.biorxiv.org/content/biorxiv/early/2024/06/28/2024.06.24.600176.full.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Biology"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-06-28",
    "authors": [
      {
        "authorId": "2309216925",
        "name": "Xinyi Lin"
      },
      {
        "authorId": "73776889",
        "name": "Gelei Deng"
      },
      {
        "authorId": "22799258",
        "name": "Yuekang Li"
      },
      {
        "authorId": "2309156452",
        "name": "Jingquan Ge"
      },
      {
        "authorId": "2309153880",
        "name": "Joshua Wing Kei Ho"
      },
      {
        "authorId": "2284875313",
        "name": "Yi Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "878032e7d2728c88f304db1f04d7e8a203c966f8",
    "url": "https://www.semanticscholar.org/paper/878032e7d2728c88f304db1f04d7e8a203c966f8",
    "title": "CorpusLM: Towards a Unified Language Model on Corpus for Knowledge-Intensive Tasks",
    "abstract": "Large language models (LLMs) have gained significant attention in various fields but prone to hallucination, especially in knowledge-intensive (KI) tasks. To address this, retrieval-augmented generation (RAG) has emerged as a popular solution to enhance factual accuracy. However, traditional retrieval modules often rely on large document index and disconnect with generative tasks. With the advent of generative retrieval (GR), language models can retrieve by directly generating document identifiers (DocIDs), offering superior performance in retrieval tasks. However, the potential relationship between GR and downstream tasks remains unexplored. In this paper, we propose \\textbf{CorpusLM}, a unified language model that leverages external corpus to tackle various knowledge-intensive tasks by integrating generative retrieval, closed-book generation, and RAG through a unified greedy decoding process. We design the following mechanisms to facilitate effective retrieval and generation, and improve the end-to-end effectiveness of KI tasks: (1) We develop a ranking-oriented DocID list generation strategy, which refines GR by directly learning from a DocID ranking list, to improve retrieval quality. (2) We design a continuous DocIDs-References-Answer generation strategy, which facilitates effective and efficient RAG. (3) We employ well-designed unsupervised DocID understanding tasks, to comprehend DocID semantics and their relevance to downstream tasks. We evaluate our approach on the widely used KILT benchmark with two variants of backbone models, i.e., T5 and Llama2. Experimental results demonstrate the superior performance of our models in both retrieval and downstream tasks.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-02",
    "authors": [
      {
        "authorId": "2144456832",
        "name": "Xiaoxi Li"
      },
      {
        "authorId": "2257039188",
        "name": "Zhicheng Dou"
      },
      {
        "authorId": "2118788278",
        "name": "Yujia Zhou"
      },
      {
        "authorId": "2282524575",
        "name": "Fangchao Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.19162312519754
  },
  {
    "paperId": "9b302002c4b764f61fa7a3d14270470f625945cf",
    "url": "https://www.semanticscholar.org/paper/9b302002c4b764f61fa7a3d14270470f625945cf",
    "title": "RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards",
    "abstract": "Retrieval-Augmented Generation (RAG) has proven its effectiveness in mitigating hallucinations in Large Language Models (LLMs) by retrieving knowledge from external resources. To adapt LLMs for RAG pipelines, current approaches use instruction tuning to optimize LLMs, improving their ability to utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses on equipping LLMs to handle diverse RAG tasks using different instructions. However, it trains RAG modules to overfit training signals and overlooks the varying data preferences among agents within the RAG system. In this paper, we propose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG systems by aligning data preferences between different RAG modules. DDR works by collecting the rewards to optimize each agent with a rollout method. This method prompts agents to sample some potential responses as perturbations, evaluates the impact of these perturbations on the whole RAG system, and subsequently optimizes the agent to produce outputs that improve the performance of the RAG system. Our experiments on various knowledge-intensive tasks demonstrate that DDR significantly outperforms the SFT method, particularly for LLMs with smaller-scale parameters that depend more on the retrieved knowledge. Additionally, DDR exhibits a stronger capability to align the data preference between RAG modules. The DDR method makes generation module more effective in extracting key information from documents and mitigating conflicts between parametric memory and external knowledge. All codes are available at https://github.com/OpenMatch/RAG-DDR.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-17",
    "authors": [
      {
        "authorId": "2261354998",
        "name": "Xinze Li"
      },
      {
        "authorId": "2124028252",
        "name": "Senkun Mei"
      },
      {
        "authorId": "49047064",
        "name": "Zhenghao Liu"
      },
      {
        "authorId": "2277242040",
        "name": "Yukun Yan"
      },
      {
        "authorId": "2267033597",
        "name": "Shuo Wang"
      },
      {
        "authorId": "2314785970",
        "name": "Shi Yu"
      },
      {
        "authorId": "1633538428",
        "name": "Zheni Zeng"
      },
      {
        "authorId": "2327546188",
        "name": "Hao Chen"
      },
      {
        "authorId": "2204644192",
        "name": "Ge Yu"
      },
      {
        "authorId": "2290295914",
        "name": "Zhiyuan Liu"
      },
      {
        "authorId": "2273551430",
        "name": "Maosong Sun"
      },
      {
        "authorId": "2139787803",
        "name": "Chenyan Xiong"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "90b337077164112e81ab91e5df1b8e4352b1ff3e",
    "url": "https://www.semanticscholar.org/paper/90b337077164112e81ab91e5df1b8e4352b1ff3e",
    "title": "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation",
    "abstract": "Despite the significant progress of large language models (LLMs) in various tasks, they often produce factual errors due to their limited internal knowledge. Retrieval-Augmented Generation (RAG), which enhances LLMs with external knowledge sources, offers a promising solution. However, these methods can be misled by irrelevant paragraphs in retrieved documents. Due to the inherent uncertainty in LLM generation, inputting the entire document may introduce off-topic information, causing the model to deviate from the central topic and affecting the relevance of the generated content. To address these issues, we propose the Retrieve-Plan-Generation (RPG) framework. RPG generates plan tokens to guide subsequent generation in the plan stage. In the answer stage, the model selects relevant fine-grained paragraphs based on the plan and uses them for further answer generation. This plan-answer process is repeated iteratively until completion, enhancing generation relevance by focusing on specific topics. To implement this framework efficiently, we utilize a simple but effective multi-task prompt-tuning method, enabling the existing LLMs to handle both planning and answering. We comprehensively compare RPG with baselines across 5 knowledge-intensive generation tasks, demonstrating the effectiveness of our approach.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-21",
    "authors": [
      {
        "authorId": "2187857206",
        "name": "Yuanjie Lyu"
      },
      {
        "authorId": "2307915655",
        "name": "Zihan Niu"
      },
      {
        "authorId": "2202470155",
        "name": "Zheyong Xie"
      },
      {
        "authorId": "2260850374",
        "name": "Chao Zhang"
      },
      {
        "authorId": "2277237058",
        "name": "Tong Xu"
      },
      {
        "authorId": "2308313519",
        "name": "Yang Wang"
      },
      {
        "authorId": "2265580543",
        "name": "Enhong Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "388fedd2e549e6f51c107162830c38c8f32902e1",
    "url": "https://www.semanticscholar.org/paper/388fedd2e549e6f51c107162830c38c8f32902e1",
    "title": "Ground Every Sentence: Improving Retrieval-Augmented LLMs with Interleaved Reference-Claim Generation",
    "abstract": "Retrieval-Augmented Generation (RAG) has been widely adopted to enhance Large Language Models (LLMs) in knowledge-intensive tasks. Recently, Attributed Text Generation (ATG) has attracted growing attention, which provides citations to support the model's responses in RAG, so as to enhance the credibility of LLM-generated content and facilitate verification. Prior methods mainly adopt coarse-grained attributions, linking to passage-level references or providing paragraph-level citations. However, these methods still fall short in verifiability and require certain time costs for fact checking. This paper proposes a fine-grained ATG method called ReClaim(Refer&Claim), which alternates the generation of references and answers step by step. Unlike traditional coarse-grained attribution, ReClaim allows the model to add sentence-level fine-grained citations to each answer sentence in long-form question-answering tasks. Our experiments encompass various training and inference methods and multiple LLMs, verifying the effectiveness of our approach.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-01",
    "authors": [
      {
        "authorId": "2309175505",
        "name": "Sirui Xia"
      },
      {
        "authorId": "2108003209",
        "name": "Xintao Wang"
      },
      {
        "authorId": "3366523",
        "name": "Jiaqing Liang"
      },
      {
        "authorId": "2309213407",
        "name": "Yifei Zhang"
      },
      {
        "authorId": "2309277002",
        "name": "Weikang Zhou"
      },
      {
        "authorId": "2310294929",
        "name": "Jiaji Deng"
      },
      {
        "authorId": "2310860800",
        "name": "Fei Yu"
      },
      {
        "authorId": "2267005966",
        "name": "Yanghua Xiao"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "a681b1085c088c51347cdb9358dd344081d29c99",
    "url": "https://www.semanticscholar.org/paper/a681b1085c088c51347cdb9358dd344081d29c99",
    "title": "Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning with Knowledge-guided Retrieval Augmented Generation",
    "abstract": "Retrieval-augmented generation (RAG) has improved large language models (LLMs) by using knowledge retrieval to overcome knowledge deficiencies. However, current RAG methods often fall short of ensuring the depth and completeness of retrieved information, which is necessary for complex reasoning tasks. In this work, we introduce Think-on-Graph 2.0 (ToG-2), a hybrid RAG framework that iteratively retrieves information from both unstructured and structured knowledge sources in a tight-coupling manner. Specifically, ToG-2 leverages knowledge graphs (KGs) to link documents via entities, facilitating deep and knowledge-guided context retrieval. Simultaneously, it utilizes documents as entity contexts to achieve precise and efficient graph retrieval. ToG-2 alternates between graph retrieval and context retrieval to search for in-depth clues relevant to the question, enabling LLMs to generate answers. We conduct a series of well-designed experiments to highlight the following advantages of ToG-2: 1) ToG-2 tightly couples the processes of context retrieval and graph retrieval, deepening context retrieval via the KG while enabling reliable graph retrieval based on contexts; 2) it achieves deep and faithful reasoning in LLMs through an iterative knowledge retrieval process of collaboration between contexts and the KG; and 3) ToG-2 is training-free and plug-and-play compatible with various LLMs. Extensive experiments demonstrate that ToG-2 achieves overall state-of-the-art (SOTA) performance on 6 out of 7 knowledge-intensive datasets with GPT-3.5, and can elevate the performance of smaller models (e.g., LLAMA-2-13B) to the level of GPT-3.5's direct reasoning. The source code is available on https://github.com/IDEA-FinAI/ToG-2.",
    "venue": "",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-07-15",
    "authors": [
      {
        "authorId": "2311556497",
        "name": "Shengjie Ma"
      },
      {
        "authorId": "2250617116",
        "name": "Chengjin Xu"
      },
      {
        "authorId": "144267788",
        "name": "Xuhui Jiang"
      },
      {
        "authorId": "2307043422",
        "name": "Muzhi Li"
      },
      {
        "authorId": "2311383154",
        "name": "Huaren Qu"
      },
      {
        "authorId": "2307038391",
        "name": "Cehao Yang"
      },
      {
        "authorId": "2289744328",
        "name": "Jiaxin Mao"
      },
      {
        "authorId": "2284217200",
        "name": "Jian Guo"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "0ca610814ebb7f775d89b012d0cae4703230507d",
    "url": "https://www.semanticscholar.org/paper/0ca610814ebb7f775d89b012d0cae4703230507d",
    "title": "Unlocking Multi-View Insights in Knowledge-Dense Retrieval-Augmented Generation",
    "abstract": "While Retrieval-Augmented Generation (RAG) plays a crucial role in the application of Large Language Models (LLMs), existing retrieval methods in knowledge-dense domains like law and medicine still suffer from a lack of multi-perspective views, which are essential for improving interpretability and reliability. Previous research on multi-view retrieval often focused solely on different semantic forms of queries, neglecting the expression of specific domain knowledge perspectives. This paper introduces a novel multi-view RAG framework, MVRAG, tailored for knowledge-dense domains that utilizes intention-aware query rewriting from multiple domain viewpoints to enhance retrieval precision, thereby improving the effectiveness of the final inference. Experiments conducted on legal and medical case retrieval demonstrate significant improvements in recall and precision rates with our framework. Our multi-perspective retrieval approach unleashes the potential of multi-view information enhancing RAG tasks, accelerating the further application of LLMs in knowledge-intensive fields.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-19",
    "authors": [
      {
        "authorId": "2297759102",
        "name": "Guanhua Chen"
      },
      {
        "authorId": "2298210228",
        "name": "Wenhan Yu"
      },
      {
        "authorId": "2297670834",
        "name": "Lei Sha"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "fc24091a9b44953c71084449b4fa0eabfb58abb3",
    "url": "https://www.semanticscholar.org/paper/fc24091a9b44953c71084449b4fa0eabfb58abb3",
    "title": "KnowPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models",
    "abstract": "By integrating external knowledge, Retrieval-Augmented Generation (RAG) has become an effective strategy for mitigating the hallucination problems that large language models (LLMs) encounter when dealing with knowledge-intensive tasks. However, in the process of integrating external non-parametric supporting evidence with internal parametric knowledge, inevitable knowledge conflicts may arise, leading to confusion in the model's responses. To enhance the knowledge selection of LLMs in various contexts, some research has focused on refining their behavior patterns through instruction-tuning. Nonetheless, due to the absence of explicit negative signals and comparative objectives, models fine-tuned in this manner may still exhibit undesirable behaviors such as contextual ignorance and contextual overinclusion. To this end, we propose a Knowledge-aware Preference Optimization strategy, dubbed KnowPO, aimed at achieving adaptive knowledge selection based on contextual relevance in real retrieval scenarios. Concretely, we proposed a general paradigm for constructing knowledge conflict datasets, which comprehensively cover various error types and learn how to avoid these negative signals through preference optimization methods. Simultaneously, we proposed a rewriting strategy and data ratio optimization strategy to address preference imbalances. Experimental results show that KnowPO outperforms previous methods for handling knowledge conflicts by over 37\\%, while also exhibiting robust generalization across various out-of-distribution datasets.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-06",
    "authors": [
      {
        "authorId": "2276704515",
        "name": "Ruizhe Zhang"
      },
      {
        "authorId": "2215472732",
        "name": "Yongxin Xu"
      },
      {
        "authorId": "2315391190",
        "name": "Yuzhen Xiao"
      },
      {
        "authorId": "2315018080",
        "name": "Runchuan Zhu"
      },
      {
        "authorId": "2181297535",
        "name": "Xinke Jiang"
      },
      {
        "authorId": "2276425819",
        "name": "Xu Chu"
      },
      {
        "authorId": "2145804723",
        "name": "Junfeng Zhao"
      },
      {
        "authorId": "2253831765",
        "name": "Yasha Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "307f5ddd10d6e25b23f9a75ace1dd7d439a2f498",
    "url": "https://www.semanticscholar.org/paper/307f5ddd10d6e25b23f9a75ace1dd7d439a2f498",
    "title": "ActiveRAG: Autonomously Knowledge Assimilation and Accommodation through Retrieval-Augmented Agents",
    "abstract": "Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to leverage external knowledge, enhancing their performance on knowledge-intensive tasks. However, existing RAG models often treat LLMs as passive recipients of information, which can lead to interference from noisy retrieved content. In this paper, we introduce ActiveRAG, a multi-agent framework that mimics human learning behavior to help LLMs actively engage with and learn from retrieved evidence. ActiveRAG designs a knowledge assimilation agent to form the knowledge understanding by associating external knowledge with the parametric memory of LLMs. Then our model employs the thought accommodation agent to calibrate the internal thought of LLMs for response refinement. Our experiments show that ActiveRAG achieves a 10\\% improvement over vanilla RAG on various question-answering benchmarks. Further analysis reveals that ActiveRAG mitigates the impact of noisy retrievals, alleviates conflicts between external knowledge and parametric memory and improves the self-consistency of LLMs in answering the question. All data and codes are available at https://github.com/OpenMatch/ActiveRAG.",
    "venue": "",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-02-21",
    "authors": [
      {
        "authorId": "2284986197",
        "name": "Zhipeng Xu"
      },
      {
        "authorId": "49047064",
        "name": "Zhenghao Liu"
      },
      {
        "authorId": "2277242040",
        "name": "Yukun Yan"
      },
      {
        "authorId": "2267033597",
        "name": "Shuo Wang"
      },
      {
        "authorId": "2314785970",
        "name": "Shi Yu"
      },
      {
        "authorId": "1633538428",
        "name": "Zheni Zeng"
      },
      {
        "authorId": "51131083",
        "name": "Chaojun Xiao"
      },
      {
        "authorId": "2290295914",
        "name": "Zhiyuan Liu"
      },
      {
        "authorId": "2204644192",
        "name": "Ge Yu"
      },
      {
        "authorId": "2139787803",
        "name": "Chenyan Xiong"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "d47b9407cf0938e9f15ce019031d7cd63b308a01",
    "url": "https://www.semanticscholar.org/paper/d47b9407cf0938e9f15ce019031d7cd63b308a01",
    "title": "Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks",
    "abstract": "Making the contents generated by Large Language Model (LLM), accurate, credible and traceable is crucial, especially in complex knowledge-intensive tasks that require multi-step reasoning and each step needs knowledge to solve. Retrieval-augmented generation is good potential to solve this problem. However, where and how to introduce Information Retrieval (IR) to LLM is a big challenge. Previous work has the problems that wrong knowledge retrieved by IR misleads the LLM and interaction between IR and LLM breaks the reasoning chain of LLM. This paper proposes a novel framework named Search-in-the-Chain (SearChain) for the interaction between LLM and IR to solve the challenges. First, LLM generates the reasoning chain named Chain-of-Query (CoQ) where each node consists of an IR-oriented query-answer pair. Second, IR verifies the answer of each node of CoQ. It corrects the answer that is not consistent with the retrieved information when IR gives high confidence, which improves the credibility. Third, LLM can indicate its missing knowledge in CoQ and rely on IR to provide this knowledge to LLM. These operations improve the accuracy in terms of reasoning and knowledge. Finally, SearChain generates the reasoning process and marks references to supporting documents for each reasoning step, which improves traceability. Interaction with IR in SearChain forms a novel reasoning path based on a tree, which enables LLM to dynamically modify the direction of reasoning. Experiments show that SearChain outperforms state-of-the-art baselines on complex knowledge-intensive tasks including multi-hop Q&A, slot filling, fact checking, and long-form Q&A.",
    "venue": "The Web Conference",
    "year": 2023,
    "citationCount": 23,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2304.14732",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-04-28",
    "authors": [
      {
        "authorId": "2202745",
        "name": "Shicheng Xu"
      },
      {
        "authorId": "2111815778",
        "name": "Liang Pang"
      },
      {
        "authorId": "2476503",
        "name": "Huawei Shen"
      },
      {
        "authorId": "2110251463",
        "name": "Xueqi Cheng"
      },
      {
        "authorId": "2281747744",
        "name": "Tat-Seng Chua"
      }
    ],
    "source": "semantic_scholar",
    "score": 117.67080745521918
  },
  {
    "paperId": "88884b8806262a4095036041e3567d450dba39f7",
    "url": "https://www.semanticscholar.org/paper/88884b8806262a4095036041e3567d450dba39f7",
    "title": "Active Retrieval Augmented Generation",
    "abstract": "Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 179,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.06983",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-11",
    "authors": [
      {
        "authorId": "2669515",
        "name": "Zhengbao Jiang"
      },
      {
        "authorId": "40027632",
        "name": "Frank F. Xu"
      },
      {
        "authorId": "49715441",
        "name": "Luyu Gao"
      },
      {
        "authorId": "48064856",
        "name": "Zhiqing Sun"
      },
      {
        "authorId": "1409707585",
        "name": "Qian Liu"
      },
      {
        "authorId": "2173509991",
        "name": "Jane Dwivedi-Yu"
      },
      {
        "authorId": "46286308",
        "name": "Yiming Yang"
      },
      {
        "authorId": "144987107",
        "name": "Jamie Callan"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ],
    "source": "semantic_scholar",
    "score": 147.89435276335314
  },
  {
    "paperId": "1cc6cc4960f7df59e7813d9a8e11098d0a0d0720",
    "url": "https://www.semanticscholar.org/paper/1cc6cc4960f7df59e7813d9a8e11098d0a0d0720",
    "title": "DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models",
    "abstract": "Dynamic retrieval augmented generation (RAG) paradigm actively decides when and what to retrieve during the text generation process of Large Language Models (LLMs). There are two key elements of this paradigm: identifying the optimal moment to activate the retrieval module (deciding when to retrieve) and crafting the appropriate query once retrieval is triggered (determining what to retrieve). However, current dynamic RAG methods fall short in both aspects. Firstly, the strategies for deciding when to retrieve often rely on static rules. Moreover, the strategies for deciding what to retrieve typically limit themselves to the LLM's most recent sentence or the last few tokens, while the LLM's real-time information needs may span across the entire context. To overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic Retrieval Augmented Generation based on the real-time Information Needs of LLMs. Our framework is specifically designed to make decisions on when and what to retrieve based on the LLM's real-time information needs during the text generation process. We evaluate DRAGIN along with existing methods comprehensively over 4 knowledge-intensive generation datasets. Experimental results show that DRAGIN achieves superior performance on all tasks, demonstrating the effectiveness of our method. We have open-sourced all the code, data, and models in GitHub: https://github.com/oneal2000/DRAGIN/tree/main",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 14,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-15",
    "authors": [
      {
        "authorId": "2147219374",
        "name": "Weihang Su"
      },
      {
        "authorId": "2291993837",
        "name": "Yichen Tang"
      },
      {
        "authorId": "2256982003",
        "name": "Qingyao Ai"
      },
      {
        "authorId": "47039225",
        "name": "Zhijing Wu"
      },
      {
        "authorId": "2260835922",
        "name": "Yiqun Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "8201d0c97667eb11711b1eb580d0a76e179b4268",
    "url": "https://www.semanticscholar.org/paper/8201d0c97667eb11711b1eb580d0a76e179b4268",
    "title": "Retrieval-Augmented Generation (RAG) and LLM Integration",
    "abstract": "Advances in Natural Language Processing (NLP) have led to the emergence of complex structures such as Large Language Models (LLM). LLMs are highly successful in understanding the subtleties of language and processing context by being trained on large datasets. However, the difficulties encountered in Information Retrieval (IR) processes have created an awareness that these models are not sufficient on their own. Traditional IR methods have generally been insufficient in understanding the complexity of natural language in responding to specific queries and retrieving appropriate information from documents or databases. Since this process is based only on keywords, it cannot fully capture the semantic meaning of the language. For this reason, it has been necessary to go beyond traditional IR methods for more precise information creation based on context and meaning. As a result of these requirements, the Retrieval-Augmented Generation (RAG) architecture has come to the fore. RAG offers the ability to create richer and contextually meaningful answers to user queries by integrating LLMs with information retrieval processes. This architecture allows the language model to instantly access external information sources; thus, it generates more accurate and contextual responses armed with existing information. These features of RAG provide appropriate solutions to users’ information-based demands by better understanding the complexity of natural language. In this study, it is emphasized that the integration of RAG architecture with information retrieval systems and LLMs provides more sensitive and accurate solutions in information-intensive tasks. This study emphasizes that the RAG architecture’s ability to retrieve information by dynamically using the learnings obtained from large datasets of LLMs strengthens applications in the field of NLP.",
    "venue": "International Service Availability Symposium",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": "2024-12-06",
    "authors": [
      {
        "authorId": "2278759076",
        "name": "Büşra Tural"
      },
      {
        "authorId": "2281134063",
        "name": "Zeynep Örpek"
      },
      {
        "authorId": "2326126993",
        "name": "Zeynep Destan"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "fc74fe92fb9e34d3ae1237bf9a6e718c723f4f3e",
    "url": "https://www.semanticscholar.org/paper/fc74fe92fb9e34d3ae1237bf9a6e718c723f4f3e",
    "title": "FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation",
    "abstract": "Retrieval-augmented generation models offer many benefits over standalone language models: besides a textual answer to a given query they provide provenance items retrieved from an updateable knowledge base. However, they are also more complex systems and need to handle long inputs. In this work, we introduce FiD-Light to strongly increase the efficiency of the state-of-the-art retrieval-augmented FiD model, while maintaining the same level of effectiveness. Our FiD-Light model constrains the information flow from the encoder (which encodes passages separately) to the decoder (using concatenated encoded representations). Furthermore, we adapt FiD-Light with re-ranking capabilities through textual source pointers, to improve the top-ranked provenance precision. Our experiments on a diverse set of seven knowledge intensive tasks (KILT) show FiD-Light consistently improves the Pareto frontier between query latency and effectiveness. FiD-Light with source pointing sets substantial new state-of-the-art results on six KILT tasks for combined text generation and provenance retrieval evaluation, while maintaining high efficiency.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2022,
    "citationCount": 66,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2209.14290",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2022-09-28",
    "authors": [
      {
        "authorId": "97393346",
        "name": "Sebastian Hofstätter"
      },
      {
        "authorId": "2809410",
        "name": "Jiecao Chen"
      },
      {
        "authorId": "2062947723",
        "name": "K. Raman"
      },
      {
        "authorId": "2499986",
        "name": "Hamed Zamani"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.07038929086448
  },
  {
    "paperId": "7581b453d9341173eb08961cc48053e57bd0cdb5",
    "url": "https://www.semanticscholar.org/paper/7581b453d9341173eb08961cc48053e57bd0cdb5",
    "title": "Supportiveness-based Knowledge Rewriting for Retrieval-augmented Language Modeling",
    "abstract": "Retrieval-augmented language models (RALMs) have recently shown great potential in mitigating the limitations of implicit knowledge in LLMs, such as untimely updating of the latest expertise and unreliable retention of long-tail knowledge. However, since the external knowledge base, as well as the retriever, can not guarantee reliability, potentially leading to the knowledge retrieved not being helpful or even misleading for LLM generation. In this paper, we introduce Supportiveness-based Knowledge Rewriting (SKR), a robust and pluggable knowledge rewriter inherently optimized for LLM generation. Specifically, we introduce the novel concept of\"supportiveness\"--which represents how effectively a knowledge piece facilitates downstream tasks--by considering the perplexity impact of augmented knowledge on the response text of a white-box LLM. Based on knowledge supportiveness, we first design a training data curation strategy for our rewriter model, effectively identifying and filtering out poor or irrelevant rewrites (e.g., with low supportiveness scores) to improve data efficacy. We then introduce the direct preference optimization (DPO) algorithm to align the generated rewrites to optimal supportiveness, guiding the rewriter model to summarize augmented content that better improves the final response. Comprehensive evaluations across six popular knowledge-intensive tasks and four LLMs have demonstrated the effectiveness and superiority of SKR. With only 7B parameters, SKR has shown better knowledge rewriting capability over GPT-4, the current state-of-the-art general-purpose LLM.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-12",
    "authors": [
      {
        "authorId": "2184030467",
        "name": "Zile Qiao"
      },
      {
        "authorId": "145235149",
        "name": "Wei Ye"
      },
      {
        "authorId": "2256747040",
        "name": "Yong Jiang"
      },
      {
        "authorId": "2187454345",
        "name": "Tong Mo"
      },
      {
        "authorId": "35930962",
        "name": "Pengjun Xie"
      },
      {
        "authorId": "2139261376",
        "name": "Weiping Li"
      },
      {
        "authorId": "2276428076",
        "name": "Fei Huang"
      },
      {
        "authorId": "1705434",
        "name": "Shikun Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "59d440de6378de10bf8da3995e64c70be23aa88e",
    "url": "https://www.semanticscholar.org/paper/59d440de6378de10bf8da3995e64c70be23aa88e",
    "title": "Visually-augmented pretrained language models for NLP tasks without images",
    "abstract": "Although pre-trained language models (PLMs) have shown impressive performance by text-only self-supervised training, they are found lack of visual semantics or commonsense. Existing solutions often rely on explicit images for visual knowledge augmentation (requiring time-consuming retrieval or generation), and they also conduct the augmentation for the whole input text, without considering whether it is actually needed in specific inputs or tasks. To address these issues, we propose a novel **V**isually-**A**ugmented fine-tuning approach that can be generally applied to various PLMs or NLP tasks, **W**ithout using any retrieved or generated **I**mages, namely **VAWI**. Experimental results show that our approach can consistently improve the performance of BERT, RoBERTa, BART, and T5 at different scales, and outperform several competitive baselines on ten tasks. Our codes and data are publicly available at https://github.com/RUCAIBox/VAWI.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2212.07937",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-12-15",
    "authors": [
      {
        "authorId": "138404610",
        "name": "Hangyu Guo"
      },
      {
        "authorId": "1423651904",
        "name": "Kun Zhou"
      },
      {
        "authorId": "2542603",
        "name": "Wayne Xin Zhao"
      },
      {
        "authorId": "2190563903",
        "name": "Qinyu Zhang"
      },
      {
        "authorId": "153693432",
        "name": "Ji-rong Wen"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "08dd6f6adf5657516d53db1946780a5d31a10676",
    "url": "https://www.semanticscholar.org/paper/08dd6f6adf5657516d53db1946780a5d31a10676",
    "title": "Retrieval-Generation Synergy Augmented Large Language Models",
    "abstract": "Large language models augmented with task-relevant documents have demonstrated impressive performance on knowledge-intensive tasks. However, regarding how to obtain effective documents, the existing methods are mainly divided into two categories. One is to retrieve from an external knowledge base, and the other is to utilize large language models to generate documents. We propose an iterative retrieval-generation collaborative framework. It is not only able to leverage both parametric and non-parametric knowledge, but also helps to find the correct reasoning path through retrieval-generation interactions, which is very important for tasks that require multi-step reasoning. We conduct experiments on four question answering datasets, including single-hop QA and multi-hop QA tasks. Empirical results show that our method significantly improves the reasoning ability of large language models and outperforms previous baselines.",
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year": 2023,
    "citationCount": 23,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.05149",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-08",
    "authors": [
      {
        "authorId": "51056532",
        "name": "Zhangyin Feng"
      },
      {
        "authorId": "2674998",
        "name": "Xiaocheng Feng"
      },
      {
        "authorId": "2258097320",
        "name": "Dezhi Zhao"
      },
      {
        "authorId": "2257128653",
        "name": "Maojin Yang"
      },
      {
        "authorId": "2257004102",
        "name": "Bing Qin"
      }
    ],
    "source": "semantic_scholar",
    "score": 123.67080745521918
  },
  {
    "paperId": "75e59a0b5173c9e7bd6fb89e457884984dbc9ab1",
    "url": "https://www.semanticscholar.org/paper/75e59a0b5173c9e7bd6fb89e457884984dbc9ab1",
    "title": "Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling",
    "abstract": "This paper studies multi-task training of retrieval-augmented generation models for knowledge-intensive tasks. We propose to clean the training set by utilizing a distinct property of knowledge-intensive generation: The connection of query-answer pairs to items in the knowledge base. We filter training examples via a threshold of confidence on the relevance labels, whether a pair is answerable by the knowledge base or not. We train a single Fusion-in-Decoder (FiD) generator on seven combined tasks of the KILT benchmark. The experimental results suggest that our simple yet effective approach substantially improves competitive baselines on two strongly imbalanced tasks; and shows either smaller improvements or no significant regression on the remaining tasks. Furthermore, we demonstrate our multi-task training with relevance label sampling scales well with increased model capacity and achieves state-of-the-art results in five out of seven KILT tasks.",
    "venue": "arXiv.org",
    "year": 2022,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2207.03030",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-07-07",
    "authors": [
      {
        "authorId": "97393346",
        "name": "Sebastian Hofstätter"
      },
      {
        "authorId": "2809410",
        "name": "Jiecao Chen"
      },
      {
        "authorId": "2062947723",
        "name": "K. Raman"
      },
      {
        "authorId": "2499986",
        "name": "Hamed Zamani"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "e6ce185330c00e3f05171bf107df03c88e2a4875",
    "url": "https://www.semanticscholar.org/paper/e6ce185330c00e3f05171bf107df03c88e2a4875",
    "title": "KGI: An Integrated Framework for Knowledge Intensive Language Tasks",
    "abstract": "In this paper, we present a system to showcase the capabilities of the latest state-of-the-art retrieval augmented generation models trained on knowledge-intensive language tasks, such as slot filling, open domain question answering, dialogue, and fact-checking. Moreover, given a user query, we show how the output from these different models can be combined to cross-examine the outputs of each other. Particularly, we show how accuracy in dialogue can be improved using the question answering model. We are also releasing all models used in the demo as a contribution of this paper. A short video demonstrating the system is available at https://ibm.box.com/v/emnlp2022-demo.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2204.03985",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-04-08",
    "authors": [
      {
        "authorId": "8576392",
        "name": "Md. Faisal Mahbub Chowdhury"
      },
      {
        "authorId": "143742133",
        "name": "Michael R. Glass"
      },
      {
        "authorId": "3415700",
        "name": "Gaetano Rossiello"
      },
      {
        "authorId": "1711133",
        "name": "A. Gliozzo"
      },
      {
        "authorId": "2689774",
        "name": "Nandana Mihindukulasooriya"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "61a955799c7429efabc46a60812ec13cb1b3fc26",
    "url": "https://www.semanticscholar.org/paper/61a955799c7429efabc46a60812ec13cb1b3fc26",
    "title": "Chain-of-Retrieval Augmented Generation",
    "abstract": "This paper introduces an approach for training o1-like RAG models that retrieve and reason over relevant information step by step before generating the final answer. Conventional RAG methods usually perform a single retrieval step before the generation process, which limits their effectiveness in addressing complex queries due to imperfect retrieval results. In contrast, our proposed method, CoRAG (Chain-of-Retrieval Augmented Generation), allows the model to dynamically reformulate the query based on the evolving state. To train CoRAG effectively, we utilize rejection sampling to automatically generate intermediate retrieval chains, thereby augmenting existing RAG datasets that only provide the correct final answer. At test time, we propose various decoding strategies to scale the model's test-time compute by controlling the length and number of sampled retrieval chains. Experimental results across multiple benchmarks validate the efficacy of CoRAG, particularly in multi-hop question answering tasks, where we observe more than 10 points improvement in EM score compared to strong baselines. On the KILT benchmark, CoRAG establishes a new state-of-the-art performance across a diverse range of knowledge-intensive tasks. Furthermore, we offer comprehensive analyses to understand the scaling behavior of CoRAG, laying the groundwork for future research aimed at developing factual and grounded foundation models.",
    "venue": "",
    "year": 2025,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2025-01-24",
    "authors": [
      {
        "authorId": "145769448",
        "name": "Liang Wang"
      },
      {
        "authorId": "2280839621",
        "name": "Haonan Chen"
      },
      {
        "authorId": "2242947624",
        "name": "Nan Yang"
      },
      {
        "authorId": "2116768132",
        "name": "Xiaolong Huang"
      },
      {
        "authorId": "2273086037",
        "name": "Zhicheng Dou"
      },
      {
        "authorId": "2257346447",
        "name": "Furu Wei"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "32c260ddd7e2d0b05c58f2e67d244b8036699db4",
    "url": "https://www.semanticscholar.org/paper/32c260ddd7e2d0b05c58f2e67d244b8036699db4",
    "title": "C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models",
    "abstract": "Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk functions under test distribution shifts. We prove that RAG achieves a lower conformal generation risk than that of a single LLM when the quality of the retrieval model and transformer is non-trivial. Our intensive empirical results demonstrate the soundness and tightness of our conformal generation risk guarantees across four widely-used NLP datasets on four state-of-the-art retrieval models.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 19,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-05",
    "authors": [
      {
        "authorId": "2153110066",
        "name": "Mintong Kang"
      },
      {
        "authorId": "51274710",
        "name": "Nezihe Merve Gurel"
      },
      {
        "authorId": "2282538184",
        "name": "Ning Yu"
      },
      {
        "authorId": "2242706269",
        "name": "D. Song"
      },
      {
        "authorId": "2267398406",
        "name": "Bo Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.93598410330986
  },
  {
    "paperId": "c58f30c054c5b48e5e4a0d1c3a20793583effa22",
    "url": "https://www.semanticscholar.org/paper/c58f30c054c5b48e5e4a0d1c3a20793583effa22",
    "title": "KITTEN: A Knowledge-Intensive Evaluation of Image Generation on Visual Entities",
    "abstract": "Recent advancements in text-to-image generation have significantly enhanced the quality of synthesized images. Despite this progress, evaluations predominantly focus on aesthetic appeal or alignment with text prompts. Consequently, there is limited understanding of whether these models can accurately represent a wide variety of realistic visual entities - a task requiring real-world knowledge. To address this gap, we propose a benchmark focused on evaluating Knowledge-InTensive image generaTion on real-world ENtities (i.e., KITTEN). Using KITTEN, we conduct a systematic study on the fidelity of entities in text-to-image generation models, focusing on their ability to generate a wide range of real-world visual entities, such as landmark buildings, aircraft, plants, and animals. We evaluate the latest text-to-image models and retrieval-augmented customization models using both automatic metrics and carefully-designed human evaluations, with an emphasis on the fidelity of entities in the generated images. Our findings reveal that even the most advanced text-to-image models often fail to generate entities with accurate visual details. Although retrieval-augmented models can enhance the fidelity of entity by incorporating reference images during testing, they often over-rely on these references and struggle to produce novel configurations of the entity as requested in creative text prompts.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-15",
    "authors": [
      {
        "authorId": "2115636909",
        "name": "Hsin-Ping Huang"
      },
      {
        "authorId": "2326171546",
        "name": "Xinyi Wang"
      },
      {
        "authorId": "1938499056",
        "name": "Yonatan Bitton"
      },
      {
        "authorId": "51258885",
        "name": "Hagai Taitelbaum"
      },
      {
        "authorId": "32012022",
        "name": "Gaurav Singh Tomar"
      },
      {
        "authorId": "2277809314",
        "name": "Ming-Wei Chang"
      },
      {
        "authorId": "2269764175",
        "name": "Xuhui Jia"
      },
      {
        "authorId": "2269731135",
        "name": "Kelvin C.K. Chan"
      },
      {
        "authorId": "2307548497",
        "name": "Hexiang Hu"
      },
      {
        "authorId": "2269866136",
        "name": "Yu-Chuan Su"
      },
      {
        "authorId": "2297139110",
        "name": "Ming-Hsuan Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "d8cbe0bfcc9c2eb1714c0e5be18a6ecaf50ea7a5",
    "url": "https://www.semanticscholar.org/paper/d8cbe0bfcc9c2eb1714c0e5be18a6ecaf50ea7a5",
    "title": "Improving Retrieval Augmented Language Model with Self-Reasoning",
    "abstract": "The Retrieval-Augmented Language Model (RALM) has shown remarkable performance on knowledge-intensive tasks by incorporating external knowledge during inference, which mitigates the factual hallucinations inherited in large language models (LLMs). Despite these advancements, challenges persist in the implementation of RALMs, particularly concerning their reliability and traceability. To be specific, the irrelevant document retrieval may result in unhelpful response generation or even deteriorate the performance of LLMs, while the lack of proper citations in generated outputs complicates efforts to verify the trustworthiness of the models. To this end, we propose a novel self-reasoning framework aimed at improving the reliability and traceability of RALMs, whose core idea is to leverage reasoning trajectories generated by the LLM itself. The framework involves constructing self-reason trajectories with three processes: a relevance-aware process, an evidence-aware selective process, and a trajectory analysis process. We have evaluated our framework across four public datasets (two short-form QA datasets, one long-form QA dataset, and one fact verification dataset) to demonstrate the superiority of our method, which can outperform existing state-of-the-art models and can achieve comparable performance with GPT-4, while only using 2,000 training samples.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-29",
    "authors": [
      {
        "authorId": "2111275391",
        "name": "Yuan Xia"
      },
      {
        "authorId": "2314078488",
        "name": "Jingbo Zhou"
      },
      {
        "authorId": "18083770",
        "name": "Zhenhui Shi"
      },
      {
        "authorId": "2153418695",
        "name": "Jun Chen"
      },
      {
        "authorId": "30898812",
        "name": "Hai-ting Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "4937ceaada8bc730949a08e5e9c908f24429643e",
    "url": "https://www.semanticscholar.org/paper/4937ceaada8bc730949a08e5e9c908f24429643e",
    "title": "Retrieval Augmented Correction of Named Entity Speech Recognition Errors",
    "abstract": "In recent years, end-to-end automatic speech recognition (ASR) systems have proven themselves remarkably accurate and performant, but these systems still have a significant error rate for entity names which appear infrequently in their training data. In parallel to the rise of end-to-end ASR systems, large language models (LLMs) have proven to be a versatile tool for various natural language processing (NLP) tasks. In NLP tasks where a database of relevant knowledge is available, retrieval augmented generation (RAG) has achieved impressive results when used with LLMs. In this work, we propose a RAG-like technique for correcting speech recognition entity name errors. Our approach uses a vector database to index a set of relevant entities. At runtime, database queries are generated from possibly errorful textual ASR hypotheses, and the entities retrieved using these queries are fed, along with the ASR hypotheses, to an LLM which has been adapted to correct ASR errors. Overall, our best system achieves 33%-39% relative word error rate reductions on synthetic test sets focused on voice assistant queries of rare music entities without regressing on the STOP test set, a publicly available voice assistant test set covering many domains.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-09",
    "authors": [
      {
        "authorId": "3093199",
        "name": "Ernest Pusateri"
      },
      {
        "authorId": "2320468380",
        "name": "Anmol Walia"
      },
      {
        "authorId": "40381032",
        "name": "Anirudh Kashi"
      },
      {
        "authorId": "153100367",
        "name": "Bortik Bandyopadhyay"
      },
      {
        "authorId": "2320457869",
        "name": "Nadia Hyder"
      },
      {
        "authorId": "1477288773",
        "name": "Sayantan Mahinder"
      },
      {
        "authorId": "1666320488",
        "name": "R. Anantha"
      },
      {
        "authorId": "2320470687",
        "name": "Daben Liu"
      },
      {
        "authorId": "2050160515",
        "name": "Sashank Gondala"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "e30b976d4c7d9d023dcef102a360d7305bc6a32b",
    "url": "https://www.semanticscholar.org/paper/e30b976d4c7d9d023dcef102a360d7305bc6a32b",
    "title": "Tool Calling: Enhancing Medication Consultation via Retrieval-Augmented Large Language Models",
    "abstract": "Large-scale language models (LLMs) have achieved remarkable success across various language tasks but suffer from hallucinations and temporal misalignment. To mitigate these shortcomings, Retrieval-augmented generation (RAG) has been utilized to provide external knowledge to facilitate the answer generation. However, applying such models to the medical domain faces several challenges due to the lack of domain-specific knowledge and the intricacy of real-world scenarios. In this study, we explore LLMs with RAG framework for knowledge-intensive tasks in the medical field. To evaluate the capabilities of LLMs, we introduce MedicineQA, a multi-round dialogue benchmark that simulates the real-world medication consultation scenario and requires LLMs to answer with retrieved evidence from the medicine database. MedicineQA contains 300 multi-round question-answering pairs, each embedded within a detailed dialogue history, highlighting the challenge posed by this knowledge-intensive task to current LLMs. We further propose a new \\textit{Distill-Retrieve-Read} framework instead of the previous \\textit{Retrieve-then-Read}. Specifically, the distillation and retrieval process utilizes a tool calling mechanism to formulate search queries that emulate the keyword-based inquiries used by search engines. With experimental results, we show that our framework brings notable performance improvements and surpasses the previous counterparts in the evidence retrieval process in terms of evidence retrieval accuracy. This advancement sheds light on applying RAG to the medical domain.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-27",
    "authors": [
      {
        "authorId": "2164396303",
        "name": "Zhongzhen Huang"
      },
      {
        "authorId": "145848004",
        "name": "Kui Xue"
      },
      {
        "authorId": "2298953258",
        "name": "Yongqi Fan"
      },
      {
        "authorId": "2298907225",
        "name": "Linjie Mu"
      },
      {
        "authorId": "2298908741",
        "name": "Ruoyu Liu"
      },
      {
        "authorId": "2287928656",
        "name": "Tong Ruan"
      },
      {
        "authorId": "2267720309",
        "name": "Shaoting Zhang"
      },
      {
        "authorId": "2288039293",
        "name": "Xiaofan Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "fc1ffbd2e2bf7b90a30f17dbc93c3d3f43ff7805",
    "url": "https://www.semanticscholar.org/paper/fc1ffbd2e2bf7b90a30f17dbc93c3d3f43ff7805",
    "title": "Unveiling and Consulting Core Experts in Retrieval-Augmented MoE-based LLMs",
    "abstract": "Retrieval-Augmented Generation (RAG) significantly improved the ability of Large Language Models (LLMs) to solve knowledge-intensive tasks. While existing research seeks to enhance RAG performance by retrieving higher-quality documents or designing RAG-specific LLMs, the internal mechanisms within LLMs that contribute to RAG’s effectiveness remain underexplored. In this paper, we aim to investigate these internal mechanisms within the popular Mixture-of-Expert (MoE)-based LLMs and demonstrate how to improve RAG by examining expert activations in these LLMs. Our controlled experiments reveal that several core groups of experts are primarily responsible for RAG-related behaviors. The activation of these core experts can signify the model’s inclination towards external/internal knowledge and adjust its behavior. For instance, we identify core experts that can (1) indicate the sufficiency of the model’s internal knowledge, (2) assess the quality of retrieved documents, and (3) enhance the model’s ability to utilize context. Based on these findings, we propose several strategies to enhance RAG’s efficiency and effectiveness through expert activation. Experimental results across various datasets and MoE LLMs show the effectiveness of our method.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-10-20",
    "authors": [
      {
        "authorId": "2148927523",
        "name": "Xin Zhou"
      },
      {
        "authorId": "2295731825",
        "name": "Ping Nie"
      },
      {
        "authorId": "2326995130",
        "name": "Yiwen Guo"
      },
      {
        "authorId": "2316804823",
        "name": "Haojie Wei"
      },
      {
        "authorId": "2316516702",
        "name": "Zhanqiu Zhang"
      },
      {
        "authorId": "2294362638",
        "name": "Pasquale Minervini"
      },
      {
        "authorId": "151482614",
        "name": "Ruotian Ma"
      },
      {
        "authorId": "2067331064",
        "name": "Tao Gui"
      },
      {
        "authorId": "2257376355",
        "name": "Qi Zhang"
      },
      {
        "authorId": "2257129989",
        "name": "Xuanjing Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "23760b6f0b2daaf71108ebb7dca3b83763f83149",
    "url": "https://www.semanticscholar.org/paper/23760b6f0b2daaf71108ebb7dca3b83763f83149",
    "title": "Learning to Rank for Multiple Retrieval-Augmented Models through Iterative Utility Maximization",
    "abstract": "This paper investigates the design of a unified search engine to serve multiple retrieval-augmented generation (RAG) agents, each with a distinct task, backbone large language model (LLM), and retrieval-augmentation strategy. We introduce an iterative approach where the search engine generates retrieval results for these RAG agents and gathers feedback on the quality of the retrieved documents during an offline phase. This feedback is then used to iteratively optimize the search engine using a novel expectation-maximization algorithm, with the goal of maximizing each agent's utility function. Additionally, we adapt this approach to an online setting, allowing the search engine to refine its behavior based on real-time individual agents feedback to better serve the results for each of them. Experiments on diverse datasets from the Knowledge-Intensive Language Tasks (KILT) benchmark demonstrates that our approach significantly on average outperforms competitive baselines across 18 RAG models. We also demonstrate that our method effectively ``personalizes'' the retrieval process for each RAG agent based on the collected feedback. Finally, we provide a comprehensive ablation study to explore various aspects of our method.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-13",
    "authors": [
      {
        "authorId": "2073044451",
        "name": "Alireza Salemi"
      },
      {
        "authorId": "2295731593",
        "name": "Hamed Zamani"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "c8f53481abdbf90bd40f642d6494242fecf976fb",
    "url": "https://www.semanticscholar.org/paper/c8f53481abdbf90bd40f642d6494242fecf976fb",
    "title": "RAVEN: Multitask Retrieval Augmented Vision-Language Learning",
    "abstract": "The scaling of large language models to encode all the world's knowledge in model parameters is unsustainable and has exacerbated resource barriers. Retrieval-Augmented Generation (RAG) presents a potential solution, yet its application to vision-language models (VLMs) is under explored. Existing methods focus on models designed for single tasks. Furthermore, they're limited by the need for resource intensive pre training, additional parameter requirements, unaddressed modality prioritization and lack of clear benefit over non-retrieval baselines. This paper introduces RAVEN, a multitask retrieval augmented VLM framework that enhances base VLMs through efficient, task specific fine-tuning. By integrating retrieval augmented samples without the need for additional retrieval-specific parameters, we show that the model acquires retrieval properties that are effective across multiple tasks. Our results and extensive ablations across retrieved modalities for the image captioning and VQA tasks indicate significant performance improvements compared to non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a +3\\% accuracy on specific VQA question types. This underscores the efficacy of applying RAG approaches to VLMs, marking a stride toward more efficient and accessible multimodal learning.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-27",
    "authors": [
      {
        "authorId": "2300845194",
        "name": "Varun Nagaraj Rao"
      },
      {
        "authorId": "2308471326",
        "name": "Siddharth Choudhary"
      },
      {
        "authorId": "2308471915",
        "name": "Aditya Deshpande"
      },
      {
        "authorId": "1710219",
        "name": "R. Satzoda"
      },
      {
        "authorId": "2261492087",
        "name": "Srikar Appalaraju"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "e3662a0e4a1bff43237c9596b2ecd7efcf0faeee",
    "url": "https://www.semanticscholar.org/paper/e3662a0e4a1bff43237c9596b2ecd7efcf0faeee",
    "title": "StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization",
    "abstract": "Retrieval-augmented generation (RAG) is a key means to effectively enhance large language models (LLMs) in many knowledge-based tasks. However, existing RAG methods struggle with knowledge-intensive reasoning tasks, because useful information required to these tasks are badly scattered. This characteristic makes it difficult for existing RAG methods to accurately identify key information and perform global reasoning with such noisy augmentation. In this paper, motivated by the cognitive theories that humans convert raw information into various structured knowledge when tackling knowledge-intensive reasoning, we proposes a new framework, StructRAG, which can identify the optimal structure type for the task at hand, reconstruct original documents into this structured format, and infer answers based on the resulting structure. Extensive experiments across various knowledge-intensive tasks show that StructRAG achieves state-of-the-art performance, particularly excelling in challenging scenarios, demonstrating its potential as an effective solution for enhancing LLMs in complex real-world applications.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-11",
    "authors": [
      {
        "authorId": "2292591497",
        "name": "Zhuoqun Li"
      },
      {
        "authorId": "49794910",
        "name": "Xuanang Chen"
      },
      {
        "authorId": "2288351000",
        "name": "Haiyang Yu"
      },
      {
        "authorId": "2116455765",
        "name": "Hongyu Lin"
      },
      {
        "authorId": "1831434",
        "name": "Yaojie Lu"
      },
      {
        "authorId": "2217344770",
        "name": "Qiaoyu Tang"
      },
      {
        "authorId": "2257407873",
        "name": "Fei Huang"
      },
      {
        "authorId": "2118233348",
        "name": "Xianpei Han"
      },
      {
        "authorId": "2110832778",
        "name": "Le Sun"
      },
      {
        "authorId": "2287833084",
        "name": "Yongbin Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "f99116659c7522941c2353f23bddd07251adaccc",
    "url": "https://www.semanticscholar.org/paper/f99116659c7522941c2353f23bddd07251adaccc",
    "title": "BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models",
    "abstract": "Retrieval augmentation addresses many critical problems in large language models such as hallucination, staleness, and privacy leaks. However, running retrieval-augmented language models (LMs) is slow and difficult to scale due to processing large amounts of retrieved text. We introduce binary token representations (BTR), which use 1-bit vectors to precompute every token in passages, significantly reducing computation during inference. Despite the potential loss of accuracy, our new calibration techniques and training objectives restore performance. Combined with offline and runtime compression, this only requires 127GB of disk space for encoding 3 billion tokens in Wikipedia. Our experiments show that on five knowledge-intensive NLP tasks, BTR accelerates state-of-the-art inference by up to 4x and reduces storage by over 100x while maintaining over 95% task performance.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.01329",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-02",
    "authors": [
      {
        "authorId": "2253406610",
        "name": "Qingqing Cao"
      },
      {
        "authorId": "48872685",
        "name": "Sewon Min"
      },
      {
        "authorId": "1705260",
        "name": "Yizhong Wang"
      },
      {
        "authorId": "2548384",
        "name": "Hannaneh Hajishirzi"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "c055baf5e8586edace51d1a7720de3c4ed345d72",
    "url": "https://www.semanticscholar.org/paper/c055baf5e8586edace51d1a7720de3c4ed345d72",
    "title": "RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models",
    "abstract": "Retrieval-augmented large language models (R-LLMs) combine pre-trained large language models (LLMs) with information retrieval systems to improve the accuracy of factual question-answering. However, current libraries for building R-LLMs provide high-level abstractions without sufficient transparency for evaluating and optimizing prompts within specific inference processes such as retrieval and generation. To address this gap, we present RaLLe, an open-source framework designed to facilitate the development, evaluation, and optimization of R-LLMs for knowledge-intensive tasks. With RaLLe, developers can easily develop and evaluate R-LLMs, improving hand-crafted prompts, assessing individual inference processes, and objectively measuring overall system performance quantitatively. By leveraging these features, developers can enhance the performance and accuracy of their R-LLMs in knowledge-intensive generation tasks. We open-source our code at https://github.com/yhoshi3/RaLLe.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.10633",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-08-21",
    "authors": [
      {
        "authorId": "2211101536",
        "name": "Yasuto Hoshi"
      },
      {
        "authorId": "2441156",
        "name": "D. Miyashita"
      },
      {
        "authorId": "20556792",
        "name": "Youyang Ng"
      },
      {
        "authorId": "2232784235",
        "name": "Kento Tatsuno"
      },
      {
        "authorId": "51194024",
        "name": "Yasuhiro Morioka"
      },
      {
        "authorId": "2422593",
        "name": "Osamu Torii"
      },
      {
        "authorId": "49192096",
        "name": "J. Deguchi"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "3406ab817245d9d56d0088820dfdfe882d696ecb",
    "url": "https://www.semanticscholar.org/paper/3406ab817245d9d56d0088820dfdfe882d696ecb",
    "title": "Opportunities for retrieval and tool augmented large language models in scientific facilities",
    "abstract": null,
    "venue": "npj Computational Materials",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-11-05",
    "authors": [
      {
        "authorId": "2269471269",
        "name": "Michael H. Prince"
      },
      {
        "authorId": "143777552",
        "name": "Henry Chan"
      },
      {
        "authorId": "66378777",
        "name": "Aikaterini Vriza"
      },
      {
        "authorId": "2303323070",
        "name": "T. Zhou"
      },
      {
        "authorId": "40135450",
        "name": "V. Sastry"
      },
      {
        "authorId": "2330167384",
        "name": "Yanqi Luo"
      },
      {
        "authorId": "91926828",
        "name": "M. Dearing"
      },
      {
        "authorId": "2255412073",
        "name": "Ross J. Harder"
      },
      {
        "authorId": "2262216487",
        "name": "Rama Vasudevan"
      },
      {
        "authorId": "12480512",
        "name": "M. Cherukara"
      }
    ],
    "source": "semantic_scholar",
    "score": 50
  },
  {
    "paperId": "d1932021a85b36144f6c355897b5ba55403751c8",
    "url": "https://www.semanticscholar.org/paper/d1932021a85b36144f6c355897b5ba55403751c8",
    "title": "ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process Rewarding",
    "abstract": "Retrieval-Augmented Generation (RAG) systems for Large Language Models (LLMs) hold promise in knowledge-intensive tasks but face limitations in complex multi-step reasoning. While recent methods have integrated RAG with chain-of-thought reasoning or test-time search using Process Reward Models (PRMs), these approaches encounter challenges such as a lack of explanations, bias in PRM training data, early-step bias in PRM scores, and insufficient post-training optimization of reasoning potential. To address these issues, we propose Retrieval-Augmented Reasoning through Trustworthy Process Rewarding (ReARTeR), a framework that enhances RAG systems' reasoning capabilities through post-training and test-time scaling. At test time, ReARTeR introduces Trustworthy Process Rewarding via a Process Reward Model for accurate scalar scoring and a Process Explanation Model (PEM) for generating natural language explanations, enabling step refinement. During post-training, it utilizes Monte Carlo Tree Search guided by Trustworthy Process Rewarding to collect high-quality step-level preference data, optimized through Iterative Preference Optimization. ReARTeR addresses three core challenges: (1) misalignment between PRM and PEM, tackled through off-policy preference learning; (2) bias in PRM training data, mitigated by balanced annotation methods and stronger annotations for challenging examples; and (3) early-step bias in PRM, resolved through a temporal-difference-based look-ahead search strategy. Experimental results on multi-step reasoning benchmarks demonstrate significant improvements, underscoring ReARTeR's potential to advance the reasoning capabilities of RAG systems.",
    "venue": "",
    "year": 2025,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2025-01-14",
    "authors": [
      {
        "authorId": "2109820280",
        "name": "ZhongXiang Sun"
      },
      {
        "authorId": "2340181458",
        "name": "Qipeng Wang"
      },
      {
        "authorId": "2118684861",
        "name": "Weijie Yu"
      },
      {
        "authorId": "2055666765",
        "name": "Xiaoxue Zang"
      },
      {
        "authorId": "2293395261",
        "name": "Kai Zheng"
      },
      {
        "authorId": "2293399145",
        "name": "Jun Xu"
      },
      {
        "authorId": "2293646334",
        "name": "Xiao Zhang"
      },
      {
        "authorId": "2340174549",
        "name": "Song Yang"
      },
      {
        "authorId": "2340361176",
        "name": "Han Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "b512451d431df9e411bea4c99f7135d010275445",
    "url": "https://www.semanticscholar.org/paper/b512451d431df9e411bea4c99f7135d010275445",
    "title": "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs",
    "abstract": "Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 88,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-10",
    "authors": [
      {
        "authorId": "40265559",
        "name": "O. Ovadia"
      },
      {
        "authorId": "2217252086",
        "name": "Menachem Brief"
      },
      {
        "authorId": "2273563114",
        "name": "Moshik Mishaeli"
      },
      {
        "authorId": "31834207",
        "name": "Oren Elisha"
      }
    ],
    "source": "semantic_scholar",
    "score": 137.3295455459821
  },
  {
    "paperId": "63a1617af179ee8b5b096b3038913a19166168d4",
    "url": "https://www.semanticscholar.org/paper/63a1617af179ee8b5b096b3038913a19166168d4",
    "title": "Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models",
    "abstract": "Retrieval-Augmented Generation (RAG) has been shown to enhance the factual accuracy of Large Language Models (LLMs), but existing methods often suffer from limited reasoning capabilities in effectively using the retrieved evidence, particularly when using open-source LLMs. To mitigate this gap, we introduce a novel framework, Open-RAG, designed to enhance reasoning capabilities in RAG with open-source LLMs. Our framework transforms an arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE) model capable of handling complex reasoning tasks, including both single- and multi-hop queries. Open-RAG uniquely trains the model to navigate challenging distractors that appear relevant but are misleading. As a result, Open-RAG leverages latent learning, dynamically selecting relevant experts and integrating external knowledge effectively for more accurate and contextually relevant responses. In addition, we propose a hybrid adaptive retrieval method to determine retrieval necessity and balance the trade-off between performance gain and inference speed. Experimental results show that the Llama2-7B-based Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT, Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source our code and models at https://openragmoe.github.io/",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-10-02",
    "authors": [
      {
        "authorId": "2232783785",
        "name": "Shayekh Bin Islam"
      },
      {
        "authorId": "2323863538",
        "name": "Md Asib Rahman"
      },
      {
        "authorId": "2323786676",
        "name": "K. S. M. T. Hossain"
      },
      {
        "authorId": "2274022429",
        "name": "Enamul Hoque"
      },
      {
        "authorId": "2708940",
        "name": "Shafiq R. Joty"
      },
      {
        "authorId": "3405393",
        "name": "Md. Rizwan Parvez"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "26569b9dfce6403f1da60eae01bde3539a53f2f1",
    "url": "https://www.semanticscholar.org/paper/26569b9dfce6403f1da60eae01bde3539a53f2f1",
    "title": "Retrieval Augmented Generation in the Wild: A System 2 Perspective",
    "abstract": "Large language models (LLMs), despite their impressive capabilities in natural language understanding tasks in open-domain, often lack e ﬀ ectiveness with similar tasks in enterprise applications due to potential hallucinations, weak multi-hop reasoning ability, and limitations in adapting to heterogeneous data types, among others. Such issues primarily arise due to the absence of private, on-premises enterprises from an LLM’s training corpus. Knowledge-intensive tasks in enterprise often require multi-step reasoning, deep contextual understanding, and integration of information stored and accessed in heterogeneous formats ( e.g. , tables, graphs, documents, and JSON), which LLMs aren’t inherently equipped to handle without signiﬁcant adaptation. To this end, retrieval augmented generation (RAG) o ﬀ ers promise in instrumenting such adaptations on demand. While RAG-based approaches focus on controlling the generation and mitigating hallucinations, existing solutions are not su ﬃ cient for the requirements of the enterprise settings. In this paper, we outline our approaches toward understanding and implementing a more e ﬀ ective RAG workﬂow in the wild. To achieve the goal, we draw on the cognitive science concepts of System 1 (fast, intuitive thinking) and System 2 (slow, deliberate, analytical thinking.) In particular, we discuss how existing RAG approaches are more aligned to System 1 and propose to shift from traditional single-model architectures to compound AI systems within a System 2 framework to improve RAG, especially in complex enterprise applications. Such compound AI systems adopt a more systematic approach by assigning specialized tasks to di ﬀ erent intelligent agents, optimizing retrieval and generation performance with a retrieval-augmented generation workﬂow.",
    "venue": "",
    "year": null,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "37455401",
        "name": "Sajjadur Rahman"
      },
      {
        "authorId": "2188417102",
        "name": "Dan Zhang"
      },
      {
        "authorId": "2284869489",
        "name": "Nikita Bhutani"
      },
      {
        "authorId": "2265753807",
        "name": "Estevam R. Hruschka"
      },
      {
        "authorId": "1781317",
        "name": "Eser Kandogan"
      },
      {
        "authorId": "2342439685",
        "name": "Megagon Labs"
      },
      {
        "authorId": "2342436461",
        "name": "Ellen Litman"
      }
    ],
    "source": "semantic_scholar",
    "score": 50
  },
  {
    "paperId": "e0dc8e113dbdd2896fb6420ac93e0b976c47f2a2",
    "url": "https://www.semanticscholar.org/paper/e0dc8e113dbdd2896fb6420ac93e0b976c47f2a2",
    "title": "Augmented Large Language Models with Parametric Knowledge Guiding",
    "abstract": "Large Language Models (LLMs) have significantly advanced natural language processing (NLP) with their impressive language understanding and generation capabilities. However, their performance may be suboptimal for domain-specific tasks that require specialized knowledge due to limited exposure to the related data. Additionally, the lack of transparency of most state-of-the-art (SOTA) LLMs, which can only be accessed via APIs, impedes further fine-tuning with domain custom data. Moreover, providing private data to the LLMs' owner leads to data privacy problems. To address these challenges, we propose the novel Parametric Knowledge Guiding (PKG) framework, which equips LLMs with a knowledge-guiding module to access relevant knowledge without altering the LLMs' parameters. Our PKG is based on open-source\"white-box\"language models, allowing offline memory of any knowledge that LLMs require. We demonstrate that our PKG framework can enhance the performance of\"black-box\"LLMs on a range of domain knowledge-intensive tasks that require factual (+7.9%), tabular (+11.9%), medical (+3.0%), and multimodal (+8.1%) knowledge.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 38,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.04757",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-08",
    "authors": [
      {
        "authorId": "23523733",
        "name": "Ziyang Luo"
      },
      {
        "authorId": "46747953",
        "name": "Can Xu"
      },
      {
        "authorId": "2007757792",
        "name": "Pu Zhao"
      },
      {
        "authorId": "2442662",
        "name": "Xiubo Geng"
      },
      {
        "authorId": "8801869",
        "name": "Chongyang Tao"
      },
      {
        "authorId": "2157405974",
        "name": "Jing Ma"
      },
      {
        "authorId": "2793487",
        "name": "Qingwei Lin"
      },
      {
        "authorId": "2086994543",
        "name": "Daxin Jiang"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.95342469194469
  },
  {
    "paperId": "0a42274653199306ddc9fa7d604ccbed58ad8254",
    "url": "https://www.semanticscholar.org/paper/0a42274653199306ddc9fa7d604ccbed58ad8254",
    "title": "Opportunities for Retrieval and Tool Augmented Large Language Models in Scientific Facilities",
    "abstract": "Upgrades to advanced scientific user facilities such as next-generation x-ray light sources, nanoscience centers, and neutron facilities are revolutionizing our understanding of materials across the spectrum of the physical sciences, from life sciences to microelectronics. However, these facility and instrument upgrades come with a significant increase in complexity. Driven by more exacting scientific needs, instruments and experiments become more intricate each year. This increased operational complexity makes it ever more challenging for domain scientists to design experiments that effectively leverage the capabilities of and operate on these advanced instruments. Large language models (LLMs) can perform complex information retrieval, assist in knowledge-intensive tasks across applications, and provide guidance on tool usage. Using x-ray light sources, leadership computing, and nanoscience centers as representative examples, we describe preliminary experiments with a Context-Aware Language Model for Science (CALMS) to assist scientists with instrument operations and complex experimentation. With the ability to retrieve relevant information from facility documentation, CALMS can answer simple questions on scientific capabilities and other operational procedures. With the ability to interface with software tools and experimental hardware, CALMS can conversationally operate scientific instruments. By making information more accessible and acting on user needs, LLMs could expand and diversify scientific facilities' users and accelerate scientific output.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Physics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-03",
    "authors": [
      {
        "authorId": "2269471269",
        "name": "Michael H. Prince"
      },
      {
        "authorId": "143777552",
        "name": "Henry Chan"
      },
      {
        "authorId": "66378777",
        "name": "Aikaterini Vriza"
      },
      {
        "authorId": "2304367160",
        "name": "Tao Zhou"
      },
      {
        "authorId": "40135450",
        "name": "V. Sastry"
      },
      {
        "authorId": "91926828",
        "name": "M. Dearing"
      },
      {
        "authorId": "2255412073",
        "name": "Ross J. Harder"
      },
      {
        "authorId": "2269471288",
        "name": "Rama K Vasudevan"
      },
      {
        "authorId": "12480512",
        "name": "M. Cherukara"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "67d0aa08d22feb0dbd36defece61256a4a1a0282",
    "url": "https://www.semanticscholar.org/paper/67d0aa08d22feb0dbd36defece61256a4a1a0282",
    "title": "Empirical evaluation of Uncertainty Quantification in Retrieval-Augmented Language Models for Science",
    "abstract": "Large language models (LLMs) have shown remarkable achievements in natural language processing tasks, producing high-quality outputs. However, LLMs still exhibit limitations, including the generation of factually incorrect information. In safety-critical applications, it is important to assess the confidence of LLM-generated content to make informed decisions. Retrieval Augmented Language Models (RALMs) is relatively a new area of research in NLP. RALMs offer potential benefits for scientific NLP tasks, as retrieved documents, can serve as evidence to support model-generated content. This inclusion of evidence enhances trustworthiness, as users can verify and explore the retrieved documents to validate model outputs. Quantifying uncertainty in RALM generations further improves trustworthiness, with retrieved text and confidence scores contributing to a comprehensive and reliable model for scientific applications. However, there is limited to no research on UQ for RALMs, particularly in scientific contexts. This study aims to address this gap by conducting a comprehensive evaluation of UQ in RALMs, focusing on scientific tasks. This research investigates how uncertainty scores vary when scientific knowledge is incorporated as pretraining and retrieval data and explores the relationship between uncertainty scores and the accuracy of model-generated outputs. We observe that an existing RALM finetuned with scientific knowledge as the retrieval data tends to be more confident in generating predictions compared to the model pretrained only with scientific knowledge. We also found that RALMs are overconfident in their predictions, making inaccurate predictions more confidently than accurate ones. Scientific knowledge provided either as pretraining or retrieval corpus does not help alleviate this issue. We released our code, data and dashboards at https://github.com/pnnl/EXPERT2.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-15",
    "authors": [
      {
        "authorId": "2054838317",
        "name": "S. Wagle"
      },
      {
        "authorId": "2258957941",
        "name": "Sai Munikoti"
      },
      {
        "authorId": "145536102",
        "name": "Anurag Acharya"
      },
      {
        "authorId": "2259724594",
        "name": "Sara Smith"
      },
      {
        "authorId": "24029613",
        "name": "Sameera Horawalavithana"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "8e90bba98fdd41a9046ba00ad527441a447c56bb",
    "url": "https://www.semanticscholar.org/paper/8e90bba98fdd41a9046ba00ad527441a447c56bb",
    "title": "Can We Further Elicit Reasoning in LLMs? Critic-Guided Planning with Retrieval-Augmentation for Solving Challenging Tasks",
    "abstract": "State-of-the-art large language models (LLMs) exhibit impressive problem-solving capabilities but may struggle with complex reasoning and factual correctness. Existing methods harness the strengths of chain-of-thought and retrieval-augmented generation (RAG) to decompose a complex problem into simpler steps and apply retrieval to improve factual correctness. These methods work well on straightforward reasoning tasks but often falter on challenging tasks such as competitive programming and mathematics, due to frequent reasoning errors and irrelevant knowledge retrieval. To address this, we introduce Critic-guided planning with Retrieval-augmentation, CR-Planner, a novel framework that leverages fine-tuned critic models to guide both reasoning and retrieval processes through planning. CR-Planner solves a problem by iteratively selecting and executing sub-goals. Initially, it identifies the most promising sub-goal from reasoning, query generation, and retrieval, guided by rewards given by a critic model named sub-goal critic. It then executes this sub-goal through sampling and selecting the optimal output based on evaluations from another critic model named execution critic. This iterative process, informed by retrieved information and critic models, enables CR-Planner to effectively navigate the solution space towards the final answer. We employ Monte Carlo Tree Search to collect the data for training the critic models, allowing for a systematic exploration of action sequences and their long-term impacts. We validate CR-Planner on challenging domain-knowledge-intensive and reasoning-heavy tasks, including competitive programming, theorem-driven math reasoning, and complex domain retrieval problems. Our experiments demonstrate that CR-Planner significantly outperforms baselines, highlighting its effectiveness in addressing challenging problems by improving both reasoning and retrieval.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-02",
    "authors": [
      {
        "authorId": "2155447436",
        "name": "Xingxuan Li"
      },
      {
        "authorId": "2313881740",
        "name": "Weiwen Xu"
      },
      {
        "authorId": "2091437375",
        "name": "Ruochen Zhao"
      },
      {
        "authorId": "1689176705",
        "name": "Fangkai Jiao"
      },
      {
        "authorId": "2708940",
        "name": "Shafiq R. Joty"
      },
      {
        "authorId": "2211459675",
        "name": "Li Bing"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "9bbcc6eb7ab49ebed302118a98c9e28ea88987b2",
    "url": "https://www.semanticscholar.org/paper/9bbcc6eb7ab49ebed302118a98c9e28ea88987b2",
    "title": "Retrieval is Accurate Generation",
    "abstract": "Standard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary. We introduce a novel method that selects context-aware phrases from a collection of supporting documents. One of the most significant challenges for this paradigm shift is determining the training oracles, because a string of text can be segmented in various ways and each segment can be retrieved from numerous possible documents. To address this, we propose to initialize the training oracles using linguistic heuristics and, more importantly, bootstrap the oracles through iterative self-reinforcement. Extensive experiments show that our model not only outperforms standard language models on a variety of knowledge-intensive tasks but also demonstrates improved generation quality in open-ended text generation. For instance, compared to the standard language model counterpart, our model raises the accuracy from 23.47% to 36.27% on OpenbookQA, and improves the MAUVE score from 42.61% to 81.58% in open-ended text generation. Remarkably, our model also achieves the best performance and the lowest latency among several retrieval-augmented baselines. In conclusion, we assert that retrieval is more accurate generation and hope that our work will encourage further research on this new paradigm shift.",
    "venue": "International Conference on Learning Representations",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-27",
    "authors": [
      {
        "authorId": "2209367631",
        "name": "Bowen Cao"
      },
      {
        "authorId": "2266753374",
        "name": "Deng Cai"
      },
      {
        "authorId": "2279792419",
        "name": "Leyang Cui"
      },
      {
        "authorId": null,
        "name": "Xuxin Cheng"
      },
      {
        "authorId": "2237804371",
        "name": "Wei Bi"
      },
      {
        "authorId": "2260859476",
        "name": "Yuexian Zou"
      },
      {
        "authorId": "2257446263",
        "name": "Shuming Shi"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "1a1e99514d8d175459f7c61cfd0c394b46e63359",
    "url": "https://www.semanticscholar.org/paper/1a1e99514d8d175459f7c61cfd0c394b46e63359",
    "title": "Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training",
    "abstract": "Prior work on Data-To-Text Generation, the task of converting knowledge graph (KG) triples into natural text, focused on domain-specific benchmark datasets. In this paper, however, we verbalize the entire English Wikidata KG, and discuss the unique challenges associated with a broad, open-domain, large-scale verbalization. We further show that verbalizing a comprehensive, encyclopedic KG like Wikidata can be used to integrate structured KGs and natural language corpora. In contrast to the many architectures that have been developed to integrate these two sources, our approach converts the KG into natural text, allowing it to be seamlessly integrated into existing language models. It carries the further advantages of improved factual accuracy and reduced toxicity in the resulting language model. We evaluate this approach by augmenting the retrieval corpus in a retrieval language model and showing significant improvements on the knowledge intensive tasks of open domain QA and the LAMA knowledge probe.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2021,
    "citationCount": 156,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2021.naacl-main.278.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-06-01",
    "authors": [
      {
        "authorId": "2257175603",
        "name": "Oshin Agarwal"
      },
      {
        "authorId": "2257216155",
        "name": "Heming Ge"
      },
      {
        "authorId": "2944868",
        "name": "Siamak Shakeri"
      },
      {
        "authorId": "1388360943",
        "name": "Rami Al-Rfou"
      }
    ],
    "source": "semantic_scholar",
    "score": 145.8436870802246
  },
  {
    "paperId": "5616be414472881c170abca7ac2472378c63fc74",
    "url": "https://www.semanticscholar.org/paper/5616be414472881c170abca7ac2472378c63fc74",
    "title": "CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation",
    "abstract": "Building high-quality datasets for specialized tasks is a time-consuming and resource-intensive process that often requires specialized domain knowledge. We propose Corpus Retrieval and Augmentation for Fine-Tuning (CRAFT), a method for generating synthetic datasets, given a small number of user-written few-shots that demonstrate the task to be performed. Given the few-shot examples, we use large-scale public web-crawled corpora and similarity-based document retrieval to find other relevant human-written documents. Lastly, instruction-tuned large language models (LLMs) augment the retrieved documents into custom-formatted task samples, which then can be used for fine-tuning. We demonstrate that CRAFT can efficiently generate large-scale task-specific training datasets for four diverse tasks: biology question-answering (QA), medicine QA and commonsense QA as well as summarization. Our experiments show that CRAFT-based models outperform or achieve comparable performance to general LLMs for QA tasks, while CRAFT-based summarization models outperform models trained on human-curated data by 46 preference points.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-03",
    "authors": [
      {
        "authorId": "2319415753",
        "name": "Ingo Ziegler"
      },
      {
        "authorId": "1999179692",
        "name": "Abdullatif Köksal"
      },
      {
        "authorId": "2326119320",
        "name": "Desmond Elliott"
      },
      {
        "authorId": "2130001188",
        "name": "Hinrich Schutze"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "a37153a5f42ee2951ad8a2c9ec86b52c4bf81c77",
    "url": "https://www.semanticscholar.org/paper/a37153a5f42ee2951ad8a2c9ec86b52c4bf81c77",
    "title": "Retrieval-Augmented Multimodal Language Modeling",
    "abstract": "Recent multimodal models such as DALL-E and\r\nCM3 have achieved remarkable progress in textto-image and image-to-text generation. However, these models store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the\r\nmodel parameters, requiring increasingly larger models and training data to capture more knowledge. To integrate knowledge in a more scalable and modular way, we propose a retrievalaugmented multimodal model, which enables a base multimodal model (generator) to refer to relevant knowledge fetched by a retriever from external memory (e.g., multimodal documents on the web). Specifically, we implement a retriever using the pretrained CLIP model and a generator using the CM3 Transformer architecture, and\r\ntrain this model using the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can retrieve and generate mixtures of text and images.\r\nWe show that RA-CM3 significantly outperforms baseline multimodal models such as DALL-E and CM3 on both image and caption generation tasks (12 FID and 17 CIDEr improvements on MSCOCO), while requiring much less compute for training (<30% of DALL-E). Moreover, we show that RA-CM3 exhibits novel capabilities such as knowledge-intensive image generation and multimodal in-context learning",
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "citationCount": 82,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2211.12561",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "19168196",
        "name": "Michihiro Yasunaga"
      },
      {
        "authorId": "2201435",
        "name": "Armen Aghajanyan"
      },
      {
        "authorId": "3040379",
        "name": "Weijia Shi"
      },
      {
        "authorId": "2191899140",
        "name": "Rich James"
      },
      {
        "authorId": "1702139",
        "name": "J. Leskovec"
      },
      {
        "authorId": "145419642",
        "name": "Percy Liang"
      },
      {
        "authorId": "35084211",
        "name": "M. Lewis"
      },
      {
        "authorId": "1982950",
        "name": "Luke Zettlemoyer"
      },
      {
        "authorId": "2072801764",
        "name": "Wen-tau Yih"
      }
    ],
    "source": "semantic_scholar",
    "score": 136.28260911694898
  },
  {
    "paperId": "d07b1a2af4403de7c3860744a976e41c0aa1e365",
    "url": "https://www.semanticscholar.org/paper/d07b1a2af4403de7c3860744a976e41c0aa1e365",
    "title": "R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large Language Models",
    "abstract": "Retrieval-augmented large language models (LLMs) leverage relevant content retrieved by information retrieval systems to generate correct responses, aiming to alleviate the hallucination problem. However, existing retriever-responder methods typically append relevant documents to the prompt of LLMs to perform text generation tasks without considering the interaction of fine-grained structural semantics between the retrieved documents and the LLMs. This issue is particularly important for accurate response generation as LLMs tend to\"lose in the middle\"when dealing with input prompts augmented with lengthy documents. In this work, we propose a new pipeline named\"Reinforced Retriever-Reorder-Responder\"(R$^4$) to learn document orderings for retrieval-augmented LLMs, thereby further enhancing their generation abilities while the large numbers of parameters of LLMs remain frozen. The reordering learning process is divided into two steps according to the quality of the generated responses: document order adjustment and document representation enhancement. Specifically, document order adjustment aims to organize retrieved document orderings into beginning, middle, and end positions based on graph attention learning, which maximizes the reinforced reward of response quality. Document representation enhancement further refines the representations of retrieved documents for responses of poor quality via document-level gradient adversarial learning. Extensive experiments demonstrate that our proposed pipeline achieves better factual question-answering performance on knowledge-intensive tasks compared to strong baselines across various public datasets. The source codes and trained models will be released upon paper acceptance.",
    "venue": "European Conference on Artificial Intelligence",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-04",
    "authors": [
      {
        "authorId": "2146342371",
        "name": "Taolin Zhang"
      },
      {
        "authorId": "2257089368",
        "name": "Dongyang Li"
      },
      {
        "authorId": "2300139675",
        "name": "Qizhou Chen"
      },
      {
        "authorId": "50097294",
        "name": "Chengyu Wang"
      },
      {
        "authorId": "2292090586",
        "name": "Longtao Huang"
      },
      {
        "authorId": "2292128230",
        "name": "Hui Xue"
      },
      {
        "authorId": "2257159827",
        "name": "Xiaofeng He"
      },
      {
        "authorId": "2272790856",
        "name": "Junyuan Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "11a11e6bee3ca1deb829f3e9abaa73182f2d3435",
    "url": "https://www.semanticscholar.org/paper/11a11e6bee3ca1deb829f3e9abaa73182f2d3435",
    "title": "Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning",
    "abstract": "While chain-of-thought (CoT) prompting has revolutionized how LLMs perform reasoning tasks, its current methods and variations (e.g, Self-consistency, ReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR)) suffer from limitations like slowness, limited context grounding, hallucination and inconsistent outputs. To overcome these challenges, we introduce Evidence to Generate (E2G), a novel single-agent, two-step prompting framework. Instead of unverified reasoning claims, this innovative approach leverages the power of\"evidence for decision making\"by first focusing exclusively on the thought sequences (the series of intermediate steps) explicitly mentioned in the context which then serve as extracted evidence, guiding the LLM's output generation process with greater precision and efficiency. This simple yet powerful approach unlocks the true potential of chain-of-thought like prompting, paving the way for faster, more reliable, and more contextually aware reasoning in LLMs. \\tool achieves remarkable results robustly across a wide range of knowledge-intensive reasoning and generation tasks, surpassing baseline approaches with state-of-the-art LLMs. For example, (i) on LogiQA benchmark using GPT-4 as backbone model, \\tool achieves a new state-of-the Accuracy of 53.8% exceeding CoT by 18%, ToT by 11%, CR by 9% (ii) a variant of E2G with PaLM2 outperforms the variable-shot performance of Gemini Ultra by 0.9 F1 points, reaching an F1 score of 83.3 on a subset of DROP.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-11",
    "authors": [
      {
        "authorId": "3405393",
        "name": "Md. Rizwan Parvez"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "cfa85c8db829dbd2384ea7f130f462e7e7f1f630",
    "url": "https://www.semanticscholar.org/paper/cfa85c8db829dbd2384ea7f130f462e7e7f1f630",
    "title": "Reliable, Adaptable, and Attributable Language Models with Retrieval",
    "abstract": "Parametric language models (LMs), which are trained on vast amounts of web data, exhibit remarkable flexibility and capability. However, they still face practical challenges such as hallucinations, difficulty in adapting to new data distributions, and a lack of verifiability. In this position paper, we advocate for retrieval-augmented LMs to replace parametric LMs as the next generation of LMs. By incorporating large-scale datastores during inference, retrieval-augmented LMs can be more reliable, adaptable, and attributable. Despite their potential, retrieval-augmented LMs have yet to be widely adopted due to several obstacles: specifically, current retrieval-augmented LMs struggle to leverage helpful text beyond knowledge-intensive tasks such as question answering, have limited interaction between retrieval and LM components, and lack the infrastructure for scaling. To address these, we propose a roadmap for developing general-purpose retrieval-augmented LMs. This involves a reconsideration of datastores and retrievers, the exploration of pipelines with improved retriever-LM interaction, and significant investment in infrastructure for efficient training and inference.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 39,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-05",
    "authors": [
      {
        "authorId": "35584853",
        "name": "Akari Asai"
      },
      {
        "authorId": "49164966",
        "name": "Zexuan Zhong"
      },
      {
        "authorId": "50536468",
        "name": "Danqi Chen"
      },
      {
        "authorId": "2303396379",
        "name": "Pang Wei Koh"
      },
      {
        "authorId": "2137813791",
        "name": "Luke S. Zettlemoyer"
      },
      {
        "authorId": "2264251662",
        "name": "Hanna Hajishirzi"
      },
      {
        "authorId": "2072801764",
        "name": "Wen-tau Yih"
      }
    ],
    "source": "semantic_scholar",
    "score": 125.33319181170904
  },
  {
    "paperId": "4c0f029efd5371eed087d0794fb0df71238600cc",
    "url": "https://www.semanticscholar.org/paper/4c0f029efd5371eed087d0794fb0df71238600cc",
    "title": "KG-CoT: Chain-of-Thought Prompting of Large Language Models over Knowledge Graphs for Knowledge-Aware Question Answering",
    "abstract": "Large language models (LLMs) encounter challenges such as hallucination and factual errors in knowledge-intensive tasks. One the one hand, LLMs sometimes struggle to generate reliable answers based on the black-box parametric knowledge, due to the lack of responsible knowledge. Moreover, fragmented knowledge facts extracted by knowledge retrievers fail to provide explicit and coherent reasoning paths for improving LLM reasoning. To address these challenges, we propose KG-CoT, a novel knowledge-augmented paradigm that leverages a small-scale step-by-step graph reasoning model to reason over knowledge graphs (KGs) and utilizes a reasoning path generation method to generate chains of reasoning with high confidence for large-scale LLMs. Extensive experiments demonstrate that our KG-CoT significantly improves the performance of LLMs on knowledge-intensive question answering tasks, such as multi-hop, single-hop, and open-domain question answering benchmarks, without fine-tuning LLMs. KG-CoT outperforms the CoT prompting as well as prior retrieval-augmented and knowledge base question answering baselines. Moreover, KG-CoT can reduce the number of API calls and cost and generalize to various LLM backbones in a lightweight plug-and-play manner.",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-08-01",
    "authors": [
      {
        "authorId": "2293901145",
        "name": "Ruilin Zhao"
      },
      {
        "authorId": "2283265839",
        "name": "Feng Zhao"
      },
      {
        "authorId": "2313356601",
        "name": "Long Wang"
      },
      {
        "authorId": "2238149841",
        "name": "Xianzhi Wang"
      },
      {
        "authorId": "2283302879",
        "name": "Guandong Xu"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "b63e97330154acece935ffa6901e3f36518e5703",
    "url": "https://www.semanticscholar.org/paper/b63e97330154acece935ffa6901e3f36518e5703",
    "title": "Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study",
    "abstract": "Large decoder-only language models (LMs) can be largely improved in terms of perplexity by retrieval (e.g., RETRO), but its impact on text generation quality and downstream task accuracy is unclear. Thus, it is still an open question: shall we pretrain large autoregressive LMs with retrieval? To answer it, we perform a comprehensive study on a scalable pre-trained retrieval-augmented LM (i.e., RETRO) compared with standard GPT and retrieval-augmented GPT incorporated at fine-tuning or inference stages. We first provide the recipe to reproduce RETRO up to 9.5B parameters while retrieving a text corpus with 330B tokens. Based on that, we have the following novel findings: i) RETRO outperforms GPT on text generation with much less degeneration (i.e., repetition), moderately higher factual accuracy, and slightly lower toxicity with a nontoxic retrieval database. ii) On the LM Evaluation Harness benchmark, RETRO largely outperforms GPT on knowledge-intensive tasks, but is on par with GPT on other tasks. Furthermore, we introduce a simple variant of the model, RETRO++, which largely improves open-domain QA results of original RETRO (e.g., EM score +8.6 on Natural Question) and significantly outperforms retrieval-augmented GPT in both fine-tuning and zero-shot evaluation settings. Our findings highlight the promising direction of pretraining autoregressive LMs with retrieval as future foundation models. We release our code and model at: https://github.com/NVIDIA/Megatron-LM/blob/main/tools/retro/README.md",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 52,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2304.06762",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-04-13",
    "authors": [
      {
        "authorId": "51454501",
        "name": "Boxin Wang"
      },
      {
        "authorId": "2056440915",
        "name": "Wei Ping"
      },
      {
        "authorId": "2092025743",
        "name": "P. Xu"
      },
      {
        "authorId": "20957879",
        "name": "Lawrence C. McAfee"
      },
      {
        "authorId": "152613855",
        "name": "Zihan Liu"
      },
      {
        "authorId": "1911755",
        "name": "M. Shoeybi"
      },
      {
        "authorId": "2156155948",
        "name": "Yi Dong"
      },
      {
        "authorId": "2787022",
        "name": "Oleksii Kuchaiev"
      },
      {
        "authorId": "71788673",
        "name": "Bo Li"
      },
      {
        "authorId": "2723309",
        "name": "Chaowei Xiao"
      },
      {
        "authorId": "2047844",
        "name": "Anima Anandkumar"
      },
      {
        "authorId": "2301680",
        "name": "Bryan Catanzaro"
      }
    ],
    "source": "semantic_scholar",
    "score": 129.55437870328183
  },
  {
    "paperId": "401ec1cf878b8846d942c2deb08e8cae146ede03",
    "url": "https://www.semanticscholar.org/paper/401ec1cf878b8846d942c2deb08e8cae146ede03",
    "title": "Re3val: Reinforced and Reranked Generative Retrieval",
    "abstract": "Generative retrieval models encode pointers to information in a corpus as an index within the model’s parameters. These models serve as part of a larger pipeline, where retrieved information conditions generation for knowledge-intensive NLP tasks. However, we identify two limitations: the generative retrieval does not account for contextual information. Secondly, the retrieval can’t be tuned for the downstream readers as decoding the page title is a non-differentiable operation. This paper introduces Re3val, trained with generative reranking and reinforcement learning using limited data. Re3val leverages context acquired via Dense Passage Retrieval to rerank the retrieved page titles and utilizes REINFORCE to maximize rewards generated by constrained decoding. Additionally, we generate questions from our pre-training dataset to mitigate epistemic uncertainty and bridge the domain gap between the pre-training and fine-tuning datasets. Subsequently, we extract and rerank contexts from the KILT database using the rerank page titles. Upon grounding the top five reranked contexts, Re3val demonstrates the Top 1 KILT scores compared to all other generative retrieval models across five KILT datasets.",
    "venue": "Findings",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-30",
    "authors": [
      {
        "authorId": "2281825075",
        "name": "EuiYul Song"
      },
      {
        "authorId": "2281880933",
        "name": "Sangryul Kim"
      },
      {
        "authorId": "2281904593",
        "name": "Haeju Lee"
      },
      {
        "authorId": "2281949233",
        "name": "Joonkee Kim"
      },
      {
        "authorId": "2263749072",
        "name": "James Thorne"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "729199093cc98a3705867abdb3cad55ce194efb9",
    "url": "https://www.semanticscholar.org/paper/729199093cc98a3705867abdb3cad55ce194efb9",
    "title": "Reverse Image Retrieval Cues Parametric Memory in Multimodal LLMs",
    "abstract": "Despite impressive advances in recent multimodal large language models (MLLMs), state-of-the-art models such as from the GPT-4 suite still struggle with knowledge-intensive tasks. To address this, we consider Reverse Image Retrieval (RIR) augmented generation, a simple yet effective strategy to augment MLLMs with web-scale reverse image search results. RIR robustly improves knowledge-intensive visual question answering (VQA) of GPT-4V by 37-43%, GPT-4 Turbo by 25-27%, and GPT-4o by 18-20% in terms of open-ended VQA evaluation metrics. To our surprise, we discover that RIR helps the model to better access its own world knowledge. Concretely, our experiments suggest that RIR augmentation helps by providing further visual and textual cues without necessarily containing the direct answer to a query. In addition, we elucidate cases in which RIR can hurt performance and conduct a human evaluation. Finally, we find that the overall advantage of using RIR makes it difficult for an agent that can choose to use RIR to perform better than an approach where RIR is the default setting.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-29",
    "authors": [
      {
        "authorId": "2303805050",
        "name": "Jialiang Xu"
      },
      {
        "authorId": "2303651689",
        "name": "Michael Moor"
      },
      {
        "authorId": "1702139",
        "name": "J. Leskovec"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "a9f8ccf9061bb98034c47bad0036fd2b97f36084",
    "url": "https://www.semanticscholar.org/paper/a9f8ccf9061bb98034c47bad0036fd2b97f36084",
    "title": "Retrieve-and-Sample: Document-level Event Argument Extraction via Hybrid Retrieval Augmentation",
    "abstract": "Recent studies have shown the effectiveness of retrieval augmentation in many generative NLP tasks. These retrieval-augmented methods allow models to explicitly acquire prior external knowledge in a non-parametric manner and regard the retrieved reference instances as cues to augment text generation. These methods use similarity-based retrieval, which is based on a simple hypothesis: the more the retrieved demonstration resembles the original input, the more likely the demonstration label resembles the input label. However, due to the complexity of event labels and sparsity of event arguments, this hypothesis does not always hold in document-level EAE. This raises an interesting question: How do we design the retrieval strategy for document-level EAE? We investigate various retrieval settings from the input and label distribution views in this paper. We further augment document-level EAE with pseudo demonstrations sampled from event semantic regions that can cover adequate alternatives in the same context and event schema. Through extensive experiments on RAMS and WikiEvents, we demonstrate the validity of our newly introduced retrieval-augmented methods and analyze why they work.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 26,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.acl-long.17.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2109978994",
        "name": "Yubing Ren"
      },
      {
        "authorId": "47184362",
        "name": "Yanan Cao"
      },
      {
        "authorId": "2075394870",
        "name": "Ping Guo"
      },
      {
        "authorId": "36595248",
        "name": "Fang Fang"
      },
      {
        "authorId": "2185915076",
        "name": "Wei Ma"
      },
      {
        "authorId": "1390641501",
        "name": "Zheng Lin"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.43755299006494
  },
  {
    "paperId": "beaf72981de56f3636302a7d9d6e2a63bc1418b8",
    "url": "https://www.semanticscholar.org/paper/beaf72981de56f3636302a7d9d6e2a63bc1418b8",
    "title": "W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering",
    "abstract": "In knowledge-intensive tasks such as open-domain question answering (OpenQA), Large Language Models (LLMs) often struggle to generate factual answers relying solely on their internal (parametric) knowledge. To address this limitation, Retrieval-Augmented Generation (RAG) systems enhance LLMs by retrieving relevant information from external sources, thereby positioning the retriever as a pivotal component. Although dense retrieval demonstrates state-of-the-art performance, its training poses challenges due to the scarcity of ground-truth evidence, largely attributed to the high costs of human annotation. In this paper, we propose W-RAG by utilizing the ranking capabilities of LLMs to create weakly labeled data for training dense retrievers. Specifically, we rerank the top-$K$ passages retrieved via BM25 by assessing the probability that LLMs will generate the correct answer based on the question and each passage. The highest-ranking passages are then used as positive training examples for dense retrieval. Our comprehensive experiments across four publicly available OpenQA datasets demonstrate that our approach enhances both retrieval and OpenQA performance compared to baseline models.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-15",
    "authors": [
      {
        "authorId": "2316325542",
        "name": "Jinming Nian"
      },
      {
        "authorId": "2113952662",
        "name": "Zhiyuan Peng"
      },
      {
        "authorId": "2260433198",
        "name": "Qifan Wang"
      },
      {
        "authorId": "2305661352",
        "name": "Yi Fang"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "dd4db8841fc2471815cc6b41bc209cefdf960491",
    "url": "https://www.semanticscholar.org/paper/dd4db8841fc2471815cc6b41bc209cefdf960491",
    "title": "ClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures",
    "abstract": "To handle the vast amounts of qualitative data produced in corporate climate communication, stakeholders increasingly rely on Retrieval Augmented Generation (RAG) systems. However, a significant gap remains in evaluating domain-specific information retrieval – the basis for answer generation. To address this challenge, this work simulates the typical tasks of a sustainability analyst by examining 30 sustainability reports with 16 detailed climate-related questions. As a result, we obtain a dataset with over 8.5K unique question-source-answer pairs labeled by different levels of relevance. Furthermore, we develop a use case with the dataset to investigate the integration of expert knowledge into information retrieval with embeddings. Although we show that incorporating expert knowledge works, we also outline the critical limitations of embeddings in knowledge-intensive downstream domains like climate change communication.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-14",
    "authors": [
      {
        "authorId": "2213295713",
        "name": "Tobias Schimanski"
      },
      {
        "authorId": "2046974354",
        "name": "Jingwei Ni"
      },
      {
        "authorId": "2306780570",
        "name": "Roberto Spacey"
      },
      {
        "authorId": "2306780708",
        "name": "Nicola Ranger"
      },
      {
        "authorId": "3073566",
        "name": "Markus Leippold"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "4d03595ce3cc61ee230511b3733c921726ea8404",
    "url": "https://www.semanticscholar.org/paper/4d03595ce3cc61ee230511b3733c921726ea8404",
    "title": "Intermediate Distillation: Data-Efficient Distillation from Black-Box LLMs for Information Retrieval",
    "abstract": "Recent research has explored distilling knowledge from large language models (LLMs) to optimize retriever models, especially within the retrieval-augmented generation (RAG) framework. However, most existing training methods rely on extracting supervision signals from LLMs' weights or their output probabilities, which is not only resource-intensive but also incompatible with black-box LLMs. In this paper, we introduce \\textit{Intermediate Distillation}, a data-efficient knowledge distillation training scheme that treats LLMs as black boxes and distills their knowledge via an innovative LLM-ranker-retriever pipeline, solely using LLMs' ranking generation as the supervision signal. Extensive experiments demonstrate that our proposed method can significantly improve the performance of retriever models with only 1,000 training instances. Moreover, our distilled retriever model significantly boosts performance in question-answering tasks within the RAG framework, demonstrating the potential of LLMs to economically and effectively train smaller models.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-18",
    "authors": [
      {
        "authorId": "2275817968",
        "name": "Zizhong Li"
      },
      {
        "authorId": "2135688409",
        "name": "Haopeng Zhang"
      },
      {
        "authorId": "2168548350",
        "name": "Jiawei Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "fa6618681476b6bec51b4233ab83658f1f9a6db7",
    "url": "https://www.semanticscholar.org/paper/fa6618681476b6bec51b4233ab83658f1f9a6db7",
    "title": "Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases",
    "abstract": "We proposed an end-to-end system design towards utilizing Retrieval Augmented Generation (RAG) to improve the factual accuracy of Large Language Models (LLMs) for domain-specific and time-sensitive queries related to private knowledge-bases. Our system integrates RAG pipeline with upstream datasets processing and downstream performance evaluation. Addressing the challenge of LLM hallucinations, we finetune models with a curated dataset which originates from CMU's extensive resources and annotated with the teacher model. Our experiments demonstrate the system's effectiveness in generating more accurate answers to domain-specific and time-sensitive inquiries. The results also revealed the limitations of fine-tuning LLMs with small-scale and skewed datasets. This research highlights the potential of RAG systems in augmenting LLMs with external datasets for improved performance in knowledge-intensive tasks. Our code and models are available on Github.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 28,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-15",
    "authors": [
      {
        "authorId": "2291983504",
        "name": "Jiarui Li"
      },
      {
        "authorId": "2291993398",
        "name": "Ye Yuan"
      },
      {
        "authorId": "2291997675",
        "name": "Zehua Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 120.5094374497971
  },
  {
    "paperId": "347604483bf70b1863dc5dd4f21fe7abe6da5abc",
    "url": "https://www.semanticscholar.org/paper/347604483bf70b1863dc5dd4f21fe7abe6da5abc",
    "title": "LEAF: Learning and Evaluation Augmented by Fact-Checking to Improve Factualness in Large Language Models",
    "abstract": "Large language models (LLMs) have shown remarkable capabilities in various natural language processing tasks, yet they often struggle with maintaining factual accuracy, particularly in knowledge-intensive domains like healthcare. This study introduces LEAF: Learning and Evaluation Augmented by Fact-Checking, a novel approach designed to enhance the factual reliability of LLMs, with a focus on medical question answering (QA). LEAF utilizes a dual strategy to enhance the factual accuracy of responses from models such as Llama 3 70B Instruct and Llama 3 8B Instruct. The first strategy, Fact-Check-Then-RAG, improves Retrieval-Augmented Generation (RAG) by incorporating fact-checking results to guide the retrieval process without updating model parameters. The second strategy, Learning from Fact-Checks via Self-Training, involves supervised fine-tuning (SFT) on fact-checked responses or applying Simple Preference Optimization (SimPO) with fact-checking as a ranking mechanism, both updating LLM parameters from supervision. These findings suggest that integrating fact-checked responses whether through RAG enhancement or self-training enhances the reliability and factual correctness of LLM outputs, offering a promising solution for applications where information accuracy is crucial.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-31",
    "authors": [
      {
        "authorId": "2263121822",
        "name": "Hieu Tran"
      },
      {
        "authorId": "2261394407",
        "name": "Junda Wang"
      },
      {
        "authorId": "2326987374",
        "name": "Yujan Ting"
      },
      {
        "authorId": "2326997381",
        "name": "Weijing Huang"
      },
      {
        "authorId": "2328639706",
        "name": "Terrence Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "26702869774950e03b10a3657bca0621aa0d8d07",
    "url": "https://www.semanticscholar.org/paper/26702869774950e03b10a3657bca0621aa0d8d07",
    "title": "KG-RAG: Bridging the Gap Between Knowledge and Creativity",
    "abstract": "Ensuring factual accuracy while maintaining the creative capabilities of Large Language Model Agents (LMAs) poses significant challenges in the development of intelligent agent systems. LMAs face prevalent issues such as information hallucinations, catastrophic forgetting, and limitations in processing long contexts when dealing with knowledge-intensive tasks. This paper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation) pipeline, a novel framework designed to enhance the knowledge capabilities of LMAs by integrating structured Knowledge Graphs (KGs) with the functionalities of LLMs, thereby significantly reducing the reliance on the latent knowledge of LLMs. The KG-RAG pipeline constructs a KG from unstructured text and then performs information retrieval over the newly created graph to perform KGQA (Knowledge Graph Question Answering). The retrieval methodology leverages a novel algorithm called Chain of Explorations (CoE) which benefits from LLMs reasoning to explore nodes and relationships within the KG sequentially. Preliminary experiments on the ComplexWebQuestions dataset demonstrate notable improvements in the reduction of hallucinated content and suggest a promising path toward developing intelligent systems adept at handling knowledge-intensive tasks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 18,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-20",
    "authors": [
      {
        "authorId": "2302321748",
        "name": "Diego Sanmartin"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.1665846874966
  },
  {
    "paperId": "1884e847881f57f48e530fe5113b7e079a4f867d",
    "url": "https://www.semanticscholar.org/paper/1884e847881f57f48e530fe5113b7e079a4f867d",
    "title": "Can Open-Source LLMs Compete with Commercial Models? Exploring the Few-Shot Performance of Current GPT Models in Biomedical Tasks",
    "abstract": "Commercial large language models (LLMs), like OpenAI's GPT-4 powering ChatGPT and Anthropic's Claude 3 Opus, have dominated natural language processing (NLP) benchmarks across different domains. New competing Open-Source alternatives like Mixtral 8x7B or Llama 3 have emerged and seem to be closing the gap while often offering higher throughput and being less costly to use. Open-Source LLMs can also be self-hosted, which makes them interesting for enterprise and clinical use cases where sensitive data should not be processed by third parties. We participated in the 12th BioASQ challenge, which is a retrieval augmented generation (RAG) setting, and explored the performance of current GPT models Claude 3 Opus, GPT-3.5-turbo and Mixtral 8x7b with in-context learning (zero-shot, few-shot) and QLoRa fine-tuning. We also explored how additional relevant knowledge from Wikipedia added to the context-window of the LLM might improve their performance. Mixtral 8x7b was competitive in the 10-shot setting, both with and without fine-tuning, but failed to produce usable results in the zero-shot setting. QLoRa fine-tuning and Wikipedia context did not lead to measurable performance gains. Our results indicate that the performance gap between commercial and open-source models in RAG setups exists mainly in the zero-shot setting and can be closed by simply collecting few-shot examples for domain-specific use cases. The code needed to rerun these experiments is available through GitHub.",
    "venue": "Conference and Labs of the Evaluation Forum",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-18",
    "authors": [
      {
        "authorId": "2220737480",
        "name": "Samy Ateia"
      },
      {
        "authorId": "2066206116",
        "name": "Udo Kruschwitz"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "8ac7df2d11170b0777b2d913d4a4b4887e127731",
    "url": "https://www.semanticscholar.org/paper/8ac7df2d11170b0777b2d913d4a4b4887e127731",
    "title": "A Comprehensive Evaluation of GPT-4V on Knowledge-Intensive Visual Question Answering",
    "abstract": "The emergence of multimodal large models (MLMs) has significantly advanced the field of visual understanding, offering remarkable capabilities in the realm of visual question answering (VQA). Yet, the true challenge lies in the domain of knowledge-intensive VQA tasks, which necessitate not just recognition of visual elements, but also a deep comprehension of the visual information in conjunction with a vast repository of learned knowledge. To uncover such capabilities of MLMs, particularly the newly introduced GPT-4V and Gemini, we provide an in-depth evaluation from three perspectives: 1) Commonsense Knowledge, which assesses how well models can understand visual cues and connect to general knowledge; 2) Fine-grained World Knowledge, which tests the model's skill in reasoning out specific knowledge from images, showcasing their proficiency across various specialized fields; 3) Comprehensive Knowledge with Decision-making Rationales, which examines model's capability to provide logical explanations for its inference, facilitating a deeper analysis from the interpretability perspective. Additionally, we utilize a visual knowledge-enhanced training strategy and multimodal retrieval-augmented generation approach to enhance MLMs, highlighting the future need for advancements in this research direction. Extensive experiments indicate that: a) GPT-4V demonstrates enhanced explanation generation when using composite images as few-shots; b) GPT-4V and other MLMs produce severe hallucinations when dealing with world knowledge; c) Visual knowledge enhanced training and prompting technicals present potential to improve performance. Codes: https://github.com/HITsz-TMG/Cognitive-Visual-Language-Mapper",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 20,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-13",
    "authors": [
      {
        "authorId": "2118046679",
        "name": "Yunxin Li"
      },
      {
        "authorId": "2111542852",
        "name": "Longyue Wang"
      },
      {
        "authorId": "33968873",
        "name": "Baotian Hu"
      },
      {
        "authorId": "2266425853",
        "name": "Xinyu Chen"
      },
      {
        "authorId": "2266437328",
        "name": "Wanqi Zhong"
      },
      {
        "authorId": "2266387313",
        "name": "Chenyang Lyu"
      },
      {
        "authorId": "2258690227",
        "name": "Min Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 115.66783656585135
  },
  {
    "paperId": "9ed84beba543c719c9b606c10ad61bb29abef372",
    "url": "https://www.semanticscholar.org/paper/9ed84beba543c719c9b606c10ad61bb29abef372",
    "title": "Causal Graphs Meet Thoughts: Enhancing Complex Reasoning in Graph-Augmented LLMs",
    "abstract": "In knowledge-intensive tasks, especially in high-stakes domains like medicine and law, it is critical not only to retrieve relevant information but also to provide causal reasoning and explainability. Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, integrating knowledge graphs with Graph Retrieval-Augmented Generation (Graph RAG) has emerged as an effective solution. Traditional Graph RAG methods often rely on simple graph traversal or semantic similarity, which do not capture causal relationships or align well with the model's internal reasoning steps. This paper proposes a novel pipeline that filters large knowledge graphs to emphasize cause-effect edges, aligns the retrieval process with the model's chain-of-thought (CoT), and enhances reasoning through multi-stage path improvements. Experiments on medical question-answering tasks show consistent gains, with up to a 10\\% absolute improvement across multiple large language models (LLMs). This approach demonstrates the value of combining causal reasoning with stepwise retrieval, leading to more interpretable and logically grounded solutions for complex queries.",
    "venue": "",
    "year": 2025,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2025-01-24",
    "authors": [
      {
        "authorId": "2343742961",
        "name": "Hang Luo"
      },
      {
        "authorId": "2342576352",
        "name": "Jian Zhang"
      },
      {
        "authorId": "2342458099",
        "name": "Chujun Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "6eb426af01b8871cf88fe01bff85f9b2ec8e5a04",
    "url": "https://www.semanticscholar.org/paper/6eb426af01b8871cf88fe01bff85f9b2ec8e5a04",
    "title": "TegTok: Augmenting Text Generation via Task-specific and Open-world Knowledge",
    "abstract": "Generating natural and informative texts has been a long-standing problem in NLP. Much effort has been dedicated into incorporating pre-trained language models (PLMs) with various open-world knowledge, such as knowledge graphs or wiki pages. However, their ability to access and manipulate the task-specific knowledge is still limited on downstream tasks, as this type of knowledge is usually not well covered in PLMs and is hard to acquire. To address the problem, we propose augmenting TExt Generation via Task-specific and Open-world Knowledge (TegTok) in a unified framework. Our model selects knowledge entries from two types of knowledge sources through dense retrieval and then injects them into the input encoding and output decoding stages respectively on the basis of PLMs. With the help of these two types of knowledge, our model can learn what and how to generate. Experiments on two text generation tasks of dialogue generation and question generation, and on two datasets show that our method achieves better performance than various baseline models.",
    "venue": "Findings",
    "year": 2022,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2203.08517",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-03-16",
    "authors": [
      {
        "authorId": "2111728713",
        "name": "Chao-Hong Tan"
      },
      {
        "authorId": "3028818",
        "name": "Jia-Chen Gu"
      },
      {
        "authorId": "8801869",
        "name": "Chongyang Tao"
      },
      {
        "authorId": "2072392338",
        "name": "Zhen-Hua Ling"
      },
      {
        "authorId": "46747953",
        "name": "Can Xu"
      },
      {
        "authorId": "2144026961",
        "name": "Huang Hu"
      },
      {
        "authorId": "2442662",
        "name": "Xiubo Geng"
      },
      {
        "authorId": "2086994543",
        "name": "Daxin Jiang"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "7726513506c3d6cae14599c0547f2ae04481cb8f",
    "url": "https://www.semanticscholar.org/paper/7726513506c3d6cae14599c0547f2ae04481cb8f",
    "title": "A graph-based approach to closed-domain natural language generation",
    "abstract": "Graph-based Natural Language Processing (NLP) methods have seen significant advancements in recent years with the development of Large Language Models (LLMs) and Retrieval Augmented Generation (RAG). LLMs are sophisticated models that recognize numerous NLP tasks by analyzing the users' natural language instructions called prompts. However, their industrial use is questionable due to such ethical concerns as false information generation called hallucinations, high risks of data breaches, and plagiarism. The paper introduces a novel NLP architecture, the Graph-Based Block-to-Block Generation (G3BG), which leverages state-of-the-art deep learning techniques, the power of attention mechanisms, distributional semantics, graph-based information retrieval, and decentralized networks. The model encodes user prompts to mitigate data breach risk, retrieves relevant information from a graph knowledge base, and forms a block for a conditional language model using LLMs to perform a new secure type of RAG. The model is closed-domain and small-scale oriented. It exhibits superior performance across low-resource NLP tasks, which makes it prominent for industrial use. The research presents a novel graph-based dataset. The dataset comprises private data features to encode and closed-domain textual information for information retrieval. The dataset is used to train and evaluate the G3BG model. The model allows cutting 100x training dataset volume achieving Perplexity ~6.51 on the Language Generation task and F1-Score ~90.3 on the Information Retrieval task comparable to most state-of-the-art language models. The experimental results prove the effectiveness of the proposed method and contribute to the algorithmic approaches toward LLM risk mitigation.",
    "venue": "RESEARCH RESULT Theoretical and Applied Linguistics",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-30",
    "authors": [
      {
        "authorId": "2324188986",
        "name": "Victoria I. Firsanova"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "c720f50a2b2a14628f808052fc93b68d0777b1e4",
    "url": "https://www.semanticscholar.org/paper/c720f50a2b2a14628f808052fc93b68d0777b1e4",
    "title": "MRAG: A Modular Retrieval Framework for Time-Sensitive Question Answering",
    "abstract": "Understanding temporal relations and answering time-sensitive questions is crucial yet a challenging task for question-answering systems powered by large language models (LLMs). Existing approaches either update the parametric knowledge of LLMs with new facts, which is resource-intensive and often impractical, or integrate LLMs with external knowledge retrieval (i.e., retrieval-augmented generation). However, off-the-shelf retrievers often struggle to identify relevant documents that require intensive temporal reasoning. To systematically study time-sensitive question answering, we introduce the TempRAGEval benchmark, which repurposes existing datasets by incorporating temporal perturbations and gold evidence labels. As anticipated, all existing retrieval methods struggle with these temporal reasoning-intensive questions. We further propose Modular Retrieval (MRAG), a trainless framework that includes three modules: (1) Question Processing that decomposes question into a main content and a temporal constraint; (2) Retrieval and Summarization that retrieves evidence and uses LLMs to summarize according to the main content; (3) Semantic-Temporal Hybrid Ranking that scores each evidence summarization based on both semantic and temporal relevance. On TempRAGEval, MRAG significantly outperforms baseline retrievers in retrieval performance, leading to further improvements in final answer accuracy.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-12-20",
    "authors": [
      {
        "authorId": "2244165381",
        "name": "Zhang Siyue"
      },
      {
        "authorId": "2337355680",
        "name": "Yuxiang Xue"
      },
      {
        "authorId": "2336831734",
        "name": "Yiming Zhang"
      },
      {
        "authorId": "2322984674",
        "name": "Xiaobao Wu"
      },
      {
        "authorId": "26336902",
        "name": "Anh Tuan Luu"
      },
      {
        "authorId": "2336816905",
        "name": "Zhao Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "7ae8f3fea5c8db1283bae48b5537a8ed7e8bec7b",
    "url": "https://www.semanticscholar.org/paper/7ae8f3fea5c8db1283bae48b5537a8ed7e8bec7b",
    "title": "ActiveRAG: Revealing the Treasures of Knowledge via Active Learning",
    "abstract": "Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks. However, current RAG models position LLMs as passive knowledge receptors, thereby restricting their capacity for learning and comprehending ex-ternal knowledge. In this paper, we present A CTIVE RAG, an innovative RAG framework that shifts from passive knowledge acquisition to an active learning mechanism. This approach utilizes the Knowledge Construction mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism to incorporate the outcomes from both chains of thought and knowledge construction, thereby calibrating the intrinsic cognition of LLMs. Our experimental results demonstrate that A CTIVE RAG surpasses previous RAG models, achieving a 5% improvement on question-answering datasets. All data and codes are available at https://github.com/ OpenMatch/ActiveRAG .",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2284986197",
        "name": "Zhipeng Xu"
      },
      {
        "authorId": "49047064",
        "name": "Zhenghao Liu"
      },
      {
        "authorId": "2284945375",
        "name": "Yibin Liu"
      },
      {
        "authorId": "2139787803",
        "name": "Chenyan Xiong"
      },
      {
        "authorId": "2277242040",
        "name": "Yukun Yan"
      },
      {
        "authorId": "2267033597",
        "name": "Shuo Wang"
      },
      {
        "authorId": "2286160362",
        "name": "Shi Yu"
      },
      {
        "authorId": "2266886975",
        "name": "Zhiyuan Liu"
      },
      {
        "authorId": "2204644192",
        "name": "Ge Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "f72423f8a468f13a0ba32972e8ab02a144306fd0",
    "url": "https://www.semanticscholar.org/paper/f72423f8a468f13a0ba32972e8ab02a144306fd0",
    "title": "AtomR: Atomic Operator-Empowered Large Language Models for Heterogeneous Knowledge Reasoning",
    "abstract": "Recent advancements in large language models (LLMs) have led to significant improvements in various natural language processing tasks, but it is still challenging for LLMs to perform knowledge-intensive complex question answering due to LLMs' inefficacy in reasoning planning and the hallucination problem. A typical solution is to employ retrieval-augmented generation (RAG) coupled with chain-of-thought (CoT) reasoning, which decomposes complex questions into chain-like sub-questions and applies iterative RAG at each sub-question. However, prior works exhibit sub-optimal reasoning planning and overlook dynamic knowledge retrieval from heterogeneous sources. In this paper, we propose AtomR, a novel heterogeneous knowledge reasoning framework that conducts multi-source reasoning at the atomic level. Drawing inspiration from the graph modeling of knowledge, AtomR leverages large language models (LLMs) to decompose complex questions into combinations of three atomic knowledge operators, significantly enhancing the reasoning process at both the planning and execution stages. We also introduce BlendQA, a novel evaluation benchmark tailored to assess complex heterogeneous knowledge reasoning. Experiments show that AtomR significantly outperforms state-of-the-art baselines across three single-source and two multi-source reasoning benchmarks, with notable performance gains of 9.4% on 2WikiMultihop and 9.5% on BlendQA.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-11-25",
    "authors": [
      {
        "authorId": "2220101512",
        "name": "Amy Xin"
      },
      {
        "authorId": "2327002994",
        "name": "Jinxin Liu"
      },
      {
        "authorId": "2273946831",
        "name": "Zijun Yao"
      },
      {
        "authorId": "2314110158",
        "name": "Zhicheng Lee"
      },
      {
        "authorId": "1712738522",
        "name": "S. Cao"
      },
      {
        "authorId": "2284777109",
        "name": "Lei Hou"
      },
      {
        "authorId": "2294164848",
        "name": "Juanzi Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "899be37ed77b5a941b06afa258c3305729870351",
    "url": "https://www.semanticscholar.org/paper/899be37ed77b5a941b06afa258c3305729870351",
    "title": "ChatTf: A Knowledge Graph-Enhanced Intelligent Q&A System for Mitigating Factuality Hallucinations in Traditional Folklore",
    "abstract": "Large language models are rapidly advancing the field of artificial intelligence, with current research focusing primarily on traditional natural language understanding tasks, such as question answering and information extraction. However, in knowledge-intensive domains, such as intangible cultural heritage, hallucination problems due to insufficient domain knowledge persist. To address this, we present ChatTf, a knowledge graph-enhanced intelligent Q&A system, exemplified by Chinese traditional folklore, aimed at reducing factuality hallucinations in this domain. Specifically, we constructed the Traditional Folklore Ontology (TFOnto) and proposed the Zero-shot Traditional Folklore Triplet Extraction (ZFTE) framework. Driven by TFOnto, ZFTE builds a Traditional Folklore Knowledge Graph (TFKG). We then proposed a dual-stage Retrieval-Augmented Generation framework (TFKG-RAG) based on TFKG to provide traditional folklore knowledge to large language models, mitigating factuality hallucinations in folklore Q&A tasks. In the experimental phase, ChatTf achieved an accuracy of 96.7% on a self-built TFCQD test set, outperforming several state-of-the-art baseline methods. This demonstrates the accuracy and reliability of folklore domain question answering.",
    "venue": "IEEE Access",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2150637221",
        "name": "Jun Xu"
      },
      {
        "authorId": "2327768424",
        "name": "Hao Zhang"
      },
      {
        "authorId": "2327704794",
        "name": "Haijing Zhang"
      },
      {
        "authorId": "2463076",
        "name": "Jiawei Lu"
      },
      {
        "authorId": "2262370344",
        "name": "Gang Xiao"
      }
    ],
    "source": "semantic_scholar",
    "score": 76
  },
  {
    "paperId": "de74cdb6348410bd3058c6089160f60b4e4a1667",
    "url": "https://www.semanticscholar.org/paper/de74cdb6348410bd3058c6089160f60b4e4a1667",
    "title": "MINTQA: A Multi-Hop Question Answering Benchmark for Evaluating LLMs on New and Tail Knowledge",
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in various reasoning tasks but face significant challenges with complex, knowledge-intensive multi-hop queries, particularly those involving new or long-tail knowledge. Existing benchmarks often fail to fully address these challenges. To bridge this gap, we introduce MINTQA (Multi-hop Question Answering on New and Tail Knowledge), a comprehensive benchmark to evaluate LLMs' capabilities in multi-hop reasoning across four critical dimensions: question handling strategy, sub-question generation, retrieval-augmented generation, and iterative or dynamic decomposition and retrieval. MINTQA comprises 10,479 question-answer pairs for evaluating new knowledge and 17,887 pairs for assessing long-tail knowledge, with each question equipped with corresponding sub-questions and answers. Our systematic evaluation of 22 state-of-the-art LLMs on MINTQA reveals significant limitations in their ability to handle complex knowledge base queries, particularly in handling new or unpopular knowledge. Our findings highlight critical challenges and offer insights for advancing multi-hop reasoning capabilities. The MINTQA benchmark is available at https://github.com/probe2/multi-hop/.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-12-22",
    "authors": [
      {
        "authorId": "2257110824",
        "name": "Jie He"
      },
      {
        "authorId": "2336877063",
        "name": "Nan Hu"
      },
      {
        "authorId": "2336876617",
        "name": "Wanqiu Long"
      },
      {
        "authorId": "2282590387",
        "name": "Jiaoyan Chen"
      },
      {
        "authorId": "2281582268",
        "name": "Jeff Z. Pan"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "482a029145258aebf2ce3d1b2a154def0a6135fd",
    "url": "https://www.semanticscholar.org/paper/482a029145258aebf2ce3d1b2a154def0a6135fd",
    "title": "Investigating on the External Knowledge in RAG for Zero-Shot Cross-Language Transfer",
    "abstract": "In the field of multilingual natural language processing (NLP), zero-shot cross-language transfer is an important research direction, which aims to enable models to effectively learn and reason without target language training data. This study explores the role of external knowledge in the Retrieval-Augmented Generation (RAG) model to improve the performance of zero-shot cross-lingual transfer. This paper proposes a new model architecture that enriches the knowledge base of the RAG model by integrating external knowledge bases, thereby enhancing its information bridging capabilities between source and target languages. In the experimental section, this paper conducts experiments using multiple cross-language tasks, including machine translation, question answering, and text summarization, to evaluate the performancen and domain of the model in different languages. The experimental results indicate that introducing external knowledge sources significantly improves the accuracy and robustness of the model, especially in resource-scarce language pairs. This research not only provides an effective solution for zero-shot cross-language transfer, but also provides new insights into understanding the role of external knowledge in improving the performance of NLP models.",
    "venue": "2024 IEEE 4th International Conference on Electronic Technology, Communication and Information (ICETCI)",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2024-05-24",
    "authors": [
      {
        "authorId": "2264417630",
        "name": "Wenmin Wang"
      },
      {
        "authorId": "2312100968",
        "name": "Peilin Zhang"
      },
      {
        "authorId": "2312099347",
        "name": "Ge Liu"
      },
      {
        "authorId": "2312275466",
        "name": "Ruihua Wu"
      },
      {
        "authorId": "2305422649",
        "name": "Guixiang Song"
      }
    ],
    "source": "semantic_scholar",
    "score": 76
  },
  {
    "paperId": "8e3cf2ba57c45de70f76075a173d4e647af226fa",
    "url": "https://www.semanticscholar.org/paper/8e3cf2ba57c45de70f76075a173d4e647af226fa",
    "title": "RAG for Question-Answering for Vocal Training Based on Domain Knowledge Base",
    "abstract": "Although Large language models (LLMs) are well-known due to their superior capacity for text generation and logical inference, they are found to be inaccurate in domain-specific question-answering tasks. The powerful generator still tends to generate content even when the LLM does not have sufficient knowledge at all, which is known as the hallucination problem. We find there is a research void in applying LLMs in the vocal training industry, which requires intensive expert knowledge in any chatbot or intelligent tutor services. This paper details employing Retrieval-Augmented Generation (RAG) technology to develop a domain-specific language model, addressing inherent challenges such as hallucination, where large models generate plausible but inaccurate content, and lack of domain specificity. By segmenting the knowledge base and establishing semantic similarities between user queries and knowledge data, the project lays a solid foundation for integrating RAG, significantly improving response accuracy and contextual relevance. The report highlights the successful implementation of RAG, enhancing system intelligence and personalization for user-specific needs, discusses challenges and solutions during the implementation process, and outlines future directions to expand RAG capabilities and improve user experiences.",
    "venue": "International Conference on Behavioral, Economic, and Socio-Cultural Computing",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-08-16",
    "authors": [
      {
        "authorId": "2335137532",
        "name": "Chun-hung Jonas Leung"
      },
      {
        "authorId": "2335207531",
        "name": "Yicheng Yi"
      },
      {
        "authorId": "2335136248",
        "name": "Le Kuai"
      },
      {
        "authorId": "2335170928",
        "name": "Zongxi Li"
      },
      {
        "authorId": "9458085",
        "name": "Siu-Kei Au Yeung"
      },
      {
        "authorId": "2335417454",
        "name": "Kwok-wah John Lee"
      },
      {
        "authorId": "2335658853",
        "name": "Ka-him Kelvin Ho"
      },
      {
        "authorId": "2333802278",
        "name": "Kevin Hung"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "f25f0489c31b680aabc295d826c1ca4f9a7ffd59",
    "url": "https://www.semanticscholar.org/paper/f25f0489c31b680aabc295d826c1ca4f9a7ffd59",
    "title": "Meta-training with Demonstration Retrieval for Efficient Few-shot Learning",
    "abstract": "Large language models show impressive results on few-shot NLP tasks. However, these models are memory and computation-intensive. Meta-training allows one to leverage smaller models for few-shot generalization in a domain-general and task-agnostic manner; however, these methods alone results in models that may not have sufficient parameterization or knowledge to adapt quickly to a large variety of tasks. To overcome this issue, we propose meta-training with demonstration retrieval, where we use a dense passage retriever to retrieve semantically similar labeled demonstrations to each example for more varied supervision. By separating external knowledge from model parameters, we can use meta-training to train parameter-efficient models that generalize well on a larger variety of tasks. We construct a meta-training set from UnifiedQA and CrossFit, and propose a demonstration bank based on UnifiedQA tasks. To our knowledge, our work is the first to combine retrieval with meta-training, to use DPR models to retrieve demonstrations, and to leverage demonstrations from many tasks simultaneously, rather than randomly sampling demonstrations from the training set of the target task. Our approach outperforms a variety of targeted parameter-efficient and retrieval-augmented few-shot methods on QA, NLI, and text classification tasks (including SQuAD, QNLI, and TREC). Our approach can be meta-trained and fine-tuned quickly on a single GPU.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2307.00119",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-06-30",
    "authors": [
      {
        "authorId": "49355602",
        "name": "Aaron Mueller"
      },
      {
        "authorId": "3089055",
        "name": "Kanika Narang"
      },
      {
        "authorId": "36299222",
        "name": "Lambert Mathias"
      },
      {
        "authorId": "2145778781",
        "name": "Qifan Wang"
      },
      {
        "authorId": "22593971",
        "name": "Hamed Firooz"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "73aa44579cb7c28d4287bf94645abc14318f03bd",
    "url": "https://www.semanticscholar.org/paper/73aa44579cb7c28d4287bf94645abc14318f03bd",
    "title": "Distill-SynthKG: Distilling Knowledge Graph Synthesis Workflow for Improved Coverage and Efficiency",
    "abstract": "Knowledge graphs (KGs) generated by large language models (LLMs) are becoming increasingly valuable for Retrieval-Augmented Generation (RAG) applications that require knowledge-intensive reasoning. However, existing KG extraction methods predominantly rely on prompt-based approaches, which are inefficient for processing large-scale corpora. These approaches often suffer from information loss, particularly with long documents, due to the lack of specialized design for KG construction. Additionally, there is a gap in evaluation datasets and methodologies for ontology-free KG construction. To overcome these limitations, we propose SynthKG, a multi-step, document-level ontology-free KG synthesis workflow based on LLMs. By fine-tuning a smaller LLM on the synthesized document-KG pairs, we streamline the multi-step process into a single-step KG generation approach called Distill-SynthKG, substantially reducing the number of LLM inference calls. Furthermore, we re-purpose existing question-answering datasets to establish KG evaluation datasets and introduce new evaluation metrics. Using KGs produced by Distill-SynthKG, we also design a novel graph-based retrieval framework for RAG. Experimental results demonstrate that Distill-SynthKG not only surpasses all baseline models in KG quality -- including models up to eight times larger -- but also consistently excels in retrieval and question-answering tasks. Our proposed graph retrieval framework also outperforms all KG-retrieval methods across multiple benchmark datasets. We release the SynthKG dataset and Distill-SynthKG model publicly to support further research and development.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-22",
    "authors": [
      {
        "authorId": "3466801",
        "name": "Prafulla Kumar Choubey"
      },
      {
        "authorId": "2263587631",
        "name": "Xin Su"
      },
      {
        "authorId": "2309218122",
        "name": "Man Luo"
      },
      {
        "authorId": "2324805044",
        "name": "Xiangyu Peng"
      },
      {
        "authorId": "2266753302",
        "name": "Caiming Xiong"
      },
      {
        "authorId": "2249909984",
        "name": "Tiep Le"
      },
      {
        "authorId": "1992685069",
        "name": "Shachar Rosenman"
      },
      {
        "authorId": "2309247409",
        "name": "Vasudev Lal"
      },
      {
        "authorId": "2122258484",
        "name": "P. Mùi"
      },
      {
        "authorId": "2327047843",
        "name": "Ricky Ho"
      },
      {
        "authorId": "2263959561",
        "name": "Phillip Howard"
      },
      {
        "authorId": "2267031643",
        "name": "Chien-Sheng Wu"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "b4db0f61380cea8caa2647ebf7e2f6bdd32f8ef0",
    "url": "https://www.semanticscholar.org/paper/b4db0f61380cea8caa2647ebf7e2f6bdd32f8ef0",
    "title": "AI Powered Legal Querying System using NLP",
    "abstract": "Abstract: The RAG-Based Legal Assistant Chatbot is a cutting-edge AI-powered solution designed to streamline legal\ninformation retrieval and document analysis. By leveraging Retrieval-Augmented Generation (RAG) and integrating\ntechnologies such as LangChain and FAISS (Facebook AI Similarity Search), the chatbot processes and indexes large volumes\nof legal documents, delivering precise and contextually relevant insights to user queries. This project addresses the challenge of\nnavigating extensive legal literature by transforming legal PDFs into searchable embeddings stored in FAISS. Users interact\nwith the system through a conversational interface built with Streamlit, which maintains query context to ensure accurate and\naccessible responses for legal professionals and non-specialists alike.The system’s backend infrastructure is implemented in\nPython, utilizing LangChain for efficient language model operations and FAISS for semantic search. Key functionalities\ninclude document ingestion, intelligent retrieval, and conversational interaction. The design prioritizes scalability and\nadaptability, enabling future integrations with diverse legal knowledge bases and supporting additional languages or\njurisdictions.The chatbot’s performance has been validated through extensive testing, showcasing high retrieval precision and\nlow response times. These features significantly enhance productivity by automating time-consuming tasks such as document\nsearch and legal analysis, allowing users to focus on critical decision-making.This project highlights the transformative\npotential of AI in the legal domain, bridging the gap between complex legal information and user accessibility. Future\ndevelopments will aim to enhance the system's natural language processing capabilities, incorporate real-time data updates, and\nintegrate advanced security measures to safeguard sensitive legal information. In conclusion, the RAG-Based Legal Assistant\nChatbot is an intelligent and robust tool that simplifies legal information access, demonstrating how AI can revolutionize\ntraditional industries through precision, scalability, and innovation",
    "venue": "International Journal for Research in Applied Science and Engineering Technology",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-12-31",
    "authors": [
      {
        "authorId": "2335600311",
        "name": "Pushpa R N"
      },
      {
        "authorId": "2335601097",
        "name": "Sanjana G Walke"
      },
      {
        "authorId": "2335583682",
        "name": "Sharadhi D"
      },
      {
        "authorId": null,
        "name": "Sharvari P K"
      },
      {
        "authorId": "2335816545",
        "name": "Shreya C S"
      }
    ],
    "source": "semantic_scholar",
    "score": 80
  },
  {
    "paperId": "959de2864ac2268c45bc1762cfb9df45ae630a1b",
    "url": "https://www.semanticscholar.org/paper/959de2864ac2268c45bc1762cfb9df45ae630a1b",
    "title": "Knowledge Discovery in the Age of LLMs",
    "abstract": "Large Language Models (LLMs) like GPT-3 are quickly changing the way NLP is done, and hence also how NLP is done for the purpose of knowledge discovery in the academic literature. Tasks that have traditionally been done by specialized NLP models, like entity extraction, summarization, and question answering, can now all be prototyped, usually with high accuracy, using zero-shot or few-shot prompting of LLMs. For example, this allows us to extract highly accurate meta-data, such as up-to-date author and affiliation, for scientific impact analysis from recent scientific papers 1 . The combination of LLMs and Information Retrieval systems has recently evolved into the paradigm of Retrieval Augmented Generation, which is one of the most promising approaches to use LLMs for Question Answering and to reduce hallucination, especially for data sets that were not accessible to LLMs during training, such as private data sources or very recent documents. Retrieval Augmented Generation can even be expanded to large document collections, like full conference proceedings, to quickly create conference summaries, blog-posts or tabular digests. At Zeta Alpha we are building a modern platform for scientific and enterprise knowledge discovery with neural search and generative language models at the core. In this talk, we show how we integrate LLMs with neural search to answer questions, explain documents while reading, summarize large document collections, and generate meta-data on the fly.",
    "venue": "BIR@ECIR",
    "year": 2023,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2279688576",
        "name": "Jakub Zavrel"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "df16e8c7599e1a5f7dbc1beebf454cd20e938a59",
    "url": "https://www.semanticscholar.org/paper/df16e8c7599e1a5f7dbc1beebf454cd20e938a59",
    "title": "README: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP",
    "abstract": "The advancement in healthcare has shifted focus toward patient-centric approaches, particularly in self-care and patient education, facilitated by access to Electronic Health Records (EHR). However, medical jargon in EHRs poses significant challenges in patient comprehension. To address this, we introduce a new task of automatically generating lay definitions, aiming to simplify complex medical terms into patient-friendly lay language. We first created the README dataset, an extensive collection of over 50,000 unique (medical term, lay definition) pairs and 300,000 mentions, each offering context-aware lay definitions manually annotated by domain experts. We have also engineered a data-centric Human-AI pipeline that synergizes data filtering, augmentation, and selection to improve data quality. We then used README as the training data for models and leveraged a Retrieval-Augmented Generation method to reduce hallucinations and improve the quality of model outputs. Our extensive automatic and human evaluations demonstrate that open-source mobile-friendly models, when fine-tuned with high-quality data, are capable of matching or even surpassing the performance of state-of-the-art closed-source large language models like ChatGPT. This research represents a significant stride in closing the knowledge gap in patient education and advancing patient-centric healthcare solutions.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-24",
    "authors": [
      {
        "authorId": "1576489304",
        "name": "Zonghai Yao"
      },
      {
        "authorId": "2276425805",
        "name": "Nandyala Siddharth Kantu"
      },
      {
        "authorId": "2249534497",
        "name": "Guanghao Wei"
      },
      {
        "authorId": "2263121822",
        "name": "Hieu Tran"
      },
      {
        "authorId": "2276425836",
        "name": "Zhangqi Duan"
      },
      {
        "authorId": "12693064",
        "name": "Sunjae Kwon"
      },
      {
        "authorId": "2261462978",
        "name": "Zhichao Yang"
      },
      {
        "authorId": "2276425268",
        "name": "Readme annotation team"
      },
      {
        "authorId": "2261455807",
        "name": "Hong Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "b6948a9e8b3eec5a56a80c69727154fcd7ececce",
    "url": "https://www.semanticscholar.org/paper/b6948a9e8b3eec5a56a80c69727154fcd7ececce",
    "title": "AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases",
    "abstract": "LLM agents have demonstrated remarkable performance across various applications, primarily due to their advanced capabilities in reasoning, utilizing external knowledge and tools, calling APIs, and executing actions to interact with environments. Current agents typically utilize a memory module or a retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and instances with similar embeddings from knowledge bases to inform task planning and execution. However, the reliance on unverified knowledge bases raises significant concerns about their safety and trustworthiness. To uncover such vulnerabilities, we propose a novel red teaming approach AgentPoison, the first backdoor attack targeting generic and RAG-based LLM agents by poisoning their long-term memory or RAG knowledge base. In particular, we form the trigger generation process as a constrained optimization to optimize backdoor triggers by mapping the triggered instances to a unique embedding space, so as to ensure that whenever a user instruction contains the optimized backdoor trigger, the malicious demonstrations are retrieved from the poisoned memory or knowledge base with high probability. In the meantime, benign instructions without the trigger will still maintain normal performance. Unlike conventional backdoor attacks, AgentPoison requires no additional model training or fine-tuning, and the optimized backdoor trigger exhibits superior transferability, in-context coherence, and stealthiness. Extensive experiments demonstrate AgentPoison's effectiveness in attacking three types of real-world LLM agents: RAG-based autonomous driving agent, knowledge-intensive QA agent, and healthcare EHRAgent. On each agent, AgentPoison achieves an average attack success rate higher than 80% with minimal impact on benign performance (less than 1%) with a poison rate less than 0.1%.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-17",
    "authors": [
      {
        "authorId": "1641681688",
        "name": "Zhaorun Chen"
      },
      {
        "authorId": "2261738344",
        "name": "Zhen Xiang"
      },
      {
        "authorId": "2312102403",
        "name": "Chaowei Xiao"
      },
      {
        "authorId": "2293597685",
        "name": "Dawn Song"
      },
      {
        "authorId": "2309303056",
        "name": "Bo Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.49820016084324
  },
  {
    "paperId": "c78d1cc1f9cba23235c8e67bac5c896f8e4708b5",
    "url": "https://www.semanticscholar.org/paper/c78d1cc1f9cba23235c8e67bac5c896f8e4708b5",
    "title": "LLMs in Biomedicine: A study on clinical Named Entity Recognition",
    "abstract": "Large Language Models (LLMs) demonstrate remarkable versatility in various NLP tasks but encounter distinct challenges in biomedical due to the complexities of language and data scarcity. This paper investigates LLMs application in the biomedical domain by exploring strategies to enhance their performance for the NER task. Our study reveals the importance of meticulously designed prompts in the biomedical. Strategic selection of in-context examples yields a marked improvement, offering ~15-20\\% increase in F1 score across all benchmark datasets for biomedical few-shot NER. Additionally, our results indicate that integrating external biomedical knowledge via prompting strategies can enhance the proficiency of general-purpose LLMs to meet the specialized needs of biomedical NER. Leveraging a medical knowledge base, our proposed method, DiRAG, inspired by Retrieval-Augmented Generation (RAG), can boost the zero-shot F1 score of LLMs for biomedical NER. Code is released at \\url{https://github.com/masoud-monajati/LLM_Bio_NER}",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-10",
    "authors": [
      {
        "authorId": "2051713098",
        "name": "Masoud Monajatipoor"
      },
      {
        "authorId": "2296218135",
        "name": "Jiaxin Yang"
      },
      {
        "authorId": "1689543587",
        "name": "Joel Stremmel"
      },
      {
        "authorId": "2295989114",
        "name": "Melika Emami"
      },
      {
        "authorId": "70437778",
        "name": "Fazlolah Mohaghegh"
      },
      {
        "authorId": "1491749242",
        "name": "Mozhdeh Rouhsedaghat"
      },
      {
        "authorId": "2296002884",
        "name": "Kai-Wei Chang"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "21620a67bbef3a4c607bf17be07d42514163dfaf",
    "url": "https://www.semanticscholar.org/paper/21620a67bbef3a4c607bf17be07d42514163dfaf",
    "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
    "abstract": "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The increasing demands of application scenarios have driven the evolution of RAG, leading to the integration of advanced retrievers, LLMs and other complementary technologies, which in turn has amplified the intricacy of RAG systems. However, the rapid advancements are outpacing the foundational RAG paradigm, with many methods struggling to be unified under the process of\"retrieve-then-generate\". In this context, this paper examines the limitations of the existing RAG paradigm and introduces the modular RAG framework. By decomposing complex RAG systems into independent modules and specialized operators, it facilitates a highly reconfigurable framework. Modular RAG transcends the traditional linear architecture, embracing a more advanced design that integrates routing, scheduling, and fusion mechanisms. Drawing on extensive research, this paper further identifies prevalent RAG patterns-linear, conditional, branching, and looping-and offers a comprehensive analysis of their respective implementation nuances. Modular RAG presents innovative opportunities for the conceptualization and deployment of RAG systems. Finally, the paper explores the potential emergence of new operators and paradigms, establishing a solid theoretical foundation and a practical roadmap for the continued evolution and practical deployment of RAG technologies.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-26",
    "authors": [
      {
        "authorId": "2280046531",
        "name": "Yunfan Gao"
      },
      {
        "authorId": "2275320371",
        "name": "Yun Xiong"
      },
      {
        "authorId": "2291409458",
        "name": "Meng Wang"
      },
      {
        "authorId": "2256769434",
        "name": "Haofen Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "91e011a952de940e5aea485fed5e49140924a8ca",
    "url": "https://www.semanticscholar.org/paper/91e011a952de940e5aea485fed5e49140924a8ca",
    "title": "FIT-RAG: Black-Box RAG with Factual Information and Token Reduction",
    "abstract": "Due to the extraordinarily large number of parameters, fine-tuning Large Language Models (LLMs) to update long-tail or out-of-date knowledge is impractical in lots of applications. To avoid fine-tuning, we can alternatively treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG. Recently, black-box RAG has achieved success in knowledge-intensive tasks and has gained much attention. Existing black-box RAG methods typically fine-tune the retriever to cater to LLMs’ preferences and concatenate all the retrieved documents as the input, which suffers from two issues: (1) Ignorance of Factual Information. The LLM preferred documents may not contain the factual information for the given question, which can mislead the retriever and hurt the effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating all the retrieved documents brings large amounts of unnecessary tokens for LLMs, which degenerates the efficiency of black-box RAG. To address these issues, this paper proposes a novel black-box RAG framework which utilizes the factual information in the retrieval and reduces the number of tokens for augmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by constructing a bi-label document scorer which takes the factual information and LLMs’ preferences as labels respectively. Besides, it reduces the tokens by introducing a self-knowledge recognizer and a sub-document-level token reducer, which enables FIT-RAG to avoid unnecessary augmentation and reduce augmentation tokens as much as possible. FIT-RAG achieves both superior effectiveness and efficiency, which is validated by extensive experiments across three open-domain question-answering datasets: TriviaQA, NQ and PopQA. FIT-RAG can improve the answering accuracy of Llama2-13B-Chat by 14.3% on TriviaQA, 19.9% on NQ and 27.5% on PopQA, respectively. Furthermore, it can save approximately half of the tokens on average across the three datasets.",
    "venue": "ACM Transactions on Information Systems",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-21",
    "authors": [
      {
        "authorId": "2293238106",
        "name": "Yuren Mao"
      },
      {
        "authorId": "2223421628",
        "name": "Xuemei Dong"
      },
      {
        "authorId": "2292411692",
        "name": "Wenyi Xu"
      },
      {
        "authorId": "2292513358",
        "name": "Yunjun Gao"
      },
      {
        "authorId": "2292401852",
        "name": "Bin Wei"
      },
      {
        "authorId": "2292448869",
        "name": "Ying Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.87639203842082
  },
  {
    "paperId": "4b0c63c86e8cdcc7c0f45246c92c3e98d77855d7",
    "url": "https://www.semanticscholar.org/paper/4b0c63c86e8cdcc7c0f45246c92c3e98d77855d7",
    "title": "RAG based Question-Answering for Contextual Response Prediction System",
    "abstract": "Large Language Models (LLMs) have shown versatility in various Natural Language Processing (NLP) tasks, including their potential as effective question-answering systems. However, to provide precise and relevant information in response to specific customer queries in industry settings, LLMs require access to a comprehensive knowledge base to avoid hallucinations. Retrieval Augmented Generation (RAG) emerges as a promising technique to address this challenge. Yet, developing an accurate question-answering framework for real-world applications using RAG entails several challenges: 1) data availability issues, 2) evaluating the quality of generated content, and 3) the costly nature of human evaluation. In this paper, we introduce an end-to-end framework that employs LLMs with RAG capabilities for industry use cases. Given a customer query, the proposed system retrieves relevant knowledge documents and leverages them, along with previous chat history, to generate response suggestions for customer service agents in the contact centers of a major retail company. Through comprehensive automated and human evaluations, we show that this solution outperforms the current BERT-based algorithms in accuracy and relevance. Our findings suggest that RAG-based LLMs can be an excellent support to human customer service representatives by lightening their workload.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-05",
    "authors": [
      {
        "authorId": "2319823357",
        "name": "Sriram Veturi"
      },
      {
        "authorId": "1491319666",
        "name": "Saurabh Vaichal"
      },
      {
        "authorId": "2319830658",
        "name": "Reshma Lal Jagadheesh"
      },
      {
        "authorId": "66674465",
        "name": "Nafis Irtiza Tripto"
      },
      {
        "authorId": "2319771117",
        "name": "Nian Yan"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "c4aeec57b9ad4fa36cbd9bdad05dbbbd340183df",
    "url": "https://www.semanticscholar.org/paper/c4aeec57b9ad4fa36cbd9bdad05dbbbd340183df",
    "title": "ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling",
    "abstract": "Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks. However, existing retrievers are often misaligned with LLMs due to their separate training processes and the black-box nature of LLMs. To address this challenge, we propose ARL2, a retriever learning technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and score relevant evidence, enabling learning the retriever from robust LLM supervision. Furthermore, ARL2 uses an adaptive self-training strategy for curating high-quality and diverse relevance data, which can effectively reduce the annotation cost. Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared to the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer learning capabilities and strong zero-shot generalization abilities. Our code will be published at \\url{https://github.com/zhanglingxi-cs/ARL2}.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-21",
    "authors": [
      {
        "authorId": "2284884352",
        "name": "Lingxi Zhang"
      },
      {
        "authorId": "2259265562",
        "name": "Yue Yu"
      },
      {
        "authorId": "2284955273",
        "name": "Kuan Wang"
      },
      {
        "authorId": "145657504",
        "name": "Chao Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "799cae3a0f79d4b15b22201ded6b5af4be2c766e",
    "url": "https://www.semanticscholar.org/paper/799cae3a0f79d4b15b22201ded6b5af4be2c766e",
    "title": "ReSLLM: Large Language Models are Strong Resource Selectors for Federated Search",
    "abstract": "Federated search, which involves integrating results from multiple independent search engines, will become increasingly pivotal in the context of Retrieval-Augmented Generation pipelines empowering LLM-based applications such as chatbots. These systems often distribute queries among various search engines, ranging from specialized (e.g., PubMed) to general (e.g., Google), based on the nature of user utterances. A critical aspect of federated search is resource selection - the selection of appropriate resources prior to issuing the query to ensure high-quality and rapid responses, and contain costs associated with calling the external search engines. However, current SOTA resource selection methodologies primarily rely on feature-based learning approaches. These methods often involve the labour intensive and expensive creation of training labels for each resource. In contrast, LLMs have exhibited strong effectiveness as zero-shot methods across NLP and IR tasks. We hypothesise that in the context of federated search LLMs can assess the relevance of resources without the need for extensive predefined labels or features. In this paper, we propose ReSLLM. Our ReSLLM method exploits LLMs to drive the selection of resources in federated search in a zero-shot setting. In addition, we devise an unsupervised fine tuning protocol, the Synthetic Label Augmentation Tuning (SLAT), where the relevance of previously logged queries and snippets from resources is predicted using an off-the-shelf LLM and then in turn used to fine-tune ReSLLM with respect to resource selection. Our empirical evaluation and analysis details the factors influencing the effectiveness of LLMs in this context. The results showcase the merits of ReSLLM for resource selection: not only competitive effectiveness in the zero-shot setting, but also obtaining large when fine-tuned using SLAT-protocol.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-31",
    "authors": [
      {
        "authorId": "2146514461",
        "name": "Shuai Wang"
      },
      {
        "authorId": "1630489015",
        "name": "Shengyao Zhuang"
      },
      {
        "authorId": "1783566",
        "name": "B. Koopman"
      },
      {
        "authorId": "1692855",
        "name": "G. Zuccon"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "7b56f03e742352c9aeca6714fdac30092ca7ddb5",
    "url": "https://www.semanticscholar.org/paper/7b56f03e742352c9aeca6714fdac30092ca7ddb5",
    "title": "Beyond Words: On Large Language Models Actionability in Mission-Critical Risk Analysis",
    "abstract": "Context. Risk analysis assesses potential risks in specific scenarios. Risk analysis principles are context-less; the same methodology can be applied to a risk connected to health and information technology security. Risk analysis requires a vast knowledge of national and international regulations and standards and is time and effort-intensive. A large language model can quickly summarize information in less time than a human and can be fine-tuned to specific tasks. Aim. Our empirical study aims to investigate the effectiveness of Retrieval-Augmented Generation and fine-tuned LLM in risk analysis. To our knowledge, no prior study has explored its capabilities in risk analysis. Method. We manually curated 193 unique scenarios leading to 1283 representative samples from over 50 mission-critical analyses archived by the industrial context team in the last five years. We compared the base GPT-3.5 and GPT-4 models versus their Retrieval-Augmented Generation and fine-tuned counterparts. We employ two human experts as competitors of the models and three other human experts to review the models and the former human experts' analysis. The reviewers analyzed 5,000 scenario analyses. Results and Conclusions. Human experts demonstrated higher accuracy, but LLMs are quicker and more actionable. Moreover, our findings show that RAG-assisted LLMs have the lowest hallucination rates, effectively uncovering hidden risks and complementing human expertise. Thus, the choice of model depends on specific needs, with FTMs for accuracy, RAG for hidden risks discovery, and base models for comprehensiveness and actionability. Therefore, experts can leverage LLMs as an effective complementing companion in risk analysis within a condensed timeframe. They can also save costs by averting unnecessary expenses associated with implementing unwarranted countermeasures.",
    "venue": "International Symposium on Empirical Software Engineering and Measurement",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-06-11",
    "authors": [
      {
        "authorId": "2180646859",
        "name": "Matteo Esposito"
      },
      {
        "authorId": "2293318055",
        "name": "Francesco Palagiano"
      },
      {
        "authorId": "2266468602",
        "name": "Valentina Lenarduzzi"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "2b74676b1a34b32c9a4d16000641eb90dc1d25dd",
    "url": "https://www.semanticscholar.org/paper/2b74676b1a34b32c9a4d16000641eb90dc1d25dd",
    "title": "RAGSys: Item-Cold-Start Recommender as RAG System",
    "abstract": "Large Language Models (LLM) hold immense promise for real-world applications, but their generic knowledge often falls short of domain-specific needs. Fine-tuning, a common approach, can suffer from catastrophic forgetting and hinder generalizability. In-Context Learning (ICL) offers an alternative, which can leverage Retrieval-Augmented Generation (RAG) to provide LLMs with relevant demonstrations for few-shot learning tasks. This paper explores the desired qualities of a demonstration retrieval system for ICL. We argue that ICL retrieval in this context resembles item-cold-start recommender systems, prioritizing discovery and maximizing information gain over strict relevance. We propose a novel evaluation method that measures the LLM's subsequent performance on NLP tasks, eliminating the need for subjective diversity scores. Our findings demonstrate the critical role of diversity and quality bias in retrieved demonstrations for effective ICL, and highlight the potential of recommender system techniques in this domain.",
    "venue": "IR-RAG@SIGIR",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-27",
    "authors": [
      {
        "authorId": "2303471987",
        "name": "Emile Contal"
      },
      {
        "authorId": "2283934772",
        "name": "Garrin McGoldrick"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "0fa1b06de73d01c2bfcfd16f0f5cbafb7fd1c553",
    "url": "https://www.semanticscholar.org/paper/0fa1b06de73d01c2bfcfd16f0f5cbafb7fd1c553",
    "title": "Learning When to Retrieve, What to Rewrite, and How to Respond in Conversational QA",
    "abstract": "Augmenting Large Language Models (LLMs) with information retrieval capabilities (i.e., Retrieval-Augmented Generation (RAG)) has proven beneficial for knowledge-intensive tasks. However, understanding users' contextual search intent when generating responses is an understudied topic for conversational question answering (QA). This conversational extension leads to additional concerns when compared to single-turn QA as it is more challenging for systems to comprehend conversational context and manage retrieved passages over multiple turns. In this work, we propose a method for enabling LLMs to decide when to retrieve in RAG settings given a conversational context. When retrieval is deemed necessary, the LLM then rewrites the conversation for passage retrieval and judges the relevance of returned passages before response generation. Operationally, we build on the single-turn SELF-RAG framework (Asai et al., 2023) and propose SELF-multi-RAG for conversational settings. SELF-multi-RAG demonstrates improved capabilities over single-turn variants with respect to retrieving relevant passages (by using summarized conversational context) and assessing the quality of generated responses. Experiments on three conversational QA datasets validate the enhanced response generation capabilities of SELF-multi-RAG, with improvements of ~13% measured by human annotation.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-09-23",
    "authors": [
      {
        "authorId": "2322506227",
        "name": "Nirmal Roy"
      },
      {
        "authorId": "10430740",
        "name": "Leonardo F. R. Ribeiro"
      },
      {
        "authorId": "2221287630",
        "name": "Rexhina Blloshmi"
      },
      {
        "authorId": "2320520948",
        "name": "Kevin Small"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "3ff32b48facacdeced0b6782646d077fad31a6cc",
    "url": "https://www.semanticscholar.org/paper/3ff32b48facacdeced0b6782646d077fad31a6cc",
    "title": "Gemini MultiPDF Chatbot: Multiple Document RAG Chatbot using Gemini Large Language Model",
    "abstract": "Abstract: The Gemini MultiPDF Chatbot represents a groundbreaking advancement in natural language processing (NLP) by integrating Retrieval-Augmented Generation (RAG) techniques with the Gemini Large Language Model. This innovative chatbot is designed to handle multiple document retrieval and generation tasks, leveraging the extensive knowledge base of the Gemini model. By harnessing RAG methods, the chatbot enhances its ability to acquire, comprehend, and generate responses across diverse knowledge sources contained within multiple PDF documents. The integration of Gemini's powerful language understanding capabilities with RAG facilitates seamless interaction with users, offering comprehensive and contextually relevant responses. This paper presents the design, implementation, and evaluation of the Gemini MultiPDF Chatbot, demonstrating its effectiveness in navigating complex information landscapes and delivering high-quality conversational experiences.",
    "venue": "International Journal for Research in Applied Science and Engineering Technology",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-31",
    "authors": [
      {
        "authorId": "2299838343",
        "name": "Mohd Kaif"
      },
      {
        "authorId": "2299894706",
        "name": "Sanskar Sharma"
      },
      {
        "authorId": "2284093020",
        "name": "Dr. Sadhana Rana"
      }
    ],
    "source": "semantic_scholar",
    "score": 80
  },
  {
    "paperId": "134b650b3ab920313d617856fdaf686a2eef3753",
    "url": "https://www.semanticscholar.org/paper/134b650b3ab920313d617856fdaf686a2eef3753",
    "title": "Aegis:An Advanced LLM-Based Multi-Agent for Intelligent Functional Safety Engineering",
    "abstract": "Functional safety is a critical aspect of automotive engineering, encompassing all phases of a vehicle’s lifecycle, including design, development, production, operation, and decommissioning. This domain involves highly knowledge-intensive tasks. This paper introduces Aegis: An Advanced LLM-Based Multi-Agent for Intelligent Functional Safety Engineering. Aegis is specifically designed to support complex functional safety tasks within the automotive sector. It is tailored to perform Hazard Analysis and Risk Assessment (HARA), document Functional Safety Requirements (FSR), and plan test cases for Automatic Emergency Braking (AEB) systems. The most advanced version, Aegis-Max, leverages Retrieval-Augmented Generation (RAG) and reflective mechanisms to enhance its capability in managing complex, knowledge-intensive tasks. Additionally, targeted prompt refinement by professional functional safety practitioners can significantly optimize Aegis’s performance in the functional safety domain. This paper demonstrates the potential of Aegis to improve the efficiency and effectiveness of functional safety processes in automotive engineering.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-10-16",
    "authors": [
      {
        "authorId": "2326124654",
        "name": "Lu Shi"
      },
      {
        "authorId": "2326117214",
        "name": "Bin Qi"
      },
      {
        "authorId": "2326260317",
        "name": "Jiarui Luo"
      },
      {
        "authorId": "2326155875",
        "name": "Yang Zhang"
      },
      {
        "authorId": "2326634132",
        "name": "Zhanzhao Liang"
      },
      {
        "authorId": "2264885230",
        "name": "Zhaowei Gao"
      },
      {
        "authorId": "2327016489",
        "name": "Wenke Deng"
      },
      {
        "authorId": "2326333701",
        "name": "Lin Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "0709e6d66e7356fc4adb63834fd9e8503978ab6b",
    "url": "https://www.semanticscholar.org/paper/0709e6d66e7356fc4adb63834fd9e8503978ab6b",
    "title": "Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception",
    "abstract": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to large language models (LLMs), often overlooks the crucial aspect of text chunking within its pipeline, which impacts the quality of knowledge-intensive tasks. This paper introduces the concept of Meta-Chunking, which refers to a granularity between sentences and paragraphs, consisting of a collection of sentences within a paragraph that have deep linguistic logical connections. To implement Meta-Chunking, we designed Perplexity (PPL) Chunking, which balances performance and speed, and precisely identifies the boundaries of text chunks by analyzing the characteristics of context perplexity distribution. Additionally, considering the inherent complexity of different texts, we propose a strategy that combines PPL Chunking with dynamic merging to achieve a balance between fine-grained and coarse-grained text chunking. Experiments conducted on eleven datasets demonstrate that Meta-Chunking can more efficiently improve the performance of single-hop and multi-hop question answering based on RAG. For instance, on the 2WikiMultihopQA dataset, it outperforms similarity chunking by 1.32 while only consuming 45.8% of the time. Furthermore, through the analysis of models of various scales and types, we observed that PPL Chunking exhibits notable flexibility and adaptability. Our code is available at https://github.com/IAAR-Shanghai/Meta-Chunking.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-16",
    "authors": [
      {
        "authorId": "2326243408",
        "name": "Jihao Zhao"
      },
      {
        "authorId": "2326171175",
        "name": "Zhiyuan Ji"
      },
      {
        "authorId": "2326117268",
        "name": "Pengnian Qi"
      },
      {
        "authorId": "2268393907",
        "name": "Simin Niu"
      },
      {
        "authorId": "2268400606",
        "name": "Bo Tang"
      },
      {
        "authorId": "2268399953",
        "name": "Feiyu Xiong"
      },
      {
        "authorId": "2268429641",
        "name": "Zhiyu Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "2c2de8c83406e3173ab414bec1aabad69c7dc74c",
    "url": "https://www.semanticscholar.org/paper/2c2de8c83406e3173ab414bec1aabad69c7dc74c",
    "title": "A RAG Approach for Generating Competency Questions in Ontology Engineering",
    "abstract": "Competency question (CQ) formulation is central to several ontology development and evaluation methodologies. Traditionally, the task of crafting these competency questions heavily relies on the effort of domain experts and knowledge engineers which is often time-consuming and labor-intensive. With the emergence of Large Language Models (LLMs), there arises the possibility to automate and enhance this process. Unlike other similar works which use existing ontologies or knowledge graphs as input to LLMs, we present a retrieval-augmented generation (RAG) approach that uses LLMs for the automatic generation of CQs given a set of scientific papers considered to be a domain knowledge base. We investigate its performance and specifically, we study the impact of different number of papers to the RAG and different temperature setting of the LLM. We conduct experiments using GPT-4 on two domain ontology engineering tasks and compare results against ground-truth CQs constructed by domain experts. Empirical assessments on the results, utilizing evaluation metrics (precision and consistency), reveal that compared to zero-shot prompting, adding relevant domain knowledge to the RAG improves the performance of LLMs on generating CQs for concrete ontology engineering tasks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-13",
    "authors": [
      {
        "authorId": "2029500336",
        "name": "Xueli Pan"
      },
      {
        "authorId": "1708506",
        "name": "J. V. Ossenbruggen"
      },
      {
        "authorId": "2238960795",
        "name": "Victor de Boer"
      },
      {
        "authorId": "2287590366",
        "name": "Zhisheng Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "a42640277dc6669bed00a7c47afb7b02ce9f939d",
    "url": "https://www.semanticscholar.org/paper/a42640277dc6669bed00a7c47afb7b02ce9f939d",
    "title": "Code-Based English Models Surprising Performance on Chinese QA Pair Extraction Task",
    "abstract": "In previous studies, code-based models have consistently outperformed text-based models in reasoning-intensive scenarios. When generating our knowledge base for Retrieval-Augmented Generation (RAG), we observed that code-based models also perform exceptionally well in Chinese QA Pair Extraction task. Further, our experiments and the metrics we designed discovered that code-based models containing a certain amount of Chinese data achieve even better performance. Additionally, the capabilities of code-based English models in specified Chinese tasks offer a distinct perspective for discussion on the philosophical\"Chinese Room\"thought experiment.",
    "venue": "",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-01-16",
    "authors": [
      {
        "authorId": "2280101298",
        "name": "Linghan Zheng"
      },
      {
        "authorId": "2280202230",
        "name": "Hui Liu"
      },
      {
        "authorId": "2280106415",
        "name": "Xiaojun Lin"
      },
      {
        "authorId": "2280110279",
        "name": "Jiayuan Dong"
      },
      {
        "authorId": "2280068180",
        "name": "Yue Sheng"
      },
      {
        "authorId": "2280067368",
        "name": "Gang Shi"
      },
      {
        "authorId": "2248831642",
        "name": "Zhiwei Liu"
      },
      {
        "authorId": "2280102996",
        "name": "Hongwei Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 65
  },
  {
    "paperId": "387cdedc3925eca868d223179b717b85b48441e4",
    "url": "https://www.semanticscholar.org/paper/387cdedc3925eca868d223179b717b85b48441e4",
    "title": "Molly: Making Large Language Model Agents Solve Python Problem More Logically",
    "abstract": "Applying large language models (LLMs) as teaching assists has attracted much attention as an integral part of intelligent education, particularly in computing courses. To reduce the gap between the LLMs and the computer programming education expert, fine-tuning and retrieval augmented generation (RAG) are the two mainstream methods in existing researches. However, fine-tuning for specific tasks is resource-intensive and may diminish the model`s generalization capabilities. RAG can perform well on reducing the illusion of LLMs, but the generation of irrelevant factual content during reasoning can cause significant confusion for learners. To address these problems, we introduce the Molly agent, focusing on solving the proposed problem encountered by learners when learning Python programming language. Our agent automatically parse the learners' questioning intent through a scenario-based interaction, enabling precise retrieval of relevant documents from the constructed knowledge base. At generation stage, the agent reflect on the generated responses to ensure that they not only align with factual content but also effectively answer the user's queries. Extensive experimentation on a constructed Chinese Python QA dataset shows the effectiveness of the Molly agent, indicating an enhancement in its performance for providing useful responses to Python questions.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-12-24",
    "authors": [
      {
        "authorId": "2283922453",
        "name": "Rui Xiao"
      },
      {
        "authorId": "2284025975",
        "name": "Jiong Wang"
      },
      {
        "authorId": "2284333853",
        "name": "Lu Han"
      },
      {
        "authorId": "2283931575",
        "name": "Na Zong"
      },
      {
        "authorId": "2337060249",
        "name": "Han Wu"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "8927115384fffb549ee5088885a4498ca1b03d41",
    "url": "https://www.semanticscholar.org/paper/8927115384fffb549ee5088885a4498ca1b03d41",
    "title": "A RAG-Based Institutional Assistant",
    "abstract": "Although large language models (LLMs) demonstrate strong text generation capabilities, they struggle in scenarios requiring access to structured knowledge bases or specific documents, limiting their effectiveness in knowledge-intensive tasks. To address this limitation, retrieval-augmented generation (RAG) models have been developed, enabling generative models to incorporate relevant document fragments into their inputs. In this paper, we design and evaluate a RAG-based virtual assistant specifically tailored for the University of S\\~ao Paulo. Our system architecture comprises two key modules: a retriever and a generative model. We experiment with different types of models for both components, adjusting hyperparameters such as chunk size and the number of retrieved documents. Our optimal retriever model achieves a Top-5 accuracy of 30%, while our most effective generative model scores 22.04\\% against ground truth answers. Notably, when the correct document chunks are supplied to the LLMs, accuracy significantly improves to 54.02%, an increase of over 30 percentage points. Conversely, without contextual input, performance declines to 13.68%. These findings highlight the critical role of database access in enhancing LLM performance. They also reveal the limitations of current semantic search methods in accurately identifying relevant documents and underscore the ongoing challenges LLMs face in generating precise responses.",
    "venue": "",
    "year": 2025,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2025-01-23",
    "authors": [
      {
        "authorId": "2341717702",
        "name": "Gustavo Kuratomi"
      },
      {
        "authorId": "2243283631",
        "name": "Paulo Pirozelli"
      },
      {
        "authorId": "70089890",
        "name": "F. G. Cozman"
      },
      {
        "authorId": "2258709103",
        "name": "S. M. Peres"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "3971af993cb750941ef8ed313764ea0e864f48a7",
    "url": "https://www.semanticscholar.org/paper/3971af993cb750941ef8ed313764ea0e864f48a7",
    "title": "Ascle—A Python Natural Language Processing Toolkit for Medical Text Generation: Development and Evaluation Study",
    "abstract": "Background Medical texts present significant domain-specific challenges, and manually curating these texts is a time-consuming and labor-intensive process. To address this, natural language processing (NLP) algorithms have been developed to automate text processing. In the biomedical field, various toolkits for text processing exist, which have greatly improved the efficiency of handling unstructured text. However, these existing toolkits tend to emphasize different perspectives, and none of them offer generation capabilities, leaving a significant gap in the current offerings. Objective This study aims to describe the development and preliminary evaluation of Ascle. Ascle is tailored for biomedical researchers and clinical staff with an easy-to-use, all-in-one solution that requires minimal programming expertise. For the first time, Ascle provides 4 advanced and challenging generative functions: question-answering, text summarization, text simplification, and machine translation. In addition, Ascle integrates 12 essential NLP functions, along with query and search capabilities for clinical databases. Methods We fine-tuned 32 domain-specific language models and evaluated them thoroughly on 27 established benchmarks. In addition, for the question-answering task, we developed a retrieval-augmented generation (RAG) framework for large language models that incorporated a medical knowledge graph with ranking techniques to enhance the reliability of generated answers. Additionally, we conducted a physician validation to assess the quality of generated content beyond automated metrics. Results The fine-tuned models and RAG framework consistently enhanced text generation tasks. For example, the fine-tuned models improved the machine translation task by 20.27 in terms of BLEU score. In the question-answering task, the RAG framework raised the ROUGE-L score by 18% over the vanilla models. Physician validation of generated answers showed high scores for readability (4.95/5) and relevancy (4.43/5), with a lower score for accuracy (3.90/5) and completeness (3.31/5). Conclusions This study introduces the development and evaluation of Ascle, a user-friendly NLP toolkit designed for medical text generation. All code is publicly available through the Ascle GitHub repository. All fine-tuned language models can be accessed through Hugging Face.",
    "venue": "Journal of Medical Internet Research",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "https://s3.ca-central-1.amazonaws.com/assets.jmir.org/assets/preprints/preprint-60601-accepted.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-16",
    "authors": [
      {
        "authorId": "2256476730",
        "name": "Rui Yang"
      },
      {
        "authorId": "2268696297",
        "name": "Qingcheng Zeng"
      },
      {
        "authorId": "2004147623",
        "name": "Keen You"
      },
      {
        "authorId": "51195616",
        "name": "Yujie Qiao"
      },
      {
        "authorId": "2162608295",
        "name": "Lucas Huang"
      },
      {
        "authorId": "2162469478",
        "name": "Chia-Chun Hsieh"
      },
      {
        "authorId": "2238888808",
        "name": "Benjamin Rosand"
      },
      {
        "authorId": "2067298269",
        "name": "Jeremy Goldwasser"
      },
      {
        "authorId": "1402690226",
        "name": "Amisha D. Dave"
      },
      {
        "authorId": "2158872251",
        "name": "T. Keenan"
      },
      {
        "authorId": "2290809077",
        "name": "Y. Ke"
      },
      {
        "authorId": "2202564904",
        "name": "Chuan Hong"
      },
      {
        "authorId": "2291759221",
        "name": "Nan Liu"
      },
      {
        "authorId": "2128368341",
        "name": "Emily Y. Chew"
      },
      {
        "authorId": "2250049814",
        "name": "Dragomir R. Radev"
      },
      {
        "authorId": "2152217328",
        "name": "Zhiyong Lu"
      },
      {
        "authorId": "2268724507",
        "name": "Hua Xu"
      },
      {
        "authorId": "2256591935",
        "name": "Qingyu Chen"
      },
      {
        "authorId": "2248289605",
        "name": "Irene Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "aa59536123b29599115cb28027d9ddb67fc1c613",
    "url": "https://www.semanticscholar.org/paper/aa59536123b29599115cb28027d9ddb67fc1c613",
    "title": "Telecom Language Models: Must They Be Large?",
    "abstract": "The increasing interest in Large Language Models (LLMs) within the telecommunications sector underscores their potential to revolutionize operational efficiency. However, the deployment of these sophisticated models is often hampered by their substantial size and computational demands, raising concerns about their viability in resource-constrained environments. Addressing this challenge, recent advancements have seen the emergence of small language models that surprisingly exhibit performance comparable to their larger counterparts in many tasks, such as coding and common-sense reasoning. Phi-2, a compact yet powerful model, exemplifies this new wave of efficient small language models. This paper conducts a comprehensive evaluation of Phi-2’s intrinsic understanding of the telecommunications domain. Recognizing the scale-related limitations, we enhance Phi-2’s capabilities through a Retrieval-Augmented Generation approach, meticulously integrating an extensive knowledge base specifically curated with telecom standard specifications. The enhanced Phi-2 model demonstrates a profound improvement in accuracy, answering questions about telecom standards with a precision that closely rivals the more resource-intensive GPT-3.5. The paper further explores the refined capabilities of Phi-2 in addressing problem-solving scenarios within the telecom sector, highlighting its potentials and limitations.",
    "venue": "IEEE International Symposium on Personal, Indoor and Mobile Radio Communications",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-07",
    "authors": [
      {
        "authorId": "3393923",
        "name": "Nicola Piovesan"
      },
      {
        "authorId": "2261362548",
        "name": "Antonio De Domenico"
      },
      {
        "authorId": "70486867",
        "name": "Fadhel Ayed"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.19162312519754
  },
  {
    "paperId": "e38d4fbfdf13532f7f58e5831cf7bc6d7d291c1a",
    "url": "https://www.semanticscholar.org/paper/e38d4fbfdf13532f7f58e5831cf7bc6d7d291c1a",
    "title": "LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction",
    "abstract": "Large Language Models (LLMs) are increasingly adopted for applications in healthcare, reaching the performance of domain experts on tasks such as question answering and document summarisation. Despite their success on these tasks, it is unclear how well LLMs perform on tasks that are traditionally pursued in the biomedical domain, such as structured information extration. To breach this gap, in this paper, we systematically benchmark LLM performance in Medical Classification and Named Entity Recognition (NER) tasks. We aim to disentangle the contribution of different factors to the performance, particularly the impact of LLMs' task knowledge and reasoning capabilities, their (parametric) domain knowledge, and addition of external knowledge. To this end we evaluate various open LLMs -- including BioMistral and Llama-2 models -- on a diverse set of biomedical datasets, using standard prompting, Chain-of-Thought (CoT) and Self-Consistency based reasoning as well as Retrieval-Augmented Generation (RAG) with PubMed and Wikipedia corpora. Counter-intuitively, our results reveal that standard prompting consistently outperforms more complex techniques across both tasks, laying bare the limitations in the current application of CoT, self-consistency and RAG in the biomedical domain. Our findings suggest that advanced prompting methods developed for knowledge- or reasoning-intensive tasks, such as CoT or RAG, are not easily portable to biomedical tasks where precise structured outputs are required. This highlights the need for more effective integration of external knowledge and reasoning mechanisms in LLMs to enhance their performance in real-world biomedical applications.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-22",
    "authors": [
      {
        "authorId": "2313095840",
        "name": "Aishik Nagar"
      },
      {
        "authorId": "2275610067",
        "name": "Viktor Schlegel"
      },
      {
        "authorId": "2117824172",
        "name": "Thanh-Tung Nguyen"
      },
      {
        "authorId": "2321703580",
        "name": "Hao Li"
      },
      {
        "authorId": "2304897005",
        "name": "Yuping Wu"
      },
      {
        "authorId": "2123039685",
        "name": "Kuluhan Binici"
      },
      {
        "authorId": "2057271731",
        "name": "Stefan Winkler"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "710f941c0fdcc488b2cafa9b82fd44ace5093ad9",
    "url": "https://www.semanticscholar.org/paper/710f941c0fdcc488b2cafa9b82fd44ace5093ad9",
    "title": "AIPatient: Simulating Patients with EHRs and LLM Powered Agentic Workflow",
    "abstract": "Simulated patient systems play a crucial role in modern medical education and research, providing safe, integrative learning environments and enabling clinical decision-making simulations. Large Language Models (LLM) could advance simulated patient systems by replicating medical conditions and patient-doctor interactions with high fidelity and low cost. However, ensuring the effectiveness and trustworthiness of these systems remains a challenge, as they require a large, diverse, and precise patient knowledgebase, along with a robust and stable knowledge diffusion to users. Here, we developed AIPatient, an advanced simulated patient system with AIPatient Knowledge Graph (AIPatient KG) as the input and the Reasoning Retrieval-Augmented Generation (Reasoning RAG) agentic workflow as the generation backbone. AIPatient KG samples data from Electronic Health Records (EHRs) in the Medical Information Mart for Intensive Care (MIMIC)-III database, producing a clinically diverse and relevant cohort of 1,495 patients with high knowledgebase validity (F1 0.89). Reasoning RAG leverages six LLM powered agents spanning tasks including retrieval, KG query generation, abstraction, checker, rewrite, and summarization. This agentic framework reaches an overall accuracy of 94.15% in EHR-based medical Question Answering (QA), outperforming benchmarks that use either no agent or only partial agent integration. Our system also presents high readability (median Flesch Reading Ease 77.23; median Flesch Kincaid Grade 5.6), robustness (ANOVA F-value 0.6126, p>0.1), and stability (ANOVA F-value 0.782, p>0.1). The promising performance of the AIPatient system highlights its potential to support a wide range of applications, including medical education, model evaluation, and system integration.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-27",
    "authors": [
      {
        "authorId": "2118683388",
        "name": "Huizi Yu"
      },
      {
        "authorId": "2295487721",
        "name": "Jiayan Zhou"
      },
      {
        "authorId": "2261065167",
        "name": "Lingyao Li"
      },
      {
        "authorId": "2334010939",
        "name": "Shan Chen"
      },
      {
        "authorId": "2111654456",
        "name": "J. Gallifant"
      },
      {
        "authorId": "2323375415",
        "name": "Anye Shi"
      },
      {
        "authorId": "2319121061",
        "name": "Xiang Li"
      },
      {
        "authorId": "2007245028",
        "name": "Wenyue Hua"
      },
      {
        "authorId": null,
        "name": "Mingyu Jin"
      },
      {
        "authorId": "2323411732",
        "name": "Guang Chen"
      },
      {
        "authorId": "2323945396",
        "name": "Yang Zhou"
      },
      {
        "authorId": "2323500272",
        "name": "Zhao Li"
      },
      {
        "authorId": "2189368286",
        "name": "Trisha P Gupte"
      },
      {
        "authorId": "2295717104",
        "name": "Ming-Li Chen"
      },
      {
        "authorId": "2295354577",
        "name": "Zahra Azizi"
      },
      {
        "authorId": "2239061409",
        "name": "Yongfeng Zhang"
      },
      {
        "authorId": "2247078038",
        "name": "T. Assimes"
      },
      {
        "authorId": "2238914465",
        "name": "Xin Ma"
      },
      {
        "authorId": "2260340486",
        "name": "D. Bitterman"
      },
      {
        "authorId": "2323462463",
        "name": "Lin Lu"
      },
      {
        "authorId": "2324920681",
        "name": "Lizhou Fan"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "1494dfcf564b09c6ebbfc772fa986937bb14d699",
    "url": "https://www.semanticscholar.org/paper/1494dfcf564b09c6ebbfc772fa986937bb14d699",
    "title": "DynamicER: Resolving Emerging Mentions to Dynamic Entities for RAG",
    "abstract": "In the rapidly evolving landscape of language, resolving new linguistic expressions in continuously updating knowledge bases remains a formidable challenge. This challenge becomes critical in retrieval-augmented generation (RAG) with knowledge bases, as emerging expressions hinder the retrieval of relevant documents, leading to generator hallucinations. To address this issue, we introduce a novel task aimed at resolving emerging mentions to dynamic entities and present DynamicER benchmark. Our benchmark includes dynamic entity mention resolution and entity-centric knowledge-intensive QA task, evaluating entity linking and RAG model’s adaptability to new expressions, respectively. We discovered that current entity linking models struggle to link these new expressions to entities. Therefore, we propose a temporal segmented clustering method with continual adaptation, effectively managing the temporal dynamics of evolving entities and emerging mentions. Extensive experiments demonstrate that our method outperforms existing baselines, enhancing RAG model performance on QA task with resolved mentions.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-10-15",
    "authors": [
      {
        "authorId": "2305648889",
        "name": "Jinyoung Kim"
      },
      {
        "authorId": "2261279448",
        "name": "Dayoon Ko"
      },
      {
        "authorId": "2261362214",
        "name": "Gunhee Kim"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "d520335537f297b50497da9b751700fce25b876d",
    "url": "https://www.semanticscholar.org/paper/d520335537f297b50497da9b751700fce25b876d",
    "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments",
    "abstract": "Retrieval Augmented Generation (RAG) is a common method for integrating external knowledge into pretrained Large Language Models (LLMs) to enhance accuracy and relevancy in question answering (QA) tasks. However, prompt engineering and resource efficiency remain significant bottlenecks in developing optimal and robust RAG solutions for real-world QA applications. Recent studies have shown success in using fine tuning to address these problems; in particular, Retrieval Augmented Fine Tuning (RAFT) applied to smaller 7B models has demonstrated superior performance compared to RAG setups with much larger models such as GPT-3.5. The combination of RAFT with parameter-efficient fine tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA), promises an even more efficient solution, yet remains an unexplored area. In this work, we combine RAFT with LoRA to reduce fine tuning and storage requirements and gain faster inference times while maintaining comparable RAG performance. This results in a more compute-efficient RAFT, or CRAFT, which is particularly useful for knowledge-intensive QA tasks in resource-constrained environments where internet access may be restricted and hardware resources limited.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-26",
    "authors": [
      {
        "authorId": "2322992640",
        "name": "Isaac Chung"
      },
      {
        "authorId": "2322982756",
        "name": "Phat Vo"
      },
      {
        "authorId": "2322991957",
        "name": "Arman Kizilkale"
      },
      {
        "authorId": "2322982549",
        "name": "Aaron Reite"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "a7ca1bce0af7fe4703f5c3296db2dcc8dc112f20",
    "url": "https://www.semanticscholar.org/paper/a7ca1bce0af7fe4703f5c3296db2dcc8dc112f20",
    "title": "FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference",
    "abstract": "Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that sets the state-of-the-art on many knowledge-intensive NLP tasks. However, the architecture used for FiD was chosen by making minimal modifications to a standard T5 model, which our analysis shows to be highly suboptimal for a retrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to the encoder, while the majority of inference time results from memory bandwidth constraints in the decoder. We propose two simple changes to the FiD architecture to alleviate memory bandwidth constraints, and speed up inference by 7x. This allows us to use a much larger decoder at modest cost. We denote FiD with the above modifications as FiDO, and show that it strongly improves performance over existing FiD models for a wide range of inference budgets. For example, FiDO-Large-XXL performs faster inference than FiD-Base and achieves better performance than FiD-Large.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "citationCount": 31,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2212.08153",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-12-15",
    "authors": [
      {
        "authorId": "21379393",
        "name": "Michiel de Jong"
      },
      {
        "authorId": "51199981",
        "name": "Yury Zemlyanskiy"
      },
      {
        "authorId": "1643737606",
        "name": "J. Ainslie"
      },
      {
        "authorId": "143883142",
        "name": "Nicholas FitzGerald"
      },
      {
        "authorId": "144074891",
        "name": "Sumit K. Sanghai"
      },
      {
        "authorId": "145757665",
        "name": "Fei Sha"
      },
      {
        "authorId": "2058480371",
        "name": "W. Cohen"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.9860385419959
  },
  {
    "paperId": "4b2fc4597b1519207fd63c2b2b33a4665c988de7",
    "url": "https://www.semanticscholar.org/paper/4b2fc4597b1519207fd63c2b2b33a4665c988de7",
    "title": "DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models",
    "abstract": "Knowledge-intensive language understanding tasks require Language Models (LMs) to integrate relevant context, mitigating their inherent weaknesses, such as incomplete or outdated knowledge. However, conflicting knowledge can be present in the LM's parameters, termed intra-memory conflict, which can affect a model's propensity to accept contextual knowledge. To study the effect of intra-memory conflict on an LM's ability to accept relevant context, we utilize two knowledge conflict measures and a novel dataset containing inherently conflicting data, DynamicQA. This dataset includes facts with a temporal dynamic nature where facts can change over time and disputable dynamic facts, which can change depending on the viewpoint. DynamicQA is the first to include real-world knowledge conflicts and provide context to study the link between the different types of knowledge conflicts. We also evaluate several measures on their ability to reflect the presence of intra-memory conflict: semantic entropy and a novel coherent persuasion score. With our extensive experiments, we verify that LMs exhibit a greater degree of intra-memory conflict with dynamic facts compared to facts that have a single truth value. Furthermore, we reveal that facts with intra-memory conflict are harder to update with context, suggesting that retrieval-augmented generation will struggle with the most commonly adapted facts.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2407.17023",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-24",
    "authors": [
      {
        "authorId": "2284760365",
        "name": "Sara Vera Marjanovi'c"
      },
      {
        "authorId": "2298957995",
        "name": "Haeun Yu"
      },
      {
        "authorId": "145676297",
        "name": "Pepa Atanasova"
      },
      {
        "authorId": "1954475",
        "name": "Maria Maistro"
      },
      {
        "authorId": "2276440036",
        "name": "Christina Lioma"
      },
      {
        "authorId": "1736067",
        "name": "Isabelle Augenstein"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "447bf7d49b1e931466e4c54fef3d5ca372a6ad06",
    "url": "https://www.semanticscholar.org/paper/447bf7d49b1e931466e4c54fef3d5ca372a6ad06",
    "title": "Intuitive or Dependent? Investigating LLMs' Behavior Style to Conflicting Prompts",
    "abstract": "This study investigates the behaviors of Large Language Models (LLMs) when faced with conflicting prompts versus their internal memory. This will not only help to understand LLMs' decision mechanism but also benefit real-world applications, such as retrieval-augmented generation (RAG). Drawing on cognitive theory, we target the first scenario of decision-making styles where there is no superiority in the conflict and categorize LLMs' preference into dependent, intuitive, and rational/irrational styles. Another scenario of factual robustness considers the correctness of prompt and memory in knowledge-intensive tasks, which can also distinguish if LLMs behave rationally or irrationally in the first scenario. To quantify them, we establish a complete benchmarking framework including a dataset, a robustness evaluation pipeline, and corresponding metrics. Extensive experiments with seven LLMs reveal their varying behaviors. And, with role play intervention, we can change the styles, but different models present distinct adaptivity and upper-bound. One of our key takeaways is to optimize models or the prompts according to the identified style. For instance, RAG models with high role play adaptability may dynamically adjust the interventions according to the quality of retrieval results -- being dependent to better leverage informative context; and, being intuitive when external prompt is noisy.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.17415",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-09-29",
    "authors": [
      {
        "authorId": "2249532957",
        "name": "Jiahao Ying"
      },
      {
        "authorId": "2273914010",
        "name": "Yixin Cao"
      },
      {
        "authorId": "2249539564",
        "name": "Kai Xiong"
      },
      {
        "authorId": "2249538512",
        "name": "Long Cui"
      },
      {
        "authorId": "2256764940",
        "name": "Yidong He"
      },
      {
        "authorId": "2249562267",
        "name": "Yongbin Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "e67c279149d956582eae6fb7ea28048b255343c2",
    "url": "https://www.semanticscholar.org/paper/e67c279149d956582eae6fb7ea28048b255343c2",
    "title": "Combining Variational Sampling and Metropolis--Hastings Sampling for Paraphrase Generation",
    "abstract": "Paraphrasing involves changing the expression of a sentence and rewording it to inform the same information as the original sentence and can occur at wordlevel, phrase-level, or sentence-level. Paraphrasing task has been attracting attention in recent years as several natural language processing (NLP) applications such as question answering, information extraction, information retrieval, and summarization benefit from the success of automatic paraphrase generation. Researchers have developed various paraphrase generation techniques, including knowledge-based approaches, supervised data-driven approaches, and unsupervised data-driven approaches. Knowledge-based approaches are labor intensive and do not generalize well and supervised approaches require massive parallel corpora of pairs of sentences and paraphrases. In this work, we propose an unsupervised paraphrasing technique that works in word-level as well as phrase-level for sentence-level paraphrase generation. Existing work either samples directly from sentence space or from a variational latent space while our work combines them both. We show the drawbacks and difficulties of techniques that work at word-level only and propose a technique consisting of three word-level operations (word replacement, word deletion, and word insertion) and a novel phrase-level paraphrasing operation (phrase replacement). The three word-level operations sample directly from the sentence space while our phrase-level operation samples from the latent space of a variational autoencoder (VAE) trained on phrases. We perform paraphrase generation iteratively with the objective of generii ating paraphrases that are 1) fluent, and 2) close in semantic information to the input sentence. We use Metropolis–Hastings (MH) algorithm, a Markov Chain Monte Carlo (MCMC) algorithm, to sample from sentence space and latent space. In each iteration, we randomly select a word/phrase and an operation to form a proposal and use MH to accept or reject the proposal and generate a paraphrase. We show the effectiveness of our approach with a series of experiments. First, we train a VAE using Stanford Natural Language Inference (SNLI) dataset [8] and Quora dataset for phrase replacement operation. Second, we evaluate our approach on the Quora dataset including 139k pairs of questions and paraphrases using iBLEU score as our main evaluation metric. The results show that our novel phrase replacement operation improves the quality of paraphrases when compared with techniques paraphrasing by direct wordlevel sampling only. We show our phrase-level operation can effectively edit multiple words at a time and generate high quality paraphrases. We also discuss the difficulties of evaluation with iBLEU score and VAE training. https://www.kaggle.com/c/quora-question-pairs/data iii To my parents, For their endless love, support, and encouragement.",
    "venue": "",
    "year": 2021,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Mathematics"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2148764345",
        "name": "Ali Hejazizo"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "28f831ef1e591a6c897c0eff192711d5f49eeba5",
    "url": "https://www.semanticscholar.org/paper/28f831ef1e591a6c897c0eff192711d5f49eeba5",
    "title": "EvoPat: A Multi-LLM-based Patents Summarization and Analysis Agent",
    "abstract": "The rapid growth of scientific techniques and knowledge is reflected in the exponential increase in new patents filed annually. While these patents drive innovation, they also present significant burden for researchers and engineers, especially newcomers. To avoid the tedious work of navigating a vast and complex landscape to identify trends and breakthroughs, researchers urgently need efficient tools to summarize, evaluate, and contextualize patents, revealing their innovative contributions and underlying scientific principles.To address this need, we present EvoPat, a multi-LLM-based patent agent designed to assist users in analyzing patents through Retrieval-Augmented Generation (RAG) and advanced search strategies. EvoPat leverages multiple Large Language Models (LLMs), each performing specialized roles such as planning, identifying innovations, and conducting comparative evaluations. The system integrates data from local databases, including patents, literature, product catalogous, and company repositories, and online searches to provide up-to-date insights. The ability to collect information not included in original database automatically is also implemented. Through extensive testing in the natural language processing (NLP) domain, we demonstrate that EvoPat outperforms GPT-4 in tasks such as patent summarization, comparative analysis, and technical evaluation. EvoPat represents a significant step toward creating AI-powered tools that empower researchers and engineers to efficiently navigate the complexities of the patent landscape.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-12-24",
    "authors": [
      {
        "authorId": "2337173723",
        "name": "Suyuan Wang"
      },
      {
        "authorId": "2337076978",
        "name": "Xueqian Yin"
      },
      {
        "authorId": "2337070077",
        "name": "Menghao Wang"
      },
      {
        "authorId": "2337050862",
        "name": "Ruofeng Guo"
      },
      {
        "authorId": "2336957181",
        "name": "Kai Nan"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "0274de2040897e66d41e7edb6a9673019ee773c8",
    "url": "https://www.semanticscholar.org/paper/0274de2040897e66d41e7edb6a9673019ee773c8",
    "title": "Intelligent Web Search Automation",
    "abstract": "The Intelligent Web Search Automation project harnesses ultramodern technologies, including Artificial Intelligence (AI), Large Language Models (LLMs), and agentic AI systems, to redefine and streamline web search processes. By utilizing frameworks like LangChain—a comprehensive toolkit for constructing and managing LLM-powered applications—and integrating custom search engines with Google APIs, this project delivers a robust solution for automated search and data extraction. LangChain’s capabilities, including OpenAI embeddings, document loaders like Cheerio Web Loader, memory vector stores, and advanced chain management, form the core of the architecture. These tools enable seamless splitting, embedding, and retrieval of large-scale data with context-aware memory storage. Agentic AI, leveraging frameworks such as ReAct for synergizing reasoning and action, further enhances automation by enabling dynamic decision-making and adaptive task execution. The system employs retrieval-augmented generation (RAG) to improve knowledge-intensive searches, integrating external data sources into the language model responses for highly relevant, structured outputs. Embedding-based search powered by Deep Lake vector stores ensures efficient handling of multimodal data, including text, images, and audio. The scalable and serverless cloud infrastructure guarantees high availability and performance, supporting both small-scale and enterprise-grade deployments. This project emphasizes automation of repetitive search tasks, reducing manual effort and enhancing efficiency. Real-time data integration and analytics empower proactive decision-making, while a user-friendly, mobile-optimized interface with multilingual support broadens accessibility. By aligning with sustainable practices and global goals, Intelligent Web Search Automation advances technological inclusivity and transforms conventional web search paradigms, making AI- driven search accessible and impactful across diverse applications.\n\nKeywords— Artificial Intelligence, Language Models, Deep Learning, NLP, Transformers, Attention Mechanism, Retrieval- Augmented Generation, Generative Agents, Text Classification.",
    "venue": "INTERANTIONAL JOURNAL OF SCIENTIFIC RESEARCH IN ENGINEERING AND MANAGEMENT",
    "year": 2025,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2025-01-17",
    "authors": [
      {
        "authorId": "2340920946",
        "name": "Devaiah K K"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "ffa809e053d8314f7b4dd621d75001eaff122c93",
    "url": "https://www.semanticscholar.org/paper/ffa809e053d8314f7b4dd621d75001eaff122c93",
    "title": "Raiders of the Lost Dependency: Fixing Dependency Conflicts in Python using LLMs",
    "abstract": "Fixing Python dependency issues is a tedious and error-prone task for developers, who must manually identify and resolve environment dependencies and version constraints of third-party modules and Python interpreters. Researchers have attempted to automate this process by relying on large knowledge graphs and database lookup tables. However, these traditional approaches face limitations due to the variety of dependency error types, large sets of possible module versions, and conflicts among transitive dependencies. This study explores the potential of using large language models (LLMs) to automatically fix dependency issues in Python programs. We introduce PLLM (pronounced\"plum\"), a novel technique that employs retrieval-augmented generation (RAG) to help an LLM infer Python versions and required modules for a given Python file. PLLM builds a testing environment that iteratively (1) prompts the LLM for module combinations, (2) tests the suggested changes, and (3) provides feedback (error messages) to the LLM to refine the fix. This feedback cycle leverages natural language processing (NLP) to intelligently parse and interpret build error messages. We benchmark PLLM on the Gistable HG2.9K dataset, a collection of challenging single-file Python gists. We compare PLLM against two state-of-the-art automatic dependency inference approaches, namely PyEGo and ReadPyE, w.r.t. the ability to resolve dependency issues. Our results indicate that PLLM can fix more dependency issues than the two baselines, with +218 (+15.97%) more fixes over ReadPyE and +281 (+21.58%) over PyEGo. Our deeper analyses suggest that PLLM is particularly beneficial for projects with many dependencies and for specific third-party numerical and machine-learning modules. Our findings demonstrate the potential of LLM-based approaches to iteratively resolve Python dependency issues.",
    "venue": "",
    "year": 2025,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2025-01-27",
    "authors": [
      {
        "authorId": "2225601829",
        "name": "Antony Bartlett"
      },
      {
        "authorId": "2342414650",
        "name": "Cynthia Liem"
      },
      {
        "authorId": "3013302",
        "name": "Annibale Panichella"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "1e909e2a8cdacdcdff125ebcc566f37cb869a1c8",
    "url": "https://www.semanticscholar.org/paper/1e909e2a8cdacdcdff125ebcc566f37cb869a1c8",
    "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
    "abstract": "The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.",
    "venue": "ACM Transactions on Information Systems",
    "year": 2023,
    "citationCount": 400,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3703155",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-11-09",
    "authors": [
      {
        "authorId": "2265930173",
        "name": "Lei Huang"
      },
      {
        "authorId": "2248673673",
        "name": "Weijiang Yu"
      },
      {
        "authorId": "2265878959",
        "name": "Weitao Ma"
      },
      {
        "authorId": "2208739098",
        "name": "Weihong Zhong"
      },
      {
        "authorId": "51056532",
        "name": "Zhangyin Feng"
      },
      {
        "authorId": "2256768984",
        "name": "Haotian Wang"
      },
      {
        "authorId": "1500384901",
        "name": "Qianglong Chen"
      },
      {
        "authorId": "2247980601",
        "name": "Weihua Peng"
      },
      {
        "authorId": "2674998",
        "name": "Xiaocheng Feng"
      },
      {
        "authorId": "2257004102",
        "name": "Bing Qin"
      },
      {
        "authorId": "2238862997",
        "name": "Ting Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 165.90942140959854
  },
  {
    "paperId": "4773398fd2fdf1feddb194aa3e90da270fd3db11",
    "url": "https://www.semanticscholar.org/paper/4773398fd2fdf1feddb194aa3e90da270fd3db11",
    "title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications",
    "abstract": "A graph is a fundamental data model to represent various entities and their complex relationships in society and nature, such as social networks, transportation networks, financial networks, and biomedical systems. Recently, large language models (LLMs) have showcased a strong generalization ability to handle various NLP and multi-mode tasks to answer users' arbitrary questions and specific-domain content generation. Compared with graph learning models, LLMs enjoy superior advantages in addressing the challenges of generalizing graph tasks by eliminating the need for training graph learning models and reducing the cost of manual annotation. In this survey, we conduct a comprehensive investigation of existing LLM studies on graph data, which summarizes the relevant graph analytics tasks solved by advanced LLM models and points out the existing remaining challenges and future directions. Specifically, we study the key problems of LLM-based generative graph analytics (LLM-GGA) with three categories: LLM-based graph query processing (LLM-GQP), LLM-based graph inference and learning (LLM-GIL), and graph-LLM-based applications. LLM-GQP focuses on an integration of graph analytics techniques and LLM prompts, including graph understanding and knowledge graph (KG) based augmented retrieval, while LLM-GIL focuses on learning and reasoning over graphs, including graph learning, graph-formed reasoning and graph representation. We summarize the useful prompts incorporated into LLM to handle different graph downstream tasks. Moreover, we give a summary of LLM model evaluation, benchmark datasets/tasks, and a deep pro and cons analysis of LLM models. We also explore open problems and future directions in this exciting interdisciplinary research area of LLMs and graph analytics.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-04-23",
    "authors": [
      {
        "authorId": "2297848659",
        "name": "Wenbo Shang"
      },
      {
        "authorId": "2298024639",
        "name": "Xin Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "201311ca82d788a09881539010cd24f2bcc071e6",
    "url": "https://www.semanticscholar.org/paper/201311ca82d788a09881539010cd24f2bcc071e6",
    "title": "Degarbayan-SC: A Colloquial Paraphrase Farsi Subtitles Dataset",
    "abstract": "Paraphrase generation and paraphrase detection are important tasks in Natural Language Processing (NLP), such as information retrieval, text simplification, question answering, and chatbots. The lack of comprehensive datasets in the Persian paraphrase is a major obstacle to progress in this area. In spite of their importance, no large-scale corpus has been made available so far, given the difficulties in its creation and the intensive labor required. In this paper, the construction process of Degarbayan-SC using movie subtitles, together with some of the difficulties we experienced during data extraction and sentence alignment, is addressed. As you know, movie subtitles are in Colloquial language. It is different from formal language. To the best of our knowledge, Degarbayan-SC is the first freely released large-scale (in the order of a million words) Persian paraphrase corpus. Furthermore, this newly introduced dataset will help the growth of Persian paraphrase. We have tested our dataset on neural network models and compared the performances of different attention-based models (transformers) and the GRU model on it. We have also declared the sentences generated by the neural networks and performed human metrics on them. The dataset is available at https://github.com/mut-deep/Degarbayan-SC",
    "venue": "International Conference on Computer and Knowledge Engineering",
    "year": 2022,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2022-11-17",
    "authors": [
      {
        "authorId": "2138534422",
        "name": "MohammadMahdi Aghajani"
      },
      {
        "authorId": "2271860",
        "name": "Mohammad Ali Keyvanrad"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "3358f057b7139201797dc34f938b99a13f99ae90",
    "url": "https://www.semanticscholar.org/paper/3358f057b7139201797dc34f938b99a13f99ae90",
    "title": "Multilingual Fact Linking",
    "abstract": "Knowledge-intensive NLP tasks can benefit from linking natural language text with facts from a Knowledge Graph (KG). Although facts themselves are language-agnostic, the fact labels (i.e., language-specific representation of the fact) in the KG are often present only in a few languages. This makes it challenging to link KG facts to sentences in languages other than the limited set of languages. To address this problem, we introduce the task of Multilingual Fact Linking (MFL) where the goal is to link fact expressed in a sentence to corresponding fact in the KG, even when the fact label in the KG is not available in the language of the sentence. To facilitate research in this area, we present a new evaluation dataset, IndicLink. This dataset contains 11,293 linked WikiData facts and 6,429 sentences spanning English and six Indian languages. We propose a Retrieval+Generation model, ReFCoG, that can scale to millions of KG facts by combining Dual Encoder based retrieval with a Seq2Seq based generation model which is constrained to output only valid KG facts. ReFCoG outperforms standard Retrieval+Re-ranking models by 10.7 pts in Precision@1. In spite of this gain, the model achieves an overall score of 52.1, showing ample scope for improvement in the task.ReFCoG code and IndicLink data are available at https://github.com/SaiKeshav/mfl",
    "venue": "Conference on Automated Knowledge Base Construction",
    "year": 2021,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-09-29",
    "authors": [
      {
        "authorId": "1500421125",
        "name": "Keshav Kolluru"
      },
      {
        "authorId": "2511068",
        "name": "Martín Rezk"
      },
      {
        "authorId": "2986975",
        "name": "Pat Verga"
      },
      {
        "authorId": "2058480371",
        "name": "W. Cohen"
      },
      {
        "authorId": "2408872",
        "name": "P. Talukdar"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "222fa32587bd676bb5e0e83a6d3c6aa0e9ca0dd7",
    "url": "https://www.semanticscholar.org/paper/222fa32587bd676bb5e0e83a6d3c6aa0e9ca0dd7",
    "title": "Knowledge-Enhanced Language Models: A Comparative Study of RAG and Embedding Methods",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, they are still somewhat limited in terms of accessing and logically employing precise knowledge, and are hence prone to hallucination. The performance of LLMs in this regard can be improved by incorporating knowledge from external sources. As methods for doing so are often focused on individually and in isolation, our project aims to explore and compare various methods to achieve this enhancement. Specifically, we investigate Retrieval-Augmented Generation (RAG) with constructed knowledge graphs on a domain-adapted BERT base model. As an alternative method, we additionally explore the KnowBERT architecture which encodes knowledge directly into a language model. Our baseline is a domain-adapted BERT large model without any knowledge graph assistance. Evaluation metrics we employ include accuracy, precision, recall, F1 score, per-plexity, and a factual recall test. We find that in general injecting knowledge into large language models improves accuracy. Between the two frameworks we examined, KnowBERT outperformed RAG-BERT on all metrics except absolute accuracy in the factual recall test. Thus, we show that enhancing language models by embedding knowledge can be a viable solution to the general problem of hallu-cination. Future work involves combining various architectural choices from RAG, KnowBERT, and other methods for further model improvement and accuracy.",
    "venue": "",
    "year": null,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2220791003",
        "name": "Stanford CS224N"
      },
      {
        "authorId": "2285220441",
        "name": "Custom Project"
      },
      {
        "authorId": "91200416",
        "name": "Adarsh Ambati"
      },
      {
        "authorId": "2314499857",
        "name": "Nikash Chhadia"
      }
    ],
    "source": "semantic_scholar",
    "score": 50
  },
  {
    "paperId": "67b955907a39e6405c2f5eae7c7b16baf2b1006c",
    "url": "https://www.semanticscholar.org/paper/67b955907a39e6405c2f5eae7c7b16baf2b1006c",
    "title": "A Novel Approach of Augmenting Training Data for Legal Text Segmentation by Leveraging Domain Knowledge",
    "abstract": null,
    "venue": "Intelligent Systems, Technologies and Applications",
    "year": 2019,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "32113796",
        "name": "R. Wagh"
      },
      {
        "authorId": "39770915",
        "name": "D. Anand"
      }
    ],
    "source": "semantic_scholar",
    "score": 74.1886522358297
  },
  {
    "paperId": "95d97e2e87103d10ca0c2ef0a2384f76e0aa6355",
    "url": "https://www.semanticscholar.org/paper/95d97e2e87103d10ca0c2ef0a2384f76e0aa6355",
    "title": "Information Retrieval in the Service of Generating Narrative Explanation - What we Want from Gallura",
    "abstract": "Information retrieval (IR) and, all the more so, knowledge discovery (KD), do not exist in isolation: it is necessary to consider the architectural context in which they are invoked in order to fulfil given kinds of tasks. This paper discusses a retrieval-intensive context of use, whose intended output is the generation of narrative explanations in a non-bona-fide, entertainment mode subject to heavy intertextuality and strictly constrained by culture-bound poetic conventions. The GALLURA project, now in the design phase, has a multiagent architecture whose modules thoroughly require IR in order to solve specialist subtasks. By their very nature, such subtasks are best subserved by efficient IR as well as mining capabilities within large textual corpora, or networks of signifiers and lexical concepts, as well as databases of narrative themes, motifs and tale types. The state of the art in AI, NLP, story-generation, computational humour, along with IR and KD, as well as the lessons of the DARSHAN project in a domain closely related to GALLURA’s, make the latter’s goals feasible in principle. 1 CONCEPTUAL & TECHNICAL BACKGROUND In the history of full-text IR, tools for retrieval from very large historical corpora in Hebrew and Aramaic were prominent, with the RESPONSA project (see e.g. Choueka, 1989a, 1989b; Choueka et al. 1971, 1987). Before the rise of Web search engines, RESPONSA tools were the ones which achieved the more far-reaching effects on society, because how they empowered the retrieval of legal precedents in rabbinic jurisprudence, thus affecting especially legal practice of family law in Israel (as for family law, in the Ottoman successor states, the usual jurisdiction is the courts of the various religious communities). Religious cultures, as being the “consumers” of religious texts, were, in a sense, the customers of a considerable portion of early projects in IR: apart from RESPONSA, whose corpora comprise the Jewish texts from the sacred sphere through the ages, this was also the case of Padre Busa’s Index Thomisticus in Milan, and of the humanities computing at the Abbey of Maredsous, in Belgium. Exegesis (such as biblical interpretations) and homiletics involve layers of texts, where a secondary text refers to and either just quotes, or discusses, some locus in the primary text; or then (as in the Jewish aggadic midrash) expands on a biblical narratives, filling the gaps where the primary text is silent. Collections of aggadic midrash from late antiquity (e.g., the Midrash Rabbah) or the Middle Ages (e.g., Yalqut Shim‘oni) are a digest of a multitude of homilies on biblical fragments of texts, developing several often alternative ideas and subnarratives. Cf. Hirshman (2006), Braude (1982), Fishbane (1993), Hartman and Budick (1986). * HyperJoseph is a hypertextual tool on the story of Joseph in Genesis, with the secondary texts elaborating on it (Nissan and Weiss, 1994). * DARSHAN is a tool that invents homilies in Hebrew (HaCohen-Kerner et al. 2007). Retrieval in DARSHAN is intensive, and so is the use of networks of lexical concepts. DARSHAN generates ranked sets of either onesentence or one-paragraph homilies. While producing its output, DARSHAN is able to quote 487 Nissan E. and HaCohen-Kerner Y.. INFORMATION RETRIEVAL IN THE SERVICE OF GENERATING NARRATIVE EXPLANATION What we Want from GALLURA. DOI: 10.5220/0003688304790484 In Proceedings of the International Conference on Knowledge Discovery and Information Retrieval (KDIR-2011), pages 479-484 ISBN: 978-989-8425-79-9 Copyright c 2011 SCITEPRESS (Science and Technology Publications, Lda.) from Scripture, to search for an occurrence elsewhere in the textual canon, to replace words or letters, to resort to puns, to interpret a word as an acronym, and so forth. Use is made of patterns which consist of canned text with places where to plug in strings obtained through IR and manipulation. The user supplies as input a biblical verse, or a sentence, or a set of words, and also specifies which devices should be applied. Filters applied to the candidate output are alert, e.g., to positive vs. negative connotation. The quality of an individual output homily is assessed as a sum of weighted factors, including: length (as an indicator of complicacy); the percentage of relevant words in the homily, out of the total of words in the homily how many sentences there are; how complex it was to insert every motif into the homily generated; how many motifs were actualized in the output homily being evaluated; how many transformations were carried out; how many words were replaced in the homily. Having mentioned acronyms, consider that HaCohen-Kerner et al. (2010b) discussed an abbreviation disambiguation system for rabbinic texts in Hebrew or Aramaic. Cf. Stock and Strapparava (2005) on the HAHA project, whose purpose is the humorous interpretation of acronyms. As to connotations, Strapparava and Valitutti (2004) described an affective extension of WordNet. 2 FUNCTIONS IN GALLURA The GALLURA project seeks to develop software that would interpret in Hebrew names by folketymology, but in the context of a generated narrative (aetiological tales, usually brief or even very brief). The most closely studied model is a large textual corpus of playfully creative writing that embodies midrashic literary devices, by explaining fancifully place-names of names for animal kinds. The GALLURA project, now in the design phase, requires, among the other things, capabilities of story-generation, and of generating a playful explanation. By themselves, these two tasks draw upon three areas in AI: explanation synthesis (for which, see e.g. Schank, 1986, 1994; Walton, 2004), story-generation (see e.g. Liu and Singh, 2002; Lönneker et al., 2005; and a long survey in Nissan, 2011a: Ch. 5), and computational humour (see e.g. Stock et al., 2002; Ritchie, 2004; Waller et al., 2009). Humour studies are interdisciplinary. Moreover, GALLURA needs skills from computational linguistics, including some that thus far were modelled by linguistics, but not computationally: folk-etymology (see e.g. Kirwin, 1985; Coates, 1994; Baldinger, 1973; Zuckermann, 2006), and phono-semantic matching (PSM), a discussion of which is found in Zuckermann (2000, 2006). For example, one of several PSM rules as occurring in neologisation by adapting a foreign term (Zuckermann 2000) is as follows (where SL is the source language. TL is the target language): SL y ‘b’ TL(+PSM) x ‘b’ TL x ‘a’ x is phonetically similar to y; a is similar to b That is to say, the PSM introduced a new sense: this was a PSM produced by shifting the meaning of a pre-existent word in the target-language (TL). Another rule of camouflaged borrowing (ibid.) is: SL y ‘b’ TL(+PSM) {x}+{z} ‘b’ TL {x} ‘a’, {z} x is a lexical morpheme (e.g. root) that is phonetically similar to y; z is a grammatical morpheme (e.g. noun-pattern); {x}+{z} is one word; a is similar to b GALLURA should also have quality evaluation capabilities, e.g., evaluating a story generated (Peinado and Gervás, 2006), or evaluating morality within a story (Reeves, 1991). We also need to resort to computational argumentation: some such current research into argumentation in computer science looks into legal narratives (Bex, 2011). Explanation as sought in GALLURA need not necessarily be realistic; it is non-bona-fide (like in humour), and must conform to a set of conventions, of which realism is just a particular case (cf. Nissan, 2008). There are constraints on style: the output text generated conforms to the early rabbinic linguistic stratum and style (thus emulating the aggadic midrash), with constraints on which lexical items or morphological forms can be selected. Rabbinic stylemes are the subject of current IR research, including in the CUISINE text classifier. So are the identification of rabbinic citations, and chronological classification based on them. In fact, HaCohen-Kerner et al. (2010a) discussed stylistic feature sets for classification in CUISINE. Automated identification of citations from rabbinic texts has been researched (HaCohen-Kerner et al., 2010c). Automated classification of rabbinic KDIR 2011 International Conference on Knowledge Discovery and Information Retrieval",
    "venue": "International Conference on Knowledge Discovery and Information Retrieval",
    "year": 2011,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1802622",
        "name": "E. Nissan"
      },
      {
        "authorId": "1401813543",
        "name": "Yaakov HaCohen-Kerner"
      }
    ],
    "source": "semantic_scholar",
    "score": 89.1886522358297
  },
  {
    "paperId": "092312cb2a59502a2db7ffea1e31c19ad2eecc7c",
    "url": "https://www.semanticscholar.org/paper/092312cb2a59502a2db7ffea1e31c19ad2eecc7c",
    "title": "H2-Golden-Retriever: Methodology and Tool for an Evidence-Based Hydrogen Research Grantsmanship",
    "abstract": "Background of Study: Hydrogen is poised to play a major role in decarbonizing the economy. The need to discover, develop, and understand low-cost, high-performance, durable materials that can help maximize the cost of electrolysis as well as the need for an intelligent tool to make evidence-based Hydrogen research funding decisions relatively easier warranted this study. Aim: In this work, we developed H2 Golden Retriever (H2GR) system for Hydrogen knowledge discovery and representation using Natural Language Processing (NLP), Knowledge Graph and Decision Intelligence. This system represents a novel methodology encapsulating state-of-the-art technique for evidence-based research grantmanship. Methods: Relevant Hydrogen papers were scraped and indexed from the web and preprocessing was done using noise and stop-words removal, language and spell check, stemming and lemmatization. The NLP tasks included Named Entity Recognition using Stanford and Spacy NER, topic modeling using Latent Dirichlet Allocation and TF-IDF. The Knowledge Graph module was used for the generation of meaningful entities and their relationships, trends and patterns in relevant H2 papers, thanks to an ontology of the hydrogen production domain. The Decision Intelligence component provides stakeholders with a simulation environment for cost and quantity dependencies. the abstract. Technical articles of interest are then read to find relevant information and further insights to orient their keyword searches in a more promising direction. This makes keeping up with the torrential pace of publications very difficult, cumbersome, time-consuming and labor-intensive. Since Hydrogen production cost depends on multiple assumptions including materials, electrolyzer design, manufacturing practices, and soft costs, it is difficult to trace back the contribution of the price of a component to the overall system cost. This makes cost of a component to vary from one technical report to another; for instance, a study[12] reported that bipolar plates is the third contributor behind the Catalyst Coated Membrane (CCM) and Porous Transport Layer (PTL) to the stack cost while another article[13] reported it as the highest contributor. It is therefore hard to crystallize on which sub-component deserves the highest focus. Lastly, experts are on a race against time. Understanding how one component affects the overall cost is one thing but how much money must be invested in versus the time it will take to reduce the overall system cost must drive the decisions of the experts. In this work, we addressed the missing link between the slew of information available in literature and the decision to fund research and, by implication, propose an AI-augmented decision tool for knowledge acquisition, knowledge extraction and an evidence-based research funding decision support tool. techniques in include word tokenization, word stemming and lemmatization, topical modeling, named-entity recognition, summarization, word cloud and keyword extraction. These tasks use both linguistics and mathematics to connect the language of with the language of computers.",
    "venue": "arXiv.org",
    "year": 2022,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2211.08614",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-11-16",
    "authors": [
      {
        "authorId": "2145259150",
        "name": "Paul Seurin"
      },
      {
        "authorId": "2088728712",
        "name": "O. Olabanjo"
      },
      {
        "authorId": "2190960040",
        "name": "Joseph Wiggins"
      },
      {
        "authorId": "144442133",
        "name": "L. Pratt"
      },
      {
        "authorId": "66423628",
        "name": "L. Rana"
      },
      {
        "authorId": "2029363250",
        "name": "Rozhin Yasaei"
      },
      {
        "authorId": "118082255",
        "name": "Gregory Renard"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "57f96b65c831dc4501c0aa1c7b0c73a292e79444",
    "url": "https://www.semanticscholar.org/paper/57f96b65c831dc4501c0aa1c7b0c73a292e79444",
    "title": "A Natural Way of Building Financial Domain Expert Agents",
    "abstract": "Financial experts possess specialized knowledge that is not easily accessible, making the acquisition of such expertise a time-intensive process. Leveraging Large Language Models (LLMs) to emulate financial domain experts offers a promising solution, which can offload routine responsibilities from human experts, allowing them to focus on more strategic tasks. However, developing a GenAI agent that matches the capabilities of a financial domain expert requires more than just LLMs with Retrieval-Augmented Generation (RAG) capabilities. The agent must interact with domain-specific data sources, perform complex analyses, and understand niche terminologies and processes. We propose a natural way of developing of GenAI-powered financial domain experts by following a zero-shot approach. Our agent’s memory layer has complimentary capabilities to few-shot prompting and provides a natural way of remembering information as it interacts with domain experts. This paper is presented as a case study where we propose a comprehensive framework for building financial domain expert agents. Our approach involves iteratively enhancing a basic LLM with data extraction layer, coding capabilities, and a memory layer to perform complex analyses. We show how addition of each layer to our LLM agent improves its performance and also address the necessary safety and governance processes to ensure the robustness and accuracy of production ready agent. We also introduce a custom dataset (having roots in the financial domain) for evaluating the agent’s performance in numerical analysis and multi-step reasoning, providing a clearer picture of the agent’s capability to mimic a financial domain expert.",
    "venue": "",
    "year": null,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2193184803",
        "name": "Gagan Kaler"
      },
      {
        "authorId": "2175649634",
        "name": "Dimitrios Vamvourellis"
      },
      {
        "authorId": "2260341840",
        "name": "Deran Onay"
      },
      {
        "authorId": "2334026168",
        "name": "Yury Krongauz"
      },
      {
        "authorId": "2258960763",
        "name": "Stefano Pasquali"
      },
      {
        "authorId": "2258962314",
        "name": "Dhagash Mehta"
      }
    ],
    "source": "semantic_scholar",
    "score": 50
  },
  {
    "paperId": "a8cbfdbe2942aafecb853bac8e7dc8edf4cde9da",
    "url": "https://www.semanticscholar.org/paper/a8cbfdbe2942aafecb853bac8e7dc8edf4cde9da",
    "title": "Data-Intensive Research Workshop (15-19 March 2010) Report",
    "abstract": "Computers are traditionally viewed as logical machines that follow precise, deterministic instructions. The real world in which they operate, however, is full of complexity, ambiguity, and uncertainty. In this year’s Turing Lecture, Professor Chris Bishop discusses the field of machine learning, and shows how uncertainty can be modelled and quantified using probabilities. Professor Bishop will look at the recent developments in probabilistic modelling that have greatly expanded the variety and scale of machine learning applications, and he explores the future potential for this technology. 4.2 Integrative Analysis of Pathology, Radiology and High Throughput Molecular Data Joel Saltz, Center for Comprehensive Informatics, Emory University The talk is available at: http://www.nesc.ac.uk/action/esi/download.cfm?index=4459. Draft 1: 17 May 2010 DIR workshop Text mining, computational biology and human disease 32 Integrative analysis of multiple complementary types of information is playing an increasingly essential role in biomedical research. The Emory led caBIG In Silico Brain Tumor Research Center is an excellent example of this. The effort is designed to leverage complementary molecular, pathology, radiology and outcome data, obtained by several large scale National Cancer Institute studies including the Cancer Genome Atlas (TCGA) study. This group is developing workflows consisting of novel image analysis algorithms and bioinformatics analyses to correlate imaging characteristics defined by Pathology and Radiology derived feature sets with underlying “omic” characteristics and with patient outcome. This effort is explicitly designed both to motivate advances in informatics and to generate useful brain tumour research results. A major technical focus of the In Silico Brain Tumor research Center is high throughput digital microscopy, which combines image acquisition with generation of semantically annotated feature sets that describe tissue characteristics of disease at a cellular scale. This is a data intensive process; a high-power microscope can easily generate several terabytes of data each day. Tissue characterisation involves workflows that employ a sequence of steps: image segmentation, feature construction, feature selection, feature extraction, classification of features, and annotation of images and image regions. The sheer size of image datasets makes gleaning information from digital microscopy slides a data-intensive process requiring efficient system support. The large number of potentially useful micro-anatomic features and the large number of image preprocessing algorithms makes it essential to develop effective semantically oriented mechanisms to manage, query and curate features and to track provenance associated with the generation of each collection of features. More information about the work is in [86, 87, 88, 89]. 4.3 Text mining, computational biology and human disease Andrey Rzhetsky, Department of Medicine, Department of Human Genetics, Computation Institute, Institute for Genomics and Systems Biology, University of Chicago The talk is available at: http://www.nesc.ac.uk/action/esi/download.cfm?index=4472. Imagine that a graduate student enters the Library of Congress with an assignment: she is to find and pull all texts relevant to protein glycosylation. Her problem is straightforward, known among text-miners as information retrieval (IR). If the student must not only find the books, but also to flag the most important concepts she encounters in each, she must perform named entity recognition (NER). Undaunted by her workload, imagine she decides to identify relations between concepts, such as protein BAD binds protein BAX, text-mining researchers call this information extraction (IE). Along with a group of additional tasks, such as question answering and text summarisation (QA and TS, respectively) which we outline below, problems of computational IR, NER and IE belong to a larger field of natural language processing (NLP)—human language understanding and generation by a computer. NLP is an integral part of artificial intelligence (AI), with its larger goal of recreating or surpassing the computational ability of the human brain. The graduate student in our example has a distinct advantage over the computer, of course—she is naturally gifted, as we all are, with the ability to parse and quickly extract meaning from language. Text mining, a new field that applies computational techniques to the problems faced by our graduate student above, can be thought of as a sub-field of both AI and NLP—but it also stakes a claim for “intellectual independence” with its emphasis on gaining new knowledge. Thus, Draft 1: 17 May 2010 DIR workshop Building a virtual data centre for the biological, ecological and environmental sciences 33 while multiple definitions exist, text mining is typically associated with information retrieval, extraction, and synthesis, with a special stress on gaining new knowledge. Moving beyond information retrieval and extraction, some proportion of published assertions can be repackaged to form synthetic ideas, that is, new compound concepts that are significantly more valuable to the scientific community than the sum of their original assertions. In 1986, researcher Don R. Swanson read a collection of scientific articles that spanned different fields of inquiry. Each of these disconnected communities had information valuable to the other, but due to the lack of crosstalk, did not appear to know it. Swanson noticed, in papers produced by researchers interested in Raynaud’s disease, that its symptoms often included increased blood viscosity and rigidity of erythrocytes. Meanwhile, in the nutrition biology community several publications proposed that dietary fish oil could reduce blood viscosity, without explicitly connecting this finding to Raynaud’s disease. In this early real-world example of synthetic idea generation, Swanson merged these findings to suggest that fish oil can be beneficial for Raynaud’s patients and published his suggestion as a medical hypothesis. The hypothesis was confirmed some years later in an independent randomised clinical trial. Even with current text mining capabilities, such synthetic ideas can be discovered automatically. A more distant but nonetheless realistic aim of the field is to trace and map more sophisticated ideas (idea isomorphisms) that are expressed differently in different scientific fields, yet represent identical problems or their solutions. Were such idea mappings made instantly available through an Internet interface, the result could be truly impressive. The diffusion of innovations across science could be markedly increased by making solutions developed in one area visible to specialists still desperately searching for them in a different field. Computationally pairing problems and solutions generated by different fields is a type of automated creativity (systematic search for synthetic ideas) that computers almost certainly will do for us in the not too distant future. Further information may be found in [90, 91, 92, 93, 94, 95] 4.4 Building a virtual data centre for the biological, ecological and environmental sciences Bill Michener, University of New Mexico, Albuquerque The talk is available at: http://www.nesc.ac.uk/action/esi/download.cfm?index=4466. Addressing the Earth’s environmental problems requires that we change the ways that we do science; harness the enormity of existing data; develop new methods to combine, analyse, and visualise diverse data resources; create new, long-lasting cyberinfrastructure; and re-envision many of our longstanding institutions. DataONE (Observation Network for Earth) represents a new virtual organisation whose goal is to enable new science and knowledge creation through universal access to data about life on earth and the environment that sustains it. DataONE is designed to be the foundation for new innovative environmental science through a distributed framework and sustainable cyberinfrastructure that meets the needs of science and society for open, persistent, robust, and secure access to well-described and easily discovered Earth observational data. Supported by the U.S. National Science Foundation, DataONE will ensure the preservation and access to multi-scale, multi-discipline, and multi-national science data. DataONE is transdisciplinary, making biological data available from the genome to the ecosystem; making environmental data available from atmospheric, ecological, hydrological, and oceanographic sources; providing secure and long-term preservation and access; and engaging scientists, land-managers, Draft 1: 17 May 2010 DIR workshop Curated databases 34 policy makers, students, educators, and the public through logical access and intuitive visualisations. Most importantly, DataONE will serve a broader range of science domains both directly and through the interoperability with the DataONE distributed network. DataONE is a five-year project that began in 2009. I identify key environmental scientific, cyberinfrastructure, and sociocultural challenges and provides a road map for how DataONE is addressing these challenges. Specific examples are based on a DataONE EVA (Exploration, Visualisation and Analysis) Working Group effort to create an integrated database that can be used for better understanding the ecology of bird migrations. The important message is that to succeed in international preservation and access to data we need to recognise the importance of data from the perspective of the scientists to understand the requirements. Scientists want fast access in terms of data and tool discovery, automated/automatic metadata annotation and rapid analysis via appropriate visualisation. They want easy access in terms of data and metadata upload; integrated, linked, and synthesised databases and interoperable and intuitive scientific workflow systems. They want cheap solutions f",
    "venue": "",
    "year": 2010,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2010-05-01",
    "authors": [
      {
        "authorId": "97702983",
        "name": "M. Atkinson"
      },
      {
        "authorId": "1708123",
        "name": "D. D. Roure"
      },
      {
        "authorId": "4369431",
        "name": "Jano van Hemert"
      },
      {
        "authorId": "1693678",
        "name": "S. Jha"
      },
      {
        "authorId": "2113828571",
        "name": "R. McNally"
      },
      {
        "authorId": "2059394415",
        "name": "Robert Mann"
      },
      {
        "authorId": "1699540",
        "name": "Stratis Viglas"
      },
      {
        "authorId": "145715698",
        "name": "Christopher K. I. Williams"
      }
    ],
    "source": "semantic_scholar",
    "score": 70.39720770839918
  },
  {
    "paperId": "0bd273fff4a31964fb83b6eb3527ab66d0f88b35",
    "url": "https://www.semanticscholar.org/paper/0bd273fff4a31964fb83b6eb3527ab66d0f88b35",
    "title": "Natural language and information systems : 13th International Conference on Applications of Natural Language to Informations Systems, NLDB 2008, London, UK, June 24-27, 2008 : proceedings",
    "abstract": "Invited Papers.- Sentence and Text Comprehension: Evidence from Human Language Processing.- Towards Semantic Search.- From Databases to Natural Language: The Unusual Direction.- Natural Language Processing and Understanding.- Division of Spanish Words into Morphemes with a Genetic Algorithm.- Abbreviation Disambiguation: Experiments with Various Variants of the One Sense per Discourse Hypothesis.- The Acquisition of Common Sense Knowledge by Being Told: An Application of NLP to Itself.- Natural Language Processing and Understanding.- Interlingua for French and German Topological Prepositions.- Ontological Profiles as Semantic Domain Representations.- A Hybrid Approach to Ontology Relationship Learning.- Automating the Generation of Semantic Annotation Tools Using a Clustering Technique.- Information Retrieval.- Exploiting Multiple Features with MEMMs for Focused Web Crawling.- Ranked-Listed or Categorized Results in IR: 2 Is Better Than 1.- Exploiting Morphological Query Structure Using Genetic Optimisation.- Generation of Query-Biased Concepts Using Content and Structure for Query Reformulation.- Comparing Several Textual Information Retrieval Systems for the Geographical Information Retrieval Task.- Querying and Question Answering.- Intensional Question Answering Using ILP: What Does an Answer Mean?.- Augmenting Data Retrieval with Information Retrieval Techniques by Using Word Similarity.- Combining Data Integration and IE Techniques to Support Partially Structured Data.- Towards Building Robust Natural Language Interfaces to Databases.- Towards a Bootstrapping NLIDB System.- Document Processing and Text Mining.- Real-Time News Event Extraction for Global Crisis Monitoring.- Topics Identification Based on Event Sequence Using Co-occurrence Words.- Topic Development Based Refinement of Audio-Segmented Television News.- Text Entailment for Logical Segmentation and Summarization.- Comparing Non-parametric Ensemble Methods for Document Clustering.- A Language Modelling Approach to Linking Criminal Styles with Offender Characteristics.- Software (Requirements) Engineering and Specification.- Towards Designing Operationalizable Models of Man-Machine Interaction Based on Concepts from Human Dialog Systems.- Using Linguistic Knowledge to Classify Non-functional Requirements in SRS documents.- A Preliminary Approach to the Automatic Extraction of Business Rules from Unrestricted Text in the Banking Industry.- Paraphrasing OCL Expressions with SBVR.- A General Architecture for Connecting NLP Frameworks and Desktop Clients Using Web Services.- Conceptual Modelling and Ontologies Related Posters.- Conceptual Model Generation from Requirements Model: A Natural Language Processing Approach.- Bivalent Verbs and Their Pragmatic Particularities.- Using Ontologies and Relatedness Metrics for Semantic Document Analysis on the Web.- Information Retrieval Related Posters.- The TSRM Approach in the Document Retrieval Application.- Enhanced Services for Targeted Information Retrieval by Event Extraction and Data Mining.- Querying and Question Answering Related Posters.- Improving Question Answering Tasks by Textual Entailment Recognition.- Supporting Named Entity Recognition and Syntactic Analysis with Full-Text Queries.- Document Processing and Text Mining Related Posters.- Multilingual Feature-Driven Opinion Extraction and Summarization from Customer Reviews.- Lexical and Semantic Methods in Inner Text Topic Segmentation: A Comparison between C99 and Transeg.- An Application of NLP and Audiovisual Content Analysis for Integration of Multimodal Databases of Current Events.- Detecting Protein-Protein Interaction Sentences Using a Mixture Model.- Using Semantic Features to Improve Task Identification in Email Messages.- Text Pre-processing for Document Clustering.- Software (Requirements) Engineering and Specification Related Posters.- Trade Oriented Enterprise Content Management: A Semantic and Collaborative Prototype Dedicated to the \"Quality, Hygiene, Safety and Environment\" Domain.- Doctoral Symposium Papers.- Mapping Natural Language into SQL in a NLIDB.- Improving Data Integration through Disambiguation Techniques.- An Ontology-Based Focused Crawler.- Impact of Term-Indexing for Arabic Document Retrieval.",
    "venue": "International Conference on Applications of Natural Language to Data Bases",
    "year": 2008,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2670451",
        "name": "E. Kapetanios"
      },
      {
        "authorId": "1802251",
        "name": "V. Sugumaran"
      },
      {
        "authorId": "2237787772",
        "name": "Myra Spiliopoulou"
      }
    ],
    "source": "semantic_scholar",
    "score": 60
  },
  {
    "paperId": "6fab37f5e00284c75370f76ba18df4f8153796e3",
    "url": "https://www.semanticscholar.org/paper/6fab37f5e00284c75370f76ba18df4f8153796e3",
    "title": "Facilitating Threat Modeling by Leveraging Large Language Models",
    "abstract": "—In recent years, security engineers in product teams have faced new challenges in threat modeling due to the increasing complexity of product design and the evolving nature of threats. First, security engineers must possess a thorough understanding of how to translate the abstract categories of threat modeling methodology into specific security threats relevant to a particular aspect of product design. Without such in-depth knowledge, applying threat modeling in practice becomes a difficult task. Second, security engineers must be aware of current vulnerabilities and be able to quickly recall those that may be relevant to the product design. Therefore, for effective threat modeling, a deep understanding of a product’s design and the background knowledge to connect real-time security events with specific design principles embedded in large volumes of technical specifications is required. This can result in a lot of human effort invested in parsing, searching, and understanding what is being built through design documents and what threats are relevant based on that information. We observe that the recent emergence of large language models (LLMs) may significantly change the landscape of threat modeling by automating and accelerating the process with their language understanding and logical reasoning capabilities. In this paper, we have developed a novel LLM-based threat modeling system by leveraging NLP techniques and an open-source LLM to decrease the required human effort above in the threat modeling process. In the evaluation, two major questions of threat modeling (MQ1 and MQ2) are considered in the proposed workflow of Task 1 and Task 2, where the NLP techniques assist in parsing and understanding design documents and threats, and the LLM analyzes and synthesizes volumes of documentation to generate responses to related threat modeling questions. Our initial findings reveal that over 75% of the responses meet the expectations of human evaluation. The Retrieval Augmented Generation (RAG)-enhanced LLM outperforms the base LLM in both tasks by responding more concisely and containing more meaningful information. This study explores a novel approach to threat modeling and demonstrates the practical applicability",
    "venue": "",
    "year": null,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2287989261",
        "name": "Isra Elsharef"
      },
      {
        "authorId": "2289192038",
        "name": "Zhen Zeng"
      },
      {
        "authorId": "2287975290",
        "name": "Zhongshu Gu"
      }
    ],
    "source": "semantic_scholar",
    "score": 60.39720770839918
  }
]