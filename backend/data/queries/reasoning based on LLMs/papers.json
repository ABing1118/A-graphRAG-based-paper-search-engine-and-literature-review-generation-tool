[
  {
    "paperId": "83ed094e60c16abae2f1eb6a1be25934dc83648d",
    "url": "https://www.semanticscholar.org/paper/83ed094e60c16abae2f1eb6a1be25934dc83648d",
    "title": "Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data",
    "abstract": "Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited. To address this gap, we introduce the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate Large Language Models' capability in statistical and causal reasoning with real-world data. The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers. To compare models' quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRText. We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models. The strongest model GPT-4 achieves an accuracy of 58%, which has much room for improvement. Among open-source models, Deepseek-coder-instruct, a code LLM pretrained on 2T tokens, gets the highest accuracy of 37%. Analysis reveals that models encounter difficulties in data analysis and causal reasoning, and struggle in using causal knowledge and provided data simultaneously. Code and data are in https://github.com/xxxiaol/QRData.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-27",
    "authors": [
      {
        "authorId": "2288030514",
        "name": "Xiao Liu"
      },
      {
        "authorId": "2109686513",
        "name": "Zirui Wu"
      },
      {
        "authorId": "2288110531",
        "name": "Xueqing Wu"
      },
      {
        "authorId": "2887562",
        "name": "Pan Lu"
      },
      {
        "authorId": "2278984743",
        "name": "Kai-Wei Chang"
      },
      {
        "authorId": "2273532365",
        "name": "Yansong Feng"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.49820016084324
  },
  {
    "paperId": "50ea1f3387e0e1a5f45722fa9d657982c78c2ce8",
    "url": "https://www.semanticscholar.org/paper/50ea1f3387e0e1a5f45722fa9d657982c78c2ce8",
    "title": "CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs for Legal Question Answering",
    "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Model (LLM) output by providing prior knowledge as context to input. This is beneficial for knowledge-intensive and expert reliant tasks, including legal question-answering, which require evidence to validate generated text outputs. We highlight that Case-Based Reasoning (CBR) presents key opportunities to structure retrieval as part of the RAG process in an LLM. We introduce CBR-RAG, where CBR cycle's initial retrieval stage, its indexing vocabulary, and similarity knowledge containers are used to enhance LLM queries with contextually relevant cases. This integration augments the original LLM query, providing a richer prompt. We present an evaluation of CBR-RAG, and examine different representations (i.e. general and domain-specific embeddings) and methods of comparison (i.e. inter, intra and hybrid similarity) on the task of legal question-answering. Our results indicate that the context provided by CBR's case reuse enforces similarity between relevant components of the questions and the evidence base leading to significant improvements in the quality of generated answers.",
    "venue": "International Conference on Case-Based Reasoning",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-04",
    "authors": [
      {
        "authorId": "1784639",
        "name": "N. Wiratunga"
      },
      {
        "authorId": "66680254",
        "name": "Ramitha Abeyratne"
      },
      {
        "authorId": "2288593555",
        "name": "Lasal Jayawardena"
      },
      {
        "authorId": "143999525",
        "name": "Kyle Martin"
      },
      {
        "authorId": "144004005",
        "name": "Stewart Massie"
      },
      {
        "authorId": "1399085102",
        "name": "Ikechukwu Nkisi-Orji"
      },
      {
        "authorId": "2837941",
        "name": "R. Weerasinghe"
      },
      {
        "authorId": "2633228",
        "name": "A. Liret"
      },
      {
        "authorId": "2217276579",
        "name": "Bruno Fleisch"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "2392195fa8213fddf37f60ae27dbb41cc2527f42",
    "url": "https://www.semanticscholar.org/paper/2392195fa8213fddf37f60ae27dbb41cc2527f42",
    "title": "Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA",
    "abstract": "Understanding data visualizations like charts and plots requires reasoning about both visual elements and numer-ics. Although strong in extractive questions, current chart visual question answering (chart VQA) models suffer on complex reasoning questions. In this work, we address the lack of reasoning ability by data augmentation. We lever-age Large Language Models (LLMs), which have shown to have strong reasoning ability, as an automatic data anno-tator that generates question-answer annotations for chart images. The key innovation in our method lies in the Syn-thesize Step-by-Step strategy: our LLM-based data genera-tor learns to decompose the complex question into step-by-step sub-questions (rationales), which are then used to de-rive the final answer using external tools, i.e. Python. This step-wise generation procedure is trained on synthetic data generated using a template-based QA generation pipeline. Experimental results highlight the significance of the pro-posed step-by-step generation. By training with the LLM- augmented data (LAMENDA), we significantly enhance the chart VQA models, achieving the state-of-the-art accuracy on the ChartQA and PlotQA datasets. In particular, our approach improves the accuracy of the previous state-of-the-art approach from 38% to 54% on the human-written questions in the ChartQA dataset, which needs strong rea-soning. We hope our work underscores the potential of syn-thetic data and encourages further exploration of data augmentation using LLMs for reasoning-heavy tasks.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2403.16385",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-25",
    "authors": [
      {
        "authorId": "2293359379",
        "name": "Zhuowan Li"
      },
      {
        "authorId": "35923416",
        "name": "Bhavan A. Jasani"
      },
      {
        "authorId": "2293714500",
        "name": "Peng Tang"
      },
      {
        "authorId": "2293717190",
        "name": "Shabnam Ghadar"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "9be02ecf206ad7494bb9531aed9491af203a3c3d",
    "url": "https://www.semanticscholar.org/paper/9be02ecf206ad7494bb9531aed9491af203a3c3d",
    "title": "LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction",
    "abstract": "Electronic health records (EHRs) contain valuable patient data for health-related prediction tasks, such as disease prediction. Traditional approaches rely on supervised learning methods that require large labeled datasets, which can be expensive and challenging to obtain. In this study, we investigate the feasibility of applying Large Language Models (LLMs) to convert structured patient visit data (e.g., diagnoses, labs, prescriptions) into natural language narratives. We evaluate the zero-shot and few-shot performance of LLMs using various EHR-prediction-oriented prompting strategies. Furthermore, we propose a novel approach that utilizes LLM agents with different roles: a predictor agent that makes predictions and generates reasoning processes and a critic agent that analyzes incorrect predictions and provides guidance for improving the reasoning of the predictor agent. Our results demonstrate that with the proposed approach, LLMs can achieve decent few-shot performance compared to traditional supervised learning methods in EHR-based disease predictions, suggesting its potential for health-oriented applications.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-19",
    "authors": [
      {
        "authorId": "2112821580",
        "name": "Hejie Cui"
      },
      {
        "authorId": "2293664100",
        "name": "Zhuocheng Shen"
      },
      {
        "authorId": "2269506893",
        "name": "Jieyu Zhang"
      },
      {
        "authorId": "2293291021",
        "name": "Hui Shao"
      },
      {
        "authorId": "2293885383",
        "name": "Lianhui Qin"
      },
      {
        "authorId": "2263536473",
        "name": "Joyce C. Ho"
      },
      {
        "authorId": "2237940940",
        "name": "Carl Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "c4985c30261c0c99eeafd97af59a301c253ad84c",
    "url": "https://www.semanticscholar.org/paper/c4985c30261c0c99eeafd97af59a301c253ad84c",
    "title": "Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs",
    "abstract": "Vision-language models (VLMs) are achieving increasingly strong performance on multimodal tasks. However, reasoning capabilities remain limited particularly for smaller VLMs, while those of large-language models (LLMs) have seen numerous improvements. We propose a technique to transfer capabilities from LLMs to VLMs. On the recently introduced ChartQA, our method obtains state-of-the-art performance when applied on the PaLI3-5B VLM by \\citet{chen2023pali3}, while also enabling much better performance on PlotQA and FigureQA. We first improve the chart representation by continuing the pre-training stage using an improved version of the chart-to-table translation task by \\citet{liu2023deplot}. We then propose constructing a 20x larger dataset than the original training set. To improve general reasoning capabilities and improve numerical operations, we synthesize reasoning traces using the table representation of charts. Lastly, our model is fine-tuned using the multitask loss introduced by \\citet{hsieh2023distilling}. Our variant ChartPaLI-5B outperforms even 10x larger models such as PaLIX-55B without using an upstream OCR system, while keeping inference time constant compared to the PaLI3-5B baseline. When rationales are further refined with a simple program-of-thought prompt \\cite{chen2023program}, our model outperforms the recently introduced Gemini Ultra and GPT-4V.",
    "venue": "NAACL-HLT",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-19",
    "authors": [
      {
        "authorId": "2237423389",
        "name": "Victor Carbune"
      },
      {
        "authorId": "2265752172",
        "name": "Hassan Mansoor"
      },
      {
        "authorId": "2292310685",
        "name": "Fangyu Liu"
      },
      {
        "authorId": "2257346647",
        "name": "Rahul Aralikatte"
      },
      {
        "authorId": "2283137677",
        "name": "Gilles Baechler"
      },
      {
        "authorId": "2283189051",
        "name": "Jindong Chen"
      },
      {
        "authorId": "150922484",
        "name": "Abhanshu Sharma"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "8826311d922135dbf0cfdb4a661ebab347e3b826",
    "url": "https://www.semanticscholar.org/paper/8826311d922135dbf0cfdb4a661ebab347e3b826",
    "title": "LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations",
    "abstract": "Can a Large Language Model (LLM) solve simple abstract reasoning problems? We explore this broad question through a systematic analysis of GPT on the Abstraction and Reasoning Corpus (ARC), a representative benchmark of abstract reasoning ability from limited examples in which solutions require some\"core knowledge\"of concepts such as objects, goal states, counting, and basic geometry. GPT-4 solves only 13/50 of the most straightforward ARC tasks when using textual encodings for their two-dimensional input-output grids. Our failure analysis reveals that GPT-4's capacity to identify objects and reason about them is significantly influenced by the sequential nature of the text that represents an object within a text encoding of a task. To test this hypothesis, we design a new benchmark, the 1D-ARC, which consists of one-dimensional (array-like) tasks that are more conducive to GPT-based reasoning, and where it indeed performs better than on the (2D) ARC. To alleviate this issue, we propose an object-based representation that is obtained through an external tool, resulting in nearly doubling the performance on solved ARC tasks and near-perfect scores on the easier 1D-ARC. Although the state-of-the-art GPT-4 is unable to\"reason\"perfectly within non-language domains such as the 1D-ARC or a simple ARC subset, our study reveals that the use of object-based representations can significantly improve its reasoning ability. Visualizations, GPT logs, and data are available at https://khalil-research.github.io/LLM4ARC.",
    "venue": "Trans. Mach. Learn. Res.",
    "year": 2023,
    "citationCount": 42,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.18354",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-26",
    "authors": [
      {
        "authorId": "47103635",
        "name": "Yudong Xu"
      },
      {
        "authorId": "2108769877",
        "name": "Wenhao Li"
      },
      {
        "authorId": "1947192",
        "name": "Pashootan Vaezipoor"
      },
      {
        "authorId": "1732536",
        "name": "S. Sanner"
      },
      {
        "authorId": "35252180",
        "name": "Elias Boutros Khalil"
      }
    ],
    "source": "semantic_scholar",
    "score": 126.41800173540344
  },
  {
    "paperId": "4f249487c670263700df7b2269cdb92a265bc21f",
    "url": "https://www.semanticscholar.org/paper/4f249487c670263700df7b2269cdb92a265bc21f",
    "title": "Effective Distillation of Table-based Reasoning Ability from LLMs",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, their enormous parameter size and extremely high requirements for compute power pose challenges for their practical deployment. Recent research has revealed that specific capabilities of LLMs, such as numerical reasoning, can be transferred to smaller models through distillation. Some studies explore the potential of leveraging LLMs to perform table-based reasoning. However, there has been no prior work focusing on table reasoning skills in smaller models specifically tailored for scientific table-to-text generation tasks. In this paper, we propose a novel table-based reasoning distillation approach, with the aim of distilling LLMs into tailored smaller models. Our experimental results have shown that a 220 million parameter model (Flan-T5-base) fine-tuned using distilled data, not only achieves a significant improvement compared to traditionally fine-tuned baselines, but also surpasses specific LLMs on a scientific table-to-text generation dataset. Our code is available at https://github.com/Bernard-Yang/DistillTableCoT.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2023,
    "citationCount": 18,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.13182",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-22",
    "authors": [
      {
        "authorId": "84537195",
        "name": "Bohao Yang"
      },
      {
        "authorId": "1488944173",
        "name": "Chen Tang"
      },
      {
        "authorId": "35679706",
        "name": "Kangning Zhao"
      },
      {
        "authorId": "2179456850",
        "name": "Chenghao Xiao"
      },
      {
        "authorId": "2268783",
        "name": "Chenghua Lin"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.1665846874966
  },
  {
    "paperId": "a1a91c24f25598f437596acf9d81b0ff182a6190",
    "url": "https://www.semanticscholar.org/paper/a1a91c24f25598f437596acf9d81b0ff182a6190",
    "title": "Natural Language based Context Modeling and Reasoning with LLMs: A Tutorial",
    "abstract": "Large language models (LLMs) have become phenomenally surging 1 , since 2018 – two decades after introducing context-awareness into computing systems [1–4]. Through taking into account the situations of ubiquitous devices, users and the societies, context-aware computing has enabled a wide spectrum of innovative applications, such as assisted living, location-based social network services and so on. To recognize contexts and make decisions for actions accordingly, various artificial intelligence technologies, such as Ontology and OWL, have been adopted as representations for context modeling and reasoning [5, 6]. Recently, with the rise of LLMs and their improved natural language understanding and reasoning capabilities, it has become feasible to model contexts using natural language and perform context reasoning by interacting with LLMs such as ChatGPT and GPT-4. In this tutorial, we demonstrate the use of texts, prompts, and autonomous agents (AutoAgents) that enable LLMs to perform context modeling and reasoning without requiring fine-tuning of the model. We organize and introduce works in the related field, and name this computing paradigm as the LLM-driven Context-aware Computing ( LCaC ). In the LCaC paradigm, users’ requests, sensors reading data, and the command to actuators are supposed to be represented as texts. Given the text of users’ request and sensor data, the AutoAgent models the context by prompting and sends to the LLM for context reasoning. LLM generates a plan of actions and responds to the AutoAgent, which later follows the action plan to foster context-awareness. To prove the concepts, we use two showcases – (1) operating a mobile z-arm in an apartment for assisted living, and (2) planning a trip and scheduling the itinerary in a context-aware and personalized manner. Furthermore, we analyze several factors that might affect the performance of LLM-driven context-awareness, and then discuss the",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.15074",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2093122747",
        "name": "Haoyi Xiong"
      },
      {
        "authorId": "2143957850",
        "name": "Jiang Bian"
      },
      {
        "authorId": "117785201",
        "name": "Sijia Yang"
      },
      {
        "authorId": "2144525327",
        "name": "Xiaofei Zhang"
      },
      {
        "authorId": "3254296",
        "name": "L. Kong"
      },
      {
        "authorId": "2115947583",
        "name": "Daqing Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "135da052891fdcf3842901b6b7585ca8c258f1c2",
    "url": "https://www.semanticscholar.org/paper/135da052891fdcf3842901b6b7585ca8c258f1c2",
    "title": "MathGenie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of LLMs",
    "abstract": "Large language models (LLMs) have exhibited great potential in mathematical reasoning. However, there remains a performance gap in this area between existing open-source models and closed-source models such as GPT-4. In this paper, we introduce MathGenie, a novel method for generating diverse and reliable math problems from a small-scale problem-solution dataset (denoted as seed data). We augment the ground-truth solutions of our seed data and train a back-translation model to translate the augmented solutions back into new questions. Subsequently, we generate code-integrated solutions for the new questions. To ensure the correctness of the code-integrated solutions, we employ rationale-based strategy for solution verification. Various pretrained models, ranging from 7B to 70B, are trained on the newly curated data to test the effectiveness of the proposed augmentation technique, resulting in a family of models known as MathGenieLM. These models consistently outperform previous open-source models across five representative mathematical reasoning datasets, achieving state-of-the-art performance. In particular, MathGenieLM-InternLM2 achieves an accuracy of 87.7% on GSM8K and 55.7% on MATH, securing the best overall score among open-source language models.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 25,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-26",
    "authors": [
      {
        "authorId": "2254361509",
        "name": "Zimu Lu"
      },
      {
        "authorId": "2276425017",
        "name": "Aojun Zhou"
      },
      {
        "authorId": "2254858928",
        "name": "Houxing Ren"
      },
      {
        "authorId": "2254440245",
        "name": "Ke Wang"
      },
      {
        "authorId": "2256723429",
        "name": "Weikang Shi"
      },
      {
        "authorId": "2279759384",
        "name": "Junting Pan"
      },
      {
        "authorId": "2284985101",
        "name": "Mingjie Zhan"
      },
      {
        "authorId": "2229147559",
        "name": "Hongsheng Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.87144807032223
  },
  {
    "paperId": "078a9c3dcd0575dccc45c861618e2363caf47986",
    "url": "https://www.semanticscholar.org/paper/078a9c3dcd0575dccc45c861618e2363caf47986",
    "title": "NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?",
    "abstract": "In evaluating the long-context capabilities of large language models (LLMs), identifying content relevant to a user's query from original long documents is a crucial prerequisite for any LLM to answer questions based on long text. We present NeedleBench, a framework consisting of a series of progressively more challenging tasks for assessing bilingual long-context capabilities, spanning multiple length intervals (4k, 8k, 32k, 128k, 200k, 1000k, and beyond) and different depth ranges, allowing the strategic insertion of critical data points in different text depth zones to rigorously test the retrieval and reasoning capabilities of models in diverse contexts. We use the NeedleBench framework to assess how well the leading open-source models can identify key information relevant to the question and apply that information to reasoning in bilingual long texts. Furthermore, we propose the Ancestral Trace Challenge (ATC) to mimic the complexity of logical reasoning challenges that are likely to be present in real-world long-context tasks, providing a simple method for evaluating LLMs in dealing with complex long-context situations. Our results suggest that current LLMs have significant room for improvement in practical long-context applications, as they struggle with the complexity of logical reasoning challenges that are likely to be present in real-world long-context tasks. All codes and resources are available at OpenCompass: https://github.com/open-compass/opencompass.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 15,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-16",
    "authors": [
      {
        "authorId": "2291515374",
        "name": "Mo Li"
      },
      {
        "authorId": "2266356137",
        "name": "Songyang Zhang"
      },
      {
        "authorId": "2311864117",
        "name": "Yunxin Liu"
      },
      {
        "authorId": "2261273470",
        "name": "Kai Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.58883083359672
  },
  {
    "paperId": "5dc1611ce19a1f6a8562da7455a5370fde148d1b",
    "url": "https://www.semanticscholar.org/paper/5dc1611ce19a1f6a8562da7455a5370fde148d1b",
    "title": "Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving",
    "abstract": "Large Language Models (LLMs) have garnered significant attention for their ability to understand text and images, generate human-like text, and perform complex reasoning tasks. However, their ability to generalize this advanced reasoning with a combination of natural language text for decision-making in dynamic situations requires further exploration. In this study, we investigate how well LLMs can adapt and apply a combination of arithmetic and common-sense reasoning, particularly in autonomous driving scenarios. We hypothesize that LLMs’ hybrid reasoning abilities can improve autonomous driving by enabling them to analyze detected object and sensor data, understand driving regulations and physical laws, and offer additional context. This addresses complex scenarios, like decisions in low visibility (due to weather conditions), where traditional methods might fall short. We evaluated LLM based on accuracy by comparing their answers with human-generated ground truth inside CARLA. The results showed that when a combination of images (detected objects) and sensor data is fed into the LLM, it can offer precise information for brake and throttle control in autonomous vehicles across various weather conditions. This formulation and answers can assist in decision-making for autopilot systems.",
    "venue": "International Conference on Control, Mechatronics and Automation",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2402.13602",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-21",
    "authors": [
      {
        "authorId": "2324780664",
        "name": "Mehdi Azarafza"
      },
      {
        "authorId": "144859797",
        "name": "M. Nayyeri"
      },
      {
        "authorId": "49960684",
        "name": "Charles Steinmetz"
      },
      {
        "authorId": "2265653443",
        "name": "Steffen Staab"
      },
      {
        "authorId": "1791562",
        "name": "A. Rettberg"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "68f4e34312ed209300e3e16a67b3b5c51e2c5dd7",
    "url": "https://www.semanticscholar.org/paper/68f4e34312ed209300e3e16a67b3b5c51e2c5dd7",
    "title": "Automated Generation of Massive Reasonable Empirical Theorems by Forward Reasoning Based on Strong Relevant Logics - A Solution to the Problem of LLM Pre-training Data Exhaustion",
    "abstract": "Recently, it is often said that the data used for the pre-training of large language models (LLMs) have been exhausted. This paper proposes a solution to the problem: Automated generation of massive reasonable empirical theorems by forward reasoning based on strong relevant logics. In fact, this can be regarded as a part of our approach to the problems of ATF (Automated Theorem Finding) and AKA (Automated Knowledge Appreciation).",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-12-16",
    "authors": [
      {
        "authorId": "2336701273",
        "name": "Jingde Cheng"
      }
    ],
    "source": "semantic_scholar",
    "score": 65
  },
  {
    "paperId": "5e77f714c534f493901f7e59e428a367c3237fc3",
    "url": "https://www.semanticscholar.org/paper/5e77f714c534f493901f7e59e428a367c3237fc3",
    "title": "JFLD: A Japanese Benchmark for Deductive Reasoning Based on Formal Logic",
    "abstract": "Large language models (LLMs) have proficiently solved a broad range of tasks with their rich knowledge but often struggle with logical reasoning. To foster the research on logical reasoning, many benchmarks have been proposed so far. However, most of these benchmarks are limited to English, hindering the evaluation of LLMs specialized for each language. To address this, we propose **JFLD** (**J**apanese **F**ormal **L**ogic **D**eduction), a deductive reasoning benchmark for Japanese. JFLD assess whether LLMs can generate logical steps to (dis-)prove a given hypothesis based on a given set of facts. Its key features are assessing pure logical reasoning abilities isolated from knowledge and assessing various reasoning rules. We evaluate various Japanese LLMs and see that they are still poor at logical reasoning, thus highlighting a substantial need for future research.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1379579811",
        "name": "Terufumi Morishita"
      },
      {
        "authorId": "145412147",
        "name": "Atsuki Yamaguchi"
      },
      {
        "authorId": "29347584",
        "name": "Gaku Morio"
      },
      {
        "authorId": "2301583670",
        "name": "Hikaru Tomonari"
      },
      {
        "authorId": "2266468746",
        "name": "Osamu Imaichi"
      },
      {
        "authorId": "2106369",
        "name": "Yasuhiro Sogawa"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "14ad24cc03b0e997f84a07547bb4c179acf896b6",
    "url": "https://www.semanticscholar.org/paper/14ad24cc03b0e997f84a07547bb4c179acf896b6",
    "title": "Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering",
    "abstract": "In this paper, we propose a modified version of the MedQA-USMLE dataset, named MEDQA-OPEN, which contains open-ended medical questions without options to mimic clinical scenarios, along with clinician-approved reasoned answers. Additionally, we implement a prompt driven by Chain of Thought (CoT) reasoning, CLINICR, to mirror the prospective process of incremental reasoning, reaching a correct response to medical questions. We empirically demonstrate how CLINICR outperforms the state-of-the-art 5-shot CoT-based prompt (Li\\'evin et al., 2022). We also present an approach that mirrors real-life clinical practice by first exploring multiple differential diagnoses through MCQ-CLINICR and subsequently narrowing down to a final diagnosis using MCQ-ELIMINATIVE. Finally, emphasizing the importance of response verification in medical settings, we utilize a reward model mechanism, replacing the elimination process performed by MCQ-ELIMINATIVE.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 19,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-07",
    "authors": [
      {
        "authorId": "2290484134",
        "name": "Ojas Gramopadhye"
      },
      {
        "authorId": "2290484369",
        "name": "Saeel Sandeep Nachane"
      },
      {
        "authorId": "2290484410",
        "name": "Prateek Chanda"
      },
      {
        "authorId": "2255574470",
        "name": "Ganesh Ramakrishnan"
      },
      {
        "authorId": "2260653191",
        "name": "K. Jadhav"
      },
      {
        "authorId": "1392630568",
        "name": "Yatin Nandwani"
      },
      {
        "authorId": "1916865",
        "name": "Dinesh Raghu"
      },
      {
        "authorId": "2243011716",
        "name": "Sachindra Joshi"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.93598410330986
  },
  {
    "paperId": "3c101920b0b2b1bd4411b6ab27d4784e26864022",
    "url": "https://www.semanticscholar.org/paper/3c101920b0b2b1bd4411b6ab27d4784e26864022",
    "title": "Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs",
    "abstract": "Integration of Large Language Models (LLMs) into visual domain tasks, resulting in visual-LLMs (V-LLMs), has enabled exceptional performance in vision-language tasks, particularly for visual question answering (VQA). However, existing V-LLMs (e.g. BLIP-2, LLaVA) demonstrate weak spatial reasoning and localization awareness. Despite generating highly descriptive and elaborate textual answers, these models fail at simple tasks like distinguishing a left vs right location. In this work, we explore how image-space coordinate based instruction fine-tuning objectives could inject spatial awareness into V-LLMs. We discover optimal coordinate representations, data-efficient instruction fine-tuning objectives, and pseudo-data generation strategies that lead to improved spatial awareness in V-LLMs. Additionally, our resulting model improves VQA across image and video domains, reduces undesired hallucination, and generates better contextual object descriptions. Experiments across 5 vision-language tasks involving 14 different datasets establish the clear performance improvements achieved by our proposed framework.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2404.07449",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-11",
    "authors": [
      {
        "authorId": "48430646",
        "name": "Kanchana Ranasinghe"
      },
      {
        "authorId": "35194054",
        "name": "Satya Narayan Shukla"
      },
      {
        "authorId": "1973062",
        "name": "Omid Poursaeed"
      },
      {
        "authorId": "1766489",
        "name": "M. Ryoo"
      },
      {
        "authorId": "2296158663",
        "name": "Tsung-Yu Lin"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "d523d86aa7ded30f777788f7ff815a79601c2e62",
    "url": "https://www.semanticscholar.org/paper/d523d86aa7ded30f777788f7ff815a79601c2e62",
    "title": "An AI-Based System Utilizing IoT-Enabled Ambient Sensors and LLMs for Complex Activity Tracking",
    "abstract": "Complex activity recognition plays an important role in elderly care assistance. However, the reasoning ability of edge devices is constrained by the classic machine learning model capacity. In this paper, we present a non-invasive ambient sensing system that can detect multiple activities and apply large language models (LLMs) to reason the activity sequences. This method effectively combines edge devices and LLMs to help elderly people in their daily activities, such as reminding them to take pills or handling emergencies like falls. The LLM-based edge device can also serve as an interface to interact with elderly people, especially with memory issue, assisting them in their daily lives. By deploying such a system, we believe that the smart sensing system can improve the quality of life for older people and provide more efficient protection.",
    "venue": "Academic Journal of Science and Technology",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": {
      "url": "https://drpress.org/ojs/index.php/ajst/article/download/23804/23348",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-02",
    "authors": [
      {
        "authorId": "2305569663",
        "name": "Yuan Sun"
      },
      {
        "authorId": "2305484057",
        "name": "Jorge Ortiz"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.58585994422887
  },
  {
    "paperId": "cddb552f6c3464a54a02b0b64b2d1af56c086606",
    "url": "https://www.semanticscholar.org/paper/cddb552f6c3464a54a02b0b64b2d1af56c086606",
    "title": "MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning",
    "abstract": "The recently released GPT-4 Code Interpreter has demonstrated remarkable proficiency in solving challenging math problems, primarily attributed to its ability to seamlessly reason with natural language, generate code, execute code, and continue reasoning based on the execution output. In this paper, we present a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities. We propose a method of generating novel and high-quality datasets with math problems and their code-based solutions, referred to as MathCodeInstruct. Each solution interleaves natural language, code, and execution results. We also introduce a customized supervised fine-tuning and inference approach. This approach yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems. Impressively, the MathCoder models achieve state-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K (83.9%) datasets, substantially outperforming other open-source alternatives. Notably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K and MATH but also outperforms GPT-4 on the competition-level MATH dataset. The dataset and models will be released at https://github.com/mathllm/MathCoder.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 63,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.03731",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-05",
    "authors": [
      {
        "authorId": "2254440245",
        "name": "Ke Wang"
      },
      {
        "authorId": "2091420783",
        "name": "Houxing Ren"
      },
      {
        "authorId": "9548994",
        "name": "Aojun Zhou"
      },
      {
        "authorId": "2254361509",
        "name": "Zimu Lu"
      },
      {
        "authorId": "2182235729",
        "name": "Sichun Luo"
      },
      {
        "authorId": "2256723429",
        "name": "Weikang Shi"
      },
      {
        "authorId": "2115713503",
        "name": "Renrui Zhang"
      },
      {
        "authorId": "2256601763",
        "name": "Linqi Song"
      },
      {
        "authorId": "2060755203",
        "name": "Mingjie Zhan"
      },
      {
        "authorId": "2229147559",
        "name": "Hongsheng Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 132.38324625039508
  },
  {
    "paperId": "35631fd55c2545615811fa8072015356ac8198e7",
    "url": "https://www.semanticscholar.org/paper/35631fd55c2545615811fa8072015356ac8198e7",
    "title": "LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities",
    "abstract": null,
    "venue": "World wide web (Bussum)",
    "year": 2023,
    "citationCount": 68,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-22",
    "authors": [
      {
        "authorId": "2127757530",
        "name": "Yuqi Zhu"
      },
      {
        "authorId": "2141660877",
        "name": "Xiaohan Wang"
      },
      {
        "authorId": "2284489311",
        "name": "Jing Chen"
      },
      {
        "authorId": "2190751119",
        "name": "Shuofei Qiao"
      },
      {
        "authorId": "2196928874",
        "name": "Yixin Ou"
      },
      {
        "authorId": "4841460",
        "name": "Yunzhi Yao"
      },
      {
        "authorId": "152931849",
        "name": "Shumin Deng"
      },
      {
        "authorId": "2144200945",
        "name": "Huajun Chen"
      },
      {
        "authorId": "2608639",
        "name": "Ningyu Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.5115975689589
  },
  {
    "paperId": "ead04f3e8a5dda83822c20f62a66624f2123abf7",
    "url": "https://www.semanticscholar.org/paper/ead04f3e8a5dda83822c20f62a66624f2123abf7",
    "title": "WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks",
    "abstract": "The ability of large language models (LLMs) to mimic human-like intelligence has led to a surge in LLM-based autonomous agents. Though recent LLMs seem capable of planning and reasoning given user instructions, their effectiveness in applying these capabilities for autonomous task solving remains underexplored. This is especially true in enterprise settings, where automated agents hold the promise of a high impact. To fill this gap, we propose WorkArena++, a novel benchmark consisting of 682 tasks corresponding to realistic workflows routinely performed by knowledge workers. WorkArena++ is designed to evaluate the planning, problem-solving, logical/arithmetic reasoning, retrieval, and contextual understanding abilities of web agents. Our empirical studies across state-of-the-art LLMs and vision-language models (VLMs), as well as human workers, reveal several challenges for such models to serve as useful assistants in the workplace. In addition to the benchmark, we provide a mechanism to effortlessly generate thousands of ground-truth observation/action traces, which can be used for fine-tuning existing models. Overall, we expect this work to serve as a useful resource to help the community progress toward capable autonomous agents. The benchmark can be found at https://github.com/ServiceNow/WorkArena/tree/workarena-plus-plus.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-07",
    "authors": [
      {
        "authorId": "2290847852",
        "name": "L'eo Boisvert"
      },
      {
        "authorId": "2264977662",
        "name": "Megh Thakkar"
      },
      {
        "authorId": "2290916316",
        "name": "Maxime Gasse"
      },
      {
        "authorId": "2310341977",
        "name": "Massimo Caccia"
      },
      {
        "authorId": "2310341775",
        "name": "Thibault Le Sellier De Chezelles"
      },
      {
        "authorId": "1907863",
        "name": "Quentin Cappart"
      },
      {
        "authorId": "2748188",
        "name": "Nicolas Chapados"
      },
      {
        "authorId": "2289400560",
        "name": "Alexandre Lacoste"
      },
      {
        "authorId": "2253399166",
        "name": "Alexandre Drouin"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "10955e63aa49fab146267949f8ebc9ebe8275183",
    "url": "https://www.semanticscholar.org/paper/10955e63aa49fab146267949f8ebc9ebe8275183",
    "title": "Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering",
    "abstract": "Equipped with Chain-of-Thought (CoT), Large language models (LLMs) have shown impressive reasoning ability in various downstream tasks. Even so, suffering from hallucinations and the inability to access external knowledge, LLMs often come with incorrect or unfaithful intermediate reasoning steps, especially in the context of answering knowledge-intensive tasks such as KBQA. To alleviate this issue, we propose a framework called Knowledge-Driven Chain-of-Thought (KD-CoT) to verify and modify reasoning traces in CoT via interaction with external knowledge, and thus overcome the hallucinations and error propagation. Concretely, we formulate the CoT rationale process of LLMs into a structured multi-round QA format. In each round, LLMs interact with a QA system that retrieves external knowledge and produce faithful reasoning traces based on retrieved precise answers. The structured CoT reasoning of LLMs is facilitated by our developed KBQA CoT collection, which serves as in-context learning demonstrations and can also be utilized as feedback augmentation to train a robust retriever. Extensive experiments on WebQSP and ComplexWebQuestion datasets demonstrate the effectiveness of proposed KD-CoT in task-solving reasoning generation, which outperforms the vanilla CoT ICL with an absolute success rate of 8.0% and 5.1%. Furthermore, our proposed feedback-augmented retriever outperforms the state-of-the-art baselines for retrieving knowledge, achieving significant improvement in Hit and recall performance. Our code and data are released on https://github.com/AdelWang/KD-CoT/tree/main.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 56,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.13259",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-08-25",
    "authors": [
      {
        "authorId": "2234024810",
        "name": "Keheng Wang"
      },
      {
        "authorId": "2141973564",
        "name": "Feiyu Duan"
      },
      {
        "authorId": "2592528",
        "name": "Sirui Wang"
      },
      {
        "authorId": "8855602",
        "name": "Peiguang Li"
      },
      {
        "authorId": "2069503881",
        "name": "Yunsen Xian"
      },
      {
        "authorId": "40548403",
        "name": "Chuantao Yin"
      },
      {
        "authorId": "21505283",
        "name": "Wenge Rong"
      },
      {
        "authorId": "2091444262",
        "name": "Zhang Xiong"
      }
    ],
    "source": "semantic_scholar",
    "score": 130.64576901751826
  },
  {
    "paperId": "7290d5bd9b0b55f8ac668764d58d466857945936",
    "url": "https://www.semanticscholar.org/paper/7290d5bd9b0b55f8ac668764d58d466857945936",
    "title": "LLMs for Relational Reasoning: How Far are We?",
    "abstract": "Large language models (LLMs) have revolutionized many areas (e.g. natural language processing, software engineering, etc.) by achieving state-of-the-art performance on extensive downstream tasks. Aiming to achieve robust and general artificial intelligence, there has been a surge of interest in investigating the reasoning ability of the LLMs. Whereas the textual and numerical reasoning benchmarks adopted by previous works are rather shallow and simple, it is hard to conclude that the LLMs possess strong reasoning ability by merely achieving positive results on these benchmarks. Recent efforts have demonstrated that the LLMs are poor at solving sequential decision-making problems that require common-sense planning by evaluating their performance on the reinforcement learning benchmarks. In this work, we conduct an in-depth assessment of several state-of-the-art LLMs’ reasoning ability based on the inductive logic programming (ILP) benchmark, which is broadly recognized as a representative and challenging measurement for evaluating logic program induction/synthesis systems as it requires inducing strict cause-effect logic to achieve robust deduction on independent and identically distributed (IID) and out-of-distribution (OOD) test samples. Our evaluations illustrate that compared with the neural program induction systems which are much smaller in model size, the state-of-the-art LLMs are much poorer in terms of reasoning ability by achieving much lower performance and generalization using either natural language prompting or truth-value matrix prompting.1",
    "venue": "2024 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code)",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-17",
    "authors": [
      {
        "authorId": "2116785673",
        "name": "Zhiming Li"
      },
      {
        "authorId": "153842976",
        "name": "Yushi Cao"
      },
      {
        "authorId": "2279768068",
        "name": "Xiufeng Xu"
      },
      {
        "authorId": "2257105366",
        "name": "Junzhe Jiang"
      },
      {
        "authorId": "2258416493",
        "name": "Xu Liu"
      },
      {
        "authorId": "2066392038",
        "name": "Yon Shin Teo"
      },
      {
        "authorId": "2261677887",
        "name": "Shang-Wei Lin"
      },
      {
        "authorId": "2257126970",
        "name": "Yang Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.53877639491068
  },
  {
    "paperId": "36e6a2fecd40f41e3c45d79d3ed44e505ea10ac3",
    "url": "https://www.semanticscholar.org/paper/36e6a2fecd40f41e3c45d79d3ed44e505ea10ac3",
    "title": "MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time",
    "abstract": "Although Large Language Models (LLMs) achieve remarkable performance across various tasks, they often struggle with complex reasoning tasks, such as answering mathematical questions. Recent efforts to address this issue have primarily focused on leveraging mathematical datasets through supervised fine-tuning or self-improvement techniques. However, these methods often depend on high-quality datasets that are difficult to prepare, or they require substantial computational resources for fine-tuning. Inspired by findings that LLMs know how to produce the right answer but struggle to select the correct reasoning path, we propose a purely inference-based searching method -- MindStar (M*). This method formulates reasoning tasks as searching problems and proposes two search ideas to identify the optimal reasoning paths. We evaluate the M* framework on both the GSM8K and MATH datasets, comparing its performance with existing open and closed-source LLMs. Our results demonstrate that M* significantly enhances the reasoning abilities of open-source models, such as Llama-2-13B and Mistral-7B, and achieves comparable performance to GPT-3.5 and Grok-1, but with substantially reduced model size and computational costs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-25",
    "authors": [
      {
        "authorId": "2277451263",
        "name": "Jikun Kang"
      },
      {
        "authorId": "2303479184",
        "name": "Xin Zhe Li"
      },
      {
        "authorId": "2277670590",
        "name": "Xi Chen"
      },
      {
        "authorId": "2303401469",
        "name": "Amirreza Kazemi"
      },
      {
        "authorId": "2303461755",
        "name": "Boxing Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "704cfb773c96f61697da2ee153d291940ebeda12",
    "url": "https://www.semanticscholar.org/paper/704cfb773c96f61697da2ee153d291940ebeda12",
    "title": "Harnessing the power of LLMs for normative reasoning in MASs",
    "abstract": "Software agents, both human and computational, do not exist in isolation and often need to collaborate or coordinate with others to achieve their goals. In human society, social mechanisms such as norms ensure efficient functioning, and these techniques have been adopted by researchers in multi-agent systems (MAS) to create socially aware agents. However, traditional techniques have limitations, such as operating in limited environments often using brittle symbolic reasoning. The advent of Large Language Models (LLMs) offers a promising solution, providing a rich and expressive vocabulary for norms and enabling norm-capable agents that can perform a range of tasks such as norm discovery, normative reasoning and decision-making. This paper examines the potential of LLM-based agents to acquire normative capabilities, drawing on recent Natural Language Processing (NLP) and LLM research. We present our vision for creating normative LLM agents. In particular, we discuss how the recently proposed\"LLM agent\"approaches can be extended to implement such normative LLM agents. We also highlight challenges in this emerging field. This paper thus aims to foster collaboration between MAS, NLP and LLM researchers in order to advance the field of normative agents.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-25",
    "authors": [
      {
        "authorId": "1736420",
        "name": "B. Savarimuthu"
      },
      {
        "authorId": "2594452",
        "name": "Surangika Ranathunga"
      },
      {
        "authorId": "1707051",
        "name": "Stephen Cranefield"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "f36df6bcc57ef01c072a9262261086755a2088b4",
    "url": "https://www.semanticscholar.org/paper/f36df6bcc57ef01c072a9262261086755a2088b4",
    "title": "Optimized Feature Generation for Tabular Data via LLMs with Decision Tree Reasoning",
    "abstract": "In tabular prediction tasks, tree-based models combined with automated feature engineering methods often outperform deep learning approaches that rely on learned representations. While these feature engineering techniques are effective, they typically depend on a pre-defined search space and primarily use validation scores for feature selection, thereby missing valuable insights from previous experiments. To address these limitations, we propose a novel tabular learning framework that utilizes large language models (LLMs), termed Optimizing Column feature generator with decision Tree reasoning (OCTree). Our key idea is to leverage the reasoning capabilities of LLMs to identify effective feature generation rules without manually specifying the search space and provide language-based reasoning information highlighting past experiments as feedback for iterative rule improvements. We use decision trees to convey this reasoning information, as they can be easily represented in natural language, effectively providing knowledge from prior experiments (i.e., the impact of the generated features on performance) to the LLMs. Our empirical results demonstrate that OCTree consistently enhances the performance of various prediction models across diverse benchmarks, outperforming competing automated feature engineering methods. Code is available at https://github.com/jaehyun513/OCTree.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-12",
    "authors": [
      {
        "authorId": "2297772367",
        "name": "Jaehyun Nam"
      },
      {
        "authorId": "2294681427",
        "name": "Kyuyoung Kim"
      },
      {
        "authorId": "2297196959",
        "name": "Seunghyuk Oh"
      },
      {
        "authorId": "1750599181",
        "name": "Jihoon Tack"
      },
      {
        "authorId": "2116671354",
        "name": "Jaehyung Kim"
      },
      {
        "authorId": "2257390972",
        "name": "Jinwoo Shin"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "f42b97cdfbf1a78c02e78cfce6f8b0e277766ae2",
    "url": "https://www.semanticscholar.org/paper/f42b97cdfbf1a78c02e78cfce6f8b0e277766ae2",
    "title": "Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities of LLMs",
    "abstract": "Reasoning encompasses two typical types: deductive reasoning and inductive reasoning. Despite extensive research into the reasoning capabilities of Large Language Models (LLMs), most studies have failed to rigorously differentiate between inductive and deductive reasoning, leading to a blending of the two. This raises an essential question: In LLM reasoning, which poses a greater challenge - deductive or inductive reasoning? While the deductive reasoning capabilities of LLMs, (i.e. their capacity to follow instructions in reasoning tasks), have received considerable attention, their abilities in true inductive reasoning remain largely unexplored. To investigate into the true inductive reasoning capabilities of LLMs, we propose a novel framework, SolverLearner. This framework enables LLMs to learn the underlying function (i.e., $y = f_w(x)$), that maps input data points $(x)$ to their corresponding output values $(y)$, using only in-context examples. By focusing on inductive reasoning and separating it from LLM-based deductive reasoning, we can isolate and investigate inductive reasoning of LLMs in its pure form via SolverLearner. Our observations reveal that LLMs demonstrate remarkable inductive reasoning capabilities through SolverLearner, achieving near-perfect performance with ACC of 1 in most cases. Surprisingly, despite their strong inductive reasoning abilities, LLMs tend to relatively lack deductive reasoning capabilities, particularly in tasks involving ``counterfactual'' reasoning.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-31",
    "authors": [
      {
        "authorId": "2260755600",
        "name": "Kewei Cheng"
      },
      {
        "authorId": "2276746186",
        "name": "Jingfeng Yang"
      },
      {
        "authorId": "2274172460",
        "name": "Haoming Jiang"
      },
      {
        "authorId": "2312598740",
        "name": "Zhengyang Wang"
      },
      {
        "authorId": "2315087538",
        "name": "Binxuan Huang"
      },
      {
        "authorId": "2257590787",
        "name": "Ruirui Li"
      },
      {
        "authorId": "2314348843",
        "name": "Shiyang Li"
      },
      {
        "authorId": "2273766311",
        "name": "Zheng Li"
      },
      {
        "authorId": "2273915435",
        "name": "Yifan Gao"
      },
      {
        "authorId": "2273816798",
        "name": "Xian Li"
      },
      {
        "authorId": "2273675661",
        "name": "Bing Yin"
      },
      {
        "authorId": "2261083995",
        "name": "Yizhou Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "813d9a7492719b47e8e932dc9c138daed8715295",
    "url": "https://www.semanticscholar.org/paper/813d9a7492719b47e8e932dc9c138daed8715295",
    "title": "An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs",
    "abstract": "Large language models (LLMs) have shown strong arithmetic reasoning capabilities when prompted with Chain-of-Thought (CoT) prompts. However, we have only a limited understanding of how they are processed by LLMs. To demystify it, prior work has primarily focused on ablating different components in the CoT prompt and empirically observing their resulting LLM performance change. Yet, the reason why these components are important to LLM reasoning is not explored. To fill this gap, in this work, we investigate ``neuron activation'' as a lens to provide a unified explanation to observations made by prior work. Specifically, we look into neurons within the feed-forward layers of LLMs that may have activated their arithmetic reasoning capabilities, using Llama2 as an example. To facilitate this investigation, we also propose an approach based on GPT-4 to automatically identify neurons that imply arithmetic reasoning. Our analyses revealed that the activation of reasoning neurons in the feed-forward layers of an LLM can explain the importance of various components in a CoT prompt, and future research can extend it for a more complete understanding.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-18",
    "authors": [
      {
        "authorId": "2203429265",
        "name": "Daking Rai"
      },
      {
        "authorId": "2307416803",
        "name": "Ziyu Yao"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "d673bf9be37a7c6b41b39439846eac444d894125",
    "url": "https://www.semanticscholar.org/paper/d673bf9be37a7c6b41b39439846eac444d894125",
    "title": "BIDTrainer: An LLMs-driven Education Tool for Enhancing the Understanding and Reasoning in Bio-inspired Design",
    "abstract": "Bio-inspired design (BID) fosters innovations in engineering. Learning BID is crucial for developing multidisciplinary innovation skills of designers and engineers. Current BID education aims to enhance learners’ understanding and analogical reasoning skills. However, it often heavily relies on the teachers’ expertise. When learners pursue independent learning using some educational tools, they face challenges in understanding and reasoning practice within this multidisciplinary field. Additionally, evaluating their learning outcomes comprehensively becomes problematic. Addressing these challenges, we introduce a LLMs-driven BID education method based on a structured ontology and three strategies: enhancing understanding through LLMs-enpowered \"learning by asking\", assisting reasoning by providing hints and feedback, and assessing learning outcomes through benchmarking against existing BID cases. Implementing the method, we developed BIDTrainer, a BID education tool. User studies indicate that learners using BIDTrainer understood BID knowledge better, reason faster with higher interactivity than the baseline, and BIDTrainer assessed the learning outcomes consistent with experts.",
    "venue": "International Conference on Human Factors in Computing Systems",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3613904.3642887",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-11",
    "authors": [
      {
        "authorId": "2173717316",
        "name": "Liuqing Chen"
      },
      {
        "authorId": "2282929649",
        "name": "Zhaojun Jiang"
      },
      {
        "authorId": "2301086401",
        "name": "Duowei Xia"
      },
      {
        "authorId": "2267903189",
        "name": "Zebin Cai"
      },
      {
        "authorId": "2110887936",
        "name": "Lingyun Sun"
      },
      {
        "authorId": "2268278681",
        "name": "Peter R. N. Childs"
      },
      {
        "authorId": "2124446242",
        "name": "H. Zuo"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "dfa72bf5095aac707e2a53997d809695eed77d15",
    "url": "https://www.semanticscholar.org/paper/dfa72bf5095aac707e2a53997d809695eed77d15",
    "title": "LLMSense: Harnessing LLMs for High-level Reasoning Over Spatiotemporal Sensor Traces",
    "abstract": "Most studies on machine learning in sensing systems focus on low-level perception tasks that process raw sensory data within a short time window. However, many practical applications, such as human routine modeling and occupancy tracking, require high-level reasoning abilities to comprehend concepts and make inferences based on long-term sensor traces. Existing machine learning-based approaches for handling such complex tasks struggle to generalize due to the limited training samples and the high dimensionality of sensor traces, necessitating the integration of human knowledge for designing first-principle models or logic reasoning methods. We pose a fundamental question: Can we harness the reasoning capabilities and world knowledge of Large Language Models (LLMs) to recognize complex events from long-term spatiotemporal sensor traces? To answer this question, we design an effective prompting framework for LLMs on high-level reasoning tasks, which can handle traces from the raw sensor data as well as the low-level perception results. We also design two strategies to enhance performance with long sensor traces, including summarization before reasoning and selective inclusion of historical traces. Our framework can be implemented in an edge-cloud setup, running small LLMs on the edge for data summarization and performing high-level reasoning on the cloud for privacy preservation. The results show that LLMSense can achieve over 80% accuracy on two high-level reasoning tasks such as dementia diagnosis with behavior traces and occupancy tracking with environmental sensor traces. This paper provides a few insights and guidelines for leveraging LLM for high-level reasoning on sensor traces and highlights several directions for future work.",
    "venue": "2024 IEEE 3rd Workshop on Machine Learning on Edge in Sensor Systems (SenSys-ML)",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-28",
    "authors": [
      {
        "authorId": "2294175329",
        "name": "Xiaomin Ouyang"
      },
      {
        "authorId": "2284986921",
        "name": "Mani Srivastava"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.87639203842082
  },
  {
    "paperId": "5f764b3c02670f8d161c841af32804b23f8d5910",
    "url": "https://www.semanticscholar.org/paper/5f764b3c02670f8d161c841af32804b23f8d5910",
    "title": "Similarity-based Neighbor Selection for Graph LLMs",
    "abstract": "Text-attributed graphs (TAGs) present unique challenges for direct processing by Language Learning Models (LLMs), yet their extensive commonsense knowledge and robust reasoning capabilities offer great promise for node classification in TAGs. Prior research in this field has grappled with issues such as over-squashing, heterophily, and ineffective graph information integration, further compounded by inconsistencies in dataset partitioning and underutilization of advanced LLMs. To address these challenges, we introduce Similarity-based Neighbor Selection (SNS). Using SimCSE and advanced neighbor selection techniques, SNS effectively improves the quality of selected neighbors, thereby improving graph representation and alleviating issues like over-squashing and heterophily. Besides, as an inductive and training-free approach, SNS demonstrates superior generalization and scalability over traditional GNN methods. Our comprehensive experiments, adhering to standard dataset partitioning practices, demonstrate that SNS, through simple prompt interactions with LLMs, consistently outperforms vanilla GNNs and achieves state-of-the-art results on datasets like PubMed in node classification, showcasing LLMs' potential in graph structure understanding. Our research further underscores the significance of graph structure integration in LLM applications and identifies key factors for their success in node classification. Code is available at https://github.com/ruili33/SNS.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-06",
    "authors": [
      {
        "authorId": "2247421824",
        "name": "Rui Li"
      },
      {
        "authorId": "2273523612",
        "name": "Jiwei Li"
      },
      {
        "authorId": "2283087587",
        "name": "Jiawei Han"
      },
      {
        "authorId": "2257458191",
        "name": "Guoyin Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "0d99fc904980136a8e6880da897543f6006fa65d",
    "url": "https://www.semanticscholar.org/paper/0d99fc904980136a8e6880da897543f6006fa65d",
    "title": "Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs",
    "abstract": "In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analyses extend across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the role of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2402.12424",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-19",
    "authors": [
      {
        "authorId": "2142468208",
        "name": "Naihao Deng"
      },
      {
        "authorId": "2284835936",
        "name": "Zhenjie Sun"
      },
      {
        "authorId": "2284762465",
        "name": "Ruiqi He"
      },
      {
        "authorId": "2284760969",
        "name": "Aman Sikka"
      },
      {
        "authorId": "2261678021",
        "name": "Yulong Chen"
      },
      {
        "authorId": "2284827687",
        "name": "Lin Ma"
      },
      {
        "authorId": "2284870290",
        "name": "Yue Zhang"
      },
      {
        "authorId": "2105984203",
        "name": "Rada Mihalcea"
      }
    ],
    "source": "semantic_scholar",
    "score": 91.87639203842082
  },
  {
    "paperId": "b190ff60af7ee4ca177ae67d8eda91e00a24f121",
    "url": "https://www.semanticscholar.org/paper/b190ff60af7ee4ca177ae67d8eda91e00a24f121",
    "title": "MR-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in LLMs",
    "abstract": "Large language models (LLMs) have shown increasing capability in problem-solving and decision-making, largely based on the step-by-step chain-of-thought reasoning processes. However, evaluating these reasoning abilities has become increasingly challenging. Existing outcome-based benchmarks are beginning to saturate, becoming less effective in tracking meaningful progress. To address this, we present a process-based benchmark MR-Ben that demands a meta-reasoning skill, where LMs are asked to locate and analyse potential errors in automatically generated reasoning steps. Our meta-reasoning paradigm is especially suited for system-2 slow thinking, mirroring the human cognitive process of carefully examining assumptions, conditions, calculations, and logic to identify mistakes.MR-Ben comprises 5,975 questions curated by human experts across a wide range of subjects, including physics, chemistry, logic, coding, and more. Through our designed metrics for assessing meta-reasoning on this benchmark, we identify interesting limitations and weaknesses of current LLMs (open-source and closed-source models). For example, with models like the o1 series from OpenAI demonstrating strong performance by effectively scrutinizing the solution space, many other state-of-the-art models fall significantly behind on MR-Ben, exposing potential shortcomings in their training strategies and inference methodologies.",
    "venue": "",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-06-20",
    "authors": [
      {
        "authorId": "2276757508",
        "name": "Zhongshen Zeng"
      },
      {
        "authorId": "2108068935",
        "name": "Yinhong Liu"
      },
      {
        "authorId": "2303955786",
        "name": "Yingjia Wan"
      },
      {
        "authorId": "2204635923",
        "name": "Jingyao Li"
      },
      {
        "authorId": "7778612",
        "name": "Pengguang Chen"
      },
      {
        "authorId": "2302368830",
        "name": "Jianbo Dai"
      },
      {
        "authorId": "2257363981",
        "name": "Yuxuan Yao"
      },
      {
        "authorId": "2158524037",
        "name": "Rongwu Xu"
      },
      {
        "authorId": "2257044451",
        "name": "Zehan Qi"
      },
      {
        "authorId": "2116008462",
        "name": "Wanru Zhao"
      },
      {
        "authorId": "2308077937",
        "name": "Linling Shen"
      },
      {
        "authorId": "2302633318",
        "name": "Jianqiao Lu"
      },
      {
        "authorId": "2303227921",
        "name": "Haochen Tan"
      },
      {
        "authorId": "2281128365",
        "name": "Yukang Chen"
      },
      {
        "authorId": "2307886574",
        "name": "Hao Zhang"
      },
      {
        "authorId": "2281651305",
        "name": "Zhan Shi"
      },
      {
        "authorId": "2307635843",
        "name": "Bailin Wang"
      },
      {
        "authorId": "2681038",
        "name": "Zhijiang Guo"
      },
      {
        "authorId": "2273012826",
        "name": "Jiaya Jia"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "240b7a93eab83806dd6af72a8a035ea5417ad8dd",
    "url": "https://www.semanticscholar.org/paper/240b7a93eab83806dd6af72a8a035ea5417ad8dd",
    "title": "LLMs as a System of Multiple Expert Agents: An Approach to Solve the Abstraction and Reasoning Corpus (ARC) Challenge",
    "abstract": "We attempt to solve the Abstraction and Reasoning Corpus (ARC) Challenge using Large Language Models (LLMs) as a system of multiple expert agents. Using the flexibility of LLMs to be prompted to do various novel tasks using zero-shot, few-shot, context-grounded prompting, we explore the feasibility of using LLMs to solve the ARC Challenge. We firstly convert the input image into multiple suitable text-based abstraction spaces. We then utilize the associative power of LLMs to derive the input-output relationship and map this to actions in the form of a working program, similar to Voyager / Ghost in the MineCraft. In addition, we use iterative environmental feedback in order to guide LLMs to solve the task. Our proposed approach achieves 50 solves out of 111 training set problems (45%) with just three abstraction spaces - grid, object and pixel - and we believe that with more abstraction spaces and learnable actions, our approach will be able to solve more.",
    "venue": "Conference on Algebraic Informatics",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2024-06-25",
    "authors": [
      {
        "authorId": "2313962912",
        "name": "John Chong Min Tan"
      },
      {
        "authorId": "1770486",
        "name": "M. Motani"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "6f2e8147938812c3c73c95e89c5d11bb87d43d60",
    "url": "https://www.semanticscholar.org/paper/6f2e8147938812c3c73c95e89c5d11bb87d43d60",
    "title": "Can LLMs Perform Structured Graph Reasoning Tasks?",
    "abstract": null,
    "venue": "International Conference on Pattern Recognition",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-02",
    "authors": [
      {
        "authorId": "2172415501",
        "name": "Palaash Agrawal"
      },
      {
        "authorId": "2282539706",
        "name": "Shavak Vasania"
      },
      {
        "authorId": "2239059415",
        "name": "Cheston Tan"
      }
    ],
    "source": "semantic_scholar",
    "score": 60.39720770839918
  },
  {
    "paperId": "ee6dc202727c2f3e414c2b4c879b9f4265ac200b",
    "url": "https://www.semanticscholar.org/paper/ee6dc202727c2f3e414c2b4c879b9f4265ac200b",
    "title": "NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data Normalization",
    "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities in parsing textual data and generating code. However, their performance in tasks involving tabular data, especially those requiring symbolic reasoning, faces challenges due to the structural variance and inconsistency in table cell values often found in web tables. In this paper, we introduce NormTab, a novel framework aimed at enhancing the symbolic reasoning performance of LLMs by normalizing web tables. We study table normalization as a stand-alone, one-time preprocessing step using LLMs to support symbolic reasoning on tabular data. Our experimental evaluation, conducted on challenging web table datasets such as WikiTableQuestion and TabFact, demonstrates that leveraging NormTab significantly improves symbolic reasoning performance, showcasing the importance and effectiveness of web table normalization for enhancing LLM-based symbolic reasoning tasks.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-25",
    "authors": [
      {
        "authorId": "31142088",
        "name": "Md Mahadi Hasan Nahid"
      },
      {
        "authorId": "2296784521",
        "name": "Davood Rafiei"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "10edb97e60935e3ed4ef4976d38356bff714fac3",
    "url": "https://www.semanticscholar.org/paper/10edb97e60935e3ed4ef4976d38356bff714fac3",
    "title": "VideoINSTA: Zero-shot Long Video Understanding via Informative Spatial-Temporal Reasoning with LLMs",
    "abstract": "In the video-language domain, recent works in leveraging zero-shot Large Language Model-based reasoning for video understanding have become competitive challengers to previous end-to-end models. However, long video understanding presents unique challenges due to the complexity of reasoning over extended timespans, even for zero-shot LLM-based approaches. The challenge of information redundancy in long videos prompts the question of what specific information is essential for large language models (LLMs) and how to leverage them for complex spatial-temporal reasoning in long-form video analysis. We propose a framework VideoINSTA, i.e. INformative Spatial-TemporAl Reasoning for zero-shot long-form video understanding. VideoINSTA contributes (1) a zero-shot framework for long video understanding using LLMs; (2) an event-based temporal reasoning and content-based spatial reasoning approach for LLMs to reason over spatial-temporal information in videos; (3) a self-reflective information reasoning scheme balancing temporal factors based on information sufficiency and prediction confidence. Our model significantly improves the state-of-the-art on three long video question-answering benchmarks: EgoSchema, NextQA, and IntentQA, and the open question answering dataset ActivityNetQA. The code is released here: https://github.com/mayhugotong/VideoINSTA.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-09-30",
    "authors": [
      {
        "authorId": "2299327546",
        "name": "Ruotong Liao"
      },
      {
        "authorId": "2323506274",
        "name": "Max Erler"
      },
      {
        "authorId": "2323538749",
        "name": "Huiyu Wang"
      },
      {
        "authorId": "2323502868",
        "name": "Guangyao Zhai"
      },
      {
        "authorId": "2267872450",
        "name": "Gengyuan Zhang"
      },
      {
        "authorId": "10684484",
        "name": "Yunpu Ma"
      },
      {
        "authorId": "2265580790",
        "name": "Volker Tresp"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "80aaa526f85fc221ebbe5ca29471ec3f56ed3256",
    "url": "https://www.semanticscholar.org/paper/80aaa526f85fc221ebbe5ca29471ec3f56ed3256",
    "title": "LongIns: A Challenging Long-context Instruction-based Exam for LLMs",
    "abstract": "The long-context capabilities of large language models (LLMs) have been a hot topic in recent years. To evaluate the performance of LLMs in different scenarios, various assessment benchmarks have emerged. However, as most of these benchmarks focus on identifying key information to answer questions, which mainly requires the retrieval ability of LLMs, these benchmarks can partially represent the reasoning performance of LLMs from large amounts of information. Meanwhile, although LLMs often claim to have context windows of 32k, 128k, 200k, or even longer, these benchmarks fail to reveal the actual supported length of these LLMs. To address these issues, we propose the LongIns benchmark dataset, a challenging long-context instruction-based exam for LLMs, which is built based on the existing instruction datasets. Specifically, in our LongIns, we introduce three evaluation settings: Global Instruction&Single Task (GIST), Local Instruction&Single Task (LIST), and Local Instruction&Multiple Tasks (LIMT). Based on LongIns, we perform comprehensive evaluations on existing LLMs and have the following important findings: (1). The top-performing GPT-4 with 128k context length performs poorly on the evaluation context window of 16k in our LongIns. (2). For the multi-hop reasoning ability of many existing LLMs, significant efforts are still needed under short context windows (less than 4k).",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-25",
    "authors": [
      {
        "authorId": "2308098543",
        "name": "Shawn Gavin"
      },
      {
        "authorId": "2300091474",
        "name": "Tuney Zheng"
      },
      {
        "authorId": "2294523552",
        "name": "Jiaheng Liu"
      },
      {
        "authorId": "2303653097",
        "name": "Quehry Que"
      },
      {
        "authorId": "2303796897",
        "name": "Noah Wang"
      },
      {
        "authorId": "2308240515",
        "name": "Jian Yang"
      },
      {
        "authorId": "2214538677",
        "name": "Chenchen Zhang"
      },
      {
        "authorId": "2239245627",
        "name": "Wenhao Huang"
      },
      {
        "authorId": "2249847177",
        "name": "Wenhu Chen"
      },
      {
        "authorId": "2308233656",
        "name": "Ge Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "291dec375a9db2108d219fb26a631deed917deb6",
    "url": "https://www.semanticscholar.org/paper/291dec375a9db2108d219fb26a631deed917deb6",
    "title": "Data-Augmentation-Based Dialectal Adaptation for LLMs",
    "abstract": "This report presents gmnlp’s participation to the Dialect-Copa shared task at VarDial 2024 (Chifu et al., 2024), which focuses on evaluating the commonsense reasoning capabilities of large language models (LLMs) on South Slavic micro-dialects. The task aims to assess how well LLMs can handle non-standard dialectal varieties, as their performance on standard languages is already well-established. We propose an approach that combines the strengths of different types of language models and leverages data augmentation techniques to improve task performance on three South Slavic dialects: Chakavian, Cherkano, and Torlak. We conduct experiments using a language-family-focused encoder-based model (BERTić) and a domain-agnostic multilingual model (AYA-101). Our results demonstrate that the proposed data augmentation techniques lead to substantial performance gains across all three test datasets in the open-source model category. This work highlights the practical utility of data augmentation and the potential of LLMs in handling non-standard dialectal varieties, contributing to the broader goal of advancing natural language understanding in low-resource and dialectal settings.",
    "venue": "Workshop on NLP for Similar Languages, Varieties and Dialects",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-11",
    "authors": [
      {
        "authorId": "48556979",
        "name": "FAHIM FAISAL"
      },
      {
        "authorId": "2273733474",
        "name": "Antonios Anastasopoulos"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "3893c0516278a602c296242fb6755914b0a782c1",
    "url": "https://www.semanticscholar.org/paper/3893c0516278a602c296242fb6755914b0a782c1",
    "title": "Keypoint-based Progressive Chain-of-Thought Distillation for LLMs",
    "abstract": "Chain-of-thought distillation is a powerful technique for transferring reasoning abilities from large language models (LLMs) to smaller student models. Previous methods typically require the student to mimic the step-by-step rationale produced by LLMs, often facing the following challenges: (i) Tokens within a rationale vary in significance, and treating them equally may fail to accurately mimic keypoint tokens, leading to reasoning errors. (ii) They usually distill knowledge by consistently predicting all the steps in a rationale, which falls short in distinguishing the learning order of step generation. This diverges from the human cognitive progression of starting with easy tasks and advancing to harder ones, resulting in sub-optimal outcomes. To this end, we propose a unified framework, called KPOD, to address these issues. Specifically, we propose a token weighting module utilizing mask learning to encourage accurate mimicry of keypoint tokens by the student during distillation. Besides, we develop an in-rationale progressive distillation strategy, starting with training the student to generate the final reasoning steps and gradually extending to cover the entire rationale. To accomplish this, a weighted token generation loss is proposed to assess step reasoning difficulty, and a value function is devised to schedule the progressive distillation by considering both step difficulty and question diversity. Extensive experiments on four reasoning benchmarks illustrate our KPOD outperforms previous methods by a large margin.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-25",
    "authors": [
      {
        "authorId": "2170165151",
        "name": "Kaituo Feng"
      },
      {
        "authorId": "2260925375",
        "name": "Changsheng Li"
      },
      {
        "authorId": "2260823600",
        "name": "Xiaolu Zhang"
      },
      {
        "authorId": "2151551686",
        "name": "Jun Zhou"
      },
      {
        "authorId": "2287785713",
        "name": "Ye Yuan"
      },
      {
        "authorId": "2168989170",
        "name": "Guoren Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "b15ef4e49d4757316337dad2c65066227f10cbd0",
    "url": "https://www.semanticscholar.org/paper/b15ef4e49d4757316337dad2c65066227f10cbd0",
    "title": "SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs in Chinese",
    "abstract": "We introduce SuperCLUE-Math6(SC-Math6), a new benchmark dataset to evaluate the mathematical reasoning abilities of Chinese language models. SC-Math6 is designed as an upgraded Chinese version of the GSM8K dataset with enhanced difficulty, diversity, and application scope. It consists of over 2000 mathematical word problems requiring multi-step reasoning and providing natural language solutions. We propose an innovative scheme to quantify the reasoning capability of large models based on performance over problems with different reasoning steps. Experiments on 13 representative Chinese models demonstrate a clear stratification of reasoning levels, with top models like GPT-4 showing superior performance. SC-Math6 fills the gap in Chinese mathematical reasoning benchmarks and provides a comprehensive testbed to advance the intelligence of Chinese language models.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-22",
    "authors": [
      {
        "authorId": "2257366407",
        "name": "Liang Xu"
      },
      {
        "authorId": "2256985675",
        "name": "Hang Xue"
      },
      {
        "authorId": "2257312015",
        "name": "Lei Zhu"
      },
      {
        "authorId": "2256983017",
        "name": "Kangkang Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "b4a6c010724f0459c9791018e34a982cf96987cf",
    "url": "https://www.semanticscholar.org/paper/b4a6c010724f0459c9791018e34a982cf96987cf",
    "title": "Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs",
    "abstract": "A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency - poll the LLM multiple times and output the most frequent solution. Existing Self-Consistency techniques always generate a constant number of samples per question, where a better approach will be to non-uniformly distribute the available budget based on the amount of agreement in the samples generated so far. In response, we introduce Adaptive-Consistency, a cost-efficient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion. Our experiments over 17 reasoning and code generation datasets and three LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 7.9 times with an average accuracy drop of less than 0.1%. Our code and data are available at https://www.sample-step-by-step.info",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 18,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.emnlp-main.761.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-19",
    "authors": [
      {
        "authorId": "2114841965",
        "name": "Pranjal Aggarwal"
      },
      {
        "authorId": "21626987",
        "name": "Aman Madaan"
      },
      {
        "authorId": "46286308",
        "name": "Yiming Yang"
      },
      {
        "authorId": "2674444",
        "name": "Mausam"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.1665846874966
  },
  {
    "paperId": "a7c0d9bf44045c9d4c41e329e2a87df0ae7e0af6",
    "url": "https://www.semanticscholar.org/paper/a7c0d9bf44045c9d4c41e329e2a87df0ae7e0af6",
    "title": "Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering",
    "abstract": "We investigate how to enhance answer precision in frequently asked questions posed by distributed users using cloud-based Large Language Models (LLMs). Our study focuses on a typical situations where users ask similar queries that involve identical mathematical reasoning steps and problem-solving procedures. Due to the unsatisfactory accuracy of LLMs' zero-shot prompting with standalone questions, we propose to improve the distributed synonymous questions using Self-Consistency (SC) and Chain-of-Thought (CoT) techniques. Specifically, we first retrieve synonymous questions from a crowd-sourced database and create a federated question pool. We call these federated synonymous questions with the same or different parameters SP-questions or DP-questions, respectively. We refer to our methods as Fed-SP-SC and Fed-DP-CoT, which can generate significantly more accurate answers for all user queries without requiring sophisticated model-tuning. Through extensive experiments, we demonstrate that our proposed methods can significantly enhance question accuracy by fully exploring the synonymous nature of the questions and the consistency of the answers.",
    "venue": "Knowledge Science, Engineering and Management",
    "year": 2023,
    "citationCount": 19,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2304.13911",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-04-27",
    "authors": [
      {
        "authorId": "2144226092",
        "name": "Xiangyang Liu"
      },
      {
        "authorId": "2215451922",
        "name": "Tianqi Pang"
      },
      {
        "authorId": "2047692",
        "name": "Chenyou Fan"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.93598410330986
  },
  {
    "paperId": "04dd492b506e48b7dd91ecb1e1fdb80c1ce30e34",
    "url": "https://www.semanticscholar.org/paper/04dd492b506e48b7dd91ecb1e1fdb80c1ce30e34",
    "title": "Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge Reasoning via Promoting Causal Consistency in LLMs",
    "abstract": "Despite advancements in LLMs, knowledge-based reasoning remains a longstanding issue due to the fragility of knowledge recall and inference. Existing methods primarily encourage LLMs to autonomously plan and solve problems or to extensively sample reasoning chains without addressing the conceptual and inferential fallacies. Attempting to alleviate inferential fallacies and drawing inspiration from multi-agent collaboration, we present a framework to increase faithfulness and causality for knowledge-based reasoning. Specifically, we propose to employ multiple intelligent agents (i.e., reasoners and an evaluator ) to work collaboratively in a reasoning-and-consensus paradigm for elevated reasoning faithfulness. The reasoners focus on providing solutions with human-like causality to solve open-domain problems. On the other hand, the evaluator agent scrutinizes if a solution is deducible from a non-causal perspective and if it still holds when challenged by a counterfactual candidate. According to the extensive and comprehensive evaluations on a variety of knowledge reasoning tasks (e.g., science question answering and commonsense reasoning), our framework outperforms all compared state-of-the-art approaches by large margins.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 18,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.11914",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2112518736",
        "name": "Ziyi Tang"
      },
      {
        "authorId": "2282983703",
        "name": "Ruilin Wang"
      },
      {
        "authorId": "2108946830",
        "name": "Weixing Chen"
      },
      {
        "authorId": "2124615864",
        "name": "Keze Wang"
      },
      {
        "authorId": "47909156",
        "name": "Y. Liu"
      },
      {
        "authorId": "1765674",
        "name": "Tianshui Chen"
      },
      {
        "authorId": "2110902045",
        "name": "Liang Lin"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.1665846874966
  },
  {
    "paperId": "4776de7b856d3b15eecc4f88666cdc13972df22e",
    "url": "https://www.semanticscholar.org/paper/4776de7b856d3b15eecc4f88666cdc13972df22e",
    "title": "Eliminating Reasoning via Inferring with Planning: A New Framework to Guide LLMs' Non-linear Thinking",
    "abstract": "Chain-of-Thought(CoT) prompting and its variants explore equipping large language models (LLMs) with high-level reasoning abilities by emulating human-like linear cognition and logic. However, the human mind is complicated and mixed with both linear and nonlinear thinking. In this work, we propose \\textbf{I}nferential \\textbf{E}xclusion \\textbf{P}rompting (IEP), a novel prompting that combines the principles of elimination and inference in order to guide LLMs to think non-linearly. IEP guides LLMs to plan and then utilize Natural Language Inference (NLI) to deduce each possible solution's entailment relation with context, commonsense, or facts, therefore yielding a broader perspective by thinking back for inferring. This forward planning and backward eliminating process allows IEP to better simulate the complex human thinking processes compared to other CoT-based methods, which only reflect linear cognitive processes. We conducted a series of empirical studies and have corroborated that IEP consistently outperforms CoT across various tasks. Additionally, we observe that integrating IEP and CoT further improves the LLMs' performance on certain tasks, highlighting the necessity of equipping LLMs with mixed logic processes. Moreover, to better evaluate comprehensive features inherent in human logic, we introduce \\textbf{M}ental-\\textbf{A}bility \\textbf{R}easoning \\textbf{B}enchmark (MARB). The benchmark comprises six novel subtasks with a total of 9,115 questions, among which 1,685 are developed with hand-crafted rationale references. We believe both \\textsc{IEP} and \\textsc{MARB} can serve as a promising direction for unveiling LLMs' logic and verbal reasoning abilities and drive further advancements. \\textsc{MARB} will be available at ~\\texttt{anonymity link} soon.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 14,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-18",
    "authors": [
      {
        "authorId": "2260336914",
        "name": "Yongqi Tong"
      },
      {
        "authorId": "2260602405",
        "name": "Yifan Wang"
      },
      {
        "authorId": "2161635474",
        "name": "Dawei Li"
      },
      {
        "authorId": "2260628159",
        "name": "Sizhe Wang"
      },
      {
        "authorId": "2260596943",
        "name": "Zi Lin"
      },
      {
        "authorId": "2260609106",
        "name": "Simeng Han"
      },
      {
        "authorId": "2257005005",
        "name": "Jingbo Shang"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "661ef7301c3c399130d3d8673098dd27f5696130",
    "url": "https://www.semanticscholar.org/paper/661ef7301c3c399130d3d8673098dd27f5696130",
    "title": "Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs",
    "abstract": "A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency – poll the LLM multiple times and output the most frequent so-lution. Existing Self-Consistency techniques always draw a constant number of samples per question, where a better approach will be to non-uniformly distribute the available bud-get based on the amount of agreement in the samples drawn so far. In response, we introduce Adaptive-Consistency, a cost-efﬁcient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion. Our experiments over 13 datasets and two LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 6.0 times with an average accuracy drop of less than 0.1%. 1",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 9,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.11860",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2114841965",
        "name": "Pranjal Aggarwal"
      },
      {
        "authorId": "21626987",
        "name": "Aman Madaan"
      },
      {
        "authorId": "2260433687",
        "name": "Yiming Yang"
      },
      {
        "authorId": "2260340500",
        "name": "Mausam"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "ca00f4056f9039d3c1a4c3a113f5ee0527149b66",
    "url": "https://www.semanticscholar.org/paper/ca00f4056f9039d3c1a4c3a113f5ee0527149b66",
    "title": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
    "abstract": "Is vision good enough for language? Recent advancements in multimodal models primarily stem from the powerful reasoning abilities of large language models (LLMs). However, the visual component typically depends only on the instance-level contrastive language-image pre-training (CLIP). Our research reveals that the visual capabilities in recent MultiModal LLMs (MLLMs) still exhibit systematic shortcomings. To understand the roots of these errors, we explore the gap between the visual embedding space of CLIP and vision-only self-supervised learning. We identify “CLIP-blind pairs”- images that CLIP perceives as similar despite their clear visual differences. With these pairs, we construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art systems, including GPT-4V, struggle with straightforward questions across nine basic visual patterns, often providing incorrect answers and hallucinated explanations. We further evaluate various CLIP-based vision-and-language models and found a notable correlation between visual patterns that challenge CLIP models and those problematic for multimodal LLMs. As an initial effort to address these issues, we propose a Mixture of Features (MoF) approach, demonstrating that integrating vision self-supervised learning features with MLLMs can significantly enhance their visual grounding capabilities. Together, our research suggests visual representation learning remains an open challenge, and accurate visual grounding is crucial for future successful multimodal systems.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2024,
    "citationCount": 178,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2401.06209",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-01-11",
    "authors": [
      {
        "authorId": "2143202419",
        "name": "Shengbang Tong"
      },
      {
        "authorId": "2109168016",
        "name": "Zhuang Liu"
      },
      {
        "authorId": "119692515",
        "name": "Yuexiang Zhai"
      },
      {
        "authorId": "2255716385",
        "name": "Yi Ma"
      },
      {
        "authorId": "2265899558",
        "name": "Yann LeCun"
      },
      {
        "authorId": "2275940976",
        "name": "Saining Xie"
      }
    ],
    "source": "semantic_scholar",
    "score": 147.81078708761132
  },
  {
    "paperId": "c608fb120ff2de5b2ed25b02731ec092882d7cf8",
    "url": "https://www.semanticscholar.org/paper/c608fb120ff2de5b2ed25b02731ec092882d7cf8",
    "title": "Rethinking Machine Ethics - Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
    "abstract": "Making moral judgments is an essential step toward developing ethical AI systems. Prevalent approaches are mostly implemented in a bottom-up manner, which uses a large set of annotated data to train models based on crowd-sourced opinions about morality. These approaches have been criticized for overgeneralizing the moral stances of a limited group of annotators and lacking explainability. This work proposes a flexible top-down framework to steer (Large) Language Models (LMs) to perform moral reasoning with well-established moral theories from interdisciplinary research. The theory-guided top-down framework can incorporate various moral theories. Our experiments demonstrate the effectiveness of the proposed framework on datasets derived from moral theories. Furthermore, we show the alignment between different moral theories and existing morality datasets. Our analysis exhibits the potential and flaws in existing resources (models and datasets) in developing explainable moral judgment-making systems.",
    "venue": "NAACL-HLT",
    "year": 2023,
    "citationCount": 18,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.15399",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-08-29",
    "authors": [
      {
        "authorId": "30887444",
        "name": "Jingyan Zhou"
      },
      {
        "authorId": "2100517979",
        "name": "Minda Hu"
      },
      {
        "authorId": "2204171275",
        "name": "Junan Li"
      },
      {
        "authorId": "2155167734",
        "name": "Xiaoying Zhang"
      },
      {
        "authorId": "1847260",
        "name": "Xixin Wu"
      },
      {
        "authorId": "145310663",
        "name": "Irwin King"
      },
      {
        "authorId": "1702243",
        "name": "Helen M. Meng"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.1665846874966
  },
  {
    "paperId": "1909ad17042a0d2af93898d427df0cef9d4483e3",
    "url": "https://www.semanticscholar.org/paper/1909ad17042a0d2af93898d427df0cef9d4483e3",
    "title": "Negotiating with LLMS: Prompt Hacks, Skill Gaps, and Reasoning Deficits",
    "abstract": "Large language models LLMs like ChatGPT have reached the 100 Mio user barrier in record time and might increasingly enter all areas of our life leading to a diverse set of interactions between those Artificial Intelligence models and humans. While many studies have discussed governance and regulations deductively from first-order principles, few studies provide an inductive, data-driven lens based on observing dialogues between humans and LLMs especially when it comes to non-collaborative, competitive situations that have the potential to pose a serious threat to people. In this work, we conduct a user study engaging over 40 individuals across all age groups in price negotiations with an LLM. We explore how people interact with an LLM, investigating differences in negotiation outcomes and strategies. Furthermore, we highlight shortcomings of LLMs with respect to their reasoning capabilities and, in turn, susceptiveness to prompt hacking, which intends to manipulate the LLM to make agreements that are against its instructions or beyond any rationality. We also show that the negotiated prices humans manage to achieve span a broad range, which points to a literacy gap in effectively interacting with LLMs.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-26",
    "authors": [
      {
        "authorId": "2258999677",
        "name": "Johannes Schneider"
      },
      {
        "authorId": "2272385229",
        "name": "Steffi Haag"
      },
      {
        "authorId": "40295265",
        "name": "Leona Chandra Kruse"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "5988806996e8d12f5d4aa911960d842cf7be0c24",
    "url": "https://www.semanticscholar.org/paper/5988806996e8d12f5d4aa911960d842cf7be0c24",
    "title": "Large Language Models are Versatile Decomposers: Decomposing Evidence and Questions for Table-based Reasoning",
    "abstract": "Table-based reasoning has shown remarkable progress in a wide range of table-based tasks. It is a challenging task, which requires reasoning over both free-form natural language (NL) questions and (semi-)structured tabular data. However, previous table-based reasoning solutions usually suffer from significant performance degradation on ''huge'' evidence (tables). In addition, most existing methods struggle to reason over complex questions since the essential information is scattered in different places. To alleviate the above challenges, we exploit large language models (LLMs) as decomposers for effective table-based reasoning, which (i) decompose huge evidence (a huge table) into sub-evidence (a small table) to mitigate the interference of useless information for table reasoning, and (ii) decompose a complex question into simpler sub-questions for text reasoning. First, we use a powerful LLM to decompose the evidence involved in the current question into the sub-evidence that retains the relevant information and excludes the remaining irrelevant information from the ''huge'' evidence. Second, we propose a novel ''parsing-execution-filling'' strategy to decompose a complex question into simper step-by-step sub-questions by generating intermediate SQL queries as a bridge to produce numerical and logical sub-questions with a powerful LLM. Finally, we leverage the decomposed sub-evidence and sub-questions to get the final answer with a few in-context prompting examples. Extensive experiments on three benchmark datasets (TabFact, WikiTableQuestion, and FetaQA) demonstrate that our method achieves significantly better results than competitive baselines for table-based reasoning. Notably, our method outperforms human performance for the first time on the TabFact dataset. In addition to impressive overall performance, our method also has the advantage of interpretability, where the returned results are to some extent tractable with the generated sub-evidence and sub-questions. For reproducibility, we release our source code and data at: https://github.com/AlibabaResearch/DAMO-ConvAI.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2023,
    "citationCount": 112,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-01-31",
    "authors": [
      {
        "authorId": "2187350260",
        "name": "Yunhu Ye"
      },
      {
        "authorId": "151471590",
        "name": "Binyuan Hui"
      },
      {
        "authorId": "2144399900",
        "name": "Min Yang"
      },
      {
        "authorId": "66200440",
        "name": "Binhua Li"
      },
      {
        "authorId": "2087380523",
        "name": "Fei Huang"
      },
      {
        "authorId": "1527090216",
        "name": "Yongbin Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 146.9108172806851
  },
  {
    "paperId": "850dadb26cafc16539074d22745783c0e0bbd01f",
    "url": "https://www.semanticscholar.org/paper/850dadb26cafc16539074d22745783c0e0bbd01f",
    "title": "DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning",
    "abstract": "In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing the demand on foundational capabilities of LLMs. Empirically, DS-Agent with GPT-4 achieves 100\\% success rate in the development stage, while attaining 36\\% improvement on average one pass rate across alternative LLMs in the deployment stage. In both stages, DS-Agent achieves the best rank in performance, costing \\$1.60 and \\$0.13 per run with GPT-4, respectively. Our data and code are open-sourced at https://github.com/guosyjlu/DS-Agent.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 15,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-27",
    "authors": [
      {
        "authorId": "2110835903",
        "name": "Siyuan Guo"
      },
      {
        "authorId": "2057947708",
        "name": "Cheng Deng"
      },
      {
        "authorId": "2283887572",
        "name": "Ying Wen"
      },
      {
        "authorId": "2280102292",
        "name": "Hechang Chen"
      },
      {
        "authorId": "2140493148",
        "name": "Yi Chang"
      },
      {
        "authorId": "2256981980",
        "name": "Jun Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.58883083359672
  },
  {
    "paperId": "d7655b20c9f88df1089edd78980dad5a257eac51",
    "url": "https://www.semanticscholar.org/paper/d7655b20c9f88df1089edd78980dad5a257eac51",
    "title": "LLM3: Large Language Model-based Task and Motion Planning with Motion Failure Reasoning",
    "abstract": "Conventional Task and Motion Planning (TAMP) approaches rely on manually designed interfaces connecting symbolic task planning with continuous motion generation. These domain-specific and labor-intensive modules are limited in addressing emerging tasks in real-world settings. Here, we present LLM3, a novel Large Language Model (LLM)-based TAMP framework featuring a domain-independent interface. Specifically, we leverage the powerful reasoning and planning capabilities of pre-trained LLMs to propose symbolic action sequences and select continuous action parameters for motion planning. Crucially, LLM3 incorporates motion planning feedback through prompting, allowing the LLM to iteratively refine its proposals by reasoning about motion failure. Consequently, LLM3 interfaces between task planning and motion planning, alleviating the intricate design process of handling domain-specific messages between them. Through a series of simulations in a box-packing domain, we quantitatively demonstrate the effectiveness of LLM3 in solving TAMP problems and the efficiency in selecting action parameters. Ablation studies underscore the significant contribution of motion failure reasoning to the success of LLM3. Furthermore, we conduct qualitative experiments on a physical manipulator, demonstrating the practical applicability of our approach in real-world settings.",
    "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
    "year": 2024,
    "citationCount": 19,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-18",
    "authors": [
      {
        "authorId": "2116931976",
        "name": "Shu Wang"
      },
      {
        "authorId": "2044532694",
        "name": "Muzhi Han"
      },
      {
        "authorId": "30901621",
        "name": "Ziyuan Jiao"
      },
      {
        "authorId": "2118689700",
        "name": "Zeyu Zhang"
      },
      {
        "authorId": "2274093892",
        "name": "Yingnian Wu"
      },
      {
        "authorId": "2285638484",
        "name": "Song-Chun Zhu"
      },
      {
        "authorId": "2118959656",
        "name": "Hangxin Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 120.93598410330986
  },
  {
    "paperId": "aa9ad5cd399434bd7e7d6fcd1ddb8f4b58b953a3",
    "url": "https://www.semanticscholar.org/paper/aa9ad5cd399434bd7e7d6fcd1ddb8f4b58b953a3",
    "title": "Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic",
    "abstract": "We study a synthetic corpus based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary, limiting the generalizability of acquired reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. Then, using the proposed corpora, which we name FLD (Formal Logic Deduction), we first evaluate and analyze the logical reasoning ability of the latest LLMs. Even GPT-4 can solve only half of the problems, suggesting that pure logical reasoning isolated from knowledge is still challenging for the LLMs, and additional training specialized in logical reasoning is indeed essential. We next empirically verify that LMs trained on FLD corpora acquire more generalizable reasoning ability. Furthermore, we identify the aspects of reasoning ability on which deduction corpora can enhance LMs and those on which they cannot, and discuss future directions on each aspect. The released corpora serve both as learning resources and as challenging benchmarks.",
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "citationCount": 14,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.07336",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-08-11",
    "authors": [
      {
        "authorId": "1379579811",
        "name": "Terufumi Morishita"
      },
      {
        "authorId": "29347584",
        "name": "Gaku Morio"
      },
      {
        "authorId": "145412147",
        "name": "Atsuki Yamaguchi"
      },
      {
        "authorId": "2106369",
        "name": "Yasuhiro Sogawa"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "b842b83a7ff5dff8e3b83915d8c15423b6085728",
    "url": "https://www.semanticscholar.org/paper/b842b83a7ff5dff8e3b83915d8c15423b6085728",
    "title": "Gemma: Open Models Based on Gemini Research and Technology",
    "abstract": "This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 298,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-13",
    "authors": [
      {
        "authorId": "2291067803",
        "name": "Gemma Team Thomas Mesnard"
      },
      {
        "authorId": "2275186843",
        "name": "Cassidy Hardin"
      },
      {
        "authorId": "51914693",
        "name": "Robert Dadashi"
      },
      {
        "authorId": "9692128",
        "name": "Surya Bhupatiraju"
      },
      {
        "authorId": "2273651441",
        "name": "Shreya Pathak"
      },
      {
        "authorId": "2175946",
        "name": "L. Sifre"
      },
      {
        "authorId": "2275177725",
        "name": "Morgane Rivière"
      },
      {
        "authorId": "26688118",
        "name": "Mihir Kale"
      },
      {
        "authorId": "2253158807",
        "name": "J Christopher Love"
      },
      {
        "authorId": "1775270",
        "name": "P. Tafti"
      },
      {
        "authorId": "122562941",
        "name": "L'eonard Hussenot"
      },
      {
        "authorId": "2841893",
        "name": "Aakanksha Chowdhery"
      },
      {
        "authorId": "2286998643",
        "name": "Adam Roberts"
      },
      {
        "authorId": "20407480",
        "name": "Aditya Barua"
      },
      {
        "authorId": "2180792930",
        "name": "Alex Botev"
      },
      {
        "authorId": "2275175484",
        "name": "Alex Castro-Ros"
      },
      {
        "authorId": "133666998",
        "name": "Ambrose Slone"
      },
      {
        "authorId": "2075284054",
        "name": "Am'elie H'eliou"
      },
      {
        "authorId": "2844530",
        "name": "Andrea Tacchetti"
      },
      {
        "authorId": "2265580818",
        "name": "Anna Bulanova"
      },
      {
        "authorId": "2291068272",
        "name": "Antonia Paterson"
      },
      {
        "authorId": "2291065122",
        "name": "Beth Tsai"
      },
      {
        "authorId": "2067577",
        "name": "Bobak Shahriari"
      },
      {
        "authorId": "153892869",
        "name": "Charline Le Lan"
      },
      {
        "authorId": "1415982317",
        "name": "Christopher A. Choquette-Choo"
      },
      {
        "authorId": "2221011518",
        "name": "Clé-ment Crepy"
      },
      {
        "authorId": "2266238595",
        "name": "Daniel Cer"
      },
      {
        "authorId": "7975935",
        "name": "Daphne Ippolito"
      },
      {
        "authorId": "2275179889",
        "name": "David Reid"
      },
      {
        "authorId": "118801223",
        "name": "Elena Buchatskaya"
      },
      {
        "authorId": "2291065245",
        "name": "Eric Ni"
      },
      {
        "authorId": "51210148",
        "name": "Eric Noland"
      },
      {
        "authorId": "2239111166",
        "name": "Geng Yan"
      },
      {
        "authorId": "2275183383",
        "name": "George Tucker"
      },
      {
        "authorId": "2291065684",
        "name": "George-Christian Muraru"
      },
      {
        "authorId": "2291067913",
        "name": "Grig-ory Rozhdestvenskiy"
      },
      {
        "authorId": "47407464",
        "name": "H. Michalewski"
      },
      {
        "authorId": "2284593233",
        "name": "Ian Tenney"
      },
      {
        "authorId": "119140047",
        "name": "Ivan Grishchenko"
      },
      {
        "authorId": "2058365883",
        "name": "Jacob Austin"
      },
      {
        "authorId": "2058168486",
        "name": "James Keeling"
      },
      {
        "authorId": "2275184618",
        "name": "Jane Labanowski"
      },
      {
        "authorId": "143783339",
        "name": "Jean-Baptiste Lespiau"
      },
      {
        "authorId": "35149729",
        "name": "J. Stanway"
      },
      {
        "authorId": "2275186701",
        "name": "Jenny Brennan"
      },
      {
        "authorId": "2275275439",
        "name": "Jeremy Chen"
      },
      {
        "authorId": "151047979",
        "name": "Johan Ferret"
      },
      {
        "authorId": "2273650801",
        "name": "Justin Chiu"
      },
      {
        "authorId": "1423275766",
        "name": "J. Mao-Jones"
      },
      {
        "authorId": "2275576896",
        "name": "Kather-ine Lee"
      },
      {
        "authorId": "2291882760",
        "name": "Kathy Yu"
      },
      {
        "authorId": "2143434227",
        "name": "Katie Millican"
      },
      {
        "authorId": "2291065624",
        "name": "Lars Lowe Sjoesund"
      },
      {
        "authorId": "2275291886",
        "name": "Lisa Lee"
      },
      {
        "authorId": "2275186508",
        "name": "Lucas Dixon"
      },
      {
        "authorId": "1557386977",
        "name": "Machel Reid"
      },
      {
        "authorId": "2291068287",
        "name": "Maciej Mikuła"
      },
      {
        "authorId": "2275185968",
        "name": "Mateo Wirth"
      },
      {
        "authorId": "2275184531",
        "name": "Michael Sharman"
      },
      {
        "authorId": "2239106707",
        "name": "Nikolai Chinaev"
      },
      {
        "authorId": "2665391",
        "name": "Nithum Thain"
      },
      {
        "authorId": "1936951",
        "name": "Olivier Bachem"
      },
      {
        "authorId": "2275186577",
        "name": "Oscar Chang"
      },
      {
        "authorId": "2038523726",
        "name": "O. Wahltinez"
      },
      {
        "authorId": "2184744789",
        "name": "Paige Bailey"
      },
      {
        "authorId": "2275185732",
        "name": "Paul Michel"
      },
      {
        "authorId": "2291065297",
        "name": "Petko Yotov"
      },
      {
        "authorId": "7281978",
        "name": "Pier Giuseppe Sessa"
      },
      {
        "authorId": "1706980",
        "name": "Rahma Chaabouni"
      },
      {
        "authorId": "89066101",
        "name": "Ramona Comanescu"
      },
      {
        "authorId": "2291065688",
        "name": "Reena Jana"
      },
      {
        "authorId": "1508890387",
        "name": "Rohan Anil"
      },
      {
        "authorId": "2275176102",
        "name": "Ross McIlroy"
      },
      {
        "authorId": "7247867",
        "name": "Ruibo Liu"
      },
      {
        "authorId": "2291067498",
        "name": "Ryan Mullins"
      },
      {
        "authorId": "2288823236",
        "name": "Samuel L Smith"
      },
      {
        "authorId": "148016269",
        "name": "Sebastian Borgeaud"
      },
      {
        "authorId": "35022714",
        "name": "Sertan Girgin"
      },
      {
        "authorId": "2269733876",
        "name": "Sholto Douglas"
      },
      {
        "authorId": "2291065472",
        "name": "Shree Pandya"
      },
      {
        "authorId": "2944868",
        "name": "Siamak Shakeri"
      },
      {
        "authorId": "2289159449",
        "name": "Soham De"
      },
      {
        "authorId": "2275184700",
        "name": "Ted Klimenko"
      },
      {
        "authorId": "2146532222",
        "name": "Tom Hennigan"
      },
      {
        "authorId": "2275181199",
        "name": "Vladimir Feinberg"
      },
      {
        "authorId": "3448463",
        "name": "Wojciech Stokowiec"
      },
      {
        "authorId": "2291314426",
        "name": "Yu-hui Chen"
      },
      {
        "authorId": "2275130349",
        "name": "Zafarali Ahmed"
      },
      {
        "authorId": "2275190668",
        "name": "Zhitao Gong"
      },
      {
        "authorId": "1986491804",
        "name": "Tris Warkentin"
      },
      {
        "authorId": "2291065300",
        "name": "Ludovic Peran"
      },
      {
        "authorId": "2275187490",
        "name": "Minh Giang"
      },
      {
        "authorId": "2265490908",
        "name": "Clément Farabet"
      },
      {
        "authorId": "1689108",
        "name": "O. Vinyals"
      },
      {
        "authorId": "2265529729",
        "name": "Jeffrey Dean"
      },
      {
        "authorId": "2645384",
        "name": "K. Kavukcuoglu"
      },
      {
        "authorId": "48987704",
        "name": "D. Hassabis"
      },
      {
        "authorId": "1983575",
        "name": "Z. Ghahramani"
      },
      {
        "authorId": "2291067974",
        "name": "Douglas Eck"
      },
      {
        "authorId": "2254701020",
        "name": "Joelle Barral"
      },
      {
        "authorId": "2291062354",
        "name": "Fernando Pereira"
      },
      {
        "authorId": "2275181648",
        "name": "Eli Collins"
      },
      {
        "authorId": "2319608",
        "name": "Armand Joulin"
      },
      {
        "authorId": "22640071",
        "name": "Noah Fiedel"
      },
      {
        "authorId": "2268665228",
        "name": "Evan Senter"
      },
      {
        "authorId": "2290741315",
        "name": "Alek Andreev"
      },
      {
        "authorId": "1914502282",
        "name": "Kathleen Kenealy"
      }
    ],
    "source": "semantic_scholar",
    "score": 155.50665360086032
  },
  {
    "paperId": "b671934871fb9a2db3ba729647f298d46958804d",
    "url": "https://www.semanticscholar.org/paper/b671934871fb9a2db3ba729647f298d46958804d",
    "title": "Large Language Model-Based Evolutionary Optimizer: Reasoning with elitism",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, prompting interest in their application as black-box optimizers. This paper asserts that LLMs possess the capability for zero-shot optimization across diverse scenarios, including multi-objective and high-dimensional problems. We introduce a novel population-based method for numerical optimization using LLMs called Language-Model-Based Evolutionary Optimizer (LEO). Our hypothesis is supported through numerical examples, spanning benchmark and industrial engineering problems such as supersonic nozzle shape optimization, heat transfer, and windfarm layout optimization. We compare our method to several gradient-based and gradient-free optimization approaches. While LLMs yield comparable results to state-of-the-art methods, their imaginative nature and propensity to hallucinate demand careful handling. We provide practical guidelines for obtaining reliable answers from LLMs and discuss method limitations and potential research directions.",
    "venue": "Neurocomputing",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-04",
    "authors": [
      {
        "authorId": "74839403",
        "name": "Shuvayan Brahmachary"
      },
      {
        "authorId": "2329728131",
        "name": "S. Joshi"
      },
      {
        "authorId": "2267240642",
        "name": "A. Panda"
      },
      {
        "authorId": "1659115161",
        "name": "K. Koneripalli"
      },
      {
        "authorId": "27070350",
        "name": "A. Sagotra"
      },
      {
        "authorId": "2267241217",
        "name": "H. Patel"
      },
      {
        "authorId": "2290111356",
        "name": "Ankush Sharma"
      },
      {
        "authorId": "2279750388",
        "name": "Ameya D. Jagtap"
      },
      {
        "authorId": "2267241044",
        "name": "K. Kalyanaraman"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "bf8491bef353df126e2306ad2fe4b898697b906a",
    "url": "https://www.semanticscholar.org/paper/bf8491bef353df126e2306ad2fe4b898697b906a",
    "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
    "abstract": "This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++ on machine translation, in a multi-turn\"prompt engineering\"fashion. We also release codebase for evaluation set extraction.",
    "venue": "International Joint Conference on Natural Language Processing",
    "year": 2023,
    "citationCount": 1184,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.ijcnlp-main.45.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-02-08",
    "authors": [
      {
        "authorId": "23672613",
        "name": "Yejin Bang"
      },
      {
        "authorId": "66986482",
        "name": "Samuel Cahyawijaya"
      },
      {
        "authorId": "40221187",
        "name": "Nayeon Lee"
      },
      {
        "authorId": "47653392",
        "name": "Wenliang Dai"
      },
      {
        "authorId": "144610224",
        "name": "Dan Su"
      },
      {
        "authorId": "150048491",
        "name": "Bryan Wilie"
      },
      {
        "authorId": "116344405",
        "name": "Holy Lovenia"
      },
      {
        "authorId": "3391272",
        "name": "Ziwei Ji"
      },
      {
        "authorId": "1660855299",
        "name": "Tiezheng Yu"
      },
      {
        "authorId": "2160716372",
        "name": "Willy Chung"
      },
      {
        "authorId": "2187874252",
        "name": "Quyet V. Do"
      },
      {
        "authorId": "98271906",
        "name": "Yan Xu"
      },
      {
        "authorId": "2057151752",
        "name": "Pascale Fung"
      }
    ],
    "source": "semantic_scholar",
    "score": 176.16247080353847
  },
  {
    "paperId": "13dc9eb9cf36c5bb287671a41cb31b7de2a4cee7",
    "url": "https://www.semanticscholar.org/paper/13dc9eb9cf36c5bb287671a41cb31b7de2a4cee7",
    "title": "Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing",
    "abstract": "Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through Direct Preference Optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. Our results on challenging logical reasoning benchmarks demonstrate the effectiveness of our learning framework, showing that our 7B model can surpass the strong counterparts like GPT-3.5-Turbo.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 14,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-01",
    "authors": [
      {
        "authorId": "1689176705",
        "name": "Fangkai Jiao"
      },
      {
        "authorId": "2084609980",
        "name": "Chengwei Qin"
      },
      {
        "authorId": "49293155",
        "name": "Zhengyuan Liu"
      },
      {
        "authorId": "2271409954",
        "name": "Nancy F. Chen"
      },
      {
        "authorId": "2708940",
        "name": "Shafiq R. Joty"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "7baaaa623d2c3011a52e2bb515e030825fa6e36c",
    "url": "https://www.semanticscholar.org/paper/7baaaa623d2c3011a52e2bb515e030825fa6e36c",
    "title": "An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration",
    "abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in a multitude of Natural Language Processing (NLP) tasks, they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process. To overcome these limitations, this study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs. This scheme first involves using LLMs to iteratively explore KG, selectively retrieving a task-relevant knowledge subgraph to support reasoning. The LLMs are then guided to further combine inherent implicit knowledge to reason on the subgraph while explicitly elucidating the reasoning process. Through such a cooperative approach, our scheme achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results. Experimental results show that our scheme significantly progressed across multiple datasets, notably achieving over a 10% improvement on the QALD10 dataset compared to the best baseline and the fine-tuned state-of-the-art (SOTA) work. Building on this success, this study hopes to offer a valuable reference for future research in the fusion of KG and LLMs, thereby enhancing LLMs' proficiency in solving complex issues.",
    "venue": "International Conference on Artificial Neural Networks",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-07",
    "authors": [
      {
        "authorId": "2283172493",
        "name": "Yihao Li"
      },
      {
        "authorId": "2262402971",
        "name": "Ru Zhang"
      },
      {
        "authorId": "2995416",
        "name": "Jianyi Liu"
      },
      {
        "authorId": "2283163709",
        "name": "Gongshen Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "1772de89a739b3abedd5ade49aa82eb378cec3c0",
    "url": "https://www.semanticscholar.org/paper/1772de89a739b3abedd5ade49aa82eb378cec3c0",
    "title": "NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning",
    "abstract": "Vision-and-Language Navigation (VLN), as a crucial research problem of Embodied AI, requires an embodied agent to navigate through complex 3D environments following natural language instructions. Recent research has highlighted the promising capacity of large language models (LLMs) in VLN by improving navigational reasoning accuracy and interpretability. However, their predominant use in an offline manner usually suffers from substantial domain gap between the VLN task and the LLM training corpus. This paper introduces a novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill parameter-efficient in-domain training to enable self-guided navigational decision, leading to a significant mitigation of the domain gap in a cost-effective manner. Specifically, at each timestep, the LLM is prompted to forecast the navigational chain-of-thought by: 1) acting as a world model to imagine the next observation according to the instruction, 2) selecting the candidate observation that best aligns with the imagination, and 3) determining the action based on the reasoning from the prior steps. Through constructing formalized labels for training, the LLM can learn to generate desired and reasonable chain-of-thought outputs for improving the action decision. Experimental results across various training settings and popular VLN benchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room (R4R)) show the significant superiority of NavCoT over the direct action prediction variants. Through simple parameter-efficient finetuning, our NavCoT outperforms a recent GPT4-based approach with ~7% relative improvement on the R2R dataset. We believe that NavCoT will help unlock more task-adaptive and scalable LLM-based embodied agents, which are helpful for developing real-world robotics applications. Code is available at https://github.com/expectorlin/NavCoT.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-12",
    "authors": [
      {
        "authorId": "2068281996",
        "name": "Bingqian Lin"
      },
      {
        "authorId": "2290906495",
        "name": "Yunshuang Nie"
      },
      {
        "authorId": "2290968842",
        "name": "Ziming Wei"
      },
      {
        "authorId": "2135666594",
        "name": "Jiaqi Chen"
      },
      {
        "authorId": "2220617506",
        "name": "Shikui Ma"
      },
      {
        "authorId": "47180442",
        "name": "Jianhua Han"
      },
      {
        "authorId": "2291076483",
        "name": "Hang Xu"
      },
      {
        "authorId": "2238658252",
        "name": "Xiaojun Chang"
      },
      {
        "authorId": "2279864033",
        "name": "Xiaodan Liang"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "34471a2fa18ea22efad5287cf4aeb18542c98a9b",
    "url": "https://www.semanticscholar.org/paper/34471a2fa18ea22efad5287cf4aeb18542c98a9b",
    "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
    "abstract": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
    "venue": "",
    "year": 2025,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2025-01-22",
    "authors": [
      {
        "authorId": "2307073216",
        "name": "DeepSeek-AI"
      },
      {
        "authorId": null,
        "name": "Daya Guo"
      },
      {
        "authorId": "2278404250",
        "name": "Dejian Yang"
      },
      {
        "authorId": "2278587222",
        "name": "Haowei Zhang"
      },
      {
        "authorId": "2258088582",
        "name": "Jun-Mei Song"
      },
      {
        "authorId": "2337775212",
        "name": "Ruoyu Zhang"
      },
      {
        "authorId": "2274917091",
        "name": "R. Xu"
      },
      {
        "authorId": "2278223869",
        "name": "Qihao Zhu"
      },
      {
        "authorId": "2278830967",
        "name": "Shirong Ma"
      },
      {
        "authorId": "2330130435",
        "name": "Peiyi Wang"
      },
      {
        "authorId": "2289212464",
        "name": "Xiaoling Bi"
      },
      {
        "authorId": "2327287516",
        "name": "Xiaokang Zhang"
      },
      {
        "authorId": "2279159169",
        "name": "Xingkai Yu"
      },
      {
        "authorId": "2335511292",
        "name": "Yu Wu"
      },
      {
        "authorId": "2316170048",
        "name": "Z. F. Wu"
      },
      {
        "authorId": "1797090",
        "name": "Zhibin Gou"
      },
      {
        "authorId": "144485528",
        "name": "Zhihong Shao"
      },
      {
        "authorId": "2309773899",
        "name": "Zhuoshu Li"
      },
      {
        "authorId": "2337771690",
        "name": "Ziyi Gao"
      },
      {
        "authorId": null,
        "name": "Aixin Liu"
      },
      {
        "authorId": "2337687235",
        "name": "Bing Xue"
      },
      {
        "authorId": "2244285627",
        "name": "Bing-Li Wang"
      },
      {
        "authorId": "2337844023",
        "name": "Bochao Wu"
      },
      {
        "authorId": "2337688451",
        "name": "Bei Feng"
      },
      {
        "authorId": "2337772642",
        "name": "Chengda Lu"
      },
      {
        "authorId": "2278389597",
        "name": "Chenggang Zhao"
      },
      {
        "authorId": "2278221484",
        "name": "C. Deng"
      },
      {
        "authorId": "2337705874",
        "name": "Chenyu Zhang"
      },
      {
        "authorId": "2278217940",
        "name": "C. Ruan"
      },
      {
        "authorId": "2307085654",
        "name": "Damai Dai"
      },
      {
        "authorId": "2274200088",
        "name": "Deli Chen"
      },
      {
        "authorId": "2325992470",
        "name": "Dong-Li Ji"
      },
      {
        "authorId": "2278218944",
        "name": "Erhang Li"
      },
      {
        "authorId": "2278589731",
        "name": "Fangyun Lin"
      },
      {
        "authorId": "2337687526",
        "name": "Fucong Dai"
      },
      {
        "authorId": "2278218736",
        "name": "Fuli Luo"
      },
      {
        "authorId": "2278219957",
        "name": "Guangbo Hao"
      },
      {
        "authorId": "2278589675",
        "name": "Guanting Chen"
      },
      {
        "authorId": "2337746209",
        "name": "Guowei Li"
      },
      {
        "authorId": "2337801868",
        "name": "H. Zhang"
      },
      {
        "authorId": "2287769900",
        "name": "Han Bao"
      },
      {
        "authorId": "2278643857",
        "name": "Hanwei Xu"
      },
      {
        "authorId": "2316174862",
        "name": "Haocheng Wang"
      },
      {
        "authorId": "2278486342",
        "name": "Honghui Ding"
      },
      {
        "authorId": "2238628841",
        "name": "Huajian Xin"
      },
      {
        "authorId": "2278395340",
        "name": "Huazuo Gao"
      },
      {
        "authorId": "2305476903",
        "name": "Hui Qu"
      },
      {
        "authorId": "2338017035",
        "name": "Hui Li"
      },
      {
        "authorId": "2278226003",
        "name": "Jianzhong Guo"
      },
      {
        "authorId": "2278337977",
        "name": "Jiashi Li"
      },
      {
        "authorId": "2219717571",
        "name": "Jiawei Wang"
      },
      {
        "authorId": "2337776990",
        "name": "Jingchang Chen"
      },
      {
        "authorId": "2337749769",
        "name": "Jingyang Yuan"
      },
      {
        "authorId": "2278355944",
        "name": "Junjie Qiu"
      },
      {
        "authorId": "2337836211",
        "name": "Junlong Li"
      },
      {
        "authorId": "2333971037",
        "name": "J. Cai"
      },
      {
        "authorId": "2332054265",
        "name": "J. Ni"
      },
      {
        "authorId": "2337771119",
        "name": "Jian Liang"
      },
      {
        "authorId": "2337786826",
        "name": "Jin Chen"
      },
      {
        "authorId": "2278218552",
        "name": "Kai Dong"
      },
      {
        "authorId": "2335579723",
        "name": "Kai Hu"
      },
      {
        "authorId": "2278218724",
        "name": "Kaige Gao"
      },
      {
        "authorId": "2278219877",
        "name": "Kang Guan"
      },
      {
        "authorId": null,
        "name": "Kexin Huang"
      },
      {
        "authorId": "2329319668",
        "name": "K. Yu"
      },
      {
        "authorId": "2337705401",
        "name": "Lean Wang"
      },
      {
        "authorId": "2278256170",
        "name": "Lecong Zhang"
      },
      {
        "authorId": "2330538675",
        "name": "Liang Zhao"
      },
      {
        "authorId": "15068726",
        "name": "Litong Wang"
      },
      {
        "authorId": "2278257096",
        "name": "Liyue Zhang"
      },
      {
        "authorId": "2337753846",
        "name": "Lei Xu"
      },
      {
        "authorId": "2337787680",
        "name": "Leyi Xia"
      },
      {
        "authorId": "2278384213",
        "name": "Mingchuan Zhang"
      },
      {
        "authorId": "2337708168",
        "name": "Minghua Zhang"
      },
      {
        "authorId": null,
        "name": "Minghui Tang"
      },
      {
        "authorId": "2338070273",
        "name": "Meng Li"
      },
      {
        "authorId": "2337775620",
        "name": "Miaojun Wang"
      },
      {
        "authorId": "2337751092",
        "name": "Mingming Li"
      },
      {
        "authorId": "2337686905",
        "name": "Ning Tian"
      },
      {
        "authorId": null,
        "name": "Panpan Huang"
      },
      {
        "authorId": "2333052802",
        "name": "Peng Zhang"
      },
      {
        "authorId": "2198632851",
        "name": "Qiancheng Wang"
      },
      {
        "authorId": "2307394725",
        "name": "Qinyu Chen"
      },
      {
        "authorId": "2278218583",
        "name": "Qiushi Du"
      },
      {
        "authorId": "2278218130",
        "name": "Ruiqi Ge"
      },
      {
        "authorId": "2337774448",
        "name": "Ruisong Zhang"
      },
      {
        "authorId": "2337687339",
        "name": "Ruizhe Pan"
      },
      {
        "authorId": "2337768965",
        "name": "Runji Wang"
      },
      {
        "authorId": "2337764886",
        "name": "R. J. Chen"
      },
      {
        "authorId": "2337687334",
        "name": "R. L. Jin"
      },
      {
        "authorId": "2288751385",
        "name": "Ruyi Chen"
      },
      {
        "authorId": "2278389939",
        "name": "Shanghao Lu"
      },
      {
        "authorId": "2278397391",
        "name": "Shangyan Zhou"
      },
      {
        "authorId": "2278219833",
        "name": "Shanhuang Chen"
      },
      {
        "authorId": null,
        "name": "Shengfeng Ye"
      },
      {
        "authorId": "2330054226",
        "name": "Shiyu Wang"
      },
      {
        "authorId": "2278408463",
        "name": "Shuiping Yu"
      },
      {
        "authorId": "2278398119",
        "name": "Shunfeng Zhou"
      },
      {
        "authorId": "2337787780",
        "name": "Shuting Pan"
      },
      {
        "authorId": "2337775628",
        "name": "S. S. Li"
      },
      {
        "authorId": "2301799594",
        "name": "Shuang Zhou"
      },
      {
        "authorId": "2157824681",
        "name": "Shao-Kang Wu"
      },
      {
        "authorId": "2337686938",
        "name": "Tao Yun"
      },
      {
        "authorId": "2278220142",
        "name": "Tian Pei"
      },
      {
        "authorId": "2340400824",
        "name": "T. Sun"
      },
      {
        "authorId": "2337757182",
        "name": "T. Wang"
      },
      {
        "authorId": null,
        "name": "Wangding Zeng"
      },
      {
        "authorId": "2257129965",
        "name": "Wanjia Zhao"
      },
      {
        "authorId": "2326440587",
        "name": "Wen Liu"
      },
      {
        "authorId": "2278618633",
        "name": "W. Liang"
      },
      {
        "authorId": "2272467392",
        "name": "W. Gao"
      },
      {
        "authorId": "2298210246",
        "name": "Wen-Xia Yu"
      },
      {
        "authorId": "2341325349",
        "name": "Wentao Zhang"
      },
      {
        "authorId": "2337790804",
        "name": "W. L. Xiao"
      },
      {
        "authorId": "2316966121",
        "name": "Wei An"
      },
      {
        "authorId": "2257346982",
        "name": "Xiaodong Liu"
      },
      {
        "authorId": "2337844470",
        "name": "Xiaohan Wang"
      },
      {
        "authorId": "2326444644",
        "name": "Xiaokang Chen"
      },
      {
        "authorId": "2278218185",
        "name": "X. Nie"
      },
      {
        "authorId": "2193630544",
        "name": "Xin Cheng"
      },
      {
        "authorId": "2336338617",
        "name": "Xin Liu"
      },
      {
        "authorId": "2239599436",
        "name": "Xin Xie"
      },
      {
        "authorId": "2326998908",
        "name": "Xingchao Liu"
      },
      {
        "authorId": "2312345351",
        "name": "Xinyu Yang"
      },
      {
        "authorId": "2337738744",
        "name": "Xinyuan Li"
      },
      {
        "authorId": null,
        "name": "Xuecheng Su"
      },
      {
        "authorId": "2337776686",
        "name": "Xuheng Lin"
      },
      {
        "authorId": "2337776625",
        "name": "X. Q. Li"
      },
      {
        "authorId": "2149112346",
        "name": "Xiangyu Jin"
      },
      {
        "authorId": "2262432943",
        "name": "Xi-Cheng Shen"
      },
      {
        "authorId": "2337775973",
        "name": "Xiaosha Chen"
      },
      {
        "authorId": "2316956218",
        "name": "Xiaowen Sun"
      },
      {
        "authorId": "2299743807",
        "name": "Xiaoxiang Wang"
      },
      {
        "authorId": "2337774374",
        "name": "Xinnan Song"
      },
      {
        "authorId": "2334871182",
        "name": "Xinyi Zhou"
      },
      {
        "authorId": "2337777409",
        "name": "Xianzu Wang"
      },
      {
        "authorId": "2337688042",
        "name": "Xinxia Shan"
      },
      {
        "authorId": "2278599324",
        "name": "Y. K. Li"
      },
      {
        "authorId": "2337725217",
        "name": "Y. Q. Wang"
      },
      {
        "authorId": "2337771864",
        "name": "Y. X. Wei"
      },
      {
        "authorId": "2338171554",
        "name": "Yang Zhang"
      },
      {
        "authorId": "2324603956",
        "name": "Yanhong Xu"
      },
      {
        "authorId": "2278381130",
        "name": "Yao Li"
      },
      {
        "authorId": "2278398317",
        "name": "Yao Zhao"
      },
      {
        "authorId": "2278377738",
        "name": "Yaofeng Sun"
      },
      {
        "authorId": "2278432120",
        "name": "Yaohui Wang"
      },
      {
        "authorId": "2338182791",
        "name": "Yi Yu"
      },
      {
        "authorId": "2337243026",
        "name": "Yichao Zhang"
      },
      {
        "authorId": "2337801571",
        "name": "Yifan Shi"
      },
      {
        "authorId": "2308644697",
        "name": "Yi Xiong"
      },
      {
        "authorId": "2317034993",
        "name": "Ying He"
      },
      {
        "authorId": "2278218072",
        "name": "Yishi Piao"
      },
      {
        "authorId": "2330376936",
        "name": "Yisong Wang"
      },
      {
        "authorId": "2316968504",
        "name": "Yixuan Tan"
      },
      {
        "authorId": "2342237203",
        "name": "Yiyang Ma"
      },
      {
        "authorId": "2278393359",
        "name": "Yiyuan Liu"
      },
      {
        "authorId": "2317130708",
        "name": "Yongqiang Guo"
      },
      {
        "authorId": "2337687328",
        "name": "Yuan Ou"
      },
      {
        "authorId": "2337765075",
        "name": "Yuduan Wang"
      },
      {
        "authorId": "2338990074",
        "name": "Yue Gong"
      },
      {
        "authorId": "2300352188",
        "name": "Yu-Jing Zou"
      },
      {
        "authorId": "2337718186",
        "name": "Yujia He"
      },
      {
        "authorId": "2326446831",
        "name": "Yunfan Xiong"
      },
      {
        "authorId": "2324528328",
        "name": "Yu-Wei Luo"
      },
      {
        "authorId": "2054452755",
        "name": "Yu-mei You"
      },
      {
        "authorId": "2337142726",
        "name": "Yuxuan Liu"
      },
      {
        "authorId": "2337775637",
        "name": "Yuyang Zhou"
      },
      {
        "authorId": "2337840145",
        "name": "Y. X. Zhu"
      },
      {
        "authorId": "2272475376",
        "name": "Yanping Huang"
      },
      {
        "authorId": "2278381130",
        "name": "Yao Li"
      },
      {
        "authorId": "2279900058",
        "name": "Yi Zheng"
      },
      {
        "authorId": "2291197888",
        "name": "Yuchen Zhu"
      },
      {
        "authorId": "2318919239",
        "name": "Yunxiang Ma"
      },
      {
        "authorId": "2337979045",
        "name": "Ying Tang"
      },
      {
        "authorId": "2337687962",
        "name": "Yukun Zha"
      },
      {
        "authorId": "2337777710",
        "name": "Yuting Yan"
      },
      {
        "authorId": "2278430558",
        "name": "Z. Ren"
      },
      {
        "authorId": "2278430558",
        "name": "Z. Ren"
      },
      {
        "authorId": "2278218357",
        "name": "Zhangli Sha"
      },
      {
        "authorId": null,
        "name": "Zhe Fu"
      },
      {
        "authorId": "2337940131",
        "name": "Zhean Xu"
      },
      {
        "authorId": "2279107352",
        "name": "Zhenda Xie"
      },
      {
        "authorId": "2148906520",
        "name": "Zhen-guo Zhang"
      },
      {
        "authorId": null,
        "name": "Zhewen Hao"
      },
      {
        "authorId": "2337695745",
        "name": "Zhicheng Ma"
      },
      {
        "authorId": "2337768819",
        "name": "Zhigang Yan"
      },
      {
        "authorId": "2326514370",
        "name": "Zhiyu Wu"
      },
      {
        "authorId": "2307422404",
        "name": "Zihui Gu"
      },
      {
        "authorId": "2337838091",
        "name": "Zijia Zhu"
      },
      {
        "authorId": "2117942065",
        "name": "Zijun Liu"
      },
      {
        "authorId": "2268382130",
        "name": "Zi-An Li"
      },
      {
        "authorId": "2279107375",
        "name": "Ziwei Xie"
      },
      {
        "authorId": "2271718109",
        "name": "Ziyang Song"
      },
      {
        "authorId": null,
        "name": "Zizheng Pan"
      },
      {
        "authorId": "2337821867",
        "name": "Zhen Huang"
      },
      {
        "authorId": "2238223352",
        "name": "Zhipeng Xu"
      },
      {
        "authorId": "2337972848",
        "name": "Zhongyu Zhang"
      },
      {
        "authorId": "2329999671",
        "name": "Zhen Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "237b0cf9d78f4a52274b868656ad011f599aeb26",
    "url": "https://www.semanticscholar.org/paper/237b0cf9d78f4a52274b868656ad011f599aeb26",
    "title": "LLM-Assist: Enhancing Closed-Loop Planning with Language-Based Reasoning",
    "abstract": "Although planning is a crucial component of the autonomous driving stack, researchers have yet to develop robust planning algorithms that are capable of safely handling the diverse range of possible driving scenarios. Learning-based planners suffer from overfitting and poor long-tail performance. On the other hand, rule-based planners generalize well, but might fail to handle scenarios that require complex driving maneuvers. To address these limitations, we investigate the possibility of leveraging the common-sense reasoning capabilities of Large Language Models (LLMs) such as GPT4 and Llama2 to generate plans for self-driving vehicles. In particular, we develop a novel hybrid planner that leverages a conventional rule-based planner in conjunction with an LLM-based planner. Guided by commonsense reasoning abilities of LLMs, our approach navigates complex scenarios which existing planners struggle with, produces well-reasoned outputs while also remaining grounded through working alongside the rule-based approach. Through extensive evaluation on the nuPlan benchmark, we achieve state-of-the-art performance, outperforming all existing pure learning- and rule-based methods across most metrics. Our code will be available at https://llmassist.github.io.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 36,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-30",
    "authors": [
      {
        "authorId": "2277248698",
        "name": "S. P. Sharan"
      },
      {
        "authorId": "3018385",
        "name": "F. Pittaluga"
      },
      {
        "authorId": "2265551529",
        "name": "G. VijayKumarB."
      },
      {
        "authorId": "2253719599",
        "name": "M. Chandraker"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.16376868966336
  },
  {
    "paperId": "3687e55cf21c4d0041d9bf0b74988319bfe3402b",
    "url": "https://www.semanticscholar.org/paper/3687e55cf21c4d0041d9bf0b74988319bfe3402b",
    "title": "Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs",
    "abstract": "Recent advancements in multimodal large language models (MLLMs) have been noteworthy, yet, these general-domain MLLMs often fall short in their ability to comprehend and interact effectively with user interface (UI) screens. In this paper, we present Ferret-UI, a new MLLM tailored for enhanced understanding of mobile UI screens, equipped with referring, grounding, and reasoning capabilities. Given that UI screens typically exhibit a more elongated aspect ratio and contain smaller objects of interest (e.g., icons, texts) than natural images, we incorporate\"any resolution\"on top of Ferret to magnify details and leverage enhanced visual features. Specifically, each screen is divided into 2 sub-images based on the original aspect ratio (i.e., horizontal division for portrait screens and vertical division for landscape screens). Both sub-images are encoded separately before being sent to LLMs. We meticulously gather training samples from an extensive range of elementary UI tasks, such as icon recognition, find text, and widget listing. These samples are formatted for instruction-following with region annotations to facilitate precise referring and grounding. To augment the model's reasoning ability, we further compile a dataset for advanced tasks, including detailed description, perception/interaction conversations, and function inference. After training on the curated datasets, Ferret-UI exhibits outstanding comprehension of UI screens and the capability to execute open-ended instructions. For model evaluation, we establish a comprehensive benchmark encompassing all the aforementioned tasks. Ferret-UI excels not only beyond most open-source UI MLLMs, but also surpasses GPT-4V on all the elementary UI tasks.",
    "venue": "European Conference on Computer Vision",
    "year": 2024,
    "citationCount": 46,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-08",
    "authors": [
      {
        "authorId": "2295669541",
        "name": "Keen You"
      },
      {
        "authorId": "2257340591",
        "name": "Haotian Zhang"
      },
      {
        "authorId": "3396325",
        "name": "E. Schoop"
      },
      {
        "authorId": "1395932715",
        "name": "Floris Weers"
      },
      {
        "authorId": "3246971",
        "name": "Amanda Swearngin"
      },
      {
        "authorId": "2057156585",
        "name": "Jeffrey Nichols"
      },
      {
        "authorId": "2249897805",
        "name": "Yinfei Yang"
      },
      {
        "authorId": "2253397669",
        "name": "Zhe Gan"
      }
    ],
    "source": "semantic_scholar",
    "score": 127.75221402565089
  },
  {
    "paperId": "b9833a5354be3a0b137ee45a39abfb49b2612129",
    "url": "https://www.semanticscholar.org/paper/b9833a5354be3a0b137ee45a39abfb49b2612129",
    "title": "A Case-Based Reasoning Framework for Adaptive Prompting in Cross-Domain Text-to-SQL",
    "abstract": ". Recent advancements in Large Language Models (LLMs), such as Codex, ChatGPT and GPT-4 have signiﬁcantly impacted the AI community, including Text-to-SQL tasks. Some evaluations and analyses on LLMs show their potential to generate SQL queries but they point out poorly designed prompts (e.g. simplistic construction or random sampling) limit LLMs’ performance and may cause unnecessary or irrelevant outputs. To address these issues, we propose CBR-ApSQL, a Case-Based Reasoning (CBR)-based framework combined with GPT-3.5 for precise control over case-relevant and case-irrelevant knowledge in Text-to-SQL tasks. We design adaptive prompts for ﬂexibly adjusting inputs for GPT-3.5, which involves (1) adaptively retrieving cases according to the question intention by de-semantizing the input question, and (2) an adaptive fallback mechanism to ensure the informativeness of the prompt, as well as the relevance between cases and the prompt. In the de-semanticization phase, we designed Semantic Domain Relevance Evaluator(SDRE), combined with Poincar´e detector(mining implicit semantics in hyperbolic space), TextAlign(discovering explicit matches), and Positector (part-of-speech detector). SDRE semantically and syntactically generates in-context exemplar annotations for the new case. On the three cross-domain datasets, our framework outperforms the state-of-the-art(SOTA) model in execution accuracy by 3.7%, 2.5%, and 8.2%, respectively",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 18,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2304.13301",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2215405126",
        "name": "Chunxi Guo"
      },
      {
        "authorId": "2257366341",
        "name": "Zhiliang Tian"
      },
      {
        "authorId": "1762106",
        "name": "Jintao Tang"
      },
      {
        "authorId": "2073437",
        "name": "Pancheng Wang"
      },
      {
        "authorId": "2256993729",
        "name": "Zhihua Wen"
      },
      {
        "authorId": "2220692725",
        "name": "Kang Yang"
      },
      {
        "authorId": "2257363292",
        "name": "Ting Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.1665846874966
  },
  {
    "paperId": "7a50db6d1393130e532f6f06adebb7fcad96681c",
    "url": "https://www.semanticscholar.org/paper/7a50db6d1393130e532f6f06adebb7fcad96681c",
    "title": "An LLM-based Knowledge Synthesis and Scientific Reasoning Framework for Biomedical Discovery",
    "abstract": "We present BioLunar, developed using the Lunar framework, as a tool for supporting biological analyses, with a particular emphasis on molecular-level evidence enrichment for biomarker discovery in oncology. The platform integrates Large Language Models (LLMs) to facilitate complex scientific reasoning across distributed evidence spaces, enhancing the capability for harmonizing and reasoning over heterogeneous data sources. Demonstrating its utility in cancer research, BioLunar leverages modular design, reusable data access and data analysis components, and a low-code user interface, enabling researchers of all programming levels to construct LLM-enabled scientific workflows. By facilitating automatic scientific discovery and inference from heterogeneous evidence, BioLunar exemplifies the potential of the integration between LLMs, specialised databases and biomedical tools to support expert-level knowledge synthesis and discovery.",
    "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Biology"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-26",
    "authors": [
      {
        "authorId": "2308466750",
        "name": "Oskar Wysocki"
      },
      {
        "authorId": "2266390451",
        "name": "Magdalena Wysocka"
      },
      {
        "authorId": "2308467817",
        "name": "Danilo S. Carvalho"
      },
      {
        "authorId": "19185501",
        "name": "Alex Bogatu"
      },
      {
        "authorId": "2308469072",
        "name": "Danilo Miranda Gusicuma"
      },
      {
        "authorId": "2266390558",
        "name": "Maxime Delmas"
      },
      {
        "authorId": "2308468190",
        "name": "Harriet Unsworth"
      },
      {
        "authorId": "2266389711",
        "name": "André Freitas"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "0e8ec196e908bb3e2b9a709ee9420f1c4d8577af",
    "url": "https://www.semanticscholar.org/paper/0e8ec196e908bb3e2b9a709ee9420f1c4d8577af",
    "title": "PHAnToM: Persona-based Prompting Has An Effect on Theory-of-Mind Reasoning in Large Language Models",
    "abstract": "The use of LLMs in natural language reasoning has shown mixed results, sometimes rivaling or even surpassing human performance in simpler classification tasks while struggling with social-cognitive reasoning, a domain where humans naturally excel. These differences have been attributed to many factors, such as variations in prompting and the specific LLMs used. However, no reasons appear conclusive, and no clear mechanisms have been established in prior work. In this study, we empirically evaluate how role-playing prompting influences Theory-of-Mind (ToM) reasoning capabilities. Grounding our rsearch in psychological theory, we propose the mechanism that, beyond the inherent variance in the complexity of reasoning tasks, performance differences arise because of socially-motivated prompting differences. In an era where prompt engineering with role-play is a typical approach to adapt LLMs to new contexts, our research advocates caution as models that adopt specific personas might potentially result in errors in social-cognitive reasoning.",
    "venue": "",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-03-04",
    "authors": [
      {
        "authorId": "2287992265",
        "name": "Fiona Anting Tan"
      },
      {
        "authorId": "2242615919",
        "name": "G. Yeo"
      },
      {
        "authorId": "2261413304",
        "name": "Fanyou Wu"
      },
      {
        "authorId": "2110546424",
        "name": "Weijie Xu"
      },
      {
        "authorId": "2212131028",
        "name": "Vinija Jain"
      },
      {
        "authorId": "2275226689",
        "name": "Aman Chadha"
      },
      {
        "authorId": "2798221",
        "name": "Kokil Jaidka"
      },
      {
        "authorId": "2290046479",
        "name": "Yang Liu"
      },
      {
        "authorId": "2282936905",
        "name": "See-Kiong Ng"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "2282ee43aa151b9496ec114a3e6bfbea852e3b2a",
    "url": "https://www.semanticscholar.org/paper/2282ee43aa151b9496ec114a3e6bfbea852e3b2a",
    "title": "A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning",
    "abstract": "Chain-of-Thought (CoT) holds a significant place in augmenting the reasoning performance for large language models (LLMs). While some studies focus on improving CoT accuracy through methods like retrieval enhancement, yet a rigorous explanation for why CoT achieves such success remains unclear. In this paper, we analyze CoT methods under two different settings by asking the following questions: (1) For zero-shot CoT, why does prompting the model with\"let's think step by step\"significantly impact its outputs? (2) For few-shot CoT, why does providing examples before questioning the model could substantially improve its reasoning ability? To answer these questions, we conduct a top-down explainable analysis from the Hopfieldian view and propose a Read-and-Control approach for controlling the accuracy of CoT. Through extensive experiments on seven datasets for three different tasks, we demonstrate that our framework can decipher the inner workings of CoT, provide reasoning error localization, and control to come up with the correct reasoning path.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-18",
    "authors": [
      {
        "authorId": "2299752648",
        "name": "Lijie Hu"
      },
      {
        "authorId": "2307209552",
        "name": "Liang Liu"
      },
      {
        "authorId": "2284694841",
        "name": "Shu Yang"
      },
      {
        "authorId": "2307213570",
        "name": "Xin Chen"
      },
      {
        "authorId": "2299824027",
        "name": "Hongru Xiao"
      },
      {
        "authorId": "2294515958",
        "name": "Mengdi Li"
      },
      {
        "authorId": "2307400302",
        "name": "Pan Zhou"
      },
      {
        "authorId": "2260606436",
        "name": "Muhammad Asif Ali"
      },
      {
        "authorId": "2260640875",
        "name": "Di Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "1a7eb221f88c2956cb5dbe77c474bad55a019738",
    "url": "https://www.semanticscholar.org/paper/1a7eb221f88c2956cb5dbe77c474bad55a019738",
    "title": "QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction",
    "abstract": "Employing Large Language Models (LLMs) for semantic parsing has achieved remarkable success. However, we find existing methods fall short in terms of reliability and efficiency when hallucinations are encountered. In this paper, we address these challenges with a framework called QueryAgent, which solves a question step-by-step and performs step-wise self-correction. We introduce an environmental feedback-based self-correction method called ERASER. Unlike traditional approaches, ERASER leverages rich environmental feedback in the intermediate steps to perform selective and differentiated self-correction only when necessary. Experimental results demonstrate that QueryAgent notably outperforms all previous few-shot methods using only one example on GrailQA and GraphQ by 7.0 and 15.0 F1. Moreover, our approach exhibits superiority in terms of efficiency, including runtime, query overhead, and API invocation costs. By leveraging ERASER, we further improve another baseline (i.e., AgentBench) by approximately 10 points, revealing the strong transferability of our approach.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-18",
    "authors": [
      {
        "authorId": "2291210811",
        "name": "Xiang Huang"
      },
      {
        "authorId": "2220695955",
        "name": "Sitao Cheng"
      },
      {
        "authorId": "2261679793",
        "name": "Shanshan Huang"
      },
      {
        "authorId": "2292215997",
        "name": "Jiayu Shen"
      },
      {
        "authorId": "2262533579",
        "name": "Yong Xu"
      },
      {
        "authorId": "2256775302",
        "name": "Chaoyun Zhang"
      },
      {
        "authorId": "2261411599",
        "name": "Yuzhong Qu"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "6da2b478ea9b6a4c193c2830cf9e99beb5d826a5",
    "url": "https://www.semanticscholar.org/paper/6da2b478ea9b6a4c193c2830cf9e99beb5d826a5",
    "title": "Vision-Language Model-based Physical Reasoning for Robot Liquid Perception",
    "abstract": "There is a growing interest in applying large language models (LLMs) in robotic tasks, due to their remarkable reasoning ability and extensive knowledge learned from vast training corpora. Grounding LLMs in the physical world remains an open challenge as they can only process textual input. Recent advancements in large vision-language models (LVLMs) have enabled a more comprehensive understanding of the physical world by incorporating visual input, which provides richer contextual information than language alone. In this work, we proposed a novel paradigm that leveraged GPT-4V(ision), the state-of-the-art LVLM by OpenAI, to enable embodied agents to perceive liquid objects via image-based environmental feedback. Specifically, we exploited the physical understanding of GPT-4V to interpret the visual representation (e.g., time-series plot) of non-visual feedback (e.g., F/T sensor data), indirectly enabling multimodal perception beyond vision and language using images as proxies. We evaluated our method using 10 common household liquids with containers of various geometry and material. Without any training or fine-tuning, we demonstrated that our method can enable the robot to indirectly perceive the physical response of liquids and estimate their viscosity. We also showed that by jointly reasoning over the visual and physical attributes learned through interactions, our method could recognize liquid objects in the absence of strong visual cues (e.g., container labels with legible text or symbols), increasing the accuracy from 69.0%—achieved by the best-performing vision-only variant—to 86.0%.",
    "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-10",
    "authors": [
      {
        "authorId": "2295891506",
        "name": "Wenqiang Lai"
      },
      {
        "authorId": "2295900199",
        "name": "Yuan Gao"
      },
      {
        "authorId": "2296540279",
        "name": "T. Lam"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.79441541679836
  },
  {
    "paperId": "5dbf93a68b7fda600521f046dea35ea8ba9e884f",
    "url": "https://www.semanticscholar.org/paper/5dbf93a68b7fda600521f046dea35ea8ba9e884f",
    "title": "AgentBench: Evaluating LLMs as Agents",
    "abstract": "Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 27 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and OSS competitors. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Training on code and high quality multi-turn alignment data could improve agent performance. Datasets, environments, and an integrated evaluation package for AgentBench are released at \\url{https://github.com/THUDM/AgentBench}.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 195,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.03688",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-08-07",
    "authors": [
      {
        "authorId": "2111312892",
        "name": "Xiao Liu"
      },
      {
        "authorId": "2110750673",
        "name": "Hao Yu"
      },
      {
        "authorId": "2228752234",
        "name": "Hanchen Zhang"
      },
      {
        "authorId": "2138973537",
        "name": "Yifan Xu"
      },
      {
        "authorId": "2181283109",
        "name": "Xuanyu Lei"
      },
      {
        "authorId": "2051311700",
        "name": "Hanyu Lai"
      },
      {
        "authorId": "2022231256",
        "name": "Yu Gu"
      },
      {
        "authorId": "2116405624",
        "name": "Yuxian Gu"
      },
      {
        "authorId": "2228633105",
        "name": "Hangliang Ding"
      },
      {
        "authorId": "2066478850",
        "name": "Kai Men"
      },
      {
        "authorId": "2218347104",
        "name": "Kejuan Yang"
      },
      {
        "authorId": "2107900499",
        "name": "Shudan Zhang"
      },
      {
        "authorId": "145924070",
        "name": "Xiang Deng"
      },
      {
        "authorId": "2051712753",
        "name": "Aohan Zeng"
      },
      {
        "authorId": "66395694",
        "name": "Zhengxiao Du"
      },
      {
        "authorId": "2146063748",
        "name": "Chenhui Zhang"
      },
      {
        "authorId": "2152107339",
        "name": "Shengqi Shen"
      },
      {
        "authorId": "1993655237",
        "name": "Tianjun Zhang"
      },
      {
        "authorId": "2191455",
        "name": "Sheng Shen"
      },
      {
        "authorId": "1758652",
        "name": "Yu Su"
      },
      {
        "authorId": "1515546612",
        "name": "Huan Sun"
      },
      {
        "authorId": "1730108",
        "name": "Minlie Huang"
      },
      {
        "authorId": "2047998",
        "name": "Yuxiao Dong"
      },
      {
        "authorId": "2148911956",
        "name": "Jie Tang"
      }
    ],
    "source": "semantic_scholar",
    "score": 149.17171988845774
  },
  {
    "paperId": "a35f1315e91513ff0bec0c488fe175214fd9636c",
    "url": "https://www.semanticscholar.org/paper/a35f1315e91513ff0bec0c488fe175214fd9636c",
    "title": "Recommender Systems in the Era of Large Language Models (LLMs)",
    "abstract": "With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an indispensable and important component, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have achieved significant advancements in enhancing recommender systems, these DNN-based methods still exhibit some limitations, such as inferior capabilities to effectively capture textual side information about users and items, difficulties in generalization to various recommendation scenarios, and reasoning on their predictions, etc. Meanwhile, the development of Large Language Models (LLMs), such as ChatGPT and GPT-4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization capabilities and reasoning skills. As a result, recent studies have actively attempted to harness the power of LLMs to enhance recommender systems. Given the rapid evolution of this research direction in recommender systems, there is a pressing need for a systematic overview that summarizes existing LLM-empowered recommender systems. Therefore, in this survey, we comprehensively review LLM-empowered recommender systems from various perspectives including pre-training, fine-tuning, and prompting paradigms. More specifically, we first introduce the representative methods to learn user and item representations, leveraging LLMs as feature encoders. Then, we systematically review the emerging advanced techniques of LLMs for enhancing recommender systems from three paradigms, namely pre-training, fine-tuning, and prompting. Finally, we comprehensively discuss the promising future directions in this emerging field.",
    "venue": "IEEE Transactions on Knowledge and Data Engineering",
    "year": 2023,
    "citationCount": 211,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.02046",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-07-05",
    "authors": [
      {
        "authorId": "41031455",
        "name": "Wenqi Fan"
      },
      {
        "authorId": "2186864321",
        "name": "Zihuai Zhao"
      },
      {
        "authorId": "2109018826",
        "name": "Jiatong Li"
      },
      {
        "authorId": "2208630682",
        "name": "Yunqing Liu"
      },
      {
        "authorId": "2221127240",
        "name": "Xiaowei Mei"
      },
      {
        "authorId": "2108941389",
        "name": "Yiqi Wang"
      },
      {
        "authorId": "1736632",
        "name": "Jiliang Tang"
      },
      {
        "authorId": "2117897052",
        "name": "Qing Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 156.34879412008019
  },
  {
    "paperId": "95ca67ba607d7859ee8eec457f4b59b115d69bf5",
    "url": "https://www.semanticscholar.org/paper/95ca67ba607d7859ee8eec457f4b59b115d69bf5",
    "title": "ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models",
    "abstract": "Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning. To improve the reasoning abilities, we propose \\textbf{ChatCoT}, a tool-augmented chain-of-thought reasoning framework for chat-based LLMs. In ChatCoT, we model the chain-of-thought~(CoT) reasoning as multi-turn conversations, to utilize tools in a more natural way through chatting. At each turn, LLMs can either interact with tools or perform the reasoning. Our approach can effectively leverage the multi-turn conversation ability of chat-based LLMs, and integrate the thought chain following and tools manipulation in a unified way. Specially, we initialize the early turns of the conversation by the tools, tasks and reasoning format, and propose an iterative \\emph{tool-augmented reasoning} step to perform step-by-step tool-augmented reasoning. The experiment results on two complex reasoning datasets (MATH and HotpotQA) have shown the effectiveness of ChatCoT on complex reasoning tasks, achieving a 6.8\\% relative improvement over the state-of-the-art baseline. Our code and data are available at: \\url{https://github.com/RUCAIBOX/ChatCoT}.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 20,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.14323",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-23",
    "authors": [
      {
        "authorId": "46842323",
        "name": "Z. Chen"
      },
      {
        "authorId": "1423651904",
        "name": "Kun Zhou"
      },
      {
        "authorId": "2107926615",
        "name": "Beichen Zhang"
      },
      {
        "authorId": "2164092564",
        "name": "Zheng Gong"
      },
      {
        "authorId": "2542603",
        "name": "Wayne Xin Zhao"
      },
      {
        "authorId": "153693432",
        "name": "Ji-rong Wen"
      }
    ],
    "source": "semantic_scholar",
    "score": 115.66783656585135
  },
  {
    "paperId": "dadcbffa71cd7c0a6930883f515dd99710fdc013",
    "url": "https://www.semanticscholar.org/paper/dadcbffa71cd7c0a6930883f515dd99710fdc013",
    "title": "QuBE: Question-based Belief Enhancement for Agentic LLM Reasoning",
    "abstract": "Despite advancements in Large Language Models (LLMs), many complex tasks are not easily solved in a single inference step, requiring the use of agentic LLMs in interactive environments. However, agentic LLMs suffer from a phenomenon known as reasoning derailment, due to the indiscriminate incorporation of observations from partially observable environments. We introduce QuBE, a method that enhances agents’ focus on task-relevant contexts, by constructing a belief state via question answering. We validate QuBE through experiments in two agentic LLM scenarios with partial observability: 1) a canonical interactive decision-making scenario using text-based game engines, and 2) an interactive retrieval-augmented generation (RAG) scenario using search engines. In the AlfWorld text-based game, QuBE outperforms established baselines by substantial margins, and in the search engine scenario, it achieves marked improvements on the BeIR zero-shot retrieval benchmark. The results demonstrate that QuBE significantly mitigates reasoning derailment, refining the decision-making process of LLM agents in partially observed environments.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2164352899",
        "name": "Minsoo Kim"
      },
      {
        "authorId": "2310518696",
        "name": "Jongyoon Kim"
      },
      {
        "authorId": "2119250848",
        "name": "Jihyuk Kim"
      },
      {
        "authorId": "2274001980",
        "name": "Seung-won Hwang"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "a814b096f45746626cf53ebf146d232e24924edc",
    "url": "https://www.semanticscholar.org/paper/a814b096f45746626cf53ebf146d232e24924edc",
    "title": "Hierarchical Deconstruction of LLM Reasoning: A Graph-Based Framework for Analyzing Knowledge Utilization",
    "abstract": "Despite the advances in large language models (LLMs), how they use their knowledge for reasoning is not yet well understood.In this study, we propose a method that deconstructs complex real-world questions into a graph, representing each question as a node with predecessors of background knowledge needed to solve the question. We develop the DepthQA dataset, deconstructing questions into three depths: (i) recalling conceptual knowledge, (ii) applying procedural knowledge, and (iii) analyzing strategic knowledge. Based on a hierarchical graph, we quantify forward discrepancy, a discrepancy in LLM performance on simpler sub-problems versus complex questions. We also measure backward discrepancy where LLMs answer complex questions but struggle with simpler ones. Our analysis shows that smaller models exhibit more discrepancies than larger models. Distinct patterns of discrepancies are observed across model capacity and possibility of training data memorization. Additionally, guiding models from simpler to complex questions through multi-turn interactions improves performance across model sizes, highlighting the importance of structured intermediate steps in knowledge reasoning. This work enhances our understanding of LLM reasoning and suggests ways to improve their problem-solving abilities.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2406.19502",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-27",
    "authors": [
      {
        "authorId": "2266468965",
        "name": "Miyoung Ko"
      },
      {
        "authorId": "2266436197",
        "name": "Sue Hyun Park"
      },
      {
        "authorId": "2309075956",
        "name": "Joonsuk Park"
      },
      {
        "authorId": "2266393906",
        "name": "Minjoon Seo"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "b994897ac3c70bc680ad1eca35efef5ef64f25dc",
    "url": "https://www.semanticscholar.org/paper/b994897ac3c70bc680ad1eca35efef5ef64f25dc",
    "title": "Thought-Like-Pro: Enhancing Reasoning of Large Language Models through Self-Driven Prolog-based Chain-of-Though",
    "abstract": "Large language models (LLMs) have shown exceptional performance as general-purpose assistants, excelling across a variety of reasoning tasks. This achievement represents a significant step toward achieving artificial general intelligence (AGI). Despite these advancements, the effectiveness of LLMs often hinges on the specific prompting strategies employed, and there remains a lack of a robust framework to facilitate learning and generalization across diverse reasoning tasks. To address these challenges, we introduce a novel learning framework, THOUGHT-LIKE-PRO In this framework, we utilize imitation learning to imitate the Chain-of-Thought (CoT) process which is verified and translated from reasoning trajectories generated by a symbolic Prolog logic engine. This framework proceeds in a self-driven manner, that enables LLMs to formulate rules and statements from given instructions and leverage the symbolic Prolog engine to derive results. Subsequently, LLMs convert Prolog-derived successive reasoning trajectories into natural language CoT for imitation learning. Our empirical findings indicate that our proposed approach substantially enhances the reasoning abilities of LLMs and demonstrates robust generalization across out-of-distribution reasoning tasks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-18",
    "authors": [
      {
        "authorId": "2310866202",
        "name": "Xiaoyu Tan"
      },
      {
        "authorId": "2268085905",
        "name": "Yongxin Deng"
      },
      {
        "authorId": "1500386397",
        "name": "Xihe Qiu"
      },
      {
        "authorId": "2248606044",
        "name": "Weidi Xu"
      },
      {
        "authorId": "2268107128",
        "name": "Chao Qu"
      },
      {
        "authorId": "2266389707",
        "name": "Wei Chu"
      },
      {
        "authorId": "2266466742",
        "name": "Yinghui Xu"
      },
      {
        "authorId": "2192603365",
        "name": "Yuan Qi"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "39e0bf77300bb6df8716ce83eb8a3f6a5e3d6b20",
    "url": "https://www.semanticscholar.org/paper/39e0bf77300bb6df8716ce83eb8a3f6a5e3d6b20",
    "title": "Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding",
    "abstract": "Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the Chain-of-Table framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, showing the reasoning process for a given tabular problem. The chain carries structured information of the intermediate results, enabling more accurate and reliable predictions. Chain-of-Table achieves new state-of-the-art performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM choices.",
    "venue": "International Conference on Learning Representations",
    "year": 2024,
    "citationCount": 58,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-09",
    "authors": [
      {
        "authorId": "1762478",
        "name": "Zilong Wang"
      },
      {
        "authorId": "2278786339",
        "name": "Hao Zhang"
      },
      {
        "authorId": "2136342754",
        "name": "Chun-Liang Li"
      },
      {
        "authorId": "117595858",
        "name": "Julian Martin Eisenschlos"
      },
      {
        "authorId": "2066252171",
        "name": "Vincent Perot"
      },
      {
        "authorId": "2278799290",
        "name": "Zifeng Wang"
      },
      {
        "authorId": "71436125",
        "name": "Lesly Miculicich"
      },
      {
        "authorId": "2114175058",
        "name": "Yasuhisa Fujii"
      },
      {
        "authorId": "2254284383",
        "name": "Jingbo Shang"
      },
      {
        "authorId": "2278969944",
        "name": "Chen-Yu Lee"
      },
      {
        "authorId": "2264567300",
        "name": "Tomas Pfister"
      }
    ],
    "source": "semantic_scholar",
    "score": 131.1630616585858
  },
  {
    "paperId": "6d4bacb69923e1e94fb4de468b939ce6db32fb51",
    "url": "https://www.semanticscholar.org/paper/6d4bacb69923e1e94fb4de468b939ce6db32fb51",
    "title": "Large Language Models Cannot Self-Correct Reasoning Yet",
    "abstract": "Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 287,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.01798",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-03",
    "authors": [
      {
        "authorId": "1490651934",
        "name": "Jie Huang"
      },
      {
        "authorId": "2238263119",
        "name": "Xinyun Chen"
      },
      {
        "authorId": "1817207",
        "name": "Swaroop Mishra"
      },
      {
        "authorId": "2253804927",
        "name": "Huaixiu Steven Zheng"
      },
      {
        "authorId": "40625240",
        "name": "Adams Wei Yu"
      },
      {
        "authorId": "2293658042",
        "name": "Xinying Song"
      },
      {
        "authorId": "2256313467",
        "name": "Denny Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 154.9444072020392
  },
  {
    "paperId": "170c97c7215f42edfb20c2248f954879e91ef86e",
    "url": "https://www.semanticscholar.org/paper/170c97c7215f42edfb20c2248f954879e91ef86e",
    "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models",
    "abstract": "Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks. At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response. We showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54% overall accuracy on ScienceQA, improving the best published few-shot result by 11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%, lifting the state of the art to 98.78%. Our analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner. The project is available at https://chameleon-llm.github.io.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 245,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2304.09842",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-04-19",
    "authors": [
      {
        "authorId": "2887562",
        "name": "Pan Lu"
      },
      {
        "authorId": "1780690",
        "name": "Baolin Peng"
      },
      {
        "authorId": "47413820",
        "name": "Hao Cheng"
      },
      {
        "authorId": "1947267",
        "name": "Michel Galley"
      },
      {
        "authorId": "2782886",
        "name": "Kai-Wei Chang"
      },
      {
        "authorId": "39092098",
        "name": "Y. Wu"
      },
      {
        "authorId": "145380991",
        "name": "Song-Chun Zhu"
      },
      {
        "authorId": "48441311",
        "name": "Jianfeng Gao"
      }
    ],
    "source": "semantic_scholar",
    "score": 159.57997303898543
  },
  {
    "paperId": "31b2918acc77682b5a499da1aa111f34e76d9603",
    "url": "https://www.semanticscholar.org/paper/31b2918acc77682b5a499da1aa111f34e76d9603",
    "title": "How to Train Data-Efficient LLMs",
    "abstract": "The training of large language models (LLMs) is expensive. In this paper, we study data-efficient approaches for pre-training LLMs, i.e., techniques that aim to optimize the Pareto frontier of model quality and training resource/data consumption. We seek to understand the tradeoffs associated with data selection routines based on (i) expensive-to-compute data-quality estimates, and (ii) maximization of coverage and diversity-based measures in the feature space. Our first technique, Ask-LLM, leverages the zero-shot reasoning capabilities of instruction-tuned LLMs to directly assess the quality of a training example. To target coverage, we propose Density sampling, which models the data distribution to select a diverse sample. In our comparison of 19 samplers, involving hundreds of evaluation tasks and pre-training runs, we find that Ask-LLM and Density are the best methods in their respective categories. Coverage sampling can recover the performance of the full data, while models trained on Ask-LLM data consistently outperform full-data training -- even when we reject 90% of the original dataset, while converging up to 70% faster.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 30,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-15",
    "authors": [
      {
        "authorId": "40705044",
        "name": "Noveen Sachdeva"
      },
      {
        "authorId": "2240537645",
        "name": "Benjamin Coleman"
      },
      {
        "authorId": "2741053",
        "name": "Wang-Cheng Kang"
      },
      {
        "authorId": "2148023",
        "name": "Jianmo Ni"
      },
      {
        "authorId": "2240649593",
        "name": "Lichan Hong"
      },
      {
        "authorId": "2254270179",
        "name": "E. Chi"
      },
      {
        "authorId": "1697232",
        "name": "James Caverlee"
      },
      {
        "authorId": "2254271546",
        "name": "Julian J. McAuley"
      },
      {
        "authorId": "48573272",
        "name": "D. Cheng"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.50980806727719
  },
  {
    "paperId": "306a312c0bae22f30a406187ab18c5724cefb661",
    "url": "https://www.semanticscholar.org/paper/306a312c0bae22f30a406187ab18c5724cefb661",
    "title": "Memory Injections: Correcting Multi-Hop Reasoning Failures During Inference in Transformer-Based Language Models",
    "abstract": "Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as “memories,” at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.",
    "venue": "BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    "year": 2023,
    "citationCount": 12,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.05605",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-11",
    "authors": [
      {
        "authorId": "2239104574",
        "name": "Mansi Sakarvadia"
      },
      {
        "authorId": "144373088",
        "name": "Aswathy Ajith"
      },
      {
        "authorId": "2239168709",
        "name": "Arham Khan"
      },
      {
        "authorId": "2239102028",
        "name": "Daniel Grzenda"
      },
      {
        "authorId": "48808283",
        "name": "Nathaniel Hudson"
      },
      {
        "authorId": "2239105163",
        "name": "André Bauer"
      },
      {
        "authorId": "3091414",
        "name": "K. Chard"
      },
      {
        "authorId": "1698701",
        "name": "Ian T Foster"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "170c5b7e311a5004ed5db5d9eee1b736669273fb",
    "url": "https://www.semanticscholar.org/paper/170c5b7e311a5004ed5db5d9eee1b736669273fb",
    "title": "DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy",
    "abstract": "Recent advances in large language models (LLMs) have revolutionized the landscape of reasoning tasks. To enhance the capabilities of LLMs to emulate human reasoning, prior studies have focused on modeling reasoning steps using various thought structures like chains, trees, or graphs. However, LLM-based reasoning still encounters the following challenges: (1) Limited adaptability of preset structures to diverse tasks; (2) Insufficient precision in exploiting known conditions to derive new ones; and (3) Inadequate consideration of historical reasoning experiences for subsequent reasoning steps. To this end, we propose DetermLR, a novel perspective that rethinks the reasoning process as an evolution from indeterminacy to determinacy. First, we categorize known conditions into two types: determinate and indeterminate premises This provides an oveall direction for the reasoning process and guides LLMs in converting indeterminate data into progressively determinate insights. Subsequently, we leverage quantitative measurements to prioritize more relevant premises to explore new insights. Furthermore, we automate the storage and extraction of available premises and reasoning paths with reasoning memory, preserving historical reasoning details for subsequent reasoning steps. Comprehensive experimental results demonstrate that DetermLR surpasses all baselines on various logical reasoning benchmarks: LogiQA, ProofWriter, FOLIO, PrOntoQA, and LogicalDeduction. Compared to previous multi-step reasoning methods, DetermLR achieves higher accuracy with fewer reasoning steps, highlighting its superior efficiency and effectiveness in solving logical reasoning tasks.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 9,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.18659",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-28",
    "authors": [
      {
        "authorId": "2257127695",
        "name": "Hongda Sun"
      },
      {
        "authorId": "2262867071",
        "name": "Weikai Xu"
      },
      {
        "authorId": "2257333016",
        "name": "Wei Liu"
      },
      {
        "authorId": "2257013742",
        "name": "Jian Luan"
      },
      {
        "authorId": "2257388949",
        "name": "Bin Wang"
      },
      {
        "authorId": "2232780079",
        "name": "Shuo Shang"
      },
      {
        "authorId": "2263887786",
        "name": "Ji-Rong Wen"
      },
      {
        "authorId": "2257014132",
        "name": "Rui Yan"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "9cbe3061ad19f2a955c239a7ccb74d7b12c5f9b6",
    "url": "https://www.semanticscholar.org/paper/9cbe3061ad19f2a955c239a7ccb74d7b12c5f9b6",
    "title": "NVLM: Open Frontier-Class Multimodal LLMs",
    "abstract": "We introduce NVLM 1.0, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training. In terms of model design, we perform a comprehensive comparison between decoder-only multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g., Flamingo). Based on the strengths and weaknesses of both approaches, we propose a novel architecture that enhances both training efficiency and multimodal reasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for tile-based dynamic high-resolution images, which significantly boosts performance on multimodal reasoning and OCR-related tasks. Regarding training data, we meticulously curate and provide detailed information on our multimodal pretraining and supervised fine-tuning datasets. Our findings indicate that dataset quality and task diversity are more important than scale, even during the pretraining phase, across all architectures. Notably, we develop production-grade multimodality for the NVLM-1.0 models, enabling them to excel in vision-language tasks while maintaining and even improving text-only performance compared to their LLM backbones. To achieve this, we craft and integrate a high-quality text-only dataset into multimodal training, alongside a substantial amount of multimodal math and reasoning data, leading to enhanced math and coding capabilities across modalities. To advance research in the field, we release the model weights at https://huggingface.co/nvidia/NVLM-D-72B and will open-source the training code for the community soon.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-17",
    "authors": [
      {
        "authorId": "47653392",
        "name": "Wenliang Dai"
      },
      {
        "authorId": "2323004150",
        "name": "Nayeon Lee"
      },
      {
        "authorId": "2256656241",
        "name": "Boxin Wang"
      },
      {
        "authorId": "2321485724",
        "name": "Zhuoling Yang"
      },
      {
        "authorId": "2256582287",
        "name": "Zihan Liu"
      },
      {
        "authorId": "2055570627",
        "name": "Jon Barker"
      },
      {
        "authorId": "2208049252",
        "name": "Tuomas Rintamaki"
      },
      {
        "authorId": "1911755",
        "name": "Mohammad Shoeybi"
      },
      {
        "authorId": "2264406909",
        "name": "Bryan Catanzaro"
      },
      {
        "authorId": "2253664013",
        "name": "Wei Ping"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.49820016084324
  },
  {
    "paperId": "1a60d3f243c785dc7f4a2b14b8b14b2cbca6a8a6",
    "url": "https://www.semanticscholar.org/paper/1a60d3f243c785dc7f4a2b14b8b14b2cbca6a8a6",
    "title": "GraphReason: Enhancing Reasoning Capabilities of Large Language Models through A Graph-Based Verification Approach",
    "abstract": "Large Language Models (LLMs) have showcased impressive reasoning capabilities, particularly when guided by specifically designed prompts in complex reasoning tasks such as math word problems. These models typically solve tasks using a chain-of-thought approach, which not only bolsters their reasoning abilities but also provides valuable insights into their problem-solving process. However, there is still significant room for enhancing the reasoning abilities of LLMs. Some studies suggest that the integration of an LLM output verifier can boost reasoning accuracy without necessitating additional model training. In this paper, we follow these studies and introduce a novel graph-based method to further augment the reasoning capabilities of LLMs. We posit that multiple solutions to a reasoning task, generated by an LLM, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths. Therefore, we propose the Reasoning Graph Verifier (GraphReason) to analyze and verify the solutions generated by LLMs. By evaluating these graphs, models can yield more accurate and reliable results.Our experimental results show that our graph-based verification method not only significantly enhances the reasoning abilities of LLMs but also outperforms existing verifier methods in terms of improving these models’ reasoning performance.",
    "venue": "NLRSE",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.09267",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-08-18",
    "authors": [
      {
        "authorId": "2158463125",
        "name": "Lang Cao"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "ac9864c44914ed08b9e674178d2814da4a55070d",
    "url": "https://www.semanticscholar.org/paper/ac9864c44914ed08b9e674178d2814da4a55070d",
    "title": "Case Repositories: Towards Case-Based Reasoning for AI Alignment",
    "abstract": "Case studies commonly form the pedagogical backbone in law, ethics, and many other domains that face complex and ambiguous societal questions informed by human values. Similar complexities and ambiguities arise when we consider how AI should be aligned in practice: when faced with vast quantities of diverse (and sometimes conflicting) values from different individuals and communities, with whose values is AI to align, and how should AI do so? We propose a complementary approach to constitutional AI alignment, grounded in ideas from case-based reasoning (CBR), that focuses on the construction of policies through judgments on a set of cases. We present a process to assemble such a case repository by: 1) gathering a set of ``seed'' cases -- questions one may ask an AI system -- in a particular domain, 2) eliciting domain-specific key dimensions for cases through workshops with domain experts, 3) using LLMs to generate variations of cases not seen in the wild, and 4) engaging with the public to judge and improve cases. We then discuss how such a case repository could assist in AI alignment, both through directly acting as precedents to ground acceptable behaviors, and as a medium for individuals and communities to engage in moral reasoning around AI.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-18",
    "authors": [
      {
        "authorId": "2267338491",
        "name": "K. Feng"
      },
      {
        "authorId": "3207763",
        "name": "Quan Ze Chen"
      },
      {
        "authorId": "2267350119",
        "name": "Inyoung Cheong"
      },
      {
        "authorId": "2267351231",
        "name": "King Xia"
      },
      {
        "authorId": "2267362985",
        "name": "Amy Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050",
    "url": "https://www.semanticscholar.org/paper/780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050",
    "title": "Multimodal Chain-of-Thought Reasoning in Language Models",
    "abstract": "Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have primarily focused on the language modality. We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. In this way, answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With Multimodal-CoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark. Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https://github.com/amazon-science/mm-cot.",
    "venue": "Trans. Mach. Learn. Res.",
    "year": 2023,
    "citationCount": 314,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2302.00923",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-02-02",
    "authors": [
      {
        "authorId": "3322871",
        "name": "Zhuosheng Zhang"
      },
      {
        "authorId": "2085709",
        "name": "Aston Zhang"
      },
      {
        "authorId": "1701799",
        "name": "Mu Li"
      },
      {
        "authorId": "2146232510",
        "name": "Hai Zhao"
      },
      {
        "authorId": "50877490",
        "name": "G. Karypis"
      },
      {
        "authorId": "78088877",
        "name": "Alexander J. Smola"
      }
    ],
    "source": "semantic_scholar",
    "score": 156.2885895823845
  },
  {
    "paperId": "10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3",
    "url": "https://www.semanticscholar.org/paper/10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3",
    "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality",
    "abstract": "The causal capabilities of large language models (LLMs) are a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We conduct a\"behavorial\"study of LLMs to benchmark their capability in generating causal arguments. Across a wide range of tasks, we find that LLMs can generate text corresponding to correct causal arguments with high probability, surpassing the best-performing existing methods. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain) and event causality (86% accuracy in determining necessary and sufficient causes in vignettes). We perform robustness checks across tasks and show that the capabilities cannot be explained by dataset memorization alone, especially since LLMs generalize to novel datasets that were created after the training cutoff date. That said, LLMs exhibit unpredictable failure modes, and we discuss the kinds of errors that may be improved and what are the fundamental limits of LLM-based answers. Overall, by operating on the text metadata, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. As a result, LLMs may be used by human domain experts to save effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. Given that LLMs ignore the actual data, our results also point to a fruitful research direction of developing algorithms that combine LLMs with existing causal techniques. Code and datasets are available at https://github.com/py-why/pywhy-llm.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 210,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.00050",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-04-28",
    "authors": [
      {
        "authorId": "3528206",
        "name": "Emre Kıcıman"
      },
      {
        "authorId": "36670968",
        "name": "R. Ness"
      },
      {
        "authorId": "2143678801",
        "name": "Amit Sharma"
      },
      {
        "authorId": "2111727675",
        "name": "Chenhao Tan"
      }
    ],
    "source": "semantic_scholar",
    "score": 150.277872002141
  },
  {
    "paperId": "b214b72f9d41b81baaab9496a3a42d1ab22292a4",
    "url": "https://www.semanticscholar.org/paper/b214b72f9d41b81baaab9496a3a42d1ab22292a4",
    "title": "Natural Language based Context Modeling and Reasoning for Ubiquitous Computing with Large Language Models: A Tutorial",
    "abstract": "Large language models (LLMs) have become phenomenally surging, since 2018--two decades after introducing context-awareness into computing systems. Through taking into account the situations of ubiquitous devices, users and the societies, context-aware computing has enabled a wide spectrum of innovative applications, such as assisted living, location-based social network services and so on. To recognize contexts and make decisions for actions accordingly, various artificial intelligence technologies, such as Ontology and OWL, have been adopted as representations for context modeling and reasoning. Recently, with the rise of LLMs and their improved natural language understanding and reasoning capabilities, it has become feasible to model contexts using natural language and perform context reasoning by interacting with LLMs such as ChatGPT and GPT-4. In this tutorial, we demonstrate the use of texts, prompts, and autonomous agents (AutoAgents) that enable LLMs to perform context modeling and reasoning without requiring fine-tuning of the model. We organize and introduce works in the related field, and name this computing paradigm as the LLM-driven Context-aware Computing (LCaC). In the LCaC paradigm, users' requests, sensors reading data, and the command to actuators are supposed to be represented as texts. Given the text of users' request and sensor data, the AutoAgent models the context by prompting and sends to the LLM for context reasoning. LLM generates a plan of actions and responds to the AutoAgent, which later follows the action plan to foster context-awareness. To prove the concepts, we use two showcases--(1) operating a mobile z-arm in an apartment for assisted living, and (2) planning a trip and scheduling the itinerary in a context-aware and personalized manner.",
    "venue": "",
    "year": 2023,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": "2023-09-24",
    "authors": [
      {
        "authorId": "2260073905",
        "name": "Haoyi Xiong"
      },
      {
        "authorId": "2143957850",
        "name": "Jiang Bian"
      },
      {
        "authorId": "117785201",
        "name": "Sijia Yang"
      },
      {
        "authorId": "2276447125",
        "name": "Xiaofei Zhang"
      },
      {
        "authorId": "2256714312",
        "name": "Linghe Kong"
      },
      {
        "authorId": "2276446113",
        "name": "Daqing Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "5ff337e94bb710bab34c340e06b0618612126961",
    "url": "https://www.semanticscholar.org/paper/5ff337e94bb710bab34c340e06b0618612126961",
    "title": "Large Language Models Can Learn Temporal Reasoning",
    "abstract": "While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they are not without their flaws and inaccuracies. Recent studies have introduced various methods to mitigate these limitations. Temporal reasoning (TR), in particular, presents a significant challenge for LLMs due to its reliance on diverse temporal concepts and intricate temporal logic. In this paper, we propose TG-LLM, a novel framework towards language-based TR. Instead of reasoning over the original context, we adopt a latent representation, temporal graph (TG) that enhances the learning of TR. A synthetic dataset (TGQA), which is fully controllable and requires minimal supervision, is constructed for fine-tuning LLMs on this text-to-TG translation task. We confirmed in experiments that the capability of TG translation learned on our dataset can be transferred to other TR tasks and benchmarks. On top of that, we teach LLM to perform deliberate reasoning over the TGs via Chain-of-Thought (CoT) bootstrapping and graph data augmentation. We observed that those strategies, which maintain a balance between usefulness and diversity, bring more reliable CoTs and final results than the vanilla CoT distillation.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 50,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-01-12",
    "authors": [
      {
        "authorId": "1739023723",
        "name": "Siheng Xiong"
      },
      {
        "authorId": "3443416",
        "name": "Ali Payani"
      },
      {
        "authorId": "2828296",
        "name": "R. Kompella"
      },
      {
        "authorId": "1730720",
        "name": "F. Fekri"
      }
    ],
    "source": "semantic_scholar",
    "score": 128.9773844908649
  },
  {
    "paperId": "b626560f19f815808a289ef5c24a17c57320da70",
    "url": "https://www.semanticscholar.org/paper/b626560f19f815808a289ef5c24a17c57320da70",
    "title": "MathPrompter: Mathematical Reasoning using Large Language Models",
    "abstract": "Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose ‘MathPrompter’, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the ‘MultiArith’ dataset (78.7% - 92.5%) evaluated using 175B parameter GPT-based LLM.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 161,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2303.05398",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-03-04",
    "authors": [
      {
        "authorId": "40203626",
        "name": "Shima Imani"
      },
      {
        "authorId": "1669396676",
        "name": "Liang Du"
      },
      {
        "authorId": "1442157164",
        "name": "H. Shrivastava"
      }
    ],
    "source": "semantic_scholar",
    "score": 146.31394502848576
  },
  {
    "paperId": "60f8a7ac53585aa2c173219e97507d6d963864e7",
    "url": "https://www.semanticscholar.org/paper/60f8a7ac53585aa2c173219e97507d6d963864e7",
    "title": "PALR: Personalization Aware LLMs for Recommendation",
    "abstract": "Large language models (LLMs) have recently received significant attention for their exceptional capabilities. Despite extensive efforts in developing general-purpose LLMs that can be utilized in various natural language processing (NLP) tasks, there has been less research exploring their potential in recommender systems. In this paper, we propose a novel framework, named PALR, which aiming to combine user history behaviors (such as clicks, purchases, ratings, etc.) with LLMs to generate user preferred items. Specifically, we first use user/item interactions as guidance for candidate retrieval. Then we adopt a LLM-based ranking model to generate recommended items. Unlike existing approaches that typically adopt general-purpose LLMs for zero/few-shot recommendation testing or training on small-sized language models (with less than 1 billion parameters), which cannot fully elicit LLMs' reasoning abilities and leverage rich item side parametric knowledge, we fine-tune a 7 billion parameters LLM for the ranking purpose. This model takes retrieval candidates in natural language format as input, with instruction which explicitly asking to select results from input candidates during inference. Our experimental results demonstrate that our solution outperforms state-of-the-art models on various sequential recommendation tasks.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 83,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.07622",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-12",
    "authors": [
      {
        "authorId": "2141144864",
        "name": "Zheng Chen"
      },
      {
        "authorId": "2112347577",
        "name": "Ziyan Jiang"
      }
    ],
    "source": "semantic_scholar",
    "score": 136.46225198264972
  },
  {
    "paperId": "237bfa636f1a575f4784d2ae81a47ac29fa38522",
    "url": "https://www.semanticscholar.org/paper/237bfa636f1a575f4784d2ae81a47ac29fa38522",
    "title": "Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning",
    "abstract": "Large Language Models (LLMs) demonstrate remarkable proficiency in comprehending and handling text-based tasks. Many efforts are being made to transfer these attributes to video modality, which are termed Video-LLMs. However, existing Video-LLMs can only capture the coarse-grained semantics and are unable to effectively handle tasks related to comprehension or localization of specific video segments. In light of these challenges, we propose Momentor, a Video-LLM capable of accomplishing fine-grained temporal understanding tasks. To support the training of Momentor, we design an automatic data generation engine to construct Moment-10M, a large-scale video instruction dataset with segment-level instruction data. We train Momentor on Moment-10M, enabling it to perform segment-level reasoning and localization. Zero-shot evaluations on several tasks demonstrate that Momentor excels in fine-grained temporally grounded comprehension and localization.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 24,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-18",
    "authors": [
      {
        "authorId": "2159713431",
        "name": "Long Qian"
      },
      {
        "authorId": "2249538542",
        "name": "Juncheng Li"
      },
      {
        "authorId": "2279975706",
        "name": "Yu Wu"
      },
      {
        "authorId": "2284935143",
        "name": "Yaobo Ye"
      },
      {
        "authorId": "2261750082",
        "name": "Hao Fei"
      },
      {
        "authorId": "2281747744",
        "name": "Tat-Seng Chua"
      },
      {
        "authorId": "2253660817",
        "name": "Yueting Zhuang"
      },
      {
        "authorId": "2118071462",
        "name": "Siliang Tang"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.28313737302301
  },
  {
    "paperId": "fc3c717987218662f49243e2be6bacc093dd47d8",
    "url": "https://www.semanticscholar.org/paper/fc3c717987218662f49243e2be6bacc093dd47d8",
    "title": "KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph",
    "abstract": "In this paper, we aim to improve the reasoning ability of large language models (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired by existing methods that design the interaction strategy between LLMs and KG, we propose an autonomous LLM-based agent framework, called KG-Agent, which enables a small LLM to actively make decisions until finishing the reasoning process over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox, KG-based executor, and knowledge memory, and develop an iteration mechanism that autonomously selects the tool then updates the memory for reasoning over KG. To guarantee the effectiveness, we leverage program language to formulate the multi-hop reasoning process over the KG, and synthesize a code-based instruction dataset to fine-tune the base LLM. Extensive experiments demonstrate that only using 10K samples for tuning LLaMA-7B can outperform state-of-the-art methods using larger LLMs or more data, on both in-domain and out-domain datasets. Our code and data will be publicly released.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 24,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-17",
    "authors": [
      {
        "authorId": "2118240359",
        "name": "Jinhao Jiang"
      },
      {
        "authorId": "2265383494",
        "name": "Kun Zhou"
      },
      {
        "authorId": "2257376413",
        "name": "Wayne Xin Zhao"
      },
      {
        "authorId": "2290226180",
        "name": "Yang Song"
      },
      {
        "authorId": "2303683715",
        "name": "Chen Zhu"
      },
      {
        "authorId": "2283086825",
        "name": "Hengshu Zhu"
      },
      {
        "authorId": "2274218622",
        "name": "Ji-Rong Wen"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.28313737302301
  },
  {
    "paperId": "bd3b3e8197d56aa8c20d18e61fae48753d618c5e",
    "url": "https://www.semanticscholar.org/paper/bd3b3e8197d56aa8c20d18e61fae48753d618c5e",
    "title": "CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs",
    "abstract": "Chart understanding plays a pivotal role when applying Multimodal Large Language Models (MLLMs) to real-world tasks such as analyzing scientific papers or financial reports. However, existing datasets often focus on oversimplified and homogeneous charts with template-based questions, leading to an over-optimistic measure of progress. We demonstrate that although open-source models can appear to outperform strong proprietary models on these benchmarks, a simple stress test with slightly different charts or questions can deteriorate performance by up to 34.5%. In this work, we propose CharXiv, a comprehensive evaluation suite involving 2,323 natural, challenging, and diverse charts from arXiv papers. CharXiv includes two types of questions: 1) descriptive questions about examining basic chart elements and 2) reasoning questions that require synthesizing information across complex visual elements in the chart. To ensure quality, all charts and questions are handpicked, curated, and verified by human experts. Our results reveal a substantial, previously underestimated gap between the reasoning skills of the strongest proprietary model (i.e., GPT-4o), which achieves 47.1% accuracy, and the strongest open-source model (i.e., InternVL Chat V1.5), which achieves 29.2%. All models lag far behind human performance of 80.5%, underscoring weaknesses in the chart understanding capabilities of existing MLLMs. We hope CharXiv facilitates future research on MLLM chart understanding by providing a more realistic and faithful measure of progress. Project page and leaderboard: https://charxiv.github.io/",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 19,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-26",
    "authors": [
      {
        "authorId": "2260308709",
        "name": "Zirui Wang"
      },
      {
        "authorId": "67284811",
        "name": "Mengzhou Xia"
      },
      {
        "authorId": "2294507804",
        "name": "Luxi He"
      },
      {
        "authorId": "2284724648",
        "name": "Howard Chen"
      },
      {
        "authorId": "2243302973",
        "name": "Yitao Liu"
      },
      {
        "authorId": "2288884828",
        "name": "Richard Zhu"
      },
      {
        "authorId": "2087743517",
        "name": "Kaiqu Liang"
      },
      {
        "authorId": "2284683822",
        "name": "Xindi Wu"
      },
      {
        "authorId": "2308072184",
        "name": "Haotian Liu"
      },
      {
        "authorId": "49288855",
        "name": "Sadhika Malladi"
      },
      {
        "authorId": "2284703052",
        "name": "Alexis Chevalier"
      },
      {
        "authorId": "2283134097",
        "name": "Sanjeev Arora"
      },
      {
        "authorId": "50536468",
        "name": "Danqi Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.93598410330986
  },
  {
    "paperId": "6a7e26c7007b8024dcc4d050649bfd7af9501453",
    "url": "https://www.semanticscholar.org/paper/6a7e26c7007b8024dcc4d050649bfd7af9501453",
    "title": "GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability",
    "abstract": "Evaluating and enhancing the general capabilities of large language models (LLMs) has been an important research topic. Graph is a common data structure in the real world, and understanding graph data is a crucial part for advancing general intelligence. To evaluate and enhance the graph understanding abilities of LLMs, in this paper, we propose a benchmark named GraphInstruct, which comprehensively includes 21 classical graph reasoning tasks, providing diverse graph generation pipelines and detailed reasoning steps. Based on GraphInstruct, we further construct GraphLM through efficient instruction-tuning, which shows prominent graph understanding capability. In order to enhance the LLM with graph reasoning capability as well, we propose a step mask training strategy, and construct a model named GraphLM+. As one of the pioneering efforts to enhance the graph understanding and reasoning abilities of LLMs, extensive experiments have demonstrated the superiority of GraphLM and GraphLM+ over other LLMs. We look forward to more researchers exploring the potential of LLMs in the graph data mining domain through GraphInstruct. Our code for generating GraphInstruct is released publicly at: https://github.com/CGCL-codes/GraphInstruct.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 21,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-07",
    "authors": [
      {
        "authorId": "2289465759",
        "name": "Zihan Luo"
      },
      {
        "authorId": "2182111071",
        "name": "Xiran Song"
      },
      {
        "authorId": "2288754117",
        "name": "Hong Huang"
      },
      {
        "authorId": "2813328",
        "name": "Jianxun Lian"
      },
      {
        "authorId": "2290238664",
        "name": "Chenhao Zhang"
      },
      {
        "authorId": "2290826449",
        "name": "Jinqi Jiang"
      },
      {
        "authorId": "2288741856",
        "name": "Xing Xie"
      },
      {
        "authorId": "2289602089",
        "name": "Hai Jin"
      }
    ],
    "source": "semantic_scholar",
    "score": 116.36563680037474
  },
  {
    "paperId": "2ddef4301dc9f9ef0f36e111e83cf8428716c562",
    "url": "https://www.semanticscholar.org/paper/2ddef4301dc9f9ef0f36e111e83cf8428716c562",
    "title": "Med42 - Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter vs. Parameter-Efficient Approaches",
    "abstract": "This study presents a comprehensive analysis and comparison of two predominant fine-tuning methodologies - full-parameter fine-tuning and parameter-efficient tuning - within the context of medical Large Language Models (LLMs). We developed and refined a series of LLMs, based on the Llama-2 architecture, specifically designed to enhance medical knowledge retrieval, reasoning, and question-answering capabilities. Our experiments systematically evaluate the effectiveness of these tuning strategies across various well-known medical benchmarks. Notably, our medical LLM Med42 showed an accuracy level of 72% on the US Medical Licensing Examination (USMLE) datasets, setting a new standard in performance for openly available medical LLMs. Through this comparative analysis, we aim to identify the most effective and efficient method for fine-tuning LLMs in the medical domain, thereby contributing significantly to the advancement of AI-driven healthcare applications.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 14,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-23",
    "authors": [
      {
        "authorId": "2297849088",
        "name": "Cl'ement Christophe"
      },
      {
        "authorId": "2297848890",
        "name": "P. K. Kanithi"
      },
      {
        "authorId": "102615578",
        "name": "Prateek Munjal"
      },
      {
        "authorId": "1840492875",
        "name": "Tathagata Raha"
      },
      {
        "authorId": "2297848425",
        "name": "Nasir Hayat"
      },
      {
        "authorId": "2297849270",
        "name": "Ronnie Rajan"
      },
      {
        "authorId": "2297848914",
        "name": "Ahmed Al-Mahrooqi"
      },
      {
        "authorId": "2298005616",
        "name": "Avani Gupta"
      },
      {
        "authorId": "2297848569",
        "name": "Muhammad Umar Salman"
      },
      {
        "authorId": "2238627171",
        "name": "Gurpreet Gosal"
      },
      {
        "authorId": "9645616",
        "name": "Bhargav Kanakiya"
      },
      {
        "authorId": "2297853602",
        "name": "Charles Chen"
      },
      {
        "authorId": "2243228182",
        "name": "Natalia Vassilieva"
      },
      {
        "authorId": "2125606",
        "name": "B. Amor"
      },
      {
        "authorId": "2308995611",
        "name": "M. A. Pimentel"
      },
      {
        "authorId": "2298728972",
        "name": "Shadab Khan"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "207f0c123e48b9e980f49e42cbc35711e14fea07",
    "url": "https://www.semanticscholar.org/paper/207f0c123e48b9e980f49e42cbc35711e14fea07",
    "title": "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models",
    "abstract": "Large language models (LLMs) have demonstrated impressive reasoning capabilities, particularly in textual mathematical problem-solving. However, existing open-source image instruction fine-tuning datasets, containing limited question-answer pairs per image, do not fully exploit visual information to enhance the multimodal mathematical reasoning capabilities of Multimodal LLMs (MLLMs). To bridge this gap, we address the lack of high-quality, diverse multimodal mathematical datasets by collecting 40K high-quality images with question-answer pairs from 24 existing datasets and synthesizing 320K new pairs, creating the MathV360K dataset, which enhances both the breadth and depth of multimodal mathematical questions. We introduce Math-LLaVA, a LLaVA-1.5-based model fine-tuned with MathV360K. This novel approach significantly improves the multimodal mathematical reasoning capabilities of LLaVA-1.5, achieving a 19-point increase and comparable performance to GPT-4V on MathVista's minitest split, and yielding leading performance on Math-V and MathVerse. Furthermore, Math-LLaVA demonstrates enhanced generalizability, showing substantial improvements on the MMMU benchmark. Our research highlights the importance of dataset diversity and synthesis in advancing MLLMs' mathematical reasoning abilities. The code and data are available at: \\url{https://github.com/HZQ950419/Math-LLaVA}.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-25",
    "authors": [
      {
        "authorId": "2187557990",
        "name": "Wenhao Shi"
      },
      {
        "authorId": "1557412457",
        "name": "Zhiqiang Hu"
      },
      {
        "authorId": "2305614095",
        "name": "Yi Bin"
      },
      {
        "authorId": "2308230787",
        "name": "Junhua Liu"
      },
      {
        "authorId": "2258938374",
        "name": "Yang Yang"
      },
      {
        "authorId": "2258761748",
        "name": "See-Kiong Ng"
      },
      {
        "authorId": "2211459675",
        "name": "Li Bing"
      },
      {
        "authorId": "2258331641",
        "name": "Roy Ka-wei Lee"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.49820016084324
  },
  {
    "paperId": "f4b5548091da7e971dd717978c8ba192e276af3a",
    "url": "https://www.semanticscholar.org/paper/f4b5548091da7e971dd717978c8ba192e276af3a",
    "title": "ComposerX: Multi-Agent Symbolic Music Composition with LLMs",
    "abstract": "Music composition represents the creative side of humanity, and itself is a complex task that requires abilities to understand and generate information with long dependency and harmony constraints. While demonstrating impressive capabilities in STEM subjects, current LLMs easily fail in this task, generating ill-written music even when equipped with modern techniques like In-Context-Learning and Chain-of-Thoughts. To further explore and enhance LLMs' potential in music composition by leveraging their reasoning ability and the large knowledge base in music history and theory, we propose ComposerX, an agent-based symbolic music generation framework. We find that applying a multi-agent approach significantly improves the music composition quality of GPT-4. The results demonstrate that ComposerX is capable of producing coherent polyphonic music compositions with captivating melodies, while adhering to user instructions.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 14,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-28",
    "authors": [
      {
        "authorId": "2298907178",
        "name": "Qixin Deng"
      },
      {
        "authorId": "2298946481",
        "name": "Qikai Yang"
      },
      {
        "authorId": "2032236274",
        "name": "Ruibin Yuan"
      },
      {
        "authorId": "2298945973",
        "name": "Yipeng Huang"
      },
      {
        "authorId": "2287877457",
        "name": "Yi Wang"
      },
      {
        "authorId": "2110814131",
        "name": "Xubo Liu"
      },
      {
        "authorId": "2157198469",
        "name": "Zeyue Tian"
      },
      {
        "authorId": "2268696176",
        "name": "Jiahao Pan"
      },
      {
        "authorId": "2143853895",
        "name": "Ge Zhang"
      },
      {
        "authorId": "2287467997",
        "name": "Hanfeng Lin"
      },
      {
        "authorId": "2129449392",
        "name": "Yizhi Li"
      },
      {
        "authorId": "2294504012",
        "name": "Ying Ma"
      },
      {
        "authorId": "2276508494",
        "name": "Jie Fu"
      },
      {
        "authorId": "2279451646",
        "name": "Chenghua Lin"
      },
      {
        "authorId": "2109397",
        "name": "Emmanouil Benetos"
      },
      {
        "authorId": "2239051433",
        "name": "Wenwu Wang"
      },
      {
        "authorId": "2298896015",
        "name": "Guangyu Xia"
      },
      {
        "authorId": "2239201089",
        "name": "Wei Xue"
      },
      {
        "authorId": "2118270918",
        "name": "Yi-Ting Guo"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "89512c767e0ca0fe64d12a436c64f15dffdad1e0",
    "url": "https://www.semanticscholar.org/paper/89512c767e0ca0fe64d12a436c64f15dffdad1e0",
    "title": "Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory",
    "abstract": "The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 57,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-27",
    "authors": [
      {
        "authorId": "2254272878",
        "name": "Niloofar Mireshghallah"
      },
      {
        "authorId": "32609381",
        "name": "Hyunwoo Kim"
      },
      {
        "authorId": "144101734",
        "name": "Xuhui Zhou"
      },
      {
        "authorId": "2258958466",
        "name": "Yulia Tsvetkov"
      },
      {
        "authorId": "2729164",
        "name": "Maarten Sap"
      },
      {
        "authorId": "2262214958",
        "name": "Reza Shokri"
      },
      {
        "authorId": "2257385140",
        "name": "Yejin Choi"
      }
    ],
    "source": "semantic_scholar",
    "score": 130.90664515819628
  },
  {
    "paperId": "b47e96762351b2dbf7e863ece4640df6194bcc0c",
    "url": "https://www.semanticscholar.org/paper/b47e96762351b2dbf7e863ece4640df6194bcc0c",
    "title": "Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning",
    "abstract": "Large language models (LLMs) have demonstrated impressive reasoning abilities in complex tasks. However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness. Knowledge graphs (KGs), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM reasoning methods only treat KGs as factual knowledge bases and overlook the importance of their structural information for reasoning. In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning. Specifically, we present a planning-retrieval-reasoning framework, where RoG first generates relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid reasoning paths from the KGs for LLMs to conduct faithful reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the reasoning ability of LLMs through training but also allows seamless integration with any arbitrary LLMs during inference. Extensive experiments on two benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art performance on KG reasoning tasks and generates faithful and interpretable reasoning results.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 132,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.01061",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-02",
    "authors": [
      {
        "authorId": "2238130759",
        "name": "Linhao Luo"
      },
      {
        "authorId": "2256011160",
        "name": "Yuan-Fang Li"
      },
      {
        "authorId": "2561045",
        "name": "Gholamreza Haffari"
      },
      {
        "authorId": "2254047333",
        "name": "Shirui Pan"
      }
    ],
    "source": "semantic_scholar",
    "score": 143.3552369233263
  },
  {
    "paperId": "8199c9d55dd998f69f703e0ad250ca0697e3ad27",
    "url": "https://www.semanticscholar.org/paper/8199c9d55dd998f69f703e0ad250ca0697e3ad27",
    "title": "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models",
    "abstract": "Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goals, integrating commonsense knowledge relevant to navigation task resolution, identifying landmarks from observed scenes, tracking navigation progress, and adapting to exceptions with plan adjustment. Furthermore, we show that LLMs is capable of generating high-quality navigational instructions from observations and actions along a path, as well as drawing accurate top-down metric trajectory given the agent's navigation history. Despite the performance of using NavGPT to zero-shot R2R tasks still falling short of trained models, we suggest adapting multi-modality inputs for LLMs to use as visual navigation agents and applying the explicit reasoning of LLMs to benefit learning-based models. Code is available at: https://github.com/GengzeZhou/NavGPT.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2023,
    "citationCount": 98,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.16986",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-26",
    "authors": [
      {
        "authorId": "2218747386",
        "name": "Gengze Zhou"
      },
      {
        "authorId": "1612421029",
        "name": "Yicong Hong"
      },
      {
        "authorId": "2143599088",
        "name": "Qi Wu"
      }
    ],
    "source": "semantic_scholar",
    "score": 138.92679775201884
  },
  {
    "paperId": "d0c69c309fbf1233b6351cd57484557c16f28427",
    "url": "https://www.semanticscholar.org/paper/d0c69c309fbf1233b6351cd57484557c16f28427",
    "title": "Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with LLMs",
    "abstract": "Large Language Models (LLMs), such as \\texttt{ChatGPT}, greatly empower dialogue systems with strong language understanding and generation capabilities. However, most of the previous works prompt the LLMs to directly generate a response based on the dialogue context, overlooking the underlying linguistic cues about the user status exhibited in the context. Such in-depth dialogue scenarios are challenging for existing LLMs to figure out the user's hidden needs and respond satisfactorily through a single-step inference. To this end, we propose a novel linguistic cue-based chain-of-thoughts (\\textit{Cue}-CoT), which enhances the LLMs inference with an intermediate reasoning step to find cues exhibited in the dialogue, aiming to provide a more personalized and engaging response. To evaluate the approach, we build a benchmark with in-depth dialogue questions, consisting of 6 datasets in both Chinese and English, targeting 3 major linguistic cues during the conversation: \\textit{personality}, \\textit{emotion}, and \\textit{psychology}. We conduct extensive experiments on the proposed benchmark with 5 LLMs under both zero-shot and one-shot settings. Empirical results demonstrate our proposed \\textit{Cue}-CoT method outperforms standard prompting methods in terms of both \\textit{helpfulness} and \\textit{acceptability} on all datasets.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 40,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.findings-emnlp.806.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-19",
    "authors": [
      {
        "authorId": "22642319",
        "name": "Hongru Wang"
      },
      {
        "authorId": "2248766573",
        "name": "Rui Wang"
      },
      {
        "authorId": "2248150493",
        "name": "Fei Mi"
      },
      {
        "authorId": "145843537",
        "name": "Yang Deng"
      },
      {
        "authorId": "2108726649",
        "name": "Zezhong Wang"
      },
      {
        "authorId": "2258715431",
        "name": "Bin Liang"
      },
      {
        "authorId": "2260333456",
        "name": "Ruifeng Xu"
      },
      {
        "authorId": "2237563835",
        "name": "Kam-Fai Wong"
      }
    ],
    "source": "semantic_scholar",
    "score": 125.70358100056461
  },
  {
    "paperId": "ea75117f34b168a20f2a4309ac2eb685ca6b1436",
    "url": "https://www.semanticscholar.org/paper/ea75117f34b168a20f2a4309ac2eb685ca6b1436",
    "title": "Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance",
    "abstract": "As large language models (LLMs) are continuously being developed, their evaluation becomes increasingly important yet challenging. This work proposes Chain-of-Thought Hub, an open-source evaluation suite on the multi-step reasoning capabilities of large language models. We are interested in this setting for two reasons: (1) from the behavior of GPT and PaLM model family, we observe that complex reasoning is likely to be a key differentiator between weaker and stronger LLMs; (2) we envisage large language models to become the next-generation computational platform and foster an ecosystem of LLM-based new applications, this naturally requires the foundation models to perform complex tasks that often involve the composition of linguistic and logical operations. Our approach is to compile a suite of challenging reasoning benchmarks to track the progress of LLMs. Our current results show that: (1) model scale clearly correlates with reasoning capabilities; (2) As of May 2023, Claude-v1.3 and PaLM-2 are the only two models that are comparable with GPT-4, while open-sourced models still lag behind; (3) LLaMA-65B performs closely to code-davinci-002, indicating that with successful further development such as reinforcement learning from human feedback (RLHF), it has great potential to be close to GPT-3.5-Turbo. Our results also suggest that for the open-source efforts to catch up, the community may focus more on building better base models and exploring RLHF.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 97,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.17306",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-26",
    "authors": [
      {
        "authorId": "46956602",
        "name": "Yao Fu"
      },
      {
        "authorId": "2203370918",
        "name": "Litu Ou"
      },
      {
        "authorId": "2218855303",
        "name": "Mingyu Chen"
      },
      {
        "authorId": "2218462633",
        "name": "Yuhao Wan"
      },
      {
        "authorId": "2293471",
        "name": "Hao-Chun Peng"
      },
      {
        "authorId": "2236429",
        "name": "Tushar Khot"
      }
    ],
    "source": "semantic_scholar",
    "score": 138.7745121800586
  },
  {
    "paperId": "ec67d5f0e236f23c6b48b926f9e25db52194dd71",
    "url": "https://www.semanticscholar.org/paper/ec67d5f0e236f23c6b48b926f9e25db52194dd71",
    "title": "Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey",
    "abstract": "The contemporary LLMs are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models. To address this critical limitation, researchers employ diverse strategies to augment the LLMs by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy. Among these strategies, leveraging knowledge graphs as a source of external information has demonstrated promising results. In this survey, we comprehensively review these knowledge-graph-based augmentation techniques in LLMs, focusing on their efficacy in mitigating hallucinations. We systematically categorize these methods into three overarching groups, offering methodological comparisons and performance evaluations. Lastly, this survey explores the current trends and challenges associated with these techniques and outlines potential avenues for future research in this emerging field.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 53,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference",
      "Review"
    ],
    "publicationDate": "2023-11-14",
    "authors": [
      {
        "authorId": "145363324",
        "name": "Garima Agrawal"
      },
      {
        "authorId": "40899329",
        "name": "Tharindu Kumarage"
      },
      {
        "authorId": "2266468495",
        "name": "Zeyad Alghami"
      },
      {
        "authorId": "2146398603",
        "name": "Huanmin Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 129.83476069846412
  },
  {
    "paperId": "9f5b8d5310a2bafad2838dcf91c88a02b0aca106",
    "url": "https://www.semanticscholar.org/paper/9f5b8d5310a2bafad2838dcf91c88a02b0aca106",
    "title": "JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models",
    "abstract": "Mathematical reasoning is an important capability of large language models~(LLMs) for real-world applications. To enhance this capability, existing work either collects large-scale math-related texts for pre-training, or relies on stronger LLMs (\\eg GPT-4) to synthesize massive math problems. Both types of work generally lead to large costs in training or synthesis. To reduce the cost, based on open-source available texts, we propose an efficient way that trains a small LLM for math problem synthesis, to efficiently generate sufficient high-quality pre-training data. To achieve it, we create a dataset using GPT-4 to distill its data synthesis capability into the small LLM. Concretely, we craft a set of prompts based on human education stages to guide GPT-4, to synthesize problems covering diverse math knowledge and difficulty levels. Besides, we adopt the gradient-based influence estimation method to select the most valuable math-related texts. The both are fed into GPT-4 for creating the knowledge distillation dataset to train the small LLM. We leverage it to synthesize 6 million math problems for pre-training our JiuZhang3.0 model, which only needs to invoke GPT-4 API 9.3k times and pre-train on 4.6B data. Experimental results have shown that JiuZhang3.0 achieves state-of-the-art performance on several mathematical reasoning datasets, under both natural language reasoning and tool manipulation settings. Our code and data will be publicly released in \\url{https://github.com/RUCAIBox/JiuZhang3.0}.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 18,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-23",
    "authors": [
      {
        "authorId": "2265383494",
        "name": "Kun Zhou"
      },
      {
        "authorId": "2107926615",
        "name": "Beichen Zhang"
      },
      {
        "authorId": "2302813110",
        "name": "Jiapeng Wang"
      },
      {
        "authorId": "2111335050",
        "name": "Zhipeng Chen"
      },
      {
        "authorId": "2257376413",
        "name": "Wayne Xin Zhao"
      },
      {
        "authorId": "2165225571",
        "name": "Jing Sha"
      },
      {
        "authorId": "2125340023",
        "name": "Zhichao Sheng"
      },
      {
        "authorId": "2302793582",
        "name": "Shijin Wang"
      },
      {
        "authorId": "2274218622",
        "name": "Ji-Rong Wen"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.1665846874966
  },
  {
    "paperId": "1b1265a7fc7debcdd0cd92fc1d9b51fc55d57fdc",
    "url": "https://www.semanticscholar.org/paper/1b1265a7fc7debcdd0cd92fc1d9b51fc55d57fdc",
    "title": "Faithful Logical Reasoning via Symbolic Chain-of-Thought",
    "abstract": "While the recent Chain-of-Thought (CoT) technique enhances the reasoning ability of large language models (LLMs) with the theory of mind, it might still struggle in handling logical reasoning that relies much on symbolic expressions and rigid deducing rules. To strengthen the logical reasoning capability of LLMs, we propose a novel Symbolic Chain-of-Thought, namely SymbCoT, a fully LLM-based framework that integrates symbolic expressions and logic rules with CoT prompting. Technically, building upon an LLM, SymbCoT 1) first translates the natural language context into the symbolic format, and then 2) derives a step-by-step plan to solve the problem with symbolic logical rules, 3) followed by a verifier to check the translation and reasoning chain. Via thorough evaluations on 5 standard datasets with both First-Order Logic and Constraint Optimization symbolic expressions, SymbCoT shows striking improvements over the CoT method consistently, meanwhile refreshing the current state-of-the-art performances. We further demonstrate that our system advances in more faithful, flexible, and explainable logical reasoning. To our knowledge, this is the first to combine symbolic expressions and rules into CoT for logical reasoning with LLMs. Code is open at https://github.com/Aiden0526/SymbCoT.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 19,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-28",
    "authors": [
      {
        "authorId": "2304023176",
        "name": "Jundong Xu"
      },
      {
        "authorId": "2303470150",
        "name": "Hao Fei"
      },
      {
        "authorId": "2304300631",
        "name": "Liangming Pan"
      },
      {
        "authorId": "2303474809",
        "name": "Qian Liu"
      },
      {
        "authorId": "2237598359",
        "name": "M. Lee"
      },
      {
        "authorId": "144793401",
        "name": "W. Hsu"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.93598410330986
  },
  {
    "paperId": "2208e506f72518a16ea86dfa604995c12fa8e4ca",
    "url": "https://www.semanticscholar.org/paper/2208e506f72518a16ea86dfa604995c12fa8e4ca",
    "title": "Premise Order Matters in Reasoning with Large Language Models",
    "abstract": "Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 19,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-14",
    "authors": [
      {
        "authorId": "2238263119",
        "name": "Xinyun Chen"
      },
      {
        "authorId": "2284066085",
        "name": "Ryan A. Chi"
      },
      {
        "authorId": "2238394232",
        "name": "Xuezhi Wang"
      },
      {
        "authorId": "2256313467",
        "name": "Denny Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.93598410330986
  },
  {
    "paperId": "44d16a076c00ecada3d425203377e4ec951c4ed0",
    "url": "https://www.semanticscholar.org/paper/44d16a076c00ecada3d425203377e4ec951c4ed0",
    "title": "MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning",
    "abstract": "Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose MedAgents, a novel multi-disciplinary collaboration framework for the medical domain. MedAgents leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine datasets (MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our proposed MedAgents framework excels at mining and harnessing the medical expertise within LLMs, as well as extending its reasoning abilities. Our code can be found at https://github.com/gersteinlab/MedAgents.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 96,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-16",
    "authors": [
      {
        "authorId": "47274259",
        "name": "Xiangru Tang"
      },
      {
        "authorId": "2187586628",
        "name": "Anni Zou"
      },
      {
        "authorId": "3322871",
        "name": "Zhuosheng Zhang"
      },
      {
        "authorId": "46316984",
        "name": "Yilun Zhao"
      },
      {
        "authorId": "2267545169",
        "name": "Xingyao Zhang"
      },
      {
        "authorId": "2266838179",
        "name": "Arman Cohan"
      },
      {
        "authorId": "2201323142",
        "name": "Mark B. Gerstein"
      }
    ],
    "source": "semantic_scholar",
    "score": 138.62066467755074
  },
  {
    "paperId": "b91645729a769c09eddda2efe2512e2f6a750723",
    "url": "https://www.semanticscholar.org/paper/b91645729a769c09eddda2efe2512e2f6a750723",
    "title": "Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation",
    "abstract": "Recent ubiquity and disruptive impacts of large language models (LLMs) have raised concerns about their potential to be misused (.i.e, generating large-scale harmful and misleading content). To combat this emerging risk of LLMs, we propose a novel\"Fighting Fire with Fire\"(F3) strategy that harnesses modern LLMs' generative and emergent reasoning capabilities to counter human-written and LLM-generated disinformation. First, we leverage GPT-3.5-turbo to synthesize authentic and deceptive LLM-generated content through paraphrase-based and perturbation-based prefix-style prompts, respectively. Second, we apply zero-shot in-context semantic reasoning techniques with cloze-style prompts to discern genuine from deceptive posts and news articles. In our extensive experiments, we observe GPT-3.5-turbo's zero-shot superiority for both in-distribution and out-of-distribution datasets, where GPT-3.5-turbo consistently achieved accuracy at 68-72%, unlike the decline observed in previous customized and fine-tuned disinformation detectors. Our codebase and dataset are available at https://github.com/mickeymst/F3.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 31,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-24",
    "authors": [
      {
        "authorId": "2260072705",
        "name": "Jason Samuel Lucas"
      },
      {
        "authorId": "150035131",
        "name": "Adaku Uchendu"
      },
      {
        "authorId": "66848311",
        "name": "Michiharu Yamashita"
      },
      {
        "authorId": "2159644449",
        "name": "Jooyoung Lee"
      },
      {
        "authorId": "40408676",
        "name": "Shaurya Rohatgi"
      },
      {
        "authorId": "2158951945",
        "name": "Dongwon Lee"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.9860385419959
  },
  {
    "paperId": "ef1d74ddfc09a367050bc54b7be846769061d95e",
    "url": "https://www.semanticscholar.org/paper/ef1d74ddfc09a367050bc54b7be846769061d95e",
    "title": "Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments",
    "abstract": "Large Language Models (LLMs) have shown potential in reasoning over structured environments, e.g., knowledge graph and table. Such tasks typically require multi-hop reasoning, i.e., match natural language utterance with instances in the environment. Previous methods leverage LLMs to incrementally build a reasoning path, where the LLMs either invoke tools or pick up schemas by step-by-step interacting with the environment. We propose Reasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently and faithfully reason over structured environments. In Readi, LLMs initially generate a reasoning path given a query, and edit the path only when necessary. We instantiate the path on structured environments and provide feedback to edit the path if anything goes wrong. Experimental results on three KGQA and two TableQA datasets show the effectiveness of Readi, significantly surpassing previous LLM-based methods (by 9.1% Hit@1 on WebQSP, 12.4% on MQA-3H and 9.5% on WTQ), comparable with state-of-the-art fine-tuned methods (67% on CWQ and 74.7% on WebQSP) and substantially boosting the vanilla LLMs (by 14.9% on CWQ). Our code will be available on https://aka.ms/readi.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-13",
    "authors": [
      {
        "authorId": "2220695955",
        "name": "Sitao Cheng"
      },
      {
        "authorId": "2296707191",
        "name": "Ziyuan Zhuang"
      },
      {
        "authorId": "2262533579",
        "name": "Yong Xu"
      },
      {
        "authorId": "2261394438",
        "name": "Fangkai Yang"
      },
      {
        "authorId": "2256775302",
        "name": "Chaoyun Zhang"
      },
      {
        "authorId": "2227603238",
        "name": "Xiaoting Qin"
      },
      {
        "authorId": "2291210811",
        "name": "Xiang Huang"
      },
      {
        "authorId": "2291133905",
        "name": "Ling Chen"
      },
      {
        "authorId": "2256132386",
        "name": "Qingwei Lin"
      },
      {
        "authorId": "2109581369",
        "name": "Dongmei Zhang"
      },
      {
        "authorId": "148121358",
        "name": "S. Rajmohan"
      },
      {
        "authorId": "2279711828",
        "name": "Qi Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "7dd8a29a5cd22c34587e1881e084ceaf581576fd",
    "url": "https://www.semanticscholar.org/paper/7dd8a29a5cd22c34587e1881e084ceaf581576fd",
    "title": "Using LLMs to Facilitate Formal Verification of RTL",
    "abstract": "Formal property verification (FPV) has existed for decades and has been shown to be effective at finding intricate RTL bugs. However, formal properties, such as those written as SystemVerilog Assertions (SVA), are time-consuming and error-prone to write, even for experienced users. Prior work has attempted to lighten this burden by raising the abstraction level so that SVA is generated from high-level specifications. However, this does not eliminate the manual effort of reasoning and writing about the detailed hardware behavior. Motivated by the increased need for FPV in the era of heterogeneous hardware and the advances in large language models (LLMs), we set out to explore whether LLMs can capture RTL behavior and generate correct SVA properties. First, we design an FPV-based evaluation framework that measures the correctness and completeness of SVA. Then, we evaluate GPT4 iteratively to craft the set of syntax and semantic rules needed to prompt it toward creating better SVA. We extend the open-source AutoSVA framework by integrating our improved GPT4-based flow to generate safety properties, in addition to facilitating their existing flow for liveness properties. Lastly, our use cases evaluate (1) the FPV coverage of GPT4-generated SVA on complex open-source RTL and (2) using generated SVA to prompt GPT4 to create RTL from scratch. Through these experiments, we find that GPT4 can generate correct SVA even for flawed RTL, without mirroring design errors. Particularly, it generated SVA that exposed a bug in the RISC-V CVA6 core that eluded the prior work's evaluation.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 29,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.09437",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-18",
    "authors": [
      {
        "authorId": "1647009803",
        "name": "Marcelo Orenes-Vera"
      },
      {
        "authorId": "1708269",
        "name": "M. Martonosi"
      },
      {
        "authorId": "1752172",
        "name": "D. Wentzlaff"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.01796072493232
  },
  {
    "paperId": "e0fb45ea254a959099156581b0913cbd0db2b432",
    "url": "https://www.semanticscholar.org/paper/e0fb45ea254a959099156581b0913cbd0db2b432",
    "title": "The Landscape and Challenges of HPC Research and LLMs",
    "abstract": "Recently, language models (LMs), especially large language models (LLMs), have revolutionized the field of deep learning. Both encoder-decoder models and prompt-based techniques have shown immense potential for natural language processing and code-based tasks. Over the past several years, many research labs and institutions have invested heavily in high-performance computing, approaching or breaching exascale performance levels. In this paper, we posit that adapting and utilizing such language model-based techniques for tasks in high-performance computing (HPC) would be very beneficial. This study presents our reasoning behind the aforementioned position and highlights how existing ideas can be improved and adapted for HPC tasks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-03",
    "authors": [
      {
        "authorId": "2283131565",
        "name": "Le Chen"
      },
      {
        "authorId": "2256997701",
        "name": "Nesreen K. Ahmed"
      },
      {
        "authorId": "2100306",
        "name": "Akashnil Dutta"
      },
      {
        "authorId": "2227729635",
        "name": "Arijit Bhattacharjee"
      },
      {
        "authorId": "2028798613",
        "name": "Sixing Yu"
      },
      {
        "authorId": "1414578549",
        "name": "Quazi Ishtiaque Mahmud"
      },
      {
        "authorId": "2142452508",
        "name": "Waqwoya Abebe"
      },
      {
        "authorId": "2282946151",
        "name": "Hung Phan"
      },
      {
        "authorId": "90019229",
        "name": "Aishwarya Sarkar"
      },
      {
        "authorId": "2282946631",
        "name": "Branden Butler"
      },
      {
        "authorId": "1789829",
        "name": "N. Hasabnis"
      },
      {
        "authorId": "151504666",
        "name": "Gal Oren"
      },
      {
        "authorId": "3369353",
        "name": "Vy A. Vo"
      },
      {
        "authorId": "2249845183",
        "name": "J. P. Muñoz"
      },
      {
        "authorId": "102992766",
        "name": "Ted Willke"
      },
      {
        "authorId": "2249170426",
        "name": "Tim Mattson"
      },
      {
        "authorId": "2266204425",
        "name": "Ali Jannesari"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "2ad8183c72a90511383a32ccaeea313eb85f4085",
    "url": "https://www.semanticscholar.org/paper/2ad8183c72a90511383a32ccaeea313eb85f4085",
    "title": "DetGPT: Detect What You Need via Reasoning",
    "abstract": "In recent years, the field of computer vision has seen significant advancements thanks to the development of large language models (LLMs). These models have enabled more effective and sophisticated interactions between humans and machines, paving the way for novel techniques that blur the lines between human and machine intelligence. In this paper, we introduce a new paradigm for object detection that we call reasoning-based object detection. Unlike conventional object detection methods that rely on specific object names, our approach enables users to interact with the system using natural language instructions, allowing for a higher level of interactivity. Our proposed method, called DetGPT, leverages state-of-the-art multi-modal models and open-vocabulary object detectors to perform reasoning within the context of the user's instructions and the visual scene. This enables DetGPT to automatically locate the object of interest based on the user's expressed desires, even if the object is not explicitly mentioned. For instance, if a user expresses a desire for a cold beverage, DetGPT can analyze the image, identify a fridge, and use its knowledge of typical fridge contents to locate the beverage. This flexibility makes our system applicable across a wide range of fields, from robotics and automation to autonomous driving. Overall, our proposed paradigm and DetGPT demonstrate the potential for more sophisticated and intuitive interactions between humans and machines. We hope that our proposed paradigm and approach will provide inspiration to the community and open the door to more interative and versatile object detection systems. Our project page is launched at detgpt.github.io.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 82,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.14167",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-23",
    "authors": [
      {
        "authorId": "2066420772",
        "name": "Renjie Pi"
      },
      {
        "authorId": "144407296",
        "name": "Jiahui Gao"
      },
      {
        "authorId": "50826757",
        "name": "Shizhe Diao"
      },
      {
        "authorId": null,
        "name": "Rui Pan"
      },
      {
        "authorId": "35279146",
        "name": "Hanze Dong"
      },
      {
        "authorId": "50561049",
        "name": "Jipeng Zhang"
      },
      {
        "authorId": "1429192500",
        "name": "Lewei Yao"
      },
      {
        "authorId": "47180442",
        "name": "Jianhua Han"
      },
      {
        "authorId": "2143534132",
        "name": "Hang Xu"
      },
      {
        "authorId": "2218224348",
        "name": "Lingpeng Kong Tong Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 136.28260911694898
  },
  {
    "paperId": "e3bf194790dc4ad4a032e97f343ff70472fe3f16",
    "url": "https://www.semanticscholar.org/paper/e3bf194790dc4ad4a032e97f343ff70472fe3f16",
    "title": "Challenging the appearance of machine intelligence: Cognitive bias in LLMs",
    "abstract": "Assessments of algorithmic bias in large language models (LLMs) are generally catered to uncovering systemic discrimination based on protected characteristics such as sex and ethnicity. However, there are over 180 documented cognitive biases that pervade human reasoning and decision making that are routinely ignored when discussing the ethical complexities of AI. We demonstrate the presence of these cognitive biases in LLMs and discuss the implications of using biased reasoning under the guise of expertise. We call for stronger education, risk management, and continued research as widespread adoption of this technology increases. Finally, we close with a set of best practices for when and how to employ this technology as widespread adoption continues to grow.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 23,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2304.01358",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-04-03",
    "authors": [
      {
        "authorId": "40867997",
        "name": "Alaina N. Talboy"
      },
      {
        "authorId": "2141617331",
        "name": "Elizabeth Fuller"
      }
    ],
    "source": "semantic_scholar",
    "score": 117.67080745521918
  },
  {
    "paperId": "f208ea909fa7f54fea82def9a92fd81dfc758c39",
    "url": "https://www.semanticscholar.org/paper/f208ea909fa7f54fea82def9a92fd81dfc758c39",
    "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions",
    "abstract": "Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, what to retrieve depends on what has already been derived, which in turn may depend on what was previously retrieved. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "citationCount": 287,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2212.10509",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-12-20",
    "authors": [
      {
        "authorId": "6365809",
        "name": "H. Trivedi"
      },
      {
        "authorId": "35217367",
        "name": "Niranjan Balasubramanian"
      },
      {
        "authorId": "2236429",
        "name": "Tushar Khot"
      },
      {
        "authorId": "48229640",
        "name": "Ashish Sabharwal"
      }
    ],
    "source": "semantic_scholar",
    "score": 154.9444072020392
  },
  {
    "paperId": "d4e3aa50b3d822021628d7c8d89519ec0389fbd8",
    "url": "https://www.semanticscholar.org/paper/d4e3aa50b3d822021628d7c8d89519ec0389fbd8",
    "title": "Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs",
    "abstract": "Although large language models (LLMs) have demonstrated remarkable proficiency in modeling text and generating human-like text, they may exhibit biases acquired from training data in doing so. Specifically, LLMs may be susceptible to a common cognitive trap in human decision-making called the representativeness heuristic. This is a concept in psychology that refers to judging the likelihood of an event based on how closely it resembles a well-known prototype or typical example, versus considering broader facts or statistical evidence. This research investigates the impact of the representativeness heuristic on LLM reasoning. We created ReHeAT (Representativeness Heuristic AI Testing), a dataset containing a series of problems spanning six common types of representativeness heuristics. Experiments reveal that four LLMs applied to ReHeAT all exhibited representativeness heuristic biases. We further identify that the model's reasoning steps are often incorrectly based on a stereotype rather than on the problem's description. Interestingly, the performance improves when adding a hint in the prompt to remind the model to use its knowledge. This suggests the uniqueness of the representativeness heuristic compared to traditional biases. It can occur even when LLMs possess the correct knowledge while falling into a cognitive trap. This highlights the importance of future research focusing on the representativeness heuristic in model reasoning and decision-making and on developing solutions to address it.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-01",
    "authors": [
      {
        "authorId": "2294722814",
        "name": "Pengda Wang"
      },
      {
        "authorId": "2186608582",
        "name": "Zilin Xiao"
      },
      {
        "authorId": "7315244",
        "name": "Hanjie Chen"
      },
      {
        "authorId": "2250843586",
        "name": "Frederick L. Oswald"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "90027ca7802645671a69b00b65e1fa94e6b63544",
    "url": "https://www.semanticscholar.org/paper/90027ca7802645671a69b00b65e1fa94e6b63544",
    "title": "ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models",
    "abstract": "Augmented Language Models (ALMs) blend the reasoning capabilities of Large Language Models (LLMs) with tools that allow for knowledge retrieval and action execution. Existing ALM systems trigger LLM thought processes while pulling observations from these tools in an interleaved fashion. Specifically, an LLM reasons to call an external tool, gets halted to fetch the tool's response, and then decides the next action based on all preceding response tokens. Such a paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution. This study addresses such challenges for the first time, proposing a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption. Comprehensive evaluations across six public NLP benchmarks and a curated dataset reveal consistent performance enhancements with our proposed methodology. Notably, ReWOO achieves 5x token efficiency and 4% accuracy improvement on HotpotQA, a multi-step reasoning benchmark. Furthermore, ReWOO demonstrates robustness under tool-failure scenarios. Beyond prompt efficiency, decoupling parametric modules from non-parametric tool calls enables instruction fine-tuning to offload LLMs into smaller language models, thus substantially reducing model parameters. Our illustrative work offloads reasoning ability from 175B GPT3.5 into 7B LLaMA, demonstrating the significant potential for truly efficient and scalable ALM systems.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 72,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.18323",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-23",
    "authors": [
      {
        "authorId": "2924359",
        "name": "Binfeng Xu"
      },
      {
        "authorId": "47015377",
        "name": "Zhiyuan Peng"
      },
      {
        "authorId": "2144399315",
        "name": "Bowen Lei"
      },
      {
        "authorId": "2153292652",
        "name": "Subhabrata Mukherjee"
      },
      {
        "authorId": "2183078810",
        "name": "Yuchen Liu"
      },
      {
        "authorId": "2116459424",
        "name": "Dongkuan Xu"
      }
    ],
    "source": "semantic_scholar",
    "score": 134.35689161722587
  },
  {
    "paperId": "a544f18fdb3a1adfd47bb9d2892b37e229193516",
    "url": "https://www.semanticscholar.org/paper/a544f18fdb3a1adfd47bb9d2892b37e229193516",
    "title": "Code Repair with LLMs gives an Exploration-Exploitation Tradeoff",
    "abstract": "Iteratively improving and repairing source code with large language models (LLMs), known as refinement, has emerged as a popular way of generating programs that would be too complex to construct in one shot. Given a bank of test cases, together with a candidate program, an LLM can improve that program by being prompted with failed test cases. But it remains an open question how to best iteratively refine code, with prior work employing simple greedy or breadth-first strategies. We show here that refinement exposes an explore-exploit tradeoff: exploit by refining the program that passes the most test cases, or explore by refining a lesser considered program. We frame this as an arm-acquiring bandit problem, which we solve with Thompson Sampling. The resulting LLM-based program synthesis algorithm is broadly applicable: Across loop invariant synthesis, visual reasoning puzzles, and competition programming problems, we find that our new method can solve more problems using fewer language model calls.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-26",
    "authors": [
      {
        "authorId": "2109238481",
        "name": "Hao Tang"
      },
      {
        "authorId": "2303624656",
        "name": "Keya Hu"
      },
      {
        "authorId": "2303726151",
        "name": "Jin Peng Zhou"
      },
      {
        "authorId": "2303463837",
        "name": "Sicheng Zhong"
      },
      {
        "authorId": "2303969843",
        "name": "Wei-Long Zheng"
      },
      {
        "authorId": "2303462714",
        "name": "Xujie Si"
      },
      {
        "authorId": "2284673917",
        "name": "Kevin Ellis"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "a9de58b1ad7e052dfc21f0363ec3809b5f9e5b7b",
    "url": "https://www.semanticscholar.org/paper/a9de58b1ad7e052dfc21f0363ec3809b5f9e5b7b",
    "title": "TheoremLlama: Transforming General-Purpose LLMs into Lean4 Experts",
    "abstract": "Proving mathematical theorems using computer-verifiable formal languages like Lean significantly impacts mathematical reasoning. One approach to formal theorem proving involves generating complete proofs using Large Language Models (LLMs) based on Natural Language (NL) proofs. However, due to the scarcity of aligned NL and Formal Language (FL) theorem-proving data most modern LLMs exhibit suboptimal performance.This scarcity results in a paucity of methodologies for training LLMs and techniques to fully utilize their capabilities in composing formal proofs. To address these challenges, this paper proposes **TheoremLlama**, an end-to-end framework that trains a general-purpose LLM to be a Lean4 expert. **TheoremLlama** includes NL-FL dataset generation and bootstrapping method to obtain aligned dataset, curriculum learning and block training techniques to train the model, and iterative proof writing method to write Lean4 proofs that work together synergistically.Using the dataset generation method in **TheoremLlama**, we provide *Open Bootstrapped Theorems* (OBT), an NL-FL aligned and bootstrapped dataset. Our novel NL-FL bootstrapping method, where NL proofs are integrated into Lean4 code for training datasets, leverages the NL reasoning ability of LLMs for formal reasoning. The **TheoremLlama** framework achieves cumulative accuracies of 36.48% and 33.61% on MiniF2F-Valid and Test datasets respectively, surpassing the GPT-4 baseline of 22.95% and 25.41%. Our code, model checkpoints, and the generated dataset is published in GitHub",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-03",
    "authors": [
      {
        "authorId": "2309657699",
        "name": "Ruida Wang"
      },
      {
        "authorId": "50561049",
        "name": "Jipeng Zhang"
      },
      {
        "authorId": "2309788655",
        "name": "Yizhen Jia"
      },
      {
        "authorId": "2192845956",
        "name": "Rui Pan"
      },
      {
        "authorId": "50826757",
        "name": "Shizhe Diao"
      },
      {
        "authorId": "2066420772",
        "name": "Renjie Pi"
      },
      {
        "authorId": "2266465257",
        "name": "Tong Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "42cf3165269ed78e8e1cc22ebeef4017176def86",
    "url": "https://www.semanticscholar.org/paper/42cf3165269ed78e8e1cc22ebeef4017176def86",
    "title": "When LLMs Meet Cunning Texts: A Fallacy Understanding Benchmark for Large Language Models",
    "abstract": "Recently, Large Language Models (LLMs) make remarkable evolutions in language understanding and generation. Following this, various benchmarks for measuring all kinds of capabilities of LLMs have sprung up. In this paper, we challenge the reasoning and understanding abilities of LLMs by proposing a FaLlacy Understanding Benchmark (FLUB) containing cunning texts that are easy for humans to understand but difficult for models to grasp. Specifically, the cunning texts that FLUB focuses on mainly consist of the tricky, humorous, and misleading texts collected from the real internet environment. And we design three tasks with increasing difficulty in the FLUB benchmark to evaluate the fallacy understanding ability of LLMs. Based on FLUB, we investigate the performance of multiple representative and advanced LLMs, reflecting our FLUB is challenging and worthy of more future study. Interesting discoveries and valuable insights are achieved in our extensive experiments and detailed analyses. We hope that our benchmark can encourage the community to improve LLMs' ability to understand fallacies. Our data and codes are available at https://github.com/THUKElab/FLUB.",
    "venue": "",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-02-16",
    "authors": [
      {
        "authorId": "2110503552",
        "name": "Yinghui Li"
      },
      {
        "authorId": "2258945345",
        "name": "Qingyu Zhou"
      },
      {
        "authorId": "2163528885",
        "name": "Yuanzhen Luo"
      },
      {
        "authorId": "2118867306",
        "name": "Shirong Ma"
      },
      {
        "authorId": "2258649570",
        "name": "Yangning Li"
      },
      {
        "authorId": "2238396445",
        "name": "Hai-Tao Zheng"
      },
      {
        "authorId": "2109906988",
        "name": "Xuming Hu"
      },
      {
        "authorId": "2284755458",
        "name": "Philip S. Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "16f55d843b1aed18cab7464580f229c07f9bd188",
    "url": "https://www.semanticscholar.org/paper/16f55d843b1aed18cab7464580f229c07f9bd188",
    "title": "Prompting Techniques for Reducing Social Bias in LLMs through System 1 and System 2 Cognitive Processes",
    "abstract": "Dual process theory posits that human cognition arises via two systems. System 1, which is a quick, emotional, and intuitive process, which is subject to cognitive biases, and System 2, is a slow, onerous, and deliberate process. NLP researchers often compare zero-shot prompting in LLMs to System 1 reasoning and chain-of-thought (CoT) prompting to System 2. In line with this interpretation, prior research has found that using CoT prompting in LLMs leads to reduced gender bias. We investigate the relationship between bias, CoT prompting, a debiasing prompt, and dual process theory in LLMs directly. We compare zero-shot CoT, debiasing, and a variety of dual process theory-based prompting strategies on two bias datasets spanning nine different social bias categories. We incorporate human and machine personas to determine whether the effects of dual process theory in LLMs exist independent of explicit persona models or are based on modeling human cognition. We find that a human persona, debiasing, System 2, and CoT prompting all tend to reduce social biases in LLMs, though the best combination of features depends on the exact model and bias category -- resulting in up to a 19 percent drop in stereotypical judgments by an LLM.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-26",
    "authors": [
      {
        "authorId": "2077526744",
        "name": "M. Kamruzzaman"
      },
      {
        "authorId": "2265517242",
        "name": "Gene Louis Kim"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "d5e9532d12a26124f019de5f1a1d6a9a6eab3d03",
    "url": "https://www.semanticscholar.org/paper/d5e9532d12a26124f019de5f1a1d6a9a6eab3d03",
    "title": "Evaluating LLMs at Detecting Errors in LLM Responses",
    "abstract": "With Large Language Models (LLMs) being widely used across various tasks, detecting errors in their responses is increasingly crucial. However, little research has been conducted on error detection of LLM responses. Collecting error annotations on LLM responses is challenging due to the subjective nature of many NLP tasks, and thus previous research focuses on tasks of little practical value (e.g., word sorting) or limited error types (e.g., faithfulness in summarization). This work introduces ReaLMistake, the first error detection benchmark consisting of objective, realistic, and diverse errors made by LLMs. ReaLMistake contains three challenging and meaningful tasks that introduce objectively assessable errors in four categories (reasoning correctness, instruction-following, context-faithfulness, and parameterized knowledge), eliciting naturally observed and diverse errors in responses of GPT-4 and Llama 2 70B annotated by experts. We use ReaLMistake to evaluate error detectors based on 12 LLMs. Our findings show: 1) Top LLMs like GPT-4 and Claude 3 detect errors made by LLMs at very low recall, and all LLM-based error detectors perform much worse than humans. 2) Explanations by LLM-based error detectors lack reliability. 3) LLMs-based error detection is sensitive to small changes in prompts but remains challenging to improve. 4) Popular approaches to improving LLMs, including self-consistency and majority vote, do not improve the error detection performance. Our benchmark and code are provided at https://github.com/psunlpgroup/ReaLMistake.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-04",
    "authors": [
      {
        "authorId": "83757854",
        "name": "Ryo Kamoi"
      },
      {
        "authorId": "1456156441",
        "name": "Sarkar Snigdha Sarathi Das"
      },
      {
        "authorId": "2118614649",
        "name": "Renze Lou"
      },
      {
        "authorId": "2295188259",
        "name": "Jihyun Janice Ahn"
      },
      {
        "authorId": "46316984",
        "name": "Yilun Zhao"
      },
      {
        "authorId": "2266470204",
        "name": "Xiaoxin Lu"
      },
      {
        "authorId": "2266469940",
        "name": "Nan Zhang"
      },
      {
        "authorId": "2145039557",
        "name": "Yusen Zhang"
      },
      {
        "authorId": "2261450671",
        "name": "Ranran Haoran Zhang"
      },
      {
        "authorId": "2294872107",
        "name": "Sujeeth Reddy Vummanthala"
      },
      {
        "authorId": "2294871756",
        "name": "Salika Dave"
      },
      {
        "authorId": "2294874707",
        "name": "Shaobo Qin"
      },
      {
        "authorId": "2266838179",
        "name": "Arman Cohan"
      },
      {
        "authorId": "2265583938",
        "name": "Wenpeng Yin"
      },
      {
        "authorId": "144142360",
        "name": "Rui Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "bc3965a843b80e720aa03116d3f2cf3896e40592",
    "url": "https://www.semanticscholar.org/paper/bc3965a843b80e720aa03116d3f2cf3896e40592",
    "title": "When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models",
    "abstract": "As large language models (LLMs) evolve, their integration with 3D spatial data (3D-LLMs) has seen rapid progress, offering unprecedented capabilities for understanding and interacting with physical spaces. This survey provides a comprehensive overview of the methodologies enabling LLMs to process, understand, and generate 3D data. Highlighting the unique advantages of LLMs, such as in-context learning, step-by-step reasoning, open-vocabulary capabilities, and extensive world knowledge, we underscore their potential to significantly advance spatial comprehension and interaction within embodied Artificial Intelligence (AI) systems. Our investigation spans various 3D data representations, from point clouds to Neural Radiance Fields (NeRFs). It examines their integration with LLMs for tasks such as 3D scene understanding, captioning, question-answering, and dialogue, as well as LLM-based agents for spatial reasoning, planning, and navigation. The paper also includes a brief review of other methods that integrate 3D and language. The meta-analysis presented in this paper reveals significant progress yet underscores the necessity for novel approaches to harness the full potential of 3D-LLMs. Hence, with this paper, we aim to chart a course for future research that explores and expands the capabilities of 3D-LLMs in understanding and interacting with the complex 3D world. To support this survey, we have established a project page where papers related to our topic are organized and listed: https://github.com/ActiveVisionLab/Awesome-LLM-3D.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-05-16",
    "authors": [
      {
        "authorId": "2301544160",
        "name": "Xianzheng Ma"
      },
      {
        "authorId": "3469024",
        "name": "Yash Bhalgat"
      },
      {
        "authorId": "2301457194",
        "name": "Brandon Smart"
      },
      {
        "authorId": "2296722898",
        "name": "Shuai Chen"
      },
      {
        "authorId": "2155447960",
        "name": "Xinghui Li"
      },
      {
        "authorId": "2301573180",
        "name": "Jian Ding"
      },
      {
        "authorId": "2301772237",
        "name": "Jindong Gu"
      },
      {
        "authorId": "73286206",
        "name": "Dave Zhenyu Chen"
      },
      {
        "authorId": "2267222520",
        "name": "Songyou Peng"
      },
      {
        "authorId": "3459992",
        "name": "Jiawang Bian"
      },
      {
        "authorId": "2257347697",
        "name": "Philip Torr"
      },
      {
        "authorId": "2300182737",
        "name": "Marc Pollefeys"
      },
      {
        "authorId": "2264249647",
        "name": "Matthias Nießner"
      },
      {
        "authorId": "2291030304",
        "name": "Ian D Reid"
      },
      {
        "authorId": "2301499896",
        "name": "Angel X. Chang"
      },
      {
        "authorId": "3422200",
        "name": "Iro Laina"
      },
      {
        "authorId": "2243268729",
        "name": "V. Prisacariu"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "f75f401f046d508753d6b207f3f19414f489bd08",
    "url": "https://www.semanticscholar.org/paper/f75f401f046d508753d6b207f3f19414f489bd08",
    "title": "Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia",
    "abstract": "Large Language Models (LLMs) have become prevalent across diverse sectors, transforming human life with their extraordinary reasoning and comprehension abilities. As they find increased use in sensitive tasks, safety concerns have gained widespread attention. Extensive efforts have been dedicated to aligning LLMs with human moral principles to ensure their safe deployment. Despite their potential, recent research indicates aligned LLMs are prone to specialized jailbreaking prompts that bypass safety measures to elicit violent and harmful content. The intrinsic discrete nature and substantial scale of contemporary LLMs pose significant challenges in automatically generating diverse, efficient, and potent jailbreaking prompts, representing a continuous obstacle. In this paper, we introduce RIPPLE (Rapid Optimization via Subconscious Exploitation and Echopraxia), a novel optimization-based method inspired by two psychological concepts: subconsciousness and echopraxia, which describe the processes of the mind that occur without conscious awareness and the involuntary mimicry of actions, respectively. Evaluations across 6 open-source LLMs and 4 commercial LLM APIs show RIPPLE achieves an average Attack Success Rate of 91.5\\%, outperforming five current methods by up to 47.0\\% with an 8x reduction in overhead. Furthermore, it displays significant transferability and stealth, successfully evading established detection mechanisms. The code of our work is available at \\url{https://github.com/SolidShen/RIPPLE_official/tree/official}",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-08",
    "authors": [
      {
        "authorId": "2052467415",
        "name": "Guangyu Shen"
      },
      {
        "authorId": "46378881",
        "name": "Siyuan Cheng"
      },
      {
        "authorId": "2277991524",
        "name": "Kai-xian Zhang"
      },
      {
        "authorId": "48927894",
        "name": "Guanhong Tao"
      },
      {
        "authorId": "2072528961",
        "name": "Shengwei An"
      },
      {
        "authorId": "2230321240",
        "name": "Lu Yan"
      },
      {
        "authorId": "2220690905",
        "name": "Zhuo Zhang"
      },
      {
        "authorId": "2268169217",
        "name": "Shiqing Ma"
      },
      {
        "authorId": "2283345552",
        "name": "Xiangyu Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "b6b6e59f3bfdda9d4a4dfe56c46b30706fd18cf3",
    "url": "https://www.semanticscholar.org/paper/b6b6e59f3bfdda9d4a4dfe56c46b30706fd18cf3",
    "title": "Enhance Reasoning for Large Language Models in the Game Werewolf",
    "abstract": "This paper presents an innovative framework that integrates Large Language Models (LLMs) with an external Thinker module to enhance the reasoning capabilities of LLM-based agents. Unlike augmenting LLMs with prompt engineering, Thinker directly harnesses knowledge from databases and employs various optimization techniques. The framework forms a reasoning hierarchy where LLMs handle intuitive System-1 tasks such as natural language processing, while the Thinker focuses on cognitive System-2 tasks that require complex logical analysis and domain-specific knowledge. Our framework is presented using a 9-player Werewolf game that demands dual-system reasoning. We introduce a communication protocol between LLMs and the Thinker, and train the Thinker using data from 18800 human sessions and reinforcement learning. Experiments demonstrate the framework's effectiveness in deductive reasoning, speech generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to surpass GPT4 when integrated with the Thinker. This paper also contributes the largest dataset for social deduction games to date.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-04",
    "authors": [
      {
        "authorId": "2175444207",
        "name": "Shuang Wu"
      },
      {
        "authorId": "2282546879",
        "name": "Liwen Zhu"
      },
      {
        "authorId": "2282675316",
        "name": "Tao Yang"
      },
      {
        "authorId": "2282607485",
        "name": "Shiwei Xu"
      },
      {
        "authorId": "2276208041",
        "name": "Qiang Fu"
      },
      {
        "authorId": "2327286620",
        "name": "Yang Wei"
      },
      {
        "authorId": "2268469",
        "name": "Haobo Fu"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "66d98dc2aad17c03532dbae21d05f098257cc2e2",
    "url": "https://www.semanticscholar.org/paper/66d98dc2aad17c03532dbae21d05f098257cc2e2",
    "title": "LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers",
    "abstract": "Logical reasoning, i.e., deductively inferring the truth value of a conclusion from a set of premises, is an important task for artificial intelligence with wide potential impacts on science, mathematics, and society. While many prompting-based strategies have been proposed to enable Large Language Models (LLMs) to do such reasoning more effectively, they still appear unsatisfactory, often failing in subtle and unpredictable ways. In this work, we investigate the validity of instead reformulating such tasks as modular neurosymbolic programming, which we call LINC: Logical Inference via Neurosymbolic Computation. In LINC, the LLM acts as a semantic parser, translating premises and conclusions from natural language to expressions in first-order logic. These expressions are then offloaded to an external theorem prover, which symbolically performs deductive inference. Leveraging this approach, we observe significant performance gains on FOLIO and a balanced subset of ProofWriter for three different models in nearly all experimental conditions we evaluate. On ProofWriter, augmenting the comparatively small open-source StarCoder+ (15.5B parameters) with LINC even outperforms GPT-3.5 and GPT-4 with Chain-of-Thought (CoT) prompting by an absolute 38% and 10%, respectively. When used with GPT-4, LINC scores 26% higher than CoT on ProofWriter while performing comparatively on FOLIO. Further analysis reveals that although both methods on average succeed roughly equally often on this dataset, they exhibit distinct and complementary failure modes. We thus provide promising evidence for how logical reasoning over natural language can be tackled through jointly leveraging LLMs alongside symbolic provers. All corresponding code is publicly available at https://github.com/benlipkin/linc",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 69,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.emnlp-main.313.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-23",
    "authors": [
      {
        "authorId": "1689656957",
        "name": "Theo X. Olausson"
      },
      {
        "authorId": "2261364306",
        "name": "Alex Gu"
      },
      {
        "authorId": "2098806399",
        "name": "Benjamin Lipkin"
      },
      {
        "authorId": "152592091",
        "name": "Cedegao Zhang"
      },
      {
        "authorId": "2117757908",
        "name": "Armando Solar-Lezama"
      },
      {
        "authorId": "2259915877",
        "name": "Josh Tenenbaum"
      },
      {
        "authorId": "2261363345",
        "name": "Roger Levy"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.7274286307404
  },
  {
    "paperId": "5c7a21e9262b62f0a27fefdc8b1270dfdcbd3912",
    "url": "https://www.semanticscholar.org/paper/5c7a21e9262b62f0a27fefdc8b1270dfdcbd3912",
    "title": "The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction",
    "abstract": "Transformer-based Large Language Models (LLMs) have become a fixture in modern machine learning. Correspondingly, significant resources are allocated towards research that aims to further advance this technology, typically resulting in models of increasing size that are trained on increasing amounts of data. This work, however, demonstrates the surprising result that it is often possible to significantly improve the performance of LLMs by selectively removing higher-order components of their weight matrices. This simple intervention, which we call LAyer-SElective Rank reduction (LASER), can be done on a model after training has completed, and requires no additional parameters or data. We show extensive experiments demonstrating the generality of this finding across language models and datasets, and provide in-depth analyses offering insights into both when LASER is effective and the mechanism by which it operates.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 68,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-21",
    "authors": [
      {
        "authorId": "50465425",
        "name": "Pratyusha Sharma"
      },
      {
        "authorId": "2275600361",
        "name": "Jordan T. Ash"
      },
      {
        "authorId": "2273648018",
        "name": "Dipendra Misra"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.5115975689589
  },
  {
    "paperId": "b31cb3d535af74fc910c42a4dd6ef0f73f8a7385",
    "url": "https://www.semanticscholar.org/paper/b31cb3d535af74fc910c42a4dd6ef0f73f8a7385",
    "title": "VISA: Reasoning Video Object Segmentation via Large Language Models",
    "abstract": "Existing Video Object Segmentation (VOS) relies on explicit user instructions, such as categories, masks, or short phrases, restricting their ability to perform complex video segmentation requiring reasoning with world knowledge. In this paper, we introduce a new task, Reasoning Video Object Segmentation (ReasonVOS). This task aims to generate a sequence of segmentation masks in response to implicit text queries that require complex reasoning abilities based on world knowledge and video contexts, which is crucial for structured environment understanding and object-centric interactions, pivotal in the development of embodied AI. To tackle ReasonVOS, we introduce VISA (Video-based large language Instructed Segmentation Assistant), to leverage the world knowledge reasoning capabilities of multi-modal LLMs while possessing the ability to segment and track objects in videos with a mask decoder. Moreover, we establish a comprehensive benchmark consisting of 35,074 instruction-mask sequence pairs from 1,042 diverse videos, which incorporates complex world knowledge reasoning into segmentation tasks for instruction-tuning and evaluation purposes of ReasonVOS models. Experiments conducted on 8 datasets demonstrate the effectiveness of VISA in tackling complex reasoning segmentation and vanilla referring segmentation in both video and image domains. The code and dataset are available at https://github.com/cilinyan/VISA.",
    "venue": "European Conference on Computer Vision",
    "year": 2024,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-16",
    "authors": [
      {
        "authorId": "2292038285",
        "name": "Cilin Yan"
      },
      {
        "authorId": "2269702782",
        "name": "Haochen Wang"
      },
      {
        "authorId": "2309422649",
        "name": "Shilin Yan"
      },
      {
        "authorId": "2290971123",
        "name": "Xiaolong Jiang"
      },
      {
        "authorId": "2309080041",
        "name": "Yao Hu"
      },
      {
        "authorId": "2304179210",
        "name": "Guoliang Kang"
      },
      {
        "authorId": "2304719201",
        "name": "Weidi Xie"
      },
      {
        "authorId": "2304222",
        "name": "E. Gavves"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "f3bcb9395d8b912361cc534f00e837c832000150",
    "url": "https://www.semanticscholar.org/paper/f3bcb9395d8b912361cc534f00e837c832000150",
    "title": "LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games",
    "abstract": "There is a growing interest in using Large Language Models (LLMs) as agents to tackle real-world tasks that may require assessing complex situations. Yet, we have a limited understanding of LLMs’ reasoning and decision-making capabilities, partly stemming from a lack of dedicated evaluation benchmarks. As negotiating and compromising are key aspects of our everyday communication and collaboration, we propose using scorable negotiation games as a new evaluation framework for LLMs. We create a testbed of diverse text-based, multi-agent, multi-issue, semantically rich negotiation games, with easily tunable difficulty. To solve the challenge, agents need to have strong arithmetic, inference, exploration, and planning capabilities, while seamlessly integrating them. Via a systematic zero-shot Chain-of-Thought prompting (CoT), we show that agents can negotiate and consistently reach successful deals. We quantify the performance with multiple metrics and observe a large gap between GPT-4 and earlier models. Importantly, we test the generalization to new games and setups. Finally, we show that these games can help evaluate other critical aspects, such as the interaction dynamics between agents in the presence of greedy and adversarial players.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 30,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.17234",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1383113350",
        "name": "Sahar Abdelnabi"
      },
      {
        "authorId": "2249532110",
        "name": "Amr Gomaa"
      },
      {
        "authorId": "31215717",
        "name": "S. Sivaprasad"
      },
      {
        "authorId": "2283490770",
        "name": "Lea Schönherr"
      },
      {
        "authorId": "2249532235",
        "name": "Mario Fritz"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.50980806727719
  },
  {
    "paperId": "396ea10d3ab89da41d02693d7165c4b98ecbb5f3",
    "url": "https://www.semanticscholar.org/paper/396ea10d3ab89da41d02693d7165c4b98ecbb5f3",
    "title": "Puzzle Solving using Reasoning of Large Language Models: A Survey",
    "abstract": "Exploring the capabilities of Large Language Models (LLMs) in puzzle solving unveils critical insights into their potential and challenges in AI, marking a significant step towards understanding their applicability in complex reasoning tasks. This survey leverages a unique taxonomy—dividing puzzles into rule-based and rule-less categories—to critically assess LLMs through various methodologies, including prompting techniques, neuro-symbolic approaches, and fine-tuning. Through a critical review of relevant datasets and benchmarks, we assess LLMs’ performance, identifying significant challenges in complex puzzle scenarios. Our findings highlight the disparity between LLM capabilities and human-like reasoning, particularly in those requiring advanced logical inference. The survey underscores the necessity for novel strategies and richer datasets to advance LLMs’ puzzle-solving proficiency and contribute to AI’s logical reasoning and creative problem-solving advancements.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 14,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference",
      "Review"
    ],
    "publicationDate": "2024-02-17",
    "authors": [
      {
        "authorId": "2284693742",
        "name": "Panagiotis Giadikiaroglou"
      },
      {
        "authorId": "2184294391",
        "name": "Maria Lymperaiou"
      },
      {
        "authorId": "2080432906",
        "name": "Giorgos Filandrianos"
      },
      {
        "authorId": "1719165",
        "name": "G. Stamou"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "a3c1f6809dd1455da73eec407bcb3be92e680112",
    "url": "https://www.semanticscholar.org/paper/a3c1f6809dd1455da73eec407bcb3be92e680112",
    "title": "Penetrative AI: Making LLMs Comprehend the Physical World",
    "abstract": "Recent developments in Large Language Models (LLMs) have demonstrated their remarkable capabilities across a range of tasks. Questions, however, persist about the nature of LLMs and their potential to integrate common-sense human knowledge when performing tasks involving information about the real physical world. This paper delves into these questions by exploring how LLMs can be extended to interact with and reason about the physical world through IoT sensors and actuators, a concept that we term \"Penetrative AI\". The paper explores such an extension at two levels of LLMs' ability to penetrate into the physical world via the processing of sensory signals. Our preliminary findings indicate that LLMs, with ChatGPT being the representative example in our exploration, have considerable and unique proficiency in employing the embedded world knowledge for interpreting IoT sensor data and reasoning over them about tasks in the physical realm. Not only this opens up new applications for LLMs beyond traditional text-based tasks, but also enables new ways of incorporating human knowledge in cyber-physical systems.",
    "venue": "Workshop on Mobile Computing Systems and Applications",
    "year": 2023,
    "citationCount": 27,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.09605",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2023-10-14",
    "authors": [
      {
        "authorId": "2249524699",
        "name": "Huatao Xu"
      },
      {
        "authorId": "2218768664",
        "name": "Liying Han"
      },
      {
        "authorId": "2306147809",
        "name": "Qirui Yang"
      },
      {
        "authorId": "2249143354",
        "name": "Mo Li"
      },
      {
        "authorId": "2258713830",
        "name": "Mani Srivastava"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.98306765262805
  },
  {
    "paperId": "6e478f94b2942c4dcd50f3dabc18f61c7821d3c0",
    "url": "https://www.semanticscholar.org/paper/6e478f94b2942c4dcd50f3dabc18f61c7821d3c0",
    "title": "Llama meets EU: Investigating the European political spectrum through the lens of LLMs",
    "abstract": "Instruction-finetuned Large Language Models inherit clear political leanings that have been shown to influence downstream task performance. We expand this line of research beyond the two-party system in the US and audit Llama Chat in the context of EU politics in various settings to analyze the model’s political knowledge and its ability to reason in context. We adapt, i.e., further fine-tune, Llama Chat on speeches of individual euro-parties from debates in the European Parliament to reevaluate its political leaning based on the EUandI questionnaire. Llama Chat shows considerable knowledge of national parties’ positions and is capable of reasoning in context. The adapted, party-specific, models are substantially re-aligned towards respective positions which we see as a starting point for using chat-based LLMs as data-driven conversational engines to assist research in political science.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-20",
    "authors": [
      {
        "authorId": "2125376289",
        "name": "Ilias Chalkidis"
      },
      {
        "authorId": "2259939211",
        "name": "Stephanie Brandl"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "d58f4d7eec0ffdc8f0b1b1ca61dd1586073553ee",
    "url": "https://www.semanticscholar.org/paper/d58f4d7eec0ffdc8f0b1b1ca61dd1586073553ee",
    "title": "E^5: Zero-shot Hierarchical Table Analysis using Augmented LLMs via Explain, Extract, Execute, Exhibit and Extrapolate",
    "abstract": "Analyzing large hierarchical tables with multi-level headers presents challenges due to their complex structure, implicit semantics, and calculation relationships. While recent advancements in large language models (LLMs) have shown promise in flat table analysis, their application to hierarchical tables is constrained by the reliance on manually curated exemplars and the model’s token capacity limitations. Addressing these challenges, we introduce a novel code-augmented LLM-based framework, E^5, for zero-shot hierarchical table question answering. This approach encompasses self-explaining the table’s hierarchical structures, code generation to extract relevant information and apply operations, external code execution to prevent hallucinations, and leveraging LLMs’ reasoning for final answer derivation. Empirical results indicate that our method, based on GPT-4, outperforms state-of-the-art fine-tuning methods with a 44.38 Exact Match improvement. Furthermore, we present F^3, an adaptive algorithm designed for token-limited scenarios, effectively condensing large tables while maintaining useful information. Our experiments prove its efficiency, enabling the processing of large tables even with models having limited context lengths. The code is available at https://github.com/zzh-SJTU/E5-Hierarchical-Table-Analysis.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2273535937",
        "name": "Zhehao Zhang"
      },
      {
        "authorId": "152673873",
        "name": "Yan Gao"
      },
      {
        "authorId": "2257431559",
        "name": "Jian-Guang Lou"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "6cd1b99ec6d399a682b01e6fe9096e2fcf450862",
    "url": "https://www.semanticscholar.org/paper/6cd1b99ec6d399a682b01e6fe9096e2fcf450862",
    "title": "EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents",
    "abstract": "Recent SOTA approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment. Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive. Instead of directly employing LLMs as agents, can we use LLMs' reasoning capabilities to adaptively create training environments to help smaller RL agents learn useful skills that they are weak at? We propose EnvGen, a novel framework to address this question. We first prompt an LLM to generate training environments by giving it the task description and simulator objectives that the agents should learn and then asking it to generate a set of environment configurations (e.g., different terrains, items initially given to agents, etc.). Next, we train a small RL agent in a mixture of the original and LLM-generated environments. Then, we enable the LLM to continuously adapt the generated environments to progressively improve the skills that the agent is weak at, by providing feedback to the LLM in the form of the agent's performance. We demonstrate the usefulness of EnvGen with comprehensive experiments in Crafter and Heist environments. We find that a small RL agent trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and learns long-horizon tasks significantly faster. We also show that using an LLM to adapt environments dynamically outperforms curriculum learning approaches and how the environments are adapted to help improve RL agents' weaker skills over time. Additionally, EnvGen is substantially more efficient as it only uses a small number of LLM calls (e.g., 4 in total), whereas LLM agents require thousands of calls. Lastly, we present detailed ablation studies for EnvGen design choices.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-18",
    "authors": [
      {
        "authorId": "2008198436",
        "name": "Abhaysinh Zala"
      },
      {
        "authorId": "2706729",
        "name": "Jaemin Cho"
      },
      {
        "authorId": "2260201860",
        "name": "Han Lin"
      },
      {
        "authorId": "13563486",
        "name": "Jaehong Yoon"
      },
      {
        "authorId": "2276608813",
        "name": "Mohit Bansal"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "4d7ba29d00d7497f7c74a2f2556e70e9af50645f",
    "url": "https://www.semanticscholar.org/paper/4d7ba29d00d7497f7c74a2f2556e70e9af50645f",
    "title": "medIKAL: Integrating Knowledge Graphs as Assistants of LLMs for Enhanced Clinical Diagnosis on EMRs",
    "abstract": "Electronic Medical Records (EMRs), while integral to modern healthcare, present challenges for clinical reasoning and diagnosis due to their complexity and information redundancy. To address this, we proposed medIKAL (Integrating Knowledge Graphs as Assistants of LLMs), a framework that combines Large Language Models (LLMs) with knowledge graphs (KGs) to enhance diagnostic capabilities. medIKAL assigns weighted importance to entities in medical records based on their type, enabling precise localization of candidate diseases within KGs. It innovatively employs a residual network-like approach, allowing initial diagnosis by the LLM to be merged into KG search results. Through a path-based reranking algorithm and a fill-in-the-blank style prompt template, it further refined the diagnostic process. We validated medIKAL's effectiveness through extensive experiments on a newly introduced open-sourced Chinese EMR dataset, demonstrating its potential to improve clinical diagnosis in real-world settings.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-20",
    "authors": [
      {
        "authorId": "2259951774",
        "name": "Mingyi Jia"
      },
      {
        "authorId": "2259993319",
        "name": "Junwen Duan"
      },
      {
        "authorId": "2307592656",
        "name": "Yan Song"
      },
      {
        "authorId": "2199815721",
        "name": "Jianxin Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "bd370edcfe7020663a05649e6f37d1c963dcdf80",
    "url": "https://www.semanticscholar.org/paper/bd370edcfe7020663a05649e6f37d1c963dcdf80",
    "title": "LLMs Know What They Need: Leveraging a Missing Information Guided Framework to Empower Retrieval-Augmented Generation",
    "abstract": "Retrieval-Augmented Generation (RAG) demonstrates great value in alleviating outdated knowledge or hallucination by supplying LLMs with updated and relevant knowledge. However, there are still several difficulties for RAG in understanding complex multi-hop query and retrieving relevant documents, which require LLMs to perform reasoning and retrieve step by step. Inspired by human's reasoning process in which they gradually search for the required information, it is natural to ask whether the LLMs could notice the missing information in each reasoning step. In this work, we first experimentally verified the ability of LLMs to extract information as well as to know the missing. Based on the above discovery, we propose a Missing Information Guided Retrieve-Extraction-Solving paradigm (MIGRES), where we leverage the identification of missing information to generate a targeted query that steers the subsequent knowledge retrieval. Besides, we design a sentence-level re-ranking filtering approach to filter the irrelevant content out from document, along with the information extraction capability of LLMs to extract useful information from cleaned-up documents, which in turn to bolster the overall efficacy of RAG. Extensive experiments conducted on multiple public datasets reveal the superiority of the proposed MIGRES method, and analytical experiments demonstrate the effectiveness of our proposed modules.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-22",
    "authors": [
      {
        "authorId": "2234024810",
        "name": "Keheng Wang"
      },
      {
        "authorId": "2290402867",
        "name": "Feiyu Duan"
      },
      {
        "authorId": "2298213582",
        "name": "Peiguang Li"
      },
      {
        "authorId": "2295934207",
        "name": "Sirui Wang"
      },
      {
        "authorId": "2290035990",
        "name": "Xunliang Cai"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "32d59939895b2437f17688625804f2be0e632871",
    "url": "https://www.semanticscholar.org/paper/32d59939895b2437f17688625804f2be0e632871",
    "title": "How Secure Are Large Language Models (LLMs) for Navigation in Urban Environments?",
    "abstract": "In the field of robotics and automation, navigation systems based on Large Language Models (LLMs) have recently shown impressive performance. However, the security aspects of these systems have received relatively less attention. This paper pioneers the exploration of vulnerabilities in LLM-based navigation models in urban outdoor environments, a critical area given the technology's widespread application in autonomous driving, logistics, and emergency services. Specifically, we introduce a novel Navigational Prompt Suffix (NPS) Attack that manipulates LLM-based navigation models by appending gradient-derived suffixes to the original navigational prompt, leading to incorrect actions. We conducted comprehensive experiments on an LLMs-based navigation model that employs various LLMs for reasoning. Our results, derived from the Touchdown and Map2Seq street-view datasets under both few-shot learning and fine-tuning configurations, demonstrate notable performance declines across three metrics in the face of both white-box and black-box attacks. These results highlight the generalizability and transferability of the NPS Attack, emphasizing the need for enhanced security in LLM-based navigation systems. As an initial countermeasure, we propose the Navigational Prompt Engineering (NPE) Defense strategy, concentrating on navigation-relevant keywords to reduce the impact of adversarial suffixes. While initial findings indicate that this strategy enhances navigational safety, there remains a critical need for the wider research community to develop stronger defense methods to effectively tackle the real-world challenges faced by these systems.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-14",
    "authors": [
      {
        "authorId": "2284219932",
        "name": "Congcong Wen"
      },
      {
        "authorId": "2284219964",
        "name": "Jiazhao Liang"
      },
      {
        "authorId": "1491104638",
        "name": "Shuaihang Yuan"
      },
      {
        "authorId": "2263911200",
        "name": "Hao Huang"
      },
      {
        "authorId": "2264345449",
        "name": "Yi Fang"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "3d6300263adb1a1e8000fd0eda55518a3642afa9",
    "url": "https://www.semanticscholar.org/paper/3d6300263adb1a1e8000fd0eda55518a3642afa9",
    "title": "FLAP: Flow-Adhering Planning with Constrained Decoding in LLMs",
    "abstract": "Planning is a crucial task for agents in task oriented dialogs (TODs). Human agents typically resolve user issues by following predefined workflows, decomposing workflow steps into actionable items, and performing actions by executing APIs in order; all of which require reasoning and planning. With the recent advances in LLMs, there have been increasing attempts to use them for task planning and API usage. However, the faithfulness of the plans to predefined workflows and API dependencies, is not guaranteed with LLMs. Moreover, workflows in real life are often custom-defined and prone to changes; hence, adaptation is desirable. To study this, we propose the problem of faithful planning in TODs that needs to resolve user intents by following predefined flows and preserving API dependencies. To solve this problem, we propose \\textbf{FLAP}, a \\textbf{Fl}ow-\\textbf{A}dhering \\textbf{P}lanning algorithm based on constrained decoding with lookahead heuristic for LLMs. Our algorithm alleviates the need for finetuning LLMs using domain specific (plan/dependency) data, enables quick adaptation to predefined flows, and outperforms other decoding and prompting-based baselines. Further, our algorithm empowers smaller LLMs (\\approx7B) to perform at par larger LLMs (\\approx30B-40B).",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-09",
    "authors": [
      {
        "authorId": "2291438644",
        "name": "Shamik Roy"
      },
      {
        "authorId": "40552391",
        "name": "Sailik Sengupta"
      },
      {
        "authorId": "3457102",
        "name": "Daniele Bonadiman"
      },
      {
        "authorId": "39674628",
        "name": "Saab Mansour"
      },
      {
        "authorId": "144877669",
        "name": "Arshit Gupta"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "553c86ba967f55db9caffe08240061b1282da893",
    "url": "https://www.semanticscholar.org/paper/553c86ba967f55db9caffe08240061b1282da893",
    "title": "Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection",
    "abstract": "Despite the promise of RLHF in aligning LLMs with human preferences, it often leads to superficial alignment, prioritizing stylistic changes over improving downstream performance of LLMs. Underspecified preferences could obscure directions to align the models. Lacking exploration restricts identification of desirable outputs to improve the models. To overcome these challenges, we propose a novel framework: Reinforcement Learning from Reflective Feedback (RLRF), which leverages fine-grained feedback based on detailed criteria to improve the core capabilities of LLMs. RLRF employs a self-reflection mechanism to systematically explore and refine LLM responses, then fine-tuning the models via a RL algorithm along with promising responses. Our experiments across Just-Eval, Factuality, and Mathematical Reasoning demonstrate the efficacy and transformative potential of RLRF beyond superficial surface-level adjustment.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-21",
    "authors": [
      {
        "authorId": "79733119",
        "name": "Kyungjae Lee"
      },
      {
        "authorId": "1474356736",
        "name": "Dasol Hwang"
      },
      {
        "authorId": "2282197642",
        "name": "Sunghyun Park"
      },
      {
        "authorId": "2288604723",
        "name": "Youngsoo Jang"
      },
      {
        "authorId": "2269856969",
        "name": "Moontae Lee"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "647f2aa9c120a200b9b91363c5677f7d89b21d4d",
    "url": "https://www.semanticscholar.org/paper/647f2aa9c120a200b9b91363c5677f7d89b21d4d",
    "title": "PTD-SQL: Partitioning and Targeted Drilling with LLMs in Text-to-SQL",
    "abstract": "Large Language Models (LLMs) have emerged as powerful tools for Text-to-SQL tasks, exhibiting remarkable reasoning capabilities. Different from tasks such as math word problem and commonsense reasoning, SQL solutions have a relatively fixed pattern. This facilitates the investigation of whether LLMs can benefit from categorical thinking, mirroring how humans acquire knowledge through inductive reasoning based on comparable examples. In this study, we propose that employing query group partitioning allows LLMs to focus on learning the thought processes specific to a single problem type, consequently enhancing their reasoning abilities across diverse difficulty levels and problem categories. Our experiments reveal that multiple advanced LLMs, when equipped with PTD-SQL, can either surpass or match previous state-of-the-art (SOTA) methods on the Spider and BIRD datasets. Intriguingly, models with varying initial performances have exhibited significant improvements mainly at the boundary of their capabilities after targeted drilling, suggesting a parallel with human progress. Code is available at https://github.com/lrlbbzl/PTD-SQL.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-09-21",
    "authors": [
      {
        "authorId": "2279024030",
        "name": "Ruilin Luo"
      },
      {
        "authorId": "1390770350",
        "name": "Liyuan Wang"
      },
      {
        "authorId": "3186130",
        "name": "Binghuai Lin"
      },
      {
        "authorId": "2279171100",
        "name": "Zicheng Lin"
      },
      {
        "authorId": "2284727148",
        "name": "Yujiu Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "44c7040c47f8d55fe77e815831170211eae57099",
    "url": "https://www.semanticscholar.org/paper/44c7040c47f8d55fe77e815831170211eae57099",
    "title": "KNowNEt:Guided Health Information Seeking from LLMs via Knowledge Graph Integration",
    "abstract": "The increasing reliance on Large Language Models (LLMs) for health information seeking can pose severe risks due to the potential for misinformation and the complexity of these topics. This paper introduces KnowNet a visualization system that integrates LLMs with Knowledge Graphs (KG) to provide enhanced accuracy and structured exploration. Specifically, for enhanced accuracy, KnowNet extracts triples (e.g., entities and their relations) from LLM outputs and maps them into the validated information and supported evidence in external KGs. For structured exploration, KnowNet provides next-step recommendations based on the neighborhood of the currently explored entities in KGs, aiming to guide a comprehensive understanding without overlooking critical aspects. To enable reasoning with both the structured data in KGs and the unstructured outputs from LLMs, KnowNet conceptualizes the understanding of a subject as the gradual construction of graph visualization. A progressive graph visualization is introduced to monitor past inquiries, and bridge the current query with the exploration history and next-step recommendations. We demonstrate the effectiveness of our system via use cases and expert interviews.",
    "venue": "IEEE Transactions on Visualization and Computer Graphics",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-18",
    "authors": [
      {
        "authorId": "2312004219",
        "name": "Youfu Yan"
      },
      {
        "authorId": "2264056306",
        "name": "Yu Hou"
      },
      {
        "authorId": "2274091250",
        "name": "Yongkang Xiao"
      },
      {
        "authorId": "2210734160",
        "name": "Rui Zhang"
      },
      {
        "authorId": "49110486",
        "name": "Qianwen Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.79441541679836
  },
  {
    "paperId": "e8ef63be9a9a07e55656bd6372cd96cc4ef37f90",
    "url": "https://www.semanticscholar.org/paper/e8ef63be9a9a07e55656bd6372cd96cc4ef37f90",
    "title": "Anomaly Detection of Tabular Data Using LLMs",
    "abstract": "Large language models (LLMs) have shown their potential in long-context understanding and mathematical reasoning. In this paper, we study the problem of using LLMs to detect tabular anomalies and show that pre-trained LLMs are zero-shot batch-level anomaly detectors. That is, without extra distribution-specific model fitting, they can discover hidden outliers in a batch of data, demonstrating their ability to identify low-density data regions. For LLMs that are not well aligned with anomaly detection and frequently output factual errors, we apply simple yet effective data-generating processes to simulate synthetic batch-level anomaly detection datasets and propose an end-to-end fine-tuning strategy to bring out the potential of LLMs in detecting real anomalies. Experiments on a large anomaly detection benchmark (ODDS) showcase i) GPT-4 has on-par performance with the state-of-the-art transductive learning-based anomaly detection methods and ii) the efficacy of our synthetic dataset and fine-tuning strategy in aligning LLMs to this task.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-24",
    "authors": [
      {
        "authorId": "27541643",
        "name": "Aodong Li"
      },
      {
        "authorId": "2308072767",
        "name": "Yunhan Zhao"
      },
      {
        "authorId": "1738476505",
        "name": "Chen Qiu"
      },
      {
        "authorId": "2749512",
        "name": "M. Kloft"
      },
      {
        "authorId": "1630595717",
        "name": "P. Smyth"
      },
      {
        "authorId": "2258717579",
        "name": "Maja Rudolph"
      },
      {
        "authorId": "1783468",
        "name": "S. Mandt"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "a42818ecc9a187862301fdad1e9c24adb6d985cb",
    "url": "https://www.semanticscholar.org/paper/a42818ecc9a187862301fdad1e9c24adb6d985cb",
    "title": "ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs",
    "abstract": "The integration of Large Language Models (LLMs) and knowledge graphs (KGs) has achieved remarkable success in various natural language processing tasks. However, existing methodologies that integrate LLMs and KGs often navigate the task-solving process solely based on the LLM's analysis of the question, overlooking the rich cognitive potential inherent in the vast knowledge encapsulated in KGs. To address this, we introduce Observation-Driven Agent (ODA), a novel AI agent framework tailored for tasks involving KGs. ODA incorporates KG reasoning abilities via global observation, which enhances reasoning capabilities through a cyclical paradigm of observation, action, and reflection. Confronting the exponential explosion of knowledge during observation, we innovatively design a recursive observation mechanism. Subsequently, we integrate the observed knowledge into the action and reflection modules. Through extensive experiments, ODA demonstrates state-of-the-art performance on several datasets, notably achieving accuracy improvements of 12.87% and 8.9%.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-11",
    "authors": [
      {
        "authorId": "2296235698",
        "name": "Lei Sun"
      },
      {
        "authorId": "2295990319",
        "name": "Zhengwei Tao"
      },
      {
        "authorId": "2296236608",
        "name": "Youdi Li"
      },
      {
        "authorId": "2295988191",
        "name": "Hiroshi Arakawa"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "8154fb1d828cdc390dc1fa442d84034948679c47",
    "url": "https://www.semanticscholar.org/paper/8154fb1d828cdc390dc1fa442d84034948679c47",
    "title": "Question Decomposition Improves the Faithfulness of Model-Generated Reasoning",
    "abstract": "As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model's stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithfulness of model-generated reasoning over CoT, while still achieving some of the performance gains of CoT. Our results show it is possible to improve the faithfulness of model-generated reasoning; continued improvements may lead to reasoning that enables us to verify the correctness and safety of LLM behavior.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 68,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.11768",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-17",
    "authors": [
      {
        "authorId": "2224616677",
        "name": "Ansh Radhakrishnan"
      },
      {
        "authorId": "2196759978",
        "name": "Karina Nguyen"
      },
      {
        "authorId": "2111073313",
        "name": "Anna Chen"
      },
      {
        "authorId": "2109064358",
        "name": "Carol Chen"
      },
      {
        "authorId": "1780754598",
        "name": "Carson E. Denison"
      },
      {
        "authorId": "39182747",
        "name": "Danny Hernandez"
      },
      {
        "authorId": "41152329",
        "name": "Esin Durmus"
      },
      {
        "authorId": "146614650",
        "name": "Evan Hubinger"
      },
      {
        "authorId": "1583434563",
        "name": "John Kernion"
      },
      {
        "authorId": "2161242438",
        "name": "Kamil.e Lukovsiut.e"
      },
      {
        "authorId": "15590401",
        "name": "Newton Cheng"
      },
      {
        "authorId": "2117706920",
        "name": "Nicholas Joseph"
      },
      {
        "authorId": "2833768",
        "name": "Nicholas Schiefer"
      },
      {
        "authorId": "2221219447",
        "name": "Oliver Rausch"
      },
      {
        "authorId": "52238703",
        "name": "Sam McCandlish"
      },
      {
        "authorId": "2154609053",
        "name": "S. E. Showk"
      },
      {
        "authorId": "46239941",
        "name": "Tamera Lanham"
      },
      {
        "authorId": "2224618184",
        "name": "Tim Maxwell"
      },
      {
        "authorId": "1728994",
        "name": "V. Chandrasekaran"
      },
      {
        "authorId": "1573482302",
        "name": "Zac Hatfield-Dodds"
      },
      {
        "authorId": "2053807409",
        "name": "Jared Kaplan"
      },
      {
        "authorId": "38732223",
        "name": "J. Brauner"
      },
      {
        "authorId": "1799822",
        "name": "Sam Bowman"
      },
      {
        "authorId": "3439053",
        "name": "Ethan Perez"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.5115975689589
  },
  {
    "paperId": "322412edc4725a46156f408540b3d82a77e623d9",
    "url": "https://www.semanticscholar.org/paper/322412edc4725a46156f408540b3d82a77e623d9",
    "title": "Misinforming LLMs: vulnerabilities, challenges and opportunities",
    "abstract": "Large Language Models (LLMs) have made significant advances in natural language processing, but their underlying mechanisms are often misunderstood. Despite exhibiting coherent answers and apparent reasoning behaviors, LLMs rely on statistical patterns in word embeddings rather than true cognitive processes. This leads to vulnerabilities such as\"hallucination\"and misinformation. The paper argues that current LLM architectures are inherently untrustworthy due to their reliance on correlations of sequential patterns of word embedding vectors. However, ongoing research into combining generative transformer-based models with fact bases and logic programming languages may lead to the development of trustworthy LLMs capable of generating statements based on given truth and explaining their self-reasoning process.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-02",
    "authors": [
      {
        "authorId": "145465576",
        "name": "Bo Zhou"
      },
      {
        "authorId": "2219274591",
        "name": "Daniel Geissler"
      },
      {
        "authorId": "143951813",
        "name": "P. Lukowicz"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "bb991838e419a3cdd8617bb6384a95f0360113a4",
    "url": "https://www.semanticscholar.org/paper/bb991838e419a3cdd8617bb6384a95f0360113a4",
    "title": "Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize Encoded Knowledge",
    "abstract": "Large Language Models (LLMs) have recently showcased remarkable generalizability in various domains. Despite their extensive knowledge, LLMs still face challenges in efficiently utilizing encoded knowledge to develop accurate and logical reasoning processes. To mitigate this problem, we introduced Hint-before-Solving Prompting (HSP), which guides the model to generate hints (e.g., specific knowledge or key ideas) for solving the problem and then generate solutions containing intermediate reasoning steps. Since HSP is orthogonal to prompting methods (e.g., Chain-of-Thought (CoT)), we applied HSP to CoT, Least-to-Most, Plan-and-Solve, and Standard promptings. The results of extensive experiments on 6 reasoning benchmarks and 4 open-source LLMs demonstrate that HSP can effectively improve the accuracy of reasoning tasks: (1) By applying high-quality hint-enhanced HSP to CoT prompting, Llama2-70B-Chat shows an improvement of 9.7. (2) Beyond exploring training-free LLM capabilities, we built the HSPMATH dataset based on HSP and fine-tuned Llemma-7B, reaching 64.3 accuracy, surpassing GPT-3.5 and WizardMath-13B. We make our code and dataset publicly available at \\url{https://github.com/jinlanfu/HSP}.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-22",
    "authors": [
      {
        "authorId": "2243662006",
        "name": "Jinlan Fu"
      },
      {
        "authorId": "2284985640",
        "name": "Shenzhen Huangfu"
      },
      {
        "authorId": "146948229",
        "name": "Hang Yan"
      },
      {
        "authorId": "2242887615",
        "name": "See-Kiong Ng"
      },
      {
        "authorId": "2256661980",
        "name": "Xipeng Qiu"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "8f7786bdda7b9341c8f1989a6810f2b7d084a70a",
    "url": "https://www.semanticscholar.org/paper/8f7786bdda7b9341c8f1989a6810f2b7d084a70a",
    "title": "A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment",
    "abstract": "As general-purpose tools, Large Language Models (LLMs) must often reason about everyday physical environments. In a question-and-answer capacity, understanding the interactions of physical objects may be necessary to give appropriate responses. Moreover, LLMs are increasingly used as reasoning engines in agentic systems, designing and controlling their action sequences. The vast majority of research has tackled this issue using static benchmarks, comprised of text or image-based questions about the physical world. However, these benchmarks do not capture the complexity and nuance of real-life physical processes. Here we advocate for a second, relatively unexplored, approach: 'embodying' the LLMs by granting them control of an agent within a 3D environment. We present the first embodied and cognitively meaningful evaluation of physical common-sense reasoning in LLMs. Our framework allows direct comparison of LLMs with other embodied agents, such as those based on Deep Reinforcement Learning, and human and non-human animals. We employ the Animal-AI (AAI) environment, a simulated 3D virtual laboratory, to study physical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a suite of experiments that replicate laboratory studies with non-human animals, to study physical reasoning capabilities including distance estimation, tracking out-of-sight objects, and tool use. We demonstrate that state-of-the-art multi-modal models with no finetuning can complete this style of task, allowing meaningful comparison to the entrants of the 2019 Animal-AI Olympics competition and to human children. Our results show that LLMs are currently outperformed by human children on these tasks. We argue that this approach allows the study of physical reasoning using ecologically valid experiments drawn directly from cognitive science, improving the predictability and reliability of LLMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-30",
    "authors": [
      {
        "authorId": "2328310696",
        "name": "Matteo G. Mecattaf"
      },
      {
        "authorId": "2328309782",
        "name": "Ben Slater"
      },
      {
        "authorId": "2328309433",
        "name": "Marko Tevsi'c"
      },
      {
        "authorId": "2328307343",
        "name": "Jonathan Prunty"
      },
      {
        "authorId": "1397160953",
        "name": "Konstantinos Voudouris"
      },
      {
        "authorId": "2243337546",
        "name": "Lucy G. Cheke"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "f4b021d6aa7a00c64e7cf88c4f9096c8c42e1540",
    "url": "https://www.semanticscholar.org/paper/f4b021d6aa7a00c64e7cf88c4f9096c8c42e1540",
    "title": "Large Language Models are Good Multi-lingual Learners : When LLMs Meet Cross-lingual Prompts",
    "abstract": "With the advent of Large Language Models (LLMs), generating rule-based data for real-world applications has become more accessible. Due to the inherent ambiguity of natural language and the complexity of rule sets, especially in long contexts, LLMs often struggle to follow all specified rules, frequently omitting at least one. To enhance the reasoning and understanding of LLMs on long and complex contexts, we propose a novel prompting strategy Multi-Lingual Prompt, namely MLPrompt, which automatically translates the error-prone rule that an LLM struggles to follow into another language, thus drawing greater attention to it. Experimental results on public datasets across various tasks have shown MLPrompt can outperform state-of-the-art prompting methods such as Chain of Thought, Tree of Thought, and Self-Consistency. Additionally, we introduce a framework integrating MLPrompt with an auto-checking mechanism for structured data generation, with a specific case study in text-to-MIP instances. Further, we extend the proposed framework for text-to-SQL to demonstrate its generation ability towards structured data synthesis.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-09-17",
    "authors": [
      {
        "authorId": "2320302662",
        "name": "Teng Wang"
      },
      {
        "authorId": "2321656322",
        "name": "Zhenqi He"
      },
      {
        "authorId": "2320304512",
        "name": "Wing-Yin Yu"
      },
      {
        "authorId": "2221337060",
        "name": "Xiaojin Fu"
      },
      {
        "authorId": "2148635550",
        "name": "Xiongwei Han"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "be974844cd1e5a441fcfebcff62f72e48af46f63",
    "url": "https://www.semanticscholar.org/paper/be974844cd1e5a441fcfebcff62f72e48af46f63",
    "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
    "abstract": "In recent years, large language models (LLMs) have spurred a new research paradigm in natural language processing. Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application. The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability. Unfortunately, Retraining LLMs repeatedly to eliminate undesirable knowledge is impractical due to their immense parameters. Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge in an efficient manner, without affecting unrelated knowledge in the model. To this end, we provide a survey of knowledge unlearning in the era of LLMs. Firstly, we formally define the knowledge unlearning problem and distinguish it from related works. Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning, and introduce details of these unlearning methods. We further present evaluation datasets used in existing methods, and finally conclude this survey by presenting the ongoing challenges and future directions.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 22,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-11-27",
    "authors": [
      {
        "authorId": "73502630",
        "name": "Nianwen Si"
      },
      {
        "authorId": "2154930608",
        "name": "Hao Zhang"
      },
      {
        "authorId": "2116152318",
        "name": "Heyu Chang"
      },
      {
        "authorId": "9047584",
        "name": "Wenlin Zhang"
      },
      {
        "authorId": "2253591545",
        "name": "Dan Qu"
      },
      {
        "authorId": "2268429659",
        "name": "Weiqiang Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 117.03241323893724
  },
  {
    "paperId": "674c5ec7b144aea1f6b143baeb17cc839f52416e",
    "url": "https://www.semanticscholar.org/paper/674c5ec7b144aea1f6b143baeb17cc839f52416e",
    "title": "Fixing Rust Compilation Errors using LLMs",
    "abstract": "The Rust programming language, with its safety guarantees, has established itself as a viable choice for low-level systems programming language over the traditional, unsafe alternatives like C/C++. These guarantees come from a strong ownership-based type system, as well as primitive support for features like closures, pattern matching, etc., that make the code more concise and amenable to reasoning. These unique Rust features also pose a steep learning curve for programmers. This paper presents a tool called RustAssistant that leverages the emergent capabilities of Large Language Models (LLMs) to automatically suggest fixes for Rust compilation errors. RustAssistant uses a careful combination of prompting techniques as well as iteration with an LLM to deliver high accuracy of fixes. RustAssistant is able to achieve an impressive peak accuracy of roughly 74% on real-world compilation errors in popular open-source Rust repositories. We plan to release our dataset of Rust compilation errors to enable further research.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 17,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.05177",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-08-09",
    "authors": [
      {
        "authorId": "2874604",
        "name": "Pantazis Deligiannis"
      },
      {
        "authorId": "1941367",
        "name": "A. Lal"
      },
      {
        "authorId": "1941756666",
        "name": "Nikita Mehrotra"
      },
      {
        "authorId": "3351977",
        "name": "Aseem Rastogi"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.35557636844247
  },
  {
    "paperId": "c7184f9a914dbfbad59faa5aeaf9ba7019dfcf74",
    "url": "https://www.semanticscholar.org/paper/c7184f9a914dbfbad59faa5aeaf9ba7019dfcf74",
    "title": "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph",
    "abstract": "Although large language models (LLMs) have achieved significant success in various tasks, they often struggle with hallucination problems, especially in scenarios requiring deep and responsible reasoning. These issues could be partially addressed by introducing external knowledge graphs (KG) in LLM reasoning. In this paper, we propose a new LLM-KG integrating paradigm ``$\\hbox{LLM}\\otimes\\hbox{KG}$'' which treats the LLM as an agent to interactively explore related entities and relations on KGs and perform reasoning based on the retrieved knowledge. We further implement this paradigm by introducing a new approach called Think-on-Graph (ToG), in which the LLM agent iteratively executes beam search on KG, discovers the most promising reasoning paths, and returns the most likely reasoning results. We use a number of well-designed experiments to examine and illustrate the following advantages of ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has the ability of knowledge traceability and knowledge correctability by leveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible plug-and-play framework for different LLMs, KGs and prompting strategies without any additional training cost; 4) the performance of ToG with small LLM models could exceed large LLM such as GPT-4 in certain scenarios and this reduces the cost of LLM deployment and application. As a training-free method with lower computational cost and better generality, ToG achieves overall SOTA in 6 out of 9 datasets where most previous SOTAs rely on additional training.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 40,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-15",
    "authors": [
      {
        "authorId": "2202454665",
        "name": "Jiashuo Sun"
      },
      {
        "authorId": "2250617116",
        "name": "Chengjin Xu"
      },
      {
        "authorId": "2253747495",
        "name": "Lumingyuan Tang"
      },
      {
        "authorId": "2211588034",
        "name": "Sai Wang"
      },
      {
        "authorId": "2249759096",
        "name": "Chen Lin"
      },
      {
        "authorId": "2254121688",
        "name": "Yeyun Gong"
      },
      {
        "authorId": "2249757348",
        "name": "Lionel M. Ni"
      },
      {
        "authorId": "93596028",
        "name": "H. Shum"
      },
      {
        "authorId": "2188226506",
        "name": "Jian Guo"
      }
    ],
    "source": "semantic_scholar",
    "score": 125.70358100056461
  },
  {
    "paperId": "717392dac099d1506b766787382d61b277863163",
    "url": "https://www.semanticscholar.org/paper/717392dac099d1506b766787382d61b277863163",
    "title": "Better Zero-Shot Reasoning with Self-Adaptive Prompting",
    "abstract": "Modern large language models (LLMs) have demonstrated impressive capabilities at sophisticated tasks, often through step-by-step reasoning similar to humans. This is made possible by their strong few and zero-shot abilities -- they can effectively learn from a handful of handcrafted, completed responses (\"in-context examples\"), or are prompted to reason spontaneously through specially designed triggers. Nonetheless, some limitations have been observed. First, performance in the few-shot setting is sensitive to the choice of examples, whose design requires significant human effort. Moreover, given the diverse downstream tasks of LLMs, it may be difficult or laborious to handcraft per-task labels. Second, while the zero-shot setting does not require handcrafting, its performance is limited due to the lack of guidance to the LLMs. To address these limitations, we propose Consistency-based Self-adaptive Prompting (COSP), a novel prompt design method for LLMs. Requiring neither handcrafted responses nor ground-truth labels, COSP selects and builds the set of examples from the LLM zero-shot outputs via carefully designed criteria that combine consistency, diversity and repetition. In the zero-shot setting for three different LLMs, we show that using only LLM predictions, COSP improves performance up to 15% compared to zero-shot baselines and matches or exceeds few-shot baselines for a range of reasoning tasks.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 43,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.14106",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-23",
    "authors": [
      {
        "authorId": "1470501980",
        "name": "Xingchen Wan"
      },
      {
        "authorId": "2068169921",
        "name": "Ruoxi Sun"
      },
      {
        "authorId": "2791430",
        "name": "H. Dai"
      },
      {
        "authorId": "2676352",
        "name": "Sercan Ö. Arik"
      },
      {
        "authorId": "1945962",
        "name": "Tomas Pfister"
      }
    ],
    "source": "semantic_scholar",
    "score": 126.76284450877392
  },
  {
    "paperId": "822f7a276a4ff7dae59b849f57b95d2603a40d99",
    "url": "https://www.semanticscholar.org/paper/822f7a276a4ff7dae59b849f57b95d2603a40d99",
    "title": "Small Language Models Need Strong Verifiers to Self-Correct Reasoning",
    "abstract": "Self-correction has emerged as a promising solution to boost the reasoning performance of large language models (LLMs), where LLMs refine their solutions using self-generated critiques that pinpoint the errors. This work explores whether small (<= 13B) language models (LMs) have the ability of self-correction on reasoning tasks with minimal inputs from stronger LMs. We propose a novel pipeline that prompts smaller LMs to collect self-correction data that supports the training of self-refinement abilities. First, we leverage correct solutions to guide the model in critiquing their incorrect responses. Second, the generated critiques, after filtering, are used for supervised fine-tuning of the self-correcting reasoner through solution refinement. Our experimental results show improved self-correction abilities of two models on five datasets spanning math and commonsense reasoning, with notable performance gains when paired with a strong GPT-4-based verifier, though limitations are identified when using a weak self-verifier for determining when to correct.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-26",
    "authors": [
      {
        "authorId": "2124948532",
        "name": "Yunxiang Zhang"
      },
      {
        "authorId": "2261284957",
        "name": "Muhammad Khalifa"
      },
      {
        "authorId": "2876316",
        "name": "Lajanugen Logeswaran"
      },
      {
        "authorId": "65924935",
        "name": "Jaekyeom Kim"
      },
      {
        "authorId": "3056520",
        "name": "Moontae Lee"
      },
      {
        "authorId": "2261393055",
        "name": "Honglak Lee"
      },
      {
        "authorId": "2261364835",
        "name": "Lu Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "6ac627f57b26354ab537734d820da4a6a7dde2c6",
    "url": "https://www.semanticscholar.org/paper/6ac627f57b26354ab537734d820da4a6a7dde2c6",
    "title": "CLadder: Assessing Causal Reasoning in Language Models",
    "abstract": "The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules. To address this, we propose a new NLP task, causal inference in natural language, inspired by the\"causal inference engine\"postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-thought prompting strategy, CausalCoT. We show that our task is highly challenging for LLMs, and we conduct an in-depth analysis to gain deeper insights into the causal reasoning abilities of LLMs. Our data is open-sourced at https://huggingface.co/datasets/causalNLP/cladder, and our code can be found at https://github.com/causalNLP/cladder.",
    "venue": "",
    "year": 2023,
    "citationCount": 42,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-12-07",
    "authors": [
      {
        "authorId": "2111472502",
        "name": "Zhijing Jin"
      },
      {
        "authorId": "2265625277",
        "name": "Yuen Chen"
      },
      {
        "authorId": "27569366",
        "name": "Felix Leeb"
      },
      {
        "authorId": "31821560",
        "name": "Luigi Gresele"
      },
      {
        "authorId": "2046878528",
        "name": "Ojasv Kamal"
      },
      {
        "authorId": "2114227440",
        "name": "Zhiheng Lyu"
      },
      {
        "authorId": "2272686866",
        "name": "Kevin Blin"
      },
      {
        "authorId": "2272838249",
        "name": "Fernando Gonzalez Adauto"
      },
      {
        "authorId": "2272856326",
        "name": "Max Kleiman-Weiner"
      },
      {
        "authorId": "2790926",
        "name": "Mrinmaya Sachan"
      },
      {
        "authorId": "2261392474",
        "name": "Bernhard Scholkopf"
      }
    ],
    "source": "semantic_scholar",
    "score": 126.41800173540344
  },
  {
    "paperId": "162e2e9ac70702c146c0aa8432e4a6806bb8c42e",
    "url": "https://www.semanticscholar.org/paper/162e2e9ac70702c146c0aa8432e4a6806bb8c42e",
    "title": "Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text",
    "abstract": "While large language models (LLMs), such as GPT-3, appear to be robust and general, their reasoning ability is not at a level to compete with the best models trained for specific natural language reasoning problems. In this study, we observe that a large language model can serve as a highly effective few-shot semantic parser. It can convert natural language sentences into a logical form that serves as input for answer set programs, a logic-based declarative knowledge representation formalism. The combination results in a robust and general system that can handle multiple question-answering tasks without requiring retraining for each new task. It only needs a few examples to guide the LLM's adaptation to a specific task, along with reusable ASP knowledge modules that can be applied to multiple tasks. We demonstrate that this method achieves state-of-the-art performance on several NLP benchmarks, including bAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot planning tasks that an LLM alone fails to solve.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 38,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.07696",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-07-15",
    "authors": [
      {
        "authorId": "2729091",
        "name": "Zhun Yang"
      },
      {
        "authorId": "51161337",
        "name": "Adam Ishay"
      },
      {
        "authorId": "46664110",
        "name": "Joohyung Lee"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.95342469194469
  },
  {
    "paperId": "d7784e9aee50148edcab64ffbeea713c19144171",
    "url": "https://www.semanticscholar.org/paper/d7784e9aee50148edcab64ffbeea713c19144171",
    "title": "Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate",
    "abstract": "Large language models (LLMs) such as ChatGPT and GPT-4 have shown impressive performance in complex reasoning tasks. However, it is difficult to know whether the models are reasoning based on deep understandings of truth and logic, or leveraging their memorized patterns in a relatively superficial way. In this work, we explore testing LLMs' reasoning by engaging with them in a debate-like conversation, where given a question, the LLM and the user need to discuss to make the correct decision starting from opposing arguments. Upon mitigating the Clever Hans effect, our task requires the LLM to not only achieve the correct answer on its own, but also be able to hold and defend its belief instead of blindly believing or getting misled by the user's (invalid) arguments and critiques, thus testing in greater depth whether the LLM grasps the essence of the reasoning required to solve the problem. Across a range of complex reasoning benchmarks spanning math, commonsense, logic and BIG-Bench tasks, we find that despite their impressive performance as reported in existing work on generating correct step-by-step solutions in the beginning, LLMs like ChatGPT cannot maintain their beliefs in truth for a significant portion of examples when challenged by oftentimes absurdly invalid arguments. Our work points to danger zones of model alignment, and also suggests more careful treatments and interpretations of the recent findings that LLMs can improve their responses based on feedback.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 48,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.findings-emnlp.795.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-22",
    "authors": [
      {
        "authorId": "7425689",
        "name": "Boshi Wang"
      },
      {
        "authorId": "145548079",
        "name": "Xiang Yue"
      },
      {
        "authorId": "1515546612",
        "name": "Huan Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 128.3773044716594
  },
  {
    "paperId": "f4f87904af97d7f567dd2f44930a73952ed4d5dd",
    "url": "https://www.semanticscholar.org/paper/f4f87904af97d7f567dd2f44930a73952ed4d5dd",
    "title": "Stop Reasoning! When Multimodal LLM with Chain-of-Thought Reasoning Meets Adversarial Image",
    "abstract": "Multimodal LLMs (MLLMs) with a great ability of text and image understanding have received great attention. To achieve better reasoning with MLLMs, Chain-of-Thought (CoT) reasoning has been widely explored, which further promotes MLLMs' explainability by giving intermediate reasoning steps. Despite the strong power demonstrated by MLLMs in multimodal reasoning, recent studies show that MLLMs still suffer from adversarial images. This raises the following open questions: Does CoT also enhance the adversarial robustness of MLLMs? What do the intermediate reasoning steps of CoT entail under adversarial attacks? To answer these questions, we first generalize existing attacks to CoT-based inferences by attacking the two main components, i.e., rationale and answer. We find that CoT indeed improves MLLMs' adversarial robustness against the existing attack methods by leveraging the multi-step reasoning process, but not substantially. Based on our findings, we further propose a novel attack method, termed as stop-reasoning attack, that attacks the model while bypassing the CoT reasoning process. Experiments on three MLLMs and two visual reasoning datasets verify the effectiveness of our proposed method. We show that stop-reasoning attack can result in misled predictions and outperform baseline attacks by a significant margin.",
    "venue": "",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-02-22",
    "authors": [
      {
        "authorId": "2286700032",
        "name": "Zefeng Wang"
      },
      {
        "authorId": "2223193538",
        "name": "Zhen Han"
      },
      {
        "authorId": "2224083544",
        "name": "Shuo Chen"
      },
      {
        "authorId": "2286294409",
        "name": "Fan Xue"
      },
      {
        "authorId": "2046761003",
        "name": "Zifeng Ding"
      },
      {
        "authorId": "2287257656",
        "name": "Xun Xiao"
      },
      {
        "authorId": "2265580790",
        "name": "Volker Tresp"
      },
      {
        "authorId": "2257347697",
        "name": "Philip Torr"
      },
      {
        "authorId": "52203056",
        "name": "Jindong Gu"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "752f684371c9901791259dc4afd04b9754e803d1",
    "url": "https://www.semanticscholar.org/paper/752f684371c9901791259dc4afd04b9754e803d1",
    "title": "Can LLM Graph Reasoning Generalize beyond Pattern Memorization?",
    "abstract": "Large language models (LLMs) demonstrate great potential for problems with implicit graphical structures, while recent works seek to enhance the graph reasoning capabilities of LLMs through specialized instruction tuning. The resulting 'graph LLMs' are evaluated with in-distribution settings only, thus it remains underexplored whether LLMs are learning generalizable graph reasoning skills or merely memorizing patterns in the synthetic training data. To this end, we propose the NLGift benchmark, an evaluation suite of LLM graph reasoning generalization: whether LLMs could go beyond semantic, numeric, structural, reasoning patterns in the synthetic training data and improve utility on real-world graph-based tasks. Extensive experiments with two LLMs across four graph reasoning tasks demonstrate that while generalization on simple patterns (semantic, numeric) is somewhat satisfactory, LLMs struggle to generalize across reasoning and real-world patterns, casting doubt on the benefit of synthetic graph tuning for real-world tasks with underlying network structures. We explore three strategies to improve LLM graph reasoning generalization, and we find that while post-training alignment is most promising for real-world tasks, empowering LLM graph reasoning to go beyond pattern memorization remains an open research question.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-23",
    "authors": [
      {
        "authorId": "2129515763",
        "name": "Yizhuo Zhang"
      },
      {
        "authorId": "2256778370",
        "name": "Heng Wang"
      },
      {
        "authorId": "2284701198",
        "name": "Shangbin Feng"
      },
      {
        "authorId": "2093186816",
        "name": "Zhaoxuan Tan"
      },
      {
        "authorId": "2257023881",
        "name": "Xiaochuang Han"
      },
      {
        "authorId": "2249540815",
        "name": "Tianxing He"
      },
      {
        "authorId": "2249583325",
        "name": "Yulia Tsvetkov"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "f859b66e55aebdd4d460e2bc1e74640eafead5ad",
    "url": "https://www.semanticscholar.org/paper/f859b66e55aebdd4d460e2bc1e74640eafead5ad",
    "title": "Zero-Shot Chain-of-Thought Reasoning Guided by Evolutionary Algorithms in Large Language Models",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across diverse tasks and exhibited impressive reasoning abilities by applying zero-shot Chain-of-Thought (CoT) prompting. However, due to the evolving nature of sentence prefixes during the pre-training phase, existing zero-shot CoT prompting methods that employ identical CoT prompting across all task instances may not be optimal. In this paper, we introduce a novel zero-shot prompting method that leverages evolutionary algorithms to generate diverse promptings for LLMs dynamically. Our approach involves initializing two CoT promptings, performing evolutionary operations based on LLMs to create a varied set, and utilizing the LLMs to select a suitable CoT prompting for a given problem. Additionally, a rewriting operation, guided by the selected CoT prompting, enhances the understanding of the LLMs about the problem. Extensive experiments conducted across ten reasoning datasets demonstrate the superior performance of our proposed method compared to current zero-shot CoT prompting methods on GPT-3.5-turbo and GPT-4. Moreover, in-depth analytical experiments underscore the adaptability and effectiveness of our method in various reasoning tasks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-08",
    "authors": [
      {
        "authorId": "2283305538",
        "name": "Feihu Jin"
      },
      {
        "authorId": "2283441447",
        "name": "Yifan Liu"
      },
      {
        "authorId": "2283526261",
        "name": "Ying Tan"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "07cdf957a11506f87fbc030dcfaaa6399847648c",
    "url": "https://www.semanticscholar.org/paper/07cdf957a11506f87fbc030dcfaaa6399847648c",
    "title": "Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations",
    "abstract": "Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When extending the rejection sampling strategy to the multilingual context, it proves effective for model performances, albeit limited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT) across multiple languages not only significantly enhances model performance multilingually but also elevates their monolingual performance. This indicates that crafting multilingual corpora can be regarded as a vital strategy for enhancing model performance in a specific language, especially in mathematical reasoning tasks. For instance, MathOctopus-7B improves its counterparts that trained on English from 42.2% to 50.8% on GSM8K testset.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 29,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-31",
    "authors": [
      {
        "authorId": "2263638230",
        "name": "Nuo Chen"
      },
      {
        "authorId": "2264209295",
        "name": "Zinan Zheng"
      },
      {
        "authorId": "2068345080",
        "name": "Ning Wu"
      },
      {
        "authorId": "24962156",
        "name": "Linjun Shou"
      },
      {
        "authorId": "50175330",
        "name": "Ming Gong"
      },
      {
        "authorId": "2263632282",
        "name": "Yangqiu Song"
      },
      {
        "authorId": "2263742987",
        "name": "Dongmei Zhang"
      },
      {
        "authorId": "2208883878",
        "name": "Jia Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.01796072493232
  },
  {
    "paperId": "f30b720e34d405f200270a6ef2d09e98585fb4d1",
    "url": "https://www.semanticscholar.org/paper/f30b720e34d405f200270a6ef2d09e98585fb4d1",
    "title": "CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models",
    "abstract": "The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules . To address this, we propose a new NLP task, causal inference in natural language , inspired by the “causal inference engine” postulated by Judea Pearl et al. We compose a large dataset, CL ADDER , with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-thought prompting strategy, C AUSAL C O T. We show that our task is highly challenging for LLMs, and we conduct an in-depth analysis to gain deeper insight into the causal reasoning abilities of LLMs. 1",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 30,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2111472502",
        "name": "Zhijing Jin"
      },
      {
        "authorId": "2265625277",
        "name": "Yuen Chen"
      },
      {
        "authorId": "27569366",
        "name": "Felix Leeb"
      },
      {
        "authorId": "31821560",
        "name": "Luigi Gresele"
      },
      {
        "authorId": "2046878528",
        "name": "Ojasv Kamal"
      },
      {
        "authorId": "2114227440",
        "name": "Zhiheng Lyu"
      },
      {
        "authorId": "2272686866",
        "name": "Kevin Blin"
      },
      {
        "authorId": "2272838249",
        "name": "Fernando Gonzalez Adauto"
      },
      {
        "authorId": "2272856326",
        "name": "Max Kleiman-Weiner"
      },
      {
        "authorId": "2790926",
        "name": "Mrinmaya Sachan"
      },
      {
        "authorId": "2261483511",
        "name": "Bernhard Schölkopf"
      }
    ],
    "source": "semantic_scholar",
    "score": 128.5098080672772
  },
  {
    "paperId": "64852dd925065dab03322d8ca90c836a0725a547",
    "url": "https://www.semanticscholar.org/paper/64852dd925065dab03322d8ca90c836a0725a547",
    "title": "HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and Reliable Medical LLMs Responses",
    "abstract": "In this paper, we investigate the retrieval-augmented generation (RAG) based on Knowledge Graphs (KGs) to improve the accuracy and reliability of Large Language Models (LLMs). Recent approaches suffer from insufficient and repetitive knowledge retrieval, tedious and time-consuming query parsing, and monotonous knowledge utilization. To this end, we develop a Hypothesis Knowledge Graph Enhanced (HyKGE) framework, which leverages LLMs' powerful reasoning capacity to compensate for the incompleteness of user queries, optimizes the interaction process with LLMs, and provides diverse retrieved knowledge. Specifically, HyKGE explores the zero-shot capability and the rich knowledge of LLMs with Hypothesis Outputs to extend feasible exploration directions in the KGs, as well as the carefully curated prompt to enhance the density and efficiency of LLMs' responses. Furthermore, we introduce the HO Fragment Granularity-aware Rerank Module to filter out noise while ensuring the balance between diversity and relevance in retrieved knowledge. Experiments on two Chinese medical multiple-choice question datasets and one Chinese open-domain medical Q&A dataset with two LLM turbos demonstrate the superiority of HyKGE in terms of accuracy and explainability.",
    "venue": "",
    "year": 2023,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-12-26",
    "authors": [
      {
        "authorId": "2181297535",
        "name": "Xinke Jiang"
      },
      {
        "authorId": "2276704515",
        "name": "Ruizhe Zhang"
      },
      {
        "authorId": "2215472732",
        "name": "Yongxin Xu"
      },
      {
        "authorId": "2276425552",
        "name": "Rihong Qiu"
      },
      {
        "authorId": "2276489740",
        "name": "Yue Fang"
      },
      {
        "authorId": "2215476252",
        "name": "Zhiyuan Wang"
      },
      {
        "authorId": "2276510531",
        "name": "Jinyi Tang"
      },
      {
        "authorId": "2256163148",
        "name": "Hongxin Ding"
      },
      {
        "authorId": "2276425819",
        "name": "Xu Chu"
      },
      {
        "authorId": "2145804723",
        "name": "Junfeng Zhao"
      },
      {
        "authorId": "2253831765",
        "name": "Yasha Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "69507bbeb8301b57b5722d3c99f8da9f08fdbee5",
    "url": "https://www.semanticscholar.org/paper/69507bbeb8301b57b5722d3c99f8da9f08fdbee5",
    "title": "Lingdan: enhancing encoding of traditional Chinese medicine knowledge for clinical reasoning tasks with large language models",
    "abstract": "OBJECTIVE\nThe recent surge in large language models (LLMs) across various fields has yet to be fully realized in traditional Chinese medicine (TCM). This study aims to bridge this gap by developing a large language model tailored to TCM knowledge, enhancing its performance and accuracy in clinical reasoning tasks such as diagnosis, treatment, and prescription recommendations.\n\n\nMATERIALS AND METHODS\nThis study harnessed a wide array of TCM data resources, including TCM ancient books, textbooks, and clinical data, to create 3 key datasets: the TCM Pre-trained Dataset, the Traditional Chinese Patent Medicine (TCPM) Question Answering Dataset, and the Spleen and Stomach Herbal Prescription Recommendation Dataset. These datasets underpinned the development of the Lingdan Pre-trained LLM and 2 specialized models: the Lingdan-TCPM-Chat Model, which uses a Chain-of-Thought process for symptom analysis and TCPM recommendation, and a Lingdan Prescription Recommendation model (Lingdan-PR) that proposes herbal prescriptions based on electronic medical records.\n\n\nRESULTS\nThe Lingdan-TCPM-Chat and the Lingdan-PR Model, fine-tuned on the Lingdan Pre-trained LLM, demonstrated state-of-the art performances for the tasks of TCM clinical knowledge answering and herbal prescription recommendation. Notably, Lingdan-PR outperformed all state-of-the-art baseline models, achieving an improvement of 18.39% in the Top@20 F1-score compared with the best baseline.\n\n\nCONCLUSION\nThis study marks a pivotal step in merging advanced LLMs with TCM, showcasing the potential of artificial intelligence to help improve clinical decision-making of medical diagnostics and treatment strategies. The success of the Lingdan Pre-trained LLM and its derivative models, Lingdan-TCPM-Chat and Lingdan-PR, not only revolutionizes TCM practices but also opens new avenues for the application of artificial intelligence in other specialized medical fields. Our project is available at https://github.com/TCMAI-BJTU/LingdanLLM.",
    "venue": "J. Am. Medical Informatics Assoc.",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-22",
    "authors": [
      {
        "authorId": "2193604347",
        "name": "Rui Hua"
      },
      {
        "authorId": "2152050798",
        "name": "Xin Dong"
      },
      {
        "authorId": "2312601449",
        "name": "Yu Wei"
      },
      {
        "authorId": "35742927",
        "name": "Zixin Shu"
      },
      {
        "authorId": "2313614653",
        "name": "Pengcheng Yang"
      },
      {
        "authorId": "2275599324",
        "name": "Yunhui Hu"
      },
      {
        "authorId": "2296439297",
        "name": "Shuiping Zhou"
      },
      {
        "authorId": "2121583990",
        "name": "He Sun"
      },
      {
        "authorId": "2312355006",
        "name": "Kaijing Yan"
      },
      {
        "authorId": "2312364628",
        "name": "Xijun Yan"
      },
      {
        "authorId": "2151394",
        "name": "Kai Chang"
      },
      {
        "authorId": "2288353970",
        "name": "Xiaodong Li"
      },
      {
        "authorId": "2311423212",
        "name": "Yuning Bai"
      },
      {
        "authorId": "3060983",
        "name": "Runshun Zhang"
      },
      {
        "authorId": "2296409161",
        "name": "Wenjia Wang"
      },
      {
        "authorId": "2249924315",
        "name": "Xuezhong Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "186c3023473ad6093b654d4f4d8da64e0749fc32",
    "url": "https://www.semanticscholar.org/paper/186c3023473ad6093b654d4f4d8da64e0749fc32",
    "title": "ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models",
    "abstract": "This paper introduces ConceptMath, a bilingual (English and Chinese), fine-grained benchmark that evaluates concept-wise mathematical reasoning of Large Language Models (LLMs). Unlike traditional benchmarks that evaluate general mathematical reasoning with an average accuracy, ConceptMath systematically organizes math problems under a hierarchy of math concepts, so that mathematical reasoning can be evaluated at different granularity with concept-wise accuracies. Based on our ConcepthMath, we evaluate a broad range of LLMs, and we observe existing LLMs, though achieving high average accuracies on traditional benchmarks, exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones. Besides, we also introduce an efficient fine-tuning strategy to enhance the weaknesses of existing LLMs. Finally, we hope ConceptMath could guide the developers to understand the fine-grained mathematical abilities of their models and facilitate the growth of foundation models.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-22",
    "authors": [
      {
        "authorId": "50117922",
        "name": "Yanan Wu"
      },
      {
        "authorId": "2285060791",
        "name": "Jie Liu"
      },
      {
        "authorId": "2284990102",
        "name": "Xingyuan Bu"
      },
      {
        "authorId": "2284731877",
        "name": "Jiaheng Liu"
      },
      {
        "authorId": "2254279326",
        "name": "Zhanhui Zhou"
      },
      {
        "authorId": "2279778502",
        "name": "Yuanxing Zhang"
      },
      {
        "authorId": "2214538677",
        "name": "Chenchen Zhang"
      },
      {
        "authorId": "2279540935",
        "name": "Zhiqi Bai"
      },
      {
        "authorId": "2284981829",
        "name": "Haibin Chen"
      },
      {
        "authorId": "2278217687",
        "name": "Tiezheng Ge"
      },
      {
        "authorId": "2254269925",
        "name": "Wanli Ouyang"
      },
      {
        "authorId": "2279560018",
        "name": "Wenbo Su"
      },
      {
        "authorId": "2279710589",
        "name": "Bo Zheng"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "51c656fef66f00e0a3ff2618601e9fb8229a5f8d",
    "url": "https://www.semanticscholar.org/paper/51c656fef66f00e0a3ff2618601e9fb8229a5f8d",
    "title": "Disentangling Logic: The Role of Context in Large Language Model Reasoning Capabilities",
    "abstract": "This study intends to systematically disentangle pure logic reasoning and text understanding by investigating the contrast across abstract and contextualized logical problems from a comprehensive set of domains. We explore whether LLMs demonstrate genuine reasoning capabilities across various domains when the underlying logical structure remains constant. We focus on two main questions (1) Can abstract logical problems alone accurately benchmark an LLM's reasoning ability in real-world scenarios, disentangled from contextual support in practical settings? (2) Does fine-tuning LLMs on abstract logic problem generalize to contextualized logic problems and vice versa? To investigate these questions, we focus on standard propositional logic, specifically propositional deductive and abductive logic reasoning. In particular, we construct instantiated datasets for deductive and abductive reasoning with 4 levels of difficulty, encompassing 12 distinct categories or domains based on the categorization of Wikipedia. Our experiments aim to provide insights into disentangling context in logical reasoning and the true reasoning capabilities of LLMs and their generalization potential. The code and dataset are available at: https://github.com/agiresearch/ContextHub.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-04",
    "authors": [
      {
        "authorId": "2007245028",
        "name": "Wenyue Hua"
      },
      {
        "authorId": "2543684",
        "name": "Kaijie Zhu"
      },
      {
        "authorId": "2261065167",
        "name": "Lingyao Li"
      },
      {
        "authorId": "2268855367",
        "name": "Lizhou Fan"
      },
      {
        "authorId": "2298216109",
        "name": "Shuhang Lin"
      },
      {
        "authorId": "2220539385",
        "name": "Mingyu Jin"
      },
      {
        "authorId": "2219696383",
        "name": "Haochen Xue"
      },
      {
        "authorId": "2109968285",
        "name": "Zelong Li"
      },
      {
        "authorId": "2285254341",
        "name": "Jindong Wang"
      },
      {
        "authorId": "2239061409",
        "name": "Yongfeng Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "bf050bcc3cc137b8057b30915b8183d4da99fdb7",
    "url": "https://www.semanticscholar.org/paper/bf050bcc3cc137b8057b30915b8183d4da99fdb7",
    "title": "GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning",
    "abstract": "The rapid advancement of large language models (LLMs) has catalyzed the deployment of LLM-powered agents across numerous applications, raising new concerns regarding their safety and trustworthiness. Existing methods for enhancing the safety of LLMs are not directly transferable to LLM-powered agents due to their diverse objectives and output modalities. In this paper, we propose GuardAgent, the first LLM agent as a guardrail to other LLM agents. Specifically, GuardAgent oversees a target LLM agent by checking whether its inputs/outputs satisfy a set of given guard requests defined by the users. GuardAgent comprises two steps: 1) creating a task plan by analyzing the provided guard requests, and 2) generating guardrail code based on the task plan and executing the code by calling APIs or using external engines. In both steps, an LLM is utilized as the core reasoning component, supplemented by in-context demonstrations retrieved from a memory module. Such knowledge-enabled reasoning allows GuardAgent to understand various textual guard requests and accurately\"translate\"them into executable code that provides reliable guardrails. Furthermore, GuardAgent is equipped with an extendable toolbox containing functions and APIs and requires no additional LLM training, which underscores its generalization capabilities and low operational overhead. Additionally, we propose two novel benchmarks: an EICU-AC benchmark for assessing privacy-related access control for healthcare agents and a Mind2Web-SC benchmark for safety evaluation for web agents. We show the effectiveness of GuardAgent on these two benchmarks with 98.7% and 90.0% accuracy in moderating invalid inputs and outputs for the two types of agents, respectively. We also show that GuardAgent is able to define novel functions in adaption to emergent LLM agents and guard requests, which underscores its strong generalization capabilities.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-13",
    "authors": [
      {
        "authorId": "2261738344",
        "name": "Zhen Xiang"
      },
      {
        "authorId": "2306335663",
        "name": "Linzhi Zheng"
      },
      {
        "authorId": "2306072975",
        "name": "Yanjie Li"
      },
      {
        "authorId": "2284689881",
        "name": "Junyuan Hong"
      },
      {
        "authorId": "2108107022",
        "name": "Qinbin Li"
      },
      {
        "authorId": "2307738654",
        "name": "Han Xie"
      },
      {
        "authorId": "2280198133",
        "name": "Jiawei Zhang"
      },
      {
        "authorId": "2155965725",
        "name": "Zidi Xiong"
      },
      {
        "authorId": "150961077",
        "name": "Chulin Xie"
      },
      {
        "authorId": "2306089622",
        "name": "Carl Yang"
      },
      {
        "authorId": "2293597685",
        "name": "Dawn Song"
      },
      {
        "authorId": "2295053351",
        "name": "Bo Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "d78763acfc8aaba3d6deecb8fb1d5b829d7c3a11",
    "url": "https://www.semanticscholar.org/paper/d78763acfc8aaba3d6deecb8fb1d5b829d7c3a11",
    "title": "Timo: Towards Better Temporal Reasoning for Language Models",
    "abstract": "Reasoning about time is essential for Large Language Models (LLMs) to understand the world. Previous works focus on solving specific tasks, primarily on time-sensitive question answering. While these methods have proven effective, they cannot generalize to a wider spectrum of temporal reasoning tasks. Therefore, we propose a crucial question: Can we build a universal framework to handle a variety of temporal reasoning tasks? To that end, we systematically study 38 temporal reasoning tasks. Based on the observation that 19 tasks are directly related to mathematics, we first leverage the available mathematical dataset to set a solid foundation for temporal reasoning. However, the in-depth study indicates that focusing solely on mathematical enhancement falls short of addressing pure temporal reasoning tasks. To mitigate this limitation, we propose a simple but effective self-critic temporal optimization method to enhance the model's temporal reasoning capabilities without sacrificing general task abilities. Finally, we develop Timo, a model designed to excel in temporal reasoning at the 7B and 13B scales. Notably, Timo outperforms the counterpart LLMs by 10.0 and 7.6 in average accuracy scores and achieves the new state-of-the-art (SOTA) performance of comparable size. Extensive experiments further validate our framework's effectiveness and its generalization across diverse temporal tasks. The code is available at https://github.com/zhaochen0110/Timo.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-20",
    "authors": [
      {
        "authorId": "2720303",
        "name": "Zhao-yu Su"
      },
      {
        "authorId": "2307329094",
        "name": "Jun Zhang"
      },
      {
        "authorId": "1914586128",
        "name": "Tong Zhu"
      },
      {
        "authorId": "2265753258",
        "name": "Xiaoye Qu"
      },
      {
        "authorId": "2267385270",
        "name": "Juntao Li"
      },
      {
        "authorId": "2306158069",
        "name": "Min Zhang"
      },
      {
        "authorId": "2307325455",
        "name": "Yu Cheng"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "694d309c4765f57bed172d91f8a844f2237dd509",
    "url": "https://www.semanticscholar.org/paper/694d309c4765f57bed172d91f8a844f2237dd509",
    "title": "Are Large Vision Language Models up to the Challenge of Chart Comprehension and Reasoning? An Extensive Investigation into the Capabilities and Limitations of LVLMs",
    "abstract": "Natural language is a powerful complementary modality of communication for data visualizations, such as bar and line charts. To facilitate chart-based reasoning using natural language, various downstream tasks have been introduced recently such as chart question answering, chart summarization, and fact-checking with charts. These tasks pose a unique challenge, demanding both vision-language reasoning and a nuanced understanding of chart data tables, visual encodings, and natural language prompts. Despite the recent success of Large Language Models (LLMs) across diverse NLP tasks, their abilities and limitations in the realm of data visualization remain under-explored, possibly due to their lack of multi-modal capabilities. To bridge the gap, this paper presents the first comprehensive evaluation of the recently developed large vision language models (LVLMs) for chart understanding and reasoning tasks. Our evaluation includes a comprehensive assessment of LVLMs, including GPT-4V and Gemini, across four major chart reasoning tasks. Furthermore, we perform a qualitative evaluation of LVLMs' performance on a diverse range of charts, aiming to provide a thorough analysis of their strengths and weaknesses. Our findings reveal that LVLMs demonstrate impressive abilities in generating fluent texts covering high-level data insights while also encountering common problems like hallucinations, factual errors, and data bias. We highlight the key strengths and limitations of chart comprehension tasks, offering insights for future research.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-01",
    "authors": [
      {
        "authorId": "2304464834",
        "name": "Mohammed Saidul Islam"
      },
      {
        "authorId": "2124881265",
        "name": "Raian Rahman"
      },
      {
        "authorId": "100832735",
        "name": "Ahmed Masry"
      },
      {
        "authorId": "46437970",
        "name": "Md Tahmid Rahman Laskar"
      },
      {
        "authorId": "1807355",
        "name": "Mir Tafseer Nayeem"
      },
      {
        "authorId": "2274022429",
        "name": "Enamul Hoque"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "c2d858527df96e269f072e3dee37ec7bae281c0f",
    "url": "https://www.semanticscholar.org/paper/c2d858527df96e269f072e3dee37ec7bae281c0f",
    "title": "NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models",
    "abstract": "Capitalizing on the remarkable advancements in Large Language Models (LLMs), there is a burgeoning initiative to harness LLMs for instruction following robotic navigation. Such a trend underscores the potential of LLMs to generalize navigational reasoning and diverse language understanding. However, a significant discrepancy in agent performance is observed when integrating LLMs in the Vision-and-Language navigation (VLN) tasks compared to previous downstream specialist models. Furthermore, the inherent capacity of language to interpret and facilitate communication in agent interactions is often underutilized in these integrations. In this work, we strive to bridge the divide between VLN-specialized models and LLM-based navigation paradigms, while maintaining the interpretative prowess of LLMs in generating linguistic navigational reasoning. By aligning visual content in a frozen LLM, we encompass visual observation comprehension for LLMs and exploit a way to incorporate LLMs and navigation policy networks for effective action predictions and navigational reasoning. We demonstrate the data efficiency of the proposed methods and eliminate the gap between LM-based agents and state-of-the-art VLN specialists.",
    "venue": "European Conference on Computer Vision",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-17",
    "authors": [
      {
        "authorId": "2218747386",
        "name": "Gengze Zhou"
      },
      {
        "authorId": "1612421029",
        "name": "Yicong Hong"
      },
      {
        "authorId": "2310518904",
        "name": "Zun Wang"
      },
      {
        "authorId": "2311825905",
        "name": "Xin Eric Wang"
      },
      {
        "authorId": "2310487211",
        "name": "Qi Wu"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "672ad7c1bd1a6e4e47e4748b878a448225f07a10",
    "url": "https://www.semanticscholar.org/paper/672ad7c1bd1a6e4e47e4748b878a448225f07a10",
    "title": "How Far Are We from Intelligent Visual Deductive Reasoning?",
    "abstract": "Vision-Language Models (VLMs) have recently demonstrated incredible strides on diverse vision language tasks. We dig into vision-based deductive reasoning, a more sophisticated but less explored realm, and find previously unexposed blindspots in the current SOTA VLMs. Specifically, we leverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop relational and deductive reasoning relying solely on visual clues. We perform comprehensive evaluations of several popular VLMs employing standard strategies such as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test, IntelligenceTest, and RAVEN. The results reveal that despite the impressive capabilities of LLMs in text-based reasoning, we are still far from achieving comparable proficiency in visual deductive reasoning. We found that certain standard strategies that are effective when applied to LLMs do not seamlessly translate to the challenges presented by visual reasoning tasks. A detailed analysis reveals that VLMs struggle to solve these tasks mainly because they are unable to perceive and comprehend multiple, confounding abstract patterns in RPM examples.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-07",
    "authors": [
      {
        "authorId": "2254045488",
        "name": "Yizhe Zhang"
      },
      {
        "authorId": "37374479",
        "name": "Richard He Bai"
      },
      {
        "authorId": "2290365185",
        "name": "Ruixiang Zhang"
      },
      {
        "authorId": "2287733778",
        "name": "Jiatao Gu"
      },
      {
        "authorId": "2443456",
        "name": "Shuangfei Zhai"
      },
      {
        "authorId": "49158771",
        "name": "J. Susskind"
      },
      {
        "authorId": "3111912",
        "name": "N. Jaitly"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "3bb853fe6bdbe889a8715c93f78dae01cf1bc65c",
    "url": "https://www.semanticscholar.org/paper/3bb853fe6bdbe889a8715c93f78dae01cf1bc65c",
    "title": "Evaluating Mathematical Reasoning Beyond Accuracy",
    "abstract": "The leaderboard of Large Language Models (LLMs) in mathematical tasks has been continuously updated. However, the majority of evaluations focus solely on the final results, neglecting the quality of the intermediate steps. This oversight can mask underlying problems, such as logical errors or unnecessary steps in the reasoning process. To measure reasoning beyond final-answer accuracy, we introduce ReasonEval, a new methodology for evaluating the quality of reasoning steps. ReasonEval employs validity and redundancy to characterize the reasoning quality, as well as accompanying LLMs to assess them automatically. We explore different design options for the LLM-based evaluators and empirically demonstrate that ReasonEval, when instantiated with base models possessing strong mathematical knowledge and trained with high-quality labeled data, consistently outperforms baseline methods in the meta-evaluation datasets. We also highlight the strong generalization capabilities of ReasonEval. By utilizing ReasonEval to evaluate LLMs specialized in math, we find that an increase in final-answer accuracy does not necessarily guarantee an improvement in the overall quality of the reasoning steps for challenging mathematical problems. Additionally, we observe that ReasonEval can play a significant role in data selection. We open-source the best-performing model, meta-evaluation script, and all evaluation results to facilitate future research.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-08",
    "authors": [
      {
        "authorId": "2295644924",
        "name": "Shijie Xia"
      },
      {
        "authorId": "2284732801",
        "name": "Xuefeng Li"
      },
      {
        "authorId": "2295676917",
        "name": "Yixin Liu"
      },
      {
        "authorId": "2295678592",
        "name": "Tongshuang Wu"
      },
      {
        "authorId": "2282138571",
        "name": "Pengfei Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "38c61bbb6b8f69395ee4592171f732f9bca3dd5c",
    "url": "https://www.semanticscholar.org/paper/38c61bbb6b8f69395ee4592171f732f9bca3dd5c",
    "title": "Reasoning with Large Language Models, a Survey",
    "abstract": "Scaling up language models to billions of parameters has opened up possibilities for in-context learning, allowing instruction tuning and few-shot learning on tasks that the model was not specifically trained for. This has achieved breakthrough performance on language tasks such as translation, summarization, and question-answering. Furthermore, in addition to these associative\"System 1\"tasks, recent advances in Chain-of-thought prompt learning have demonstrated strong\"System 2\"reasoning abilities, answering a question in the field of artificial general intelligence whether LLMs can reason. The field started with the question whether LLMs can solve grade school math word problems. This paper reviews the rapidly expanding field of prompt-based reasoning with LLMs. Our taxonomy identifies different ways to generate, evaluate, and control multi-step reasoning. We provide an in-depth coverage of core approaches and open problems, and we propose a research agenda for the near future. Finally, we highlight the relation between reasoning and prompt-based learning, and we discuss the relation between reasoning, sequential decision processes, and reinforcement learning. We find that self-improvement, self-reflection, and some metacognitive abilities of the reasoning processes are possible through the judicious use of prompts. True self-improvement and self-reasoning, to go from reasoning with LLMs to reasoning by LLMs, remains future work.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-07-16",
    "authors": [
      {
        "authorId": "2562595",
        "name": "A. Plaat"
      },
      {
        "authorId": "2117067803",
        "name": "Annie Wong"
      },
      {
        "authorId": "2239104085",
        "name": "Suzan Verberne"
      },
      {
        "authorId": "2311509515",
        "name": "Joost Broekens"
      },
      {
        "authorId": "2218156728",
        "name": "N. V. Stein"
      },
      {
        "authorId": "2237990304",
        "name": "T. Back"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "be6f989f0104fb8f6b9322073b07c2cfeffbacbf",
    "url": "https://www.semanticscholar.org/paper/be6f989f0104fb8f6b9322073b07c2cfeffbacbf",
    "title": "From Beginner to Expert: Modeling Medical Knowledge into General LLMs",
    "abstract": "Recently, large language model (LLM) based artificial intelligence (AI) systems have demonstrated remarkable capabilities in natural language understanding and generation. However, these models face a significant challenge when it comes to sensitive applications, such as reasoning over medical knowledge and answering medical questions in a physician-like manner. Prior studies attempted to overcome this challenge by increasing the model size (>100B) to learn more general medical knowledge, while there is still room for improvement in LLMs with smaller-scale model sizes (<100B). In this work, we start from a pre-trained general LLM model (AntGLM-10B) and fine-tune it from a medical beginner towards a medical expert (called AntGLM-Med-10B), which leverages a 3-stage optimization procedure, i.e., general medical knowledge injection, medical domain instruction tuning, and specific medical task adaptation. Our contributions are threefold: (1) We specifically investigate how to adapt a pre-trained general LLM in medical domain, especially for a specific medical task. (2) We collect and construct large-scale medical datasets for each stage of the optimization process. These datasets encompass various data types and tasks, such as question-answering, medical reasoning, multi-choice questions, and medical conversations. (3) Specifically for multi-choice questions in the medical domain, we propose a novel Verification-of-Choice approach for prompting engineering, which significantly enhances the reasoning ability of LLMs. Remarkably, by combining the above approaches, our AntGLM-Med-10B model can outperform the most of LLMs on PubMedQA, including both general and medical LLMs, even when these LLMs have larger model size.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-02",
    "authors": [
      {
        "authorId": "2171247031",
        "name": "Qiang Li"
      },
      {
        "authorId": "2215685191",
        "name": "Xiaoyan Yang"
      },
      {
        "authorId": "2268638594",
        "name": "Haowen Wang"
      },
      {
        "authorId": "2269511483",
        "name": "Qin Wang"
      },
      {
        "authorId": "2267013360",
        "name": "Lei Liu"
      },
      {
        "authorId": "2269510514",
        "name": "Junjie Wang"
      },
      {
        "authorId": "2145957136",
        "name": "Yang Zhang"
      },
      {
        "authorId": "2269473798",
        "name": "Mingyuan Chu"
      },
      {
        "authorId": "2260288151",
        "name": "Sen Hu"
      },
      {
        "authorId": "2257103276",
        "name": "Yicheng Chen"
      },
      {
        "authorId": "2180240771",
        "name": "Yue Shen"
      },
      {
        "authorId": "2268644119",
        "name": "Cong Fan"
      },
      {
        "authorId": "2260010886",
        "name": "Wangshu Zhang"
      },
      {
        "authorId": "2260601133",
        "name": "Teng Xu"
      },
      {
        "authorId": "2269769748",
        "name": "Jinjie Gu"
      },
      {
        "authorId": "2257349883",
        "name": "Jing Zheng"
      },
      {
        "authorId": "2269750295",
        "name": "Guannan Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "d567a0370977fc6c1030cd02ba80564906f9e9da",
    "url": "https://www.semanticscholar.org/paper/d567a0370977fc6c1030cd02ba80564906f9e9da",
    "title": "Logic-Scaffolding: Personalized Aspect-Instructed Recommendation Explanation Generation using LLMs",
    "abstract": "The unique capabilities of Large Language Models (LLMs), such as the natural language text generation ability, position them as strong candidates for providing explanation for recommendations. However, despite the size of the LLM, most existing models struggle to produce zero-shot explanations reliably. To address this issue, we propose a framework called Logic-Scaffolding, that combines the ideas of aspect-based explanation and chain-of-thought prompting to generate explanations through intermediate reasoning steps. In this paper, we share our experience in building the framework and present an interactive demonstration for exploring our results.",
    "venue": "Web Search and Data Mining",
    "year": 2023,
    "citationCount": 9,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3616855.3635689",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2023-12-22",
    "authors": [
      {
        "authorId": "23598119",
        "name": "Behnam Rahdari"
      },
      {
        "authorId": "2263944122",
        "name": "Hao Ding"
      },
      {
        "authorId": "2276324990",
        "name": "Ziwei Fan"
      },
      {
        "authorId": "2276335652",
        "name": "Yifei Ma"
      },
      {
        "authorId": "2276249197",
        "name": "Zhuotong Chen"
      },
      {
        "authorId": "1713801",
        "name": "Anoop Deoras"
      },
      {
        "authorId": "1681967",
        "name": "B. Kveton"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.53877639491068
  },
  {
    "paperId": "fd2cb81c01d9ab51c78de69b53f8b337775e290c",
    "url": "https://www.semanticscholar.org/paper/fd2cb81c01d9ab51c78de69b53f8b337775e290c",
    "title": "DocFinQA: A Long-Context Financial Reasoning Dataset",
    "abstract": "For large language models (LLMs) to be effective in the financial domain -- where each decision can have a significant impact -- it is necessary to investigate realistic tasks and data. Financial professionals often interact with documents that are hundreds of pages long, but most financial research datasets only deal with short excerpts from these documents. To address this, we introduce a long-document financial QA task. We augment 7,437 questions from the existing FinQA dataset with the full-document context, extending the average context length from under 700 words in FinQA to 123k words in DocFinQA. We conduct extensive experiments over retrieval-based QA pipelines and long-context language models. DocFinQA proves a significant challenge for even state-of-the-art systems. We also provide a case-study on the longest documents in DocFinQA and find that models particularly struggle on these documents. Addressing these challenges may have a wide reaching impact across applications where specificity and long-range contexts are critical, like gene sequences and legal document contract analysis.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-01-12",
    "authors": [
      {
        "authorId": "2266430123",
        "name": "Varshini Reddy"
      },
      {
        "authorId": "2266399409",
        "name": "Rik Koncel-Kedziorski"
      },
      {
        "authorId": "2266397553",
        "name": "V. Lai"
      },
      {
        "authorId": "2266398345",
        "name": "Chris Tanner"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "c29dbfbc17fa190b787a2662d49f08a38c8bd166",
    "url": "https://www.semanticscholar.org/paper/c29dbfbc17fa190b787a2662d49f08a38c8bd166",
    "title": "ARB: Advanced Reasoning Benchmark for Large Language Models",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance on various quantitative reasoning and knowledge benchmarks. However, many of these benchmarks are losing utility as LLMs get increasingly high scores, despite not yet reaching expert performance in these domains. We introduce ARB, a novel benchmark composed of advanced reasoning problems in multiple fields. ARB presents a more challenging test than prior benchmarks, featuring problems in mathematics, physics, biology, chemistry, and law. As a subset of ARB, we introduce a challenging set of math and physics problems which require advanced symbolic reasoning and domain knowledge. We evaluate recent models such as GPT-4 and Claude on ARB and demonstrate that current models score well below 50% on more demanding tasks. In order to improve both automatic and assisted evaluation capabilities, we introduce a rubric-based evaluation approach, allowing GPT-4 to score its own intermediate reasoning steps. Further, we conduct a human evaluation of the symbolic subset of ARB, finding promising agreement between annotators and GPT-4 rubric evaluation scores.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 32,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.13692",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-25",
    "authors": [
      {
        "authorId": "2224816573",
        "name": "Tomohiro Sawada"
      },
      {
        "authorId": "2175557610",
        "name": "Daniel Paleka"
      },
      {
        "authorId": "2273535229",
        "name": "Alexander Havrilla"
      },
      {
        "authorId": "2224856627",
        "name": "Pranav Tadepalli"
      },
      {
        "authorId": "2224856637",
        "name": "Paula Vidas"
      },
      {
        "authorId": "2224856648",
        "name": "Alexander Kranias"
      },
      {
        "authorId": "1996084",
        "name": "John J. Nay"
      },
      {
        "authorId": "2066789106",
        "name": "Kshitij Gupta"
      },
      {
        "authorId": "51891020",
        "name": "Aran Komatsuzaki"
      }
    ],
    "source": "semantic_scholar",
    "score": 122.4476134219972
  },
  {
    "paperId": "cd64f536e2d0a9d262accd73f2ae2222d8ea8ca8",
    "url": "https://www.semanticscholar.org/paper/cd64f536e2d0a9d262accd73f2ae2222d8ea8ca8",
    "title": "CRUSH: Cybersecurity Research using Universal LLMs and Semantic Hypernetworks",
    "abstract": "Enterprise Knowledge Graphs (EKG) are powerful tools for representing and reasoning about complex and dynamic domains, such as cyber threat intelligence. However, designing and constructing such graphs can be challenging, especially when dealing with heterogeneous and noisy data sources. This paper presents our novel approach to using Large Language Models (LLM) for EKG design and development based on our experience building a Threat Intelligence Graph (TIG) using GPT3.5/ GPT4/ ChatGPT. We show how LLMs can automatically extract, infer, validate, and summarize information from various sources, such as threat reports, literature, scripts, etc., and populate the EKG with relevant entities, relationships, and properties. We also demonstrate how LLMs can identify malicious intents in any script file and map it to the TIG to detect any malicious techniques linked to the script. We demonstrate that an LLM-EKG-based approach could deliver up to 99% recall on the task of detection of malicious scripts and a consistent 90%+ recall on the task of detection of specific threat types across all script-based cybersecurity threats.",
    "venue": "EKG-LLM@CIKM",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2274336414",
        "name": "Mohit Sewak"
      },
      {
        "authorId": "4818740",
        "name": "Vamsi K Emani"
      },
      {
        "authorId": "2274341706",
        "name": "Annam Naresh"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "227341fc8872a5de917673ae8e81e0264d39ff72",
    "url": "https://www.semanticscholar.org/paper/227341fc8872a5de917673ae8e81e0264d39ff72",
    "title": "Explore the Potential of LLMs in Misinformation Detection: An Empirical Study",
    "abstract": "Large Language Models (LLMs) have garnered significant attention for their powerful ability in natural language understanding and reasoning. In this paper, we present a comprehensive empirical study to explore the performance of LLMs on misinformation detection tasks. This study stands as the pioneering investigation into the understanding capabilities of multiple LLMs regarding both content and propagation across social media platforms. Our empirical studies on eight misinformation detection datasets show that LLM-based detectors can achieve comparable performance in text-based misinformation detection but exhibit notably constrained capabilities in comprehending propagation structure compared to existing models in propagation-based misinformation detection. Our experiments further demonstrate that LLMs exhibit great potential to enhance existing misinformation detection models. These findings highlight the potential ability of LLMs to detect misinformation.",
    "venue": "",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-11-21",
    "authors": [
      {
        "authorId": "2267694909",
        "name": "Mengyang Chen"
      },
      {
        "authorId": "2267862082",
        "name": "Lingwei Wei"
      },
      {
        "authorId": "2268054136",
        "name": "Han Cao"
      },
      {
        "authorId": "2256662678",
        "name": "Wei Zhou"
      },
      {
        "authorId": "2115190434",
        "name": "Song Hu"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "c5bf84f715eb9a80fe0b5809649ff03a4cfd2cd7",
    "url": "https://www.semanticscholar.org/paper/c5bf84f715eb9a80fe0b5809649ff03a4cfd2cd7",
    "title": "CHASE-SQL: Multi-Path Reasoning and Preference Optimized Candidate Selection in Text-to-SQL",
    "abstract": "In tackling the challenges of large language model (LLM) performance for Text-to-SQL tasks, we introduce CHASE-SQL, a new framework that employs innovative strategies, using test-time compute in multi-agent modeling to improve candidate generation and selection. CHASE-SQL leverages LLMs' intrinsic knowledge to generate diverse and high-quality SQL candidates using different LLM generators with: (1) a divide-and-conquer method that decomposes complex queries into manageable sub-queries in a single LLM call; (2) chain-of-thought reasoning based on query execution plans, reflecting the steps a database engine takes during execution; and (3) a unique instance-aware synthetic example generation technique, which offers specific few-shot demonstrations tailored to test questions.To identify the best candidate, a selection agent is employed to rank the candidates through pairwise comparisons with a fine-tuned binary-candidates selection LLM. This selection approach has been demonstrated to be more robust over alternatives. The proposed generators-selector framework not only enhances the quality and diversity of SQL queries but also outperforms previous methods. Overall, our proposed CHASE-SQL achieves the state-of-the-art execution accuracy of 73.0% and 73.01% on the test set and development set of the notable BIRD Text-to-SQL dataset benchmark, rendering CHASE-SQL the top submission of the leaderboard (at the time of paper submission).",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-02",
    "authors": [
      {
        "authorId": "2192284724",
        "name": "Mohammadreza Pourreza"
      },
      {
        "authorId": "2316892037",
        "name": "Hailong Li"
      },
      {
        "authorId": "2068169921",
        "name": "Ruoxi Sun"
      },
      {
        "authorId": "2324868116",
        "name": "Yeounoh Chung"
      },
      {
        "authorId": "2171655817",
        "name": "Shayan Talaei"
      },
      {
        "authorId": "2169797763",
        "name": "G. T. Kakkar"
      },
      {
        "authorId": "2324056256",
        "name": "Yu Gan"
      },
      {
        "authorId": "2303415064",
        "name": "Amin Saberi"
      },
      {
        "authorId": "2324057763",
        "name": "Fatma Ozcan"
      },
      {
        "authorId": "2676352",
        "name": "Sercan Ö. Arik"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "7e1a823a3ee88e0964a18b89015614d8cc82d560",
    "url": "https://www.semanticscholar.org/paper/7e1a823a3ee88e0964a18b89015614d8cc82d560",
    "title": "Extracting Self-Consistent Causal Insights from Users Feedback with LLMs and In-context Learning",
    "abstract": "Microsoft Windows Feedback Hub is designed to receive customer feedback on a wide variety of subjects including critical topics such as power and battery. Feedback is one of the most effective ways to have a grasp of users' experience with Windows and its ecosystem. However, the sheer volume of feedback received by Feedback Hub makes it immensely challenging to diagnose the actual cause of reported issues. To better understand and triage issues, we leverage Double Machine Learning (DML) to associate users' feedback with telemetry signals. One of the main challenges we face in the DML pipeline is the necessity of domain knowledge for model design (e.g., causal graph), which sometimes is either not available or hard to obtain. In this work, we take advantage of reasoning capabilities in Large Language Models (LLMs) to generate a prior model that which to some extent compensates for the lack of domain knowledge and could be used as a heuristic for measuring feedback informativeness. Our LLM-based approach is able to extract previously known issues, uncover new bugs, and identify sequences of events that lead to a bug, while minimizing out-of-domain outputs.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-11",
    "authors": [
      {
        "authorId": "2290848807",
        "name": "Sara Abdali"
      },
      {
        "authorId": "2227962001",
        "name": "Anjali Parikh"
      },
      {
        "authorId": "2273923461",
        "name": "Steve Lim"
      },
      {
        "authorId": "2264962872",
        "name": "Emre Kiciman"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "97afc428341eca1011a142daf269c9b01230f410",
    "url": "https://www.semanticscholar.org/paper/97afc428341eca1011a142daf269c9b01230f410",
    "title": "Reverse Chain: A Generic-Rule for LLMs to Master Multi-API Planning",
    "abstract": "While enabling large language models to implement function calling (known as APIs) can greatly enhance the performance of Large Language Models (LLMs), function calling is still a challenging task due to the complicated relations between different APIs, especially in a context-learning setting without fine-tuning. This paper introduces ``Reverse Chain'', a controllable, target-driven approach designed to empower LLMs with the capability to operate external APIs only via prompts. Recognizing that most LLMs have limited tool-use capabilities, Reverse Chain limits LLMs to executing simple tasks, e.g., API Selection and Argument Completion. Furthermore, to manage a controllable multi-function calling, Reverse Chain adopts a generic rule based on a backward reasoning process. This rule determines when to do API selection or Argument completion. To evaluate the multi-tool-use capability of LLMs, we have released a compositional multi-tool task dataset, available at \\url{https://anonymous.4open.science/r/reverse-chain-8681}. Extensive numerical experiments validate the remarkable proficiency of Reverse Chain in managing multiple API calls.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.04474",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-06",
    "authors": [
      {
        "authorId": "2257102265",
        "name": "Yinger Zhang"
      },
      {
        "authorId": "2256984989",
        "name": "Hui Cai"
      },
      {
        "authorId": "2257103276",
        "name": "Yicheng Chen"
      },
      {
        "authorId": "2257157842",
        "name": "Rui Sun"
      },
      {
        "authorId": "2257349883",
        "name": "Jing Zheng"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "6a932dd04c39c361bcacf730b3812b56df0e9b8a",
    "url": "https://www.semanticscholar.org/paper/6a932dd04c39c361bcacf730b3812b56df0e9b8a",
    "title": "Case2Code: Learning Inductive Reasoning with Synthetic Data",
    "abstract": "Complex reasoning is an impressive ability shown by large language models (LLMs). Most LLMs are skilled in deductive reasoning, such as chain-of-thought prompting or iterative tool-using to solve challenging tasks step-by-step. In this paper, we hope to focus on evaluating and teaching LLMs to conduct inductive reasoning, that is, LLMs are supposed to infer underlying rules by observing examples or sequential transformations. However, collecting large-scale and diverse human-generated inductive data is challenging. We focus on data synthesis in the code domain and propose a \\textbf{Case2Code} task by exploiting the expressiveness and correctness of programs. Specifically, we collect a diverse set of executable programs, synthesize input-output transformations for each program, and force LLMs to infer the underlying code implementations based on the synthetic I/O cases. We first evaluate representative LLMs on the synthesized Case2Code task and demonstrate that the Case-to-code induction is challenging for LLMs. Then, we synthesize large-scale Case2Code training samples to train LLMs to perform inductive reasoning. Experimental results show that such induction training benefits not only in distribution Case2Code performance but also enhances various coding abilities of trained LLMs, demonstrating the great potential of learning inductive reasoning via synthetic data.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-17",
    "authors": [
      {
        "authorId": "95329799",
        "name": "Yunfan Shao"
      },
      {
        "authorId": "2107897400",
        "name": "Linyang Li"
      },
      {
        "authorId": "2283837879",
        "name": "Yichuan Ma"
      },
      {
        "authorId": "2311748053",
        "name": "Peiji Li"
      },
      {
        "authorId": "2042948910",
        "name": "Demin Song"
      },
      {
        "authorId": "1834133",
        "name": "Qinyuan Cheng"
      },
      {
        "authorId": "2278813969",
        "name": "Shimin Li"
      },
      {
        "authorId": "50080067",
        "name": "Xiaonan Li"
      },
      {
        "authorId": "2199600227",
        "name": "Pengyu Wang"
      },
      {
        "authorId": "2303800191",
        "name": "Qipeng Guo"
      },
      {
        "authorId": "2302528360",
        "name": "Hang Yan"
      },
      {
        "authorId": "2256661980",
        "name": "Xipeng Qiu"
      },
      {
        "authorId": "2284750473",
        "name": "Xuanjing Huang"
      },
      {
        "authorId": "2258618409",
        "name": "Dahua Lin"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "62207dfc2f579de2e35852352e4beb5675869524",
    "url": "https://www.semanticscholar.org/paper/62207dfc2f579de2e35852352e4beb5675869524",
    "title": "Towards Generating UI Design Feedback with LLMs",
    "abstract": "Feedback on user interface (UI) mockups is crucial for the design process, and designers often seek and leverage feedback to improve their UIs. However, human feedback is not always readily available. Given the recent emergence of LLMs, which have been shown to be proficient in rule-based reasoning, we explore the potential of LLMs to provide feedback automatically. In particular, we investigate automating heuristic evaluation, which currently entails a human expert assessing how well a UI adheres to a given set of design guidelines. We build an LLM-based heuristic evaluation plugin for Figma, which designers can use to evaluate their UI mockups. The plugin queries the LLM with the guidelines and a JSON representation of the UI mockup and then renders the identified guideline violations as constructive suggestions for design improvements. Future work is needed to study what types of usability problems can be successfully identified by LLM-driven heuristic evaluation.",
    "venue": "ACM Symposium on User Interface Software and Technology",
    "year": 2023,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-29",
    "authors": [
      {
        "authorId": "2261845318",
        "name": "Peitong Duan"
      },
      {
        "authorId": "2261901237",
        "name": "Jeremy Warner"
      },
      {
        "authorId": "2261891291",
        "name": "Bjoern Hartmann"
      }
    ],
    "source": "semantic_scholar",
    "score": 100.1415686865115
  },
  {
    "paperId": "f1a3cd5cc340f47e3e966709f7dfddef23460aa2",
    "url": "https://www.semanticscholar.org/paper/f1a3cd5cc340f47e3e966709f7dfddef23460aa2",
    "title": "Strategic Reasoning with Language Models",
    "abstract": "Strategic reasoning enables agents to cooperate, communicate, and compete with other agents in diverse situations. Existing approaches to solving strategic games rely on extensive training, yielding strategies that do not generalize to new scenarios or games without retraining. Large Language Models (LLMs), with their ability to comprehend and generate complex, context-rich language, could prove powerful as tools for strategic gameplay. This paper introduces an approach that uses pretrained LLMs with few-shot chain-of-thought examples to enable strategic reasoning for AI agents. Our approach uses systematically generated demonstrations of reasoning about states, values, and beliefs to prompt the model. Using extensive variations of simple matrix games, we show that strategies that are derived based on systematically generated prompts generalize almost perfectly to new game structures, alternate objectives, and hidden information. Additionally, we demonstrate our approach can lead to human-like negotiation strategies in realistic scenarios without any extra training or fine-tuning. Our results highlight the ability of LLMs, guided by systematic reasoning demonstrations, to adapt and excel in diverse strategic scenarios.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 28,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.19165",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-30",
    "authors": [
      {
        "authorId": "144841994",
        "name": "Kanishk Gandhi"
      },
      {
        "authorId": "1779671",
        "name": "Dorsa Sadigh"
      },
      {
        "authorId": "144002017",
        "name": "Noah D. Goodman"
      }
    ],
    "source": "semantic_scholar",
    "score": 120.5094374497971
  },
  {
    "paperId": "22d5459d1f47341b355feeb1becc37208d6ec365",
    "url": "https://www.semanticscholar.org/paper/22d5459d1f47341b355feeb1becc37208d6ec365",
    "title": "RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought",
    "abstract": "Large language Models (LLMs) have achieved promising performance on arithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting. However, LLMs face challenges in maintaining factual consistency during reasoning, exhibiting tendencies to condition overlooking, question misinterpretation, and condition hallucination over given problems. Existing methods use coarse-grained feedback (e.g., whether the answer is correct) to improve factual consistency. In this work, we propose RCoT (Reversing Chain-of-Thought), a novel method to improve LLMs' reasoning abilities by automatically detecting and rectifying factual inconsistency in LLMs, generated solutions. To detect factual inconsistency, RCoT first asks LLMs to reconstruct the problem based on generated solutions. Then fine-grained comparisons between the original problem and the reconstructed problem expose the factual inconsistency in the original solutions. To rectify the solution, RCoT formulates detected factual inconsistency into fine-grained feedback to guide LLMs in revising solutions. Experimental results demonstrate improvements of RCoT over standard CoT, Self-Consistency and Self-Refine across seven arithmetic datasets. Moreover, we find that manually written fine-grained feedback can dramatically improve LLMs' reasoning abilities (e.g., ChatGPT reaches 94.6% accuracy on GSM8K), encouraging the community to further explore the fine-grained feedback generation methods.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 27,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2305.11499",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-19",
    "authors": [
      {
        "authorId": "2217950757",
        "name": "Tianci Xue"
      },
      {
        "authorId": "1390880371",
        "name": "Ziqi Wang"
      },
      {
        "authorId": "2052036545",
        "name": "Zhenhailong Wang"
      },
      {
        "authorId": "2118642562",
        "name": "Chi Han"
      },
      {
        "authorId": "144808890",
        "name": "Pengfei Yu"
      },
      {
        "authorId": "2072975661",
        "name": "Heng Ji"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.98306765262805
  },
  {
    "paperId": "03c3737116ec11828fd63b182723ec54fc2985da",
    "url": "https://www.semanticscholar.org/paper/03c3737116ec11828fd63b182723ec54fc2985da",
    "title": "Improving LLM Reasoning through Scaling Inference Computation with Collaborative Verification",
    "abstract": "Despite significant advancements in the general capability of large language models (LLMs), they continue to struggle with consistent and accurate reasoning, especially in complex tasks such as mathematical and code reasoning. One key limitation is that LLMs are trained primarily on correct solutions, reducing their ability to detect and learn from errors, which hampers their ability to reliably verify and rank outputs. To address this, we scale up the inference-time computation by generating multiple reasoning paths and employing verifiers to assess and rank the generated outputs by correctness. To facilitate this, we introduce a comprehensive dataset consisting of correct and incorrect solutions for math and code tasks, generated by multiple LLMs. This diverse set of solutions enables verifiers to more effectively distinguish and rank correct answers from erroneous outputs. The training methods for building verifiers were selected based on an extensive comparison of existing approaches. Moreover, to leverage the unique strengths of different reasoning strategies, we propose a novel collaborative method integrating Chain-of-Thought (CoT) and Program-of-Thought (PoT) solutions for verification. CoT provides a clear, step-by-step reasoning process that enhances interpretability, while PoT, being executable, offers a precise and error-sensitive validation mechanism. By taking both of their strengths, our approach significantly improves the accuracy and reliability of reasoning verification. Our verifiers, Math-Rev and Code-Rev, demonstrate substantial performance gains to existing LLMs, achieving state-of-the-art results on benchmarks such as GSM8k and MATH and even outperforming GPT-4o with Qwen-72B-Instruct as the reasoner.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-05",
    "authors": [
      {
        "authorId": "151474408",
        "name": "Zhenwen Liang"
      },
      {
        "authorId": "2238385821",
        "name": "Ye Liu"
      },
      {
        "authorId": "2263863440",
        "name": "Tong Niu"
      },
      {
        "authorId": "2248274721",
        "name": "Xiangliang Zhang"
      },
      {
        "authorId": "2118860628",
        "name": "Yingbo Zhou"
      },
      {
        "authorId": "3014143",
        "name": "Semih Yavuz"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "8dc5789bc06768bb5da10cdcaa2af808d5017be0",
    "url": "https://www.semanticscholar.org/paper/8dc5789bc06768bb5da10cdcaa2af808d5017be0",
    "title": "Strategic Deductive Reasoning in Large Language Models: A Dual-Agent Approach",
    "abstract": "This study explores the enhancement of deductive reasoning capabilities in Large Language Models (LLMs) through a strategic dual-agent framework. In this framework, one agent acts as a questioner and another as an answerer, with both employing advanced linguistic and logical processing to optimize information exchange. Utilizing a structured environment that limits query opportunities, our approach emphasizes the development of LLMs that can efficiently generate and interpret questions to deduce hidden information effectively. The models, which incorporate self-defined agents with a combination of pretraining and llama-3-8b enhancements, demonstrate a remarkable ability to navigate the complexities of logical deduction. Performance evaluations, based on a series of simulated interactions, illustrate the agents' improved precision and strategic acumen in narrowing down possibilities through targeted inquiries. These findings underscore the potential of LLMs in tasks requiring intricate reasoning and collaboration, marking a significant step towards more intelligent and autonomous systems.",
    "venue": "2024 IEEE 6th International Conference on Power, Intelligent Computing and Systems (ICPICS)",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2024-07-26",
    "authors": [
      {
        "authorId": "2332942068",
        "name": "Siyue Li"
      },
      {
        "authorId": "2337220578",
        "name": "Xiaofan Zhou"
      },
      {
        "authorId": "2337241018",
        "name": "Zhizhong Wu"
      },
      {
        "authorId": "2337257626",
        "name": "Yuiian Long"
      },
      {
        "authorId": "2323050601",
        "name": "Yanxin Shen"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.87639203842082
  },
  {
    "paperId": "22be742ae03eba2e6fdf7fbd92a7522cba0d87fc",
    "url": "https://www.semanticscholar.org/paper/22be742ae03eba2e6fdf7fbd92a7522cba0d87fc",
    "title": "Reason from Fallacy: Enhancing Large Language Models' Logical Reasoning through Logical Fallacy Understanding",
    "abstract": "Large Language Models (LLMs) have demonstrated good performance in many reasoning tasks, but they still struggle with some complicated reasoning tasks including logical reasoning. One non-negligible reason for LLMs' suboptimal performance on logical reasoning is their overlooking of understanding logical fallacies correctly. To evaluate LLMs' capability of logical fallacy understanding (LFU), we propose five concrete tasks from three cognitive dimensions of WHAT, WHY, and HOW in this paper. Towards these LFU tasks, we have successfully constructed a new dataset LFUD based on GPT-4 accompanied by a little human effort. Our extensive experiments justify that our LFUD can be used not only to evaluate LLMs' LFU capability, but also to fine-tune LLMs to obtain significantly enhanced performance on logical reasoning.",
    "venue": "NAACL-HLT",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-04",
    "authors": [
      {
        "authorId": "2295681496",
        "name": "Yanda Li"
      },
      {
        "authorId": "2295743328",
        "name": "Dixuan Wang"
      },
      {
        "authorId": "3366523",
        "name": "Jiaqing Liang"
      },
      {
        "authorId": "2295745672",
        "name": "Guochao Jiang"
      },
      {
        "authorId": "2152880833",
        "name": "Qi He"
      },
      {
        "authorId": "2267005966",
        "name": "Yanghua Xiao"
      },
      {
        "authorId": "1944126000",
        "name": "Deqing Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "e606b277ede9d184412ea7e296899b7894cf76cd",
    "url": "https://www.semanticscholar.org/paper/e606b277ede9d184412ea7e296899b7894cf76cd",
    "title": "FinLLMs: A Framework for Financial Reasoning Dataset Generation with Large Language Models",
    "abstract": "Large Language models (LLMs) usually rely on extensive training datasets. In the financial domain, creating numerical reasoning datasets that include a mix of tables and long text often involves substantial manual annotation expenses. To address the limited data resources and reduce the annotation cost, we introduce FinLLMs, a method for generating financial question-answering data based on common financial formulas using Large Language Models. First, we compile a list of common financial formulas and construct a graph based on the variables these formulas employ. We then augment the formula set by combining those that share identical variables as new elements. Specifically, we explore formulas obtained by manual annotation and merge those formulas with shared variables by traversing the constructed graph. Finally, utilizing GPT-3.5, we generate financial question-answering data that encompasses both tabular information and long textual content, building on the collected formula set. Our experiments demonstrate that synthetic data generated by FinLLMs effectively enhances the performance of several large-scale numerical reasoning models in the financial domain, outperforming two established benchmark financial question-answering datasets.",
    "venue": "IEEE Transactions on Big Data",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-19",
    "authors": [
      {
        "authorId": "2280089724",
        "name": "Ziqiang Yuan"
      },
      {
        "authorId": "2280074135",
        "name": "Kaiyuan Wang"
      },
      {
        "authorId": "2280912184",
        "name": "Shoutai Zhu"
      },
      {
        "authorId": "2280290976",
        "name": "Ye Yuan"
      },
      {
        "authorId": "2280133145",
        "name": "Jingya Zhou"
      },
      {
        "authorId": "2280068008",
        "name": "Yanlin Zhu"
      },
      {
        "authorId": "2280105691",
        "name": "Wenqi Wei"
      }
    ],
    "source": "semantic_scholar",
    "score": 100.1415686865115
  },
  {
    "paperId": "02e10a3033489858a47da0279344cce0c034b00a",
    "url": "https://www.semanticscholar.org/paper/02e10a3033489858a47da0279344cce0c034b00a",
    "title": "Image-of-Thought Prompting for Visual Reasoning Refinement in Multimodal Large Language Models",
    "abstract": "Recent advancements in Chain-of-Thought (CoT) and related rationale-based works have significantly improved the performance of Large Language Models (LLMs) in complex reasoning tasks. With the evolution of Multimodal Large Language Models (MLLMs), enhancing their capability to tackle complex multimodal reasoning problems is a crucial frontier. However, incorporating multimodal rationales in CoT has yet to be thoroughly investigated. We propose the Image-of-Thought (IoT) prompting method, which helps MLLMs to extract visual rationales step-by-step. Specifically, IoT prompting can automatically design critical visual information extraction operations based on the input images and questions. Each step of visual information refinement identifies specific visual rationales that support answers to complex visual reasoning questions. Beyond the textual CoT, IoT simultaneously utilizes visual and textual rationales to help MLLMs understand complex multimodal information. IoT prompting has improved zero-shot visual reasoning performance across various visual understanding tasks in different MLLMs. Moreover, the step-by-step visual feature explanations generated by IoT prompting elucidate the visual reasoning process, aiding in analyzing the cognitive processes of large multimodal models",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-22",
    "authors": [
      {
        "authorId": "2289467055",
        "name": "Qiji Zhou"
      },
      {
        "authorId": "2304707334",
        "name": "Ruochen Zhou"
      },
      {
        "authorId": "2302793096",
        "name": "Zike Hu"
      },
      {
        "authorId": "2302897040",
        "name": "Panzhong Lu"
      },
      {
        "authorId": "2302812340",
        "name": "Siyang Gao"
      },
      {
        "authorId": "2303379291",
        "name": "Yue Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "5c118e57b5398224ca4401c902cda33da7d29ec3",
    "url": "https://www.semanticscholar.org/paper/5c118e57b5398224ca4401c902cda33da7d29ec3",
    "title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models",
    "abstract": "The alignment of reasoning abilities between smaller and larger Language Models are largely conducted via supervised fine-tuning using demonstrations generated from robust Large Language Models (LLMs). Although these approaches deliver more performant models, they do not show sufficiently strong generalization ability as the training only relies on the provided demonstrations.In this paper, we propose the Self-refine Instruction-tuning method that elicits Smaller Language Models to self-improve their abilities.Our approach is based on a two-stage process, where reasoning abilities are first transferred between LLMs and Small Language Models (SLMs) via Instruction-tuning on synthetic demonstrations provided by LLMs, and then the instructed models self-improve their abilities through preference optimization strategies.In particular, the second phase operates refinement heuristics based on Direct Preference Optimization, where the SLMs are elicited to deliver a series of reasoning paths by automatically sampling the generated responses and providing rewards using ground truths from the LLMs.Results obtained on commonsense and math reasoning tasks show that this approach consistently outperforms Instruction-tuning in both in-domain and out-domain scenarios, aligning the reasoning abilities of Smaller and Larger language models.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-01",
    "authors": [
      {
        "authorId": "2008183566",
        "name": "Leonardo Ranaldi"
      },
      {
        "authorId": "2277458933",
        "name": "André Freitas"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "4243f5e3e2fb405aba2648512eb950c06a4fd900",
    "url": "https://www.semanticscholar.org/paper/4243f5e3e2fb405aba2648512eb950c06a4fd900",
    "title": "Assessing the Reasoning Abilities of ChatGPT in the Context of Claim Verification",
    "abstract": "The reasoning capabilities of LLMs are currently hotly debated. We examine the issue from the perspective of claim/rumour verification. We propose the first logical reasoning framework designed to break down any claim or rumour paired with evidence into the atomic reasoning steps necessary for verification. Based on our framework, we curate two annotated collections of such claim/evidence pairs: a synthetic dataset from Wikipedia and a real-world set stemming from rumours circulating on Twitter. We use them to evaluate the reasoning capabilities of GPT-3.5-Turbo and GPT-4 (hereinafter referred to as ChatGPT) within the context of our framework, providing a thorough analysis. Our results show that ChatGPT struggles in abductive reasoning, although this can be somewhat mitigated by using manual Chain of Thought (CoT) as opposed to Zero-Shot (ZS) and ZS CoT approaches. Our study contributes to the growing body of research suggesting that ChatGPT's reasoning processes are unlikely to mirror human-like reasoning, and that LLMs need to be more rigorously evaluated to distinguish between hype and actual capabilities, especially in high-stakes real-world tasks such as claim verification.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-16",
    "authors": [
      {
        "authorId": "2121340562",
        "name": "John Dougrez-Lewis"
      },
      {
        "authorId": "2155170907",
        "name": "Mahmud Elahi Akhter"
      },
      {
        "authorId": "2257185783",
        "name": "Yulan He"
      },
      {
        "authorId": "48717312",
        "name": "M. Liakata"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "6fb5fc5df5e755fdef9fb6280e9c690e0a7f1a6f",
    "url": "https://www.semanticscholar.org/paper/6fb5fc5df5e755fdef9fb6280e9c690e0a7f1a6f",
    "title": "Fusion-Eval: Integrating Evaluators with LLMs",
    "abstract": "Evaluating Large Language Models (LLMs) is a complex task, especially considering the intricacies of natural language understanding and the expectations for high-level reasoning. Traditional evaluations typically lean on human-based, model-based, or automatic-metrics-based paradigms, each with its own advantages and shortcomings. We introduce \"Fusion-Eval\", a system that employs LLMs not solely for direct evaluations, but to skill-fully integrate insights from diverse evaluators. This gives Fusion-Eval flexibility, enabling it to work effectively across diverse tasks and make optimal use of multiple references. In testing on the SummEval dataset, Fusion-Eval achieved a Spearman correlation of 0.96, out-performing other evaluators. The success of Fusion-Eval underscores the potential of LLMs to produce evaluations that closely align human perspectives, setting a new standard in the field of LLM evaluation.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2257004117",
        "name": "Lei Shu"
      },
      {
        "authorId": "50981270",
        "name": "Nevan Wichers"
      },
      {
        "authorId": "2256991052",
        "name": "Liangchen Luo"
      },
      {
        "authorId": "2257195450",
        "name": "Yun Zhu"
      },
      {
        "authorId": "2257122720",
        "name": "Yinxiao Liu"
      },
      {
        "authorId": "2261676962",
        "name": "Jindong Chen"
      },
      {
        "authorId": "2218226973",
        "name": "Lei Meng"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "8b5bdbbe470913e828aaa703ffbc3c02af698654",
    "url": "https://www.semanticscholar.org/paper/8b5bdbbe470913e828aaa703ffbc3c02af698654",
    "title": "MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models",
    "abstract": "Multi-agent interactions between Large Language Model (LLM) agents have shown major improvements on diverse reasoning tasks. However, these involve long generations from multiple models across several rounds, making them expensive. Moreover, these multi-agent approaches fail to provide a final, single model for efficient inference. To address this, we introduce MAGDi, a new method for structured distillation of the reasoning interactions between multiple LLMs into smaller LMs. MAGDi teaches smaller models by representing multi-agent interactions as graphs, augmenting a base student model with a graph encoder, and distilling knowledge using three objective functions: next-token prediction, a contrastive loss between correct and incorrect reasoning, and a graph-based objective to model the interaction structure. Experiments on seven widely used commonsense and math reasoning benchmarks show that MAGDi improves the reasoning capabilities of smaller models, outperforming several methods that distill from a single teacher and multiple teachers. Moreover, MAGDi also demonstrates an order of magnitude higher efficiency over its teachers. We conduct extensive analyses to show that MAGDi (1) enhances the generalizability to out-of-domain tasks, (2) scales positively with the size and strength of the base student model, and (3) obtains larger improvements (via our multi-teacher training) when applying self-consistency -- an inference technique that relies on model diversity.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-02",
    "authors": [
      {
        "authorId": "2244771469",
        "name": "Justin Chih-Yao Chen"
      },
      {
        "authorId": "35106509",
        "name": "Swarnadeep Saha"
      },
      {
        "authorId": "2281825070",
        "name": "Elias Stengel-Eskin"
      },
      {
        "authorId": "2281826842",
        "name": "Mohit Bansal"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "4362edfd3907204cf1b7ec8e3c16c56db5cd14cf",
    "url": "https://www.semanticscholar.org/paper/4362edfd3907204cf1b7ec8e3c16c56db5cd14cf",
    "title": "Large Language Models-guided Dynamic Adaptation for Temporal Knowledge Graph Reasoning",
    "abstract": "Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing temporal information to capture complex relations within a Temporal Knowledge Graph (TKG) to infer new knowledge. Conventional methods in TKGR typically depend on deep learning algorithms or temporal logical rules. However, deep learning-based TKGRs often lack interpretability, whereas rule-based TKGRs struggle to effectively learn temporal rules that capture temporal patterns. Recently, Large Language Models (LLMs) have demonstrated extensive knowledge and remarkable proficiency in temporal reasoning. Consequently, the employment of LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing interest among researchers. Nonetheless, LLMs are known to function as black boxes, making it challenging to comprehend their reasoning process. Additionally, due to the resource-intensive nature of fine-tuning, promptly updating LLMs to integrate evolving knowledge within TKGs for reasoning is impractical. To address these challenges, in this paper, we propose a Large Language Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on TKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze historical data and extract temporal logical rules. These rules unveil temporal patterns and facilitate interpretable reasoning. To account for the evolving nature of TKGs, a dynamic adaptation strategy is proposed to update the LLM-generated rules with the latest events. This ensures that the extracted rules always incorporate the most recent knowledge and better generalize to the predictions on future events. Experimental results show that without the need of fine-tuning, LLM-DA significantly improves the accuracy of reasoning over several common datasets, providing a robust framework for TKGR tasks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-23",
    "authors": [
      {
        "authorId": "2163833353",
        "name": "Jiapu Wang"
      },
      {
        "authorId": "2220259324",
        "name": "Kai Sun"
      },
      {
        "authorId": "2238130759",
        "name": "Linhao Luo"
      },
      {
        "authorId": "2302892265",
        "name": "Wei Wei"
      },
      {
        "authorId": "2140879677",
        "name": "Yongli Hu"
      },
      {
        "authorId": "2243282363",
        "name": "Alan Wee-Chung Liew"
      },
      {
        "authorId": "2294378361",
        "name": "Shirui Pan"
      },
      {
        "authorId": "2239088112",
        "name": "Baocai Yin"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "9ab904669c4aeb349b9f573053ff82608dd226a1",
    "url": "https://www.semanticscholar.org/paper/9ab904669c4aeb349b9f573053ff82608dd226a1",
    "title": "Follow the Rules: Reasoning for Video Anomaly Detection with Large Language Models",
    "abstract": "Video Anomaly Detection (VAD) is crucial for applications such as security surveillance and autonomous driving. However, existing VAD methods provide little rationale behind detection, hindering public trust in real-world deployments. In this paper, we approach VAD with a reasoning framework. Although Large Language Models (LLMs) have shown revolutionary reasoning ability, we find that their direct use falls short of VAD. Specifically, the implicit knowledge pre-trained in LLMs focuses on general context and thus may not apply to every specific real-world VAD scenario, leading to inflexibility and inaccuracy. To address this, we propose AnomalyRuler, a novel rule-based reasoning framework for VAD with LLMs. AnomalyRuler comprises two main stages: induction and deduction. In the induction stage, the LLM is fed with few-shot normal reference samples and then summarizes these normal patterns to induce a set of rules for detecting anomalies. The deduction stage follows the induced rules to spot anomalous frames in test videos. Additionally, we design rule aggregation, perception smoothing, and robust reasoning strategies to further enhance AnomalyRuler's robustness. AnomalyRuler is the first reasoning approach for the one-class VAD task, which requires only few-normal-shot prompting without the need for full-shot training, thereby enabling fast adaption to various VAD scenarios. Comprehensive experiments across four VAD benchmarks demonstrate AnomalyRuler's state-of-the-art detection performance and reasoning ability. AnomalyRuler is open-source and available at: https://github.com/Yuchen413/AnomalyRuler",
    "venue": "European Conference on Computer Vision",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-14",
    "authors": [
      {
        "authorId": "2266419161",
        "name": "Yuchen Yang"
      },
      {
        "authorId": "2311411529",
        "name": "Kwonjoon Lee"
      },
      {
        "authorId": "1387979739",
        "name": "Behzad Dariush"
      },
      {
        "authorId": "2254266035",
        "name": "Yinzhi Cao"
      },
      {
        "authorId": "2303843360",
        "name": "Shao-Yuan Lo"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "426f728b9023a1815ac273d06ed281e97be74cc6",
    "url": "https://www.semanticscholar.org/paper/426f728b9023a1815ac273d06ed281e97be74cc6",
    "title": "ArgMed-Agents: Explainable Clinical Decision Reasoning with Large Language Models via Argumentation Schemes",
    "abstract": "There are two main barriers to using large language models (LLMs) in clinical reasoning. Firstly, while LLMs exhibit significant promise in Natural Language Processing (NLP) tasks, their performance in complex reasoning and planning falls short of expectations. Secondly, LLMs use uninterpretable methods to make clinical decisions that are fundamentally different from the clinician’s cognitive processes. This leads to user distrust. In this paper, we present a multi-agent framework called ArgMed-Agents, which aims to enable LLM-based agents to make explainable clinical decision reasoning through interaction. ArgMed-Agents performs self-argumentation iterations via Argumen-tation Scheme for Clinical Decision (a reasoning mechanism for modeling cognitive processes in clinical reasoning), and then constructs the argumentation process as a directed graph representing conflicting relationships. Ultimately, Reasoner(a symbolic solver) identify a series of rational and coherent arguments to support decision. ArgMed-Agents enables LLMs to mimic the process of clinical argumentative reasoning by generating explanations of reasoning in a self-directed manner. The setup experiments show that ArgMed-Agents not only improves accuracy in complex clinical decision reasoning problems compared to other prompt methods, but more importantly, it provides users with decision explanations that increase their confidence.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2279932265",
        "name": "Shengxin Hong"
      },
      {
        "authorId": "2280081126",
        "name": "Liang Xiao"
      },
      {
        "authorId": "2290897459",
        "name": "Xin Zhang"
      },
      {
        "authorId": "2242983688",
        "name": "Jian-Xing Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "86e453a20fba63fa9b803dff7c3bb91e189e984f",
    "url": "https://www.semanticscholar.org/paper/86e453a20fba63fa9b803dff7c3bb91e189e984f",
    "title": "R2-Guard: Robust Reasoning Enabled LLM Guardrail via Knowledge-Enhanced Logical Reasoning",
    "abstract": "As LLMs become increasingly prevalent across various applications, it is critical to establish safety guardrails to moderate input/output content of LLMs. Existing guardrail models treat various safety categories independently and fail to explicitly capture the intercorrelations among them. This has led to limitations such as ineffectiveness due to inadequate training on long-tail data from correlated safety categories, susceptibility to jailbreaking attacks, and inflexibility regarding new safety categories. To address these limitations, we propose $R^2$-Guard, a robust reasoning enabled LLM guardrail via knowledge-enhanced logical reasoning. Specifically, $R^2$-Guard comprises two parts: data-driven category-specific learning and reasoning components. The data-driven guardrail models provide unsafety probabilities of moderated content on different safety categories. We then encode safety knowledge among different categories as first-order logical rules and embed them into a probabilistic graphic model (PGM) based reasoning component. The unsafety probabilities of different categories from data-driven guardrail models are sent to the reasoning component for final inference. We employ two types of PGMs: Markov logic networks (MLNs) and probabilistic circuits (PCs), and optimize PCs to achieve precision-efficiency balance via improved graph structure. To further perform stress tests for guardrail models, we employ a pairwise construction method to construct a new safety benchmark TwinSafety, which features principled categories. We demonstrate the effectiveness of $R^2$-Guard by comparisons with eight strong guardrail models on six safety benchmarks, and demonstrate the robustness of $R^2$-Guard against four SOTA jailbreaking attacks. $R^2$-Guard significantly surpasses SOTA method LlamaGuard by 30.2% on ToxicChat and by 59.5% against jailbreaking attacks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-08",
    "authors": [
      {
        "authorId": "2153110066",
        "name": "Mintong Kang"
      },
      {
        "authorId": "2292209429",
        "name": "Bo Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "587f22e4e04d77ba0750deea69192fbfb73d7435",
    "url": "https://www.semanticscholar.org/paper/587f22e4e04d77ba0750deea69192fbfb73d7435",
    "title": "Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning",
    "abstract": "Chain-of-thought prompting~(CoT) and tool augmentation have been validated in recent work as effective practices for improving large language models~(LLMs) to perform step-by-step reasoning on complex math-related tasks. However, most existing math reasoning datasets may be not able to fully evaluate and analyze the ability of LLMs in manipulating tools and performing reasoning, as they may only require very few invocations of tools or miss annotations for evaluating intermediate reasoning steps. To address the issue, we construct \\textbf{CARP}, a new Chinese dataset consisting of 4,886 computation-intensive algebra problems with formulated annotations on intermediate steps. In CARP, we test four LLMs with CoT prompting, and find that they are all prone to make mistakes at the early steps of the solution, leading to wrong answers. Based on this finding, we propose a new approach that can deliberate the reasoning steps with tool interfaces, namely \\textbf{DELI}. In DELI, we first initialize a step-by-step solution based on retrieved exemplars, then iterate two deliberation procedures that check and refine the intermediate steps of the generated solution, from the perspectives of tool manipulation and natural language reasoning, until obtaining converged solutions or reaching the maximum turn. Experimental results on CARP and six other datasets show that the proposed DELI mostly outperforms competitive baselines, and can further boost the performance of existing CoT methods. Our data and code are available in \\url{https://github.com/RUCAIBox/CARP}.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 24,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.02408",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-04",
    "authors": [
      {
        "authorId": "2107926615",
        "name": "Beichen Zhang"
      },
      {
        "authorId": "1423651904",
        "name": "Kun Zhou"
      },
      {
        "authorId": "2422218",
        "name": "Xilin Wei"
      },
      {
        "authorId": "2542603",
        "name": "Wayne Xin Zhao"
      },
      {
        "authorId": "2165225571",
        "name": "Jing Sha"
      },
      {
        "authorId": "2108620507",
        "name": "Shijin Wang"
      },
      {
        "authorId": "153693432",
        "name": "Ji-rong Wen"
      }
    ],
    "source": "semantic_scholar",
    "score": 125.28313737302301
  },
  {
    "paperId": "70240feed2c732abef3838aabe760b51975ac832",
    "url": "https://www.semanticscholar.org/paper/70240feed2c732abef3838aabe760b51975ac832",
    "title": "A LLM Benchmark based on the Minecraft Builder Dialog Agent Task",
    "abstract": "In this work we proposing adapting the Minecraft builder task into an LLM benchmark suitable for evaluating LLM ability in spatially orientated tasks, and informing builder agent design. Previous works have proposed corpora with varying complex structures, and human written instructions. We instead attempt to provide a comprehensive synthetic benchmark for testing builder agents over a series of distinct tasks that comprise of common building operations. We believe this approach allows us to probe specific strengths and weaknesses of different agents, and test the ability of LLMs in the challenging area of spatial reasoning and vector based math.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-17",
    "authors": [
      {
        "authorId": "26658473",
        "name": "Chris Madge"
      },
      {
        "authorId": "2271214844",
        "name": "Massimo Poesio"
      }
    ],
    "source": "semantic_scholar",
    "score": 81.47918433002164
  },
  {
    "paperId": "e384224d841be009da20969f0ca9b4b471e291ee",
    "url": "https://www.semanticscholar.org/paper/e384224d841be009da20969f0ca9b4b471e291ee",
    "title": "Prompt Fix: Vulnerability Automatic Repair Technology Based on Prompt Engineering",
    "abstract": "With the emergence of large-scale language models (LLM), the powerful capabilities of LLM in natural language processing have attracted attention. Based on programming language LLM (Programming Language Model, PLM), we use prompt templates to explore its potential in the field of automatic vulnerability repair, and combine it with a special workflow to improve its efficiency in automatic vulnerability repair tasks. Specifically, we design four prompt templates for handling vulner-able code, and design an iterative reasoning method to improve the efficiency of vulnerability fixing. We selected multiple typical LLMs for evaluation on multiple data sets. The results show that reasonable prompt templates can effectively improve the efficiency of automatic vulnerability repair, which is significantly improved compared with neural machine translation technology. In addition, we also discussed previous bug fixing related work and our work, and pointed out some of our shortcomings and directions for future improvements.",
    "venue": "International Conference on Computing, Networking and Communications",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-19",
    "authors": [
      {
        "authorId": "2283925850",
        "name": "Peng Liu"
      },
      {
        "authorId": "2149698611",
        "name": "He Wang"
      },
      {
        "authorId": "2307743539",
        "name": "Chen Zheng"
      },
      {
        "authorId": "2118333616",
        "name": "Yuqing Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "cfb53c6cf51d5523d97b02f03422bee6a058644b",
    "url": "https://www.semanticscholar.org/paper/cfb53c6cf51d5523d97b02f03422bee6a058644b",
    "title": "Divide and Conquer for Large Language Models Reasoning",
    "abstract": "Large language models (LLMs) have shown impressive performance in various reasoning benchmarks with the emergence of Chain-of-Thought (CoT) and its derivative methods, particularly in tasks involving multi-choice questions (MCQs). However, current works all process data uniformly without considering the problem-solving difficulty, which means an excessive focus on simple questions while insufficient to intricate ones. To address this challenge, we inspired by humans using heuristic strategies to categorize tasks and handle them individually, propose to apply the Divide and Conquer to LLMs reasoning. First, we divide questions into different subsets based on the statistical confidence score ( CS ), then fix nearly resolved sets and conquer demanding nuanced process ones with elaborately designed methods, including Prior Knowledge based Reasoning (PKR) and Filter Choices based Reasoning (FCR), as well as their integration variants. Our experiments demonstrate that this proposed strategy significantly boosts the models’ reasoning abilities across nine datasets involving arithmetic, commonsense, and logic tasks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2278829306",
        "name": "Zijie Meng"
      },
      {
        "authorId": "2258684069",
        "name": "Yan Zhang"
      },
      {
        "authorId": "2266490792",
        "name": "Zhaopeng Feng"
      },
      {
        "authorId": "2253854049",
        "name": "Yang Feng"
      },
      {
        "authorId": "2256931282",
        "name": "Gaoang Wang"
      },
      {
        "authorId": "2253900280",
        "name": "J. Zhou"
      },
      {
        "authorId": "2290238577",
        "name": "Jian Wu"
      },
      {
        "authorId": "2258784763",
        "name": "Zuozhu Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "243550d8184eed2fd7627eae133bccc3c72dcd02",
    "url": "https://www.semanticscholar.org/paper/243550d8184eed2fd7627eae133bccc3c72dcd02",
    "title": "Dual Instruction Tuning with Large Language Models for Mathematical Reasoning",
    "abstract": "Recent advancements highlight the success of instruction tuning with large language models (LLMs) utilizing Chain-of-Thought (CoT) data for mathematical reasoning tasks. Despite the fine-tuned LLMs, challenges persist, such as incorrect, missing, and redundant steps in CoT generation leading to inaccuracies in answer predictions. To alleviate this problem, we propose a dual instruction tuning strategy to meticulously model mathematical reasoning from both forward and reverse directions. This involves introducing the Intermediate Reasoning State Prediction task (forward reasoning) and the Instruction Reconstruction task (reverse reasoning) to enhance the LLMs' understanding and execution of instructions. Training instances for these tasks are constructed based on existing mathematical instruction tuning datasets. Subsequently, LLMs undergo multi-task fine-tuning using both existing mathematical instructions and the newly created data. Comprehensive experiments validate the effectiveness and domain generalization of the dual instruction tuning strategy across various mathematical reasoning tasks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-27",
    "authors": [
      {
        "authorId": "2145494002",
        "name": "Yongwei Zhou"
      },
      {
        "authorId": "2293771556",
        "name": "Tiejun Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "287aa18cec7808fb5364d6ec25099f59c5014db7",
    "url": "https://www.semanticscholar.org/paper/287aa18cec7808fb5364d6ec25099f59c5014db7",
    "title": "Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large Language Model Reasoning",
    "abstract": "Iterative human engagement is a common and effective means of leveraging the advanced language processing power of large language models (LLMs). Using well-structured prompts in a conversational manner, human users can effectively influence an LLM to develop more thoughtful and accurate responses. Motivated by this insight, we propose the Iteration of Thought (IoT) framework for enhancing LLM responses by generating\"thought\"-provoking prompts vis a vis an input query and the current iteration of an LLM's response. Unlike static or semi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT), IoT adapts its reasoning path dynamically, based on evolving context, and without generating alternate explorative thoughts which are ultimately discarded. The three components of the IoT framework are (1) an Inner Dialogue Agent (IDA) responsible for generating instructive, context-specific prompts; (2) an LLM Agent (LLMA) that processes these prompts to refine its responses; and (3) an iterative prompting loop that implements a conversation between the former two components. We introduce two variants of our framework: Autonomous Iteration of Thought (AIoT), where an LLM decides when to stop iterating, and Guided Iteration of Thought (GIoT), which always forces a fixed number iterations. We investigate the performance of IoT across various datasets, spanning complex reasoning tasks from the GPQA dataset, explorative problem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop question answering from the HotpotQA dataset. Our results show that IoT represents a viable paradigm for autonomous response refinement in LLMs, showcasing significant improvements over CoT and thereby enabling more adaptive and efficient reasoning systems that minimize human intervention.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-19",
    "authors": [
      {
        "authorId": "46254996",
        "name": "Santosh Kumar Radha"
      },
      {
        "authorId": "2321873048",
        "name": "Yasamin Nouri Jelyani"
      },
      {
        "authorId": "13829278",
        "name": "A. Ghukasyan"
      },
      {
        "authorId": "1768149875",
        "name": "O. Goktas"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "cfb53c6cf51d5523d97b02f03422bee6a058644b",
    "url": "https://www.semanticscholar.org/paper/cfb53c6cf51d5523d97b02f03422bee6a058644b",
    "title": "Divide and Conquer for Large Language Models Reasoning",
    "abstract": "Large language models (LLMs) have shown impressive performance in various reasoning benchmarks with the emergence of Chain-of-Thought (CoT) and its derivative methods, particularly in tasks involving multi-choice questions (MCQs). However, current works all process data uniformly without considering the problem-solving difficulty, which means an excessive focus on simple questions while insufficient to intricate ones. To address this challenge, we inspired by humans using heuristic strategies to categorize tasks and handle them individually, propose to apply the Divide and Conquer to LLMs reasoning. First, we divide questions into different subsets based on the statistical confidence score ( CS ), then fix nearly resolved sets and conquer demanding nuanced process ones with elaborately designed methods, including Prior Knowledge based Reasoning (PKR) and Filter Choices based Reasoning (FCR), as well as their integration variants. Our experiments demonstrate that this proposed strategy significantly boosts the models’ reasoning abilities across nine datasets involving arithmetic, commonsense, and logic tasks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2278829306",
        "name": "Zijie Meng"
      },
      {
        "authorId": "2258684069",
        "name": "Yan Zhang"
      },
      {
        "authorId": "2266490792",
        "name": "Zhaopeng Feng"
      },
      {
        "authorId": "2253854049",
        "name": "Yang Feng"
      },
      {
        "authorId": "2256931282",
        "name": "Gaoang Wang"
      },
      {
        "authorId": "2253900280",
        "name": "J. Zhou"
      },
      {
        "authorId": "2290238577",
        "name": "Jian Wu"
      },
      {
        "authorId": "2258784763",
        "name": "Zuozhu Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "a0978ef43f0eca1e0c2c46f6e2d97316191cc78d",
    "url": "https://www.semanticscholar.org/paper/a0978ef43f0eca1e0c2c46f6e2d97316191cc78d",
    "title": "Sibyl: Simple yet Effective Agent Framework for Complex Real-world Reasoning",
    "abstract": "Existing agents based on large language models (LLMs) demonstrate robust problem-solving capabilities by integrating LLMs' inherent knowledge, strong in-context learning and zero-shot capabilities, and the use of tools combined with intricately designed LLM invocation workflows by humans. However, these agents still exhibit shortcomings in long-term reasoning and under-use the potential of existing tools, leading to noticeable deficiencies in complex real-world reasoning scenarios. To address these limitations, we introduce Sibyl, a simple yet powerful LLM-based agent framework designed to tackle complex reasoning tasks by efficiently leveraging a minimal set of tools. Drawing inspiration from Global Workspace Theory, Sibyl incorporates a global workspace to enhance the management and sharing of knowledge and conversation history throughout the system. Furthermore, guided by Society of Mind Theory, Sibyl implements a multi-agent debate-based jury to self-refine the final answers, ensuring a comprehensive and balanced approach. This approach aims to reduce system complexity while expanding the scope of problems solvable-from matters typically resolved by humans in minutes to those requiring hours or even days, thus facilitating a shift from System-1 to System-2 thinking. Sibyl has been designed with a focus on scalability and ease of debugging by incorporating the concept of reentrancy from functional programming from its inception, with the aim of seamless and low effort integration in other LLM applications to improve capabilities. Our experimental results on the GAIA benchmark test set reveal that the Sibyl agent instantiated with GPT-4 achieves state-of-the-art performance with an average score of 34.55%, compared to other agents based on GPT-4. We hope that Sibyl can inspire more reliable and reusable LLM-based agent solutions to address complex real-world reasoning tasks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-15",
    "authors": [
      {
        "authorId": "2283441039",
        "name": "Yulong Wang"
      },
      {
        "authorId": "2311447171",
        "name": "Tianhao Shen"
      },
      {
        "authorId": "2283444751",
        "name": "Lifeng Liu"
      },
      {
        "authorId": "2242192116",
        "name": "Jian Xie"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "287aa18cec7808fb5364d6ec25099f59c5014db7",
    "url": "https://www.semanticscholar.org/paper/287aa18cec7808fb5364d6ec25099f59c5014db7",
    "title": "Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large Language Model Reasoning",
    "abstract": "Iterative human engagement is a common and effective means of leveraging the advanced language processing power of large language models (LLMs). Using well-structured prompts in a conversational manner, human users can effectively influence an LLM to develop more thoughtful and accurate responses. Motivated by this insight, we propose the Iteration of Thought (IoT) framework for enhancing LLM responses by generating\"thought\"-provoking prompts vis a vis an input query and the current iteration of an LLM's response. Unlike static or semi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT), IoT adapts its reasoning path dynamically, based on evolving context, and without generating alternate explorative thoughts which are ultimately discarded. The three components of the IoT framework are (1) an Inner Dialogue Agent (IDA) responsible for generating instructive, context-specific prompts; (2) an LLM Agent (LLMA) that processes these prompts to refine its responses; and (3) an iterative prompting loop that implements a conversation between the former two components. We introduce two variants of our framework: Autonomous Iteration of Thought (AIoT), where an LLM decides when to stop iterating, and Guided Iteration of Thought (GIoT), which always forces a fixed number iterations. We investigate the performance of IoT across various datasets, spanning complex reasoning tasks from the GPQA dataset, explorative problem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop question answering from the HotpotQA dataset. Our results show that IoT represents a viable paradigm for autonomous response refinement in LLMs, showcasing significant improvements over CoT and thereby enabling more adaptive and efficient reasoning systems that minimize human intervention.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-19",
    "authors": [
      {
        "authorId": "46254996",
        "name": "Santosh Kumar Radha"
      },
      {
        "authorId": "2321873048",
        "name": "Yasamin Nouri Jelyani"
      },
      {
        "authorId": "13829278",
        "name": "A. Ghukasyan"
      },
      {
        "authorId": "1768149875",
        "name": "O. Goktas"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "eb76a0f47202e7d535d78d978d3cb9629e49159f",
    "url": "https://www.semanticscholar.org/paper/eb76a0f47202e7d535d78d978d3cb9629e49159f",
    "title": "Interpretable Contrastive Monte Carlo Tree Search Reasoning",
    "abstract": "We propose SC-MCTS*: a novel Monte Carlo Tree Search (MCTS) reasoning algorithm for Large Language Models (LLMs), significantly improves both reasoning accuracy and speed. Our motivation comes from: 1. Previous MCTS LLM reasoning works often overlooked its biggest drawback--slower speed compared to CoT; 2. Previous research mainly used MCTS as a tool for LLM reasoning on various tasks with limited quantitative analysis or ablation studies of its components from reasoning interpretability perspective. 3. The reward model is the most crucial component in MCTS, however previous work has rarely conducted in-depth study or improvement of MCTS's reward models. Thus, we conducted extensive ablation studies and quantitative analysis on components of MCTS, revealing the impact of each component on the MCTS reasoning performance of LLMs. Building on this, (i) we designed a highly interpretable reward model based on the principle of contrastive decoding and (ii) achieved an average speed improvement of 51.9% per node using speculative decoding. Additionally, (iii) we improved UCT node selection strategy and backpropagation used in previous works, resulting in significant performance improvement. We outperformed o1-mini by an average of 17.4% on the Blocksworld multi-step reasoning dataset using Llama-3.1-70B with SC-MCTS*. Our code is available at https://github.com/zitian-gao/SC-MCTS.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-02",
    "authors": [
      {
        "authorId": "2301532663",
        "name": "Zitian Gao"
      },
      {
        "authorId": "2323793102",
        "name": "Boye Niu"
      },
      {
        "authorId": "2323898312",
        "name": "Xuzheng He"
      },
      {
        "authorId": "2324643067",
        "name": "Haotian Xu"
      },
      {
        "authorId": "2323848951",
        "name": "Hongzhang Liu"
      },
      {
        "authorId": "10017193",
        "name": "Aiwei Liu"
      },
      {
        "authorId": "2109906988",
        "name": "Xuming Hu"
      },
      {
        "authorId": "2284684419",
        "name": "Lijie Wen"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "073d5e2a7da5b8991dc1030ea813f6a94a731777",
    "url": "https://www.semanticscholar.org/paper/073d5e2a7da5b8991dc1030ea813f6a94a731777",
    "title": "Self-Consistency Boosts Calibration for Math Reasoning",
    "abstract": "Calibration, which establishes the correlation between accuracy and model confidence, is important for LLM development. We design three off-the-shelf calibration methods based on self-consistency (Wang et al., 2022) for math reasoning tasks. Evaluation on two popular benchmarks (GSM8K and MathQA) using strong open-source LLMs (Mistral and LLaMA2), our methods better bridge model confidence and accuracy than existing methods based on p(True) (Kadavath et al., 2022) or logit (Kadavath et al., 2022).",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-14",
    "authors": [
      {
        "authorId": "1754106063",
        "name": "Ante Wang"
      },
      {
        "authorId": "2273672063",
        "name": "Linfeng Song"
      },
      {
        "authorId": "2243391254",
        "name": "Ye Tian"
      },
      {
        "authorId": "2284066568",
        "name": "Baolin Peng"
      },
      {
        "authorId": "50496698",
        "name": "Lifeng Jin"
      },
      {
        "authorId": "2238955130",
        "name": "Haitao Mi"
      },
      {
        "authorId": "2274092718",
        "name": "Jinsong Su"
      },
      {
        "authorId": "2239076081",
        "name": "Dong Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 85.79441541679836
  },
  {
    "paperId": "2c6896c025b29a2cc3d90bbf9b77646b255b2090",
    "url": "https://www.semanticscholar.org/paper/2c6896c025b29a2cc3d90bbf9b77646b255b2090",
    "title": "TPD: Enhancing Student Language Model Reasoning via Principle Discovery and Guidance",
    "abstract": "Large Language Models (LLMs) have recently showcased remarkable reasoning abilities. However, larger models often surpass their smaller counterparts in reasoning tasks, posing the challenge of effectively transferring these capabilities from larger models. Existing approaches heavily rely on extensive fine-tuning data or continuous interactions with a superior teacher LLM during inference. We introduce a principle-based teacher-student framework called ``Teaching via Principle Discovery'' (TPD) to address these limitations. Inspired by human learning mechanisms, TPD mimics the interaction between a teacher and a student using a principle-based approach. The teacher LLM generates problem-solving instructions and corrective principles based on the student LLM's errors. These principles guide the refinement of instructions and the selection of instructive examples from a validation set. This enables the student model to learn from both the teacher's guidance and its own mistakes. Once the student model begins making inferences, TPD requires no further intervention from the teacher LLM or humans. Through extensive experiments across eight reasoning tasks, we demonstrate the effectiveness of TPD. Compared to standard chain-of-thought prompting, TPD significantly improves the student model's performance, achieving $6.2\\%$ improvement on average.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-24",
    "authors": [
      {
        "authorId": "2266420540",
        "name": "Haorui Wang"
      },
      {
        "authorId": "46752897",
        "name": "Rongzhi Zhang"
      },
      {
        "authorId": "1527089853",
        "name": "Yinghao Li"
      },
      {
        "authorId": "2865034",
        "name": "Lingkai Kong"
      },
      {
        "authorId": "8103389",
        "name": "Yuchen Zhuang"
      },
      {
        "authorId": "29963551",
        "name": "Xiusi Chen"
      },
      {
        "authorId": "2267156626",
        "name": "Chao Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "243550d8184eed2fd7627eae133bccc3c72dcd02",
    "url": "https://www.semanticscholar.org/paper/243550d8184eed2fd7627eae133bccc3c72dcd02",
    "title": "Dual Instruction Tuning with Large Language Models for Mathematical Reasoning",
    "abstract": "Recent advancements highlight the success of instruction tuning with large language models (LLMs) utilizing Chain-of-Thought (CoT) data for mathematical reasoning tasks. Despite the fine-tuned LLMs, challenges persist, such as incorrect, missing, and redundant steps in CoT generation leading to inaccuracies in answer predictions. To alleviate this problem, we propose a dual instruction tuning strategy to meticulously model mathematical reasoning from both forward and reverse directions. This involves introducing the Intermediate Reasoning State Prediction task (forward reasoning) and the Instruction Reconstruction task (reverse reasoning) to enhance the LLMs' understanding and execution of instructions. Training instances for these tasks are constructed based on existing mathematical instruction tuning datasets. Subsequently, LLMs undergo multi-task fine-tuning using both existing mathematical instructions and the newly created data. Comprehensive experiments validate the effectiveness and domain generalization of the dual instruction tuning strategy across various mathematical reasoning tasks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-27",
    "authors": [
      {
        "authorId": "2145494002",
        "name": "Yongwei Zhou"
      },
      {
        "authorId": "2293771556",
        "name": "Tiejun Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "4e6bc1a794fc59da3373cfd1d0cedd48a2f6534b",
    "url": "https://www.semanticscholar.org/paper/4e6bc1a794fc59da3373cfd1d0cedd48a2f6534b",
    "title": "Evaluating Interventional Reasoning Capabilities of Large Language Models",
    "abstract": "Numerous decision-making tasks require estimating causal effects under interventions on different parts of a system. As practitioners consider using large language models (LLMs) to automate decisions, studying their causal reasoning capabilities becomes crucial. A recent line of work evaluates LLMs ability to retrieve commonsense causal facts, but these evaluations do not sufficiently assess how LLMs reason about interventions. Motivated by the role that interventions play in causal inference, in this paper, we conduct empirical analyses to evaluate whether LLMs can accurately update their knowledge of a data-generating process in response to an intervention. We create benchmarks that span diverse causal graphs (e.g., confounding, mediation) and variable types, and enable a study of intervention-based reasoning. These benchmarks allow us to isolate the ability of LLMs to accurately predict changes resulting from their ability to memorize facts or find other shortcuts. We evaluate six LLMs on the benchmarks, finding that GPT models show promising accuracy at predicting the intervention effects.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-08",
    "authors": [
      {
        "authorId": "2295669214",
        "name": "Tejas Kasetty"
      },
      {
        "authorId": "133841722",
        "name": "Divyat Mahajan"
      },
      {
        "authorId": "2533850",
        "name": "G. Dziugaite"
      },
      {
        "authorId": "2295670715",
        "name": "Alexandre Drouin"
      },
      {
        "authorId": "2308018690",
        "name": "Dhanya Sridhar"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "b933a1a9ff0ab4079494caa7811b8e4f2a06301d",
    "url": "https://www.semanticscholar.org/paper/b933a1a9ff0ab4079494caa7811b8e4f2a06301d",
    "title": "Software Engineering Methods for AI-Driven Deductive Legal Reasoning",
    "abstract": "The recent proliferation of generative artificial intelligence (AI) technologies such as pre-trained large language models (LLMs) has opened up new frontiers in computational law. An exciting area of development is the use of AI to automate the deductive rule-based reasoning inherent in statutory and contract law. This paper argues that such automated deductive legal reasoning can now be viewed from the lens of software engineering, treating LLMs as interpreters of natural-language programs with natural-language inputs. We show how it is possible to apply principled software engineering techniques to enhance AI-driven legal reasoning of complex statutes and to unlock new applications in automated meta-reasoning such as mutation-guided example generation and metamorphic property-based testing.",
    "venue": "Proceedings of the 2024 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3689492.3690050",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-04-15",
    "authors": [
      {
        "authorId": "3130527",
        "name": "Rohan Padhye"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.79441541679836
  },
  {
    "paperId": "b2561fe791760431e2e8c4263a1e58fb594e6288",
    "url": "https://www.semanticscholar.org/paper/b2561fe791760431e2e8c4263a1e58fb594e6288",
    "title": "GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning",
    "abstract": "Large Language Models (LLMs) are increasingly used for various tasks with graph structures. Though LLMs can process graph information in a textual format, they overlook the rich vision modality, which is an intuitive way for humans to comprehend structural information and conduct general graph reasoning. The potential benefits and capabilities of representing graph structures as visual images (i.e., $\\textit{visual graph}$) are still unexplored. To fill the gap, we innovatively propose an end-to-end framework, called $\\textbf{G}$raph to v$\\textbf{I}$sual and $\\textbf{T}$extual Integr$\\textbf{A}$tion (GITA), which firstly incorporates visual graphs into general graph reasoning. Besides, we establish $\\textbf{G}$raph-based $\\textbf{V}$ision-$\\textbf{L}$anguage $\\textbf{Q}$uestion $\\textbf{A}$nswering (GVLQA) dataset from existing graph data, which is the first vision-language dataset for general graph reasoning purposes. Extensive experiments on the GVLQA dataset and five real-world datasets show that GITA outperforms mainstream LLMs in terms of general graph reasoning capabilities. Moreover, We highlight the effectiveness of the layout augmentation on visual graphs and pretraining on the GVLQA dataset.",
    "venue": "",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-02-03",
    "authors": [
      {
        "authorId": "2273826353",
        "name": "Yanbin Wei"
      },
      {
        "authorId": "2274132285",
        "name": "Shuai Fu"
      },
      {
        "authorId": "2152123946",
        "name": "Weisen Jiang"
      },
      {
        "authorId": "2243335442",
        "name": "James T. Kwok"
      },
      {
        "authorId": "2153638098",
        "name": "Yu Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "b8f3e8b85bef7b0abb187ca4cbcca677cccbcb44",
    "url": "https://www.semanticscholar.org/paper/b8f3e8b85bef7b0abb187ca4cbcca677cccbcb44",
    "title": "Evaluation of large language model performance on the Biomedical Language Understanding and Reasoning Benchmark",
    "abstract": "Background: The ability of large language models (LLMs) to interpret and generate human-like text has been accompanied with speculation about their application in medicine and clinical research. There is limited data available to inform evidence-based decisions on the appropriateness for specific use cases. Methods: We evaluated and compared four general-purpose LLMs (GPT-4, GPT-3.5-turbo, Flan-T5-XXL, and Zephyr-7B-Beta) and a healthcare-specific LLM (MedLLaMA-13B) on a set of 13 datasets - referred to as the Biomedical Language Understanding and Reasoning Benchmark (BLURB) - covering six commonly needed medical natural language processing tasks: named entity recognition (NER); relation extraction; population, interventions, comparators, and outcomes (PICO); sentence similarity; document classification; and question-answering. All models were evaluated without modification. Model performance was assessed according to a range of prompting strategies (formalised as a systematic, reusable prompting framework) and relied on the standard, task-specific evaluation metrics defined by BLURB. Results: Across all tasks, GPT-4 outperformed other LLMs, followed by Flan-T5-XXL and GPT-3.5- turbo, then Zephyr-7b-Beta and MedLLaMA-13B. The most performant prompts for GPT-4 and Flan-T5-XXL both outperformed the previously-reported best results for the PubMedQA task. The domain-specific MedLLaMA-13B achieved lower scores for most tasks except for question-answering tasks. We observed a substantial impact of strategically editing the prompt describing the task and a consistent improvement in performance when including examples semantically similar to the input text in the prompt. Conclusion: These results provide evidence of the potential LLMs may have for medical application and highlight the importance of robust evaluation before adopting LLMs for any specific use cases. Continuing to explore how these emerging technologies can be adapted for the healthcare setting, paired with human expertise, and enhanced through quality control measures will be important research to allow responsible innovation with LLMs in the medical area.",
    "venue": "medRxiv",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://www.medrxiv.org/content/medrxiv/early/2024/05/17/2024.05.17.24307411.full.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-05-17",
    "authors": [
      {
        "authorId": "2302818048",
        "name": "Hui Feng"
      },
      {
        "authorId": "2302410294",
        "name": "Francesco Ronzano"
      },
      {
        "authorId": "2302410044",
        "name": "Jude LaFleur"
      },
      {
        "authorId": "2302409246",
        "name": "Matthew Garber"
      },
      {
        "authorId": "2302817730",
        "name": "Rodrigo de Oliveira"
      },
      {
        "authorId": "2296687891",
        "name": "Kathryn Rough"
      },
      {
        "authorId": "2302409056",
        "name": "Katharine Roth"
      },
      {
        "authorId": "2302409209",
        "name": "Jay Nanavati"
      },
      {
        "authorId": "2302409150",
        "name": "Khaldoun Zine"
      },
      {
        "authorId": "2302410051",
        "name": "El Abidine"
      },
      {
        "authorId": "2302410361",
        "name": "Christina Mack"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "93e2bf97b2664443b108e03950829cc79f3f8419",
    "url": "https://www.semanticscholar.org/paper/93e2bf97b2664443b108e03950829cc79f3f8419",
    "title": "VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment",
    "abstract": "Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art reinforcement learning (RL) algorithm used for LLM finetuning, employs value networks to tackle credit assignment. However, value networks face challenges in predicting the expected cumulative rewards accurately in complex reasoning tasks, often leading to high-variance updates and suboptimal performance. In this work, we systematically evaluate the efficacy of value networks and reveal their significant shortcomings in reasoning-heavy LLM tasks, showing that they barely outperform a random baseline when comparing alternative steps. To address this, we propose VinePPO, a straightforward approach that leverages the flexibility of language environments to compute unbiased Monte Carlo-based estimates, bypassing the need for large value networks. Our method consistently outperforms PPO and other RL-free baselines across MATH and GSM8K datasets with fewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These results emphasize the importance of accurate credit assignment in RL finetuning of LLM and demonstrate VinePPO's potential as a superior alternative.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-02",
    "authors": [
      {
        "authorId": "1754452702",
        "name": "Amirhossein Kazemnejad"
      },
      {
        "authorId": "113383974",
        "name": "Milad Aghajohari"
      },
      {
        "authorId": "27729635",
        "name": "Eva Portelance"
      },
      {
        "authorId": "2041695",
        "name": "Alessandro Sordoni"
      },
      {
        "authorId": "145732771",
        "name": "Siva Reddy"
      },
      {
        "authorId": "2285877959",
        "name": "Aaron C. Courville"
      },
      {
        "authorId": "2299948386",
        "name": "Nicolas Le Roux"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "33038d55738f84187bb8ded80851720ceb29a8b4",
    "url": "https://www.semanticscholar.org/paper/33038d55738f84187bb8ded80851720ceb29a8b4",
    "title": "A Theory Guided Scaffolding Instruction Framework for LLM-Enabled Metaphor Reasoning",
    "abstract": "Metaphor detection is a challenging task in figurative language processing, which aims to distinguish between metaphorical and literal expressions in text. Existing methods tackle metaphor detection via training or fine-tuning discriminative models on labeled data. However, these approaches struggle to explain the underlying reasoning process behind the metaphorical/literal judgment. Recently, large language models (LLMs) have shown promise in language reasoning tasks. Although promising, LLM-based methods for metaphor detection and reasoning are still faced with the challenging issue of bringing the explainable concepts for metaphor reasoning and their linguistic manifestation. To fill this gap, we propose a novel Theory guided Scaffolding Instruction (TSI) framework that instructs an LLM to infer the underlying reasoning process of metaphor detection guided by metaphor theories for the first time. Our work is inspired by a pedagogical strategy called scaffolding instruction, which encourages educators to provide questioning and support as scaffolding so as to assist learners in constructing the understanding of pedagogical goals step by step. We first construct a metaphor knowledge graph grounded in metaphor theory which serves as the instructional structure to obtain a series of scaffolding questions, directing the LLM to incrementally generate the reasoning process for metaphor understanding through dialogue interactions. During this theory guided instruction process, we explore the LLM’s mastery boundary and provide the relevant knowledge as scaffolding support when the question is beyond the LLM’s capability. Experimental results verify that our method significantly outperforms both the LLM-based reasoning methods and the SOTA methods in metaphor detection, indicating the facilitation of metaphor and instruction theories in guiding LLM-based reasoning process.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "144966427",
        "name": "Yuan Tian"
      },
      {
        "authorId": "2150038278",
        "name": "Nan Xu"
      },
      {
        "authorId": "2264558046",
        "name": "Wenji Mao"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "6f826d007e851f0c694fbe0c378d5e0097aa18a2",
    "url": "https://www.semanticscholar.org/paper/6f826d007e851f0c694fbe0c378d5e0097aa18a2",
    "title": "A Survey of Table Reasoning with Large Language Models",
    "abstract": "Table reasoning, which aims to generate the corresponding answer to the question following the user requirement according to the provided table, and optionally a text description of the table, effectively improving the efficiency of obtaining information. Recently, using Large Language Models (LLMs) has become the mainstream method for table reasoning, because it not only significantly reduces the annotation cost but also exceeds the performance of previous methods. However, existing research still lacks a summary of LLM-based table reasoning works. Due to the existing lack of research, questions about which techniques can improve table reasoning performance in the era of LLMs, why LLMs excel at table reasoning, and how to enhance table reasoning abilities in the future, remain largely unexplored. This gap significantly limits progress in research. To answer the above questions and advance table reasoning research with LLMs, we present this survey to analyze existing research, inspiring future work. In this paper, we analyze the mainstream techniques used to improve table reasoning performance in the LLM era, and the advantages of LLMs compared to pre-LLMs for solving table reasoning. We provide research directions from both the improvement of existing methods and the expansion of practical applications to inspire future research.",
    "venue": "Frontiers of Computer Science",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-02-13",
    "authors": [
      {
        "authorId": "2284033114",
        "name": "Xuanliang Zhang"
      },
      {
        "authorId": "2159084043",
        "name": "Dingzirui Wang"
      },
      {
        "authorId": "49093992",
        "name": "Longxu Dou"
      },
      {
        "authorId": "2283927956",
        "name": "Qingfu Zhu"
      },
      {
        "authorId": "2283920108",
        "name": "Wanxiang Che"
      }
    ],
    "source": "semantic_scholar",
    "score": 100.79441541679836
  },
  {
    "paperId": "42bfc69cb6742ced4a0004e7f1596ac176aaa66d",
    "url": "https://www.semanticscholar.org/paper/42bfc69cb6742ced4a0004e7f1596ac176aaa66d",
    "title": "DrHouse: An LLM-empowered Diagnostic Reasoning System through Harnessing Outcomes from Sensor Data and Expert Knowledge",
    "abstract": "Large language models (LLMs) have the potential to transform digital healthcare, as evidenced by recent advances in LLM-based virtual doctors. However, current approaches rely on patient's subjective descriptions of symptoms, causing increased misdiagnosis. Recognizing the value of daily data from smart devices, we introduce a novel LLM-based multi-turn consultation virtual doctor system, DrHouse, which incorporates three significant contributions: 1) It utilizes sensor data from smart devices in the diagnosis process, enhancing accuracy and reliability. 2) DrHouse leverages continuously updating medical knowledge bases to ensure its model remains at diagnostic standard's forefront. 3) DrHouse introduces a novel diagnostic algorithm that concurrently evaluates potential diseases and their likelihood, facilitating more nuanced and informed medical assessments. Through multi-turn interactions, DrHouse determines the next steps, such as accessing daily data from smart devices or requesting in-lab tests, and progressively refines its diagnoses. Evaluations on three public datasets and our self-collected datasets show that DrHouse can achieve up to an 31.5% increase in diagnosis accuracy over the state-of-the-art baselines. The results of a 32-participant user study show that 75% medical experts and 91.7% test subjects are willing to use DrHouse.",
    "venue": "Proceedings of the ACM on Interactive Mobile Wearable and Ubiquitous Technologies",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-21",
    "authors": [
      {
        "authorId": "2267429742",
        "name": "Bufang Yang"
      },
      {
        "authorId": "2302470640",
        "name": "Siyang Jiang"
      },
      {
        "authorId": "2302513213",
        "name": "Lilin Xu"
      },
      {
        "authorId": "2294927293",
        "name": "Kaiwei Liu"
      },
      {
        "authorId": "2302457501",
        "name": "Hai Li"
      },
      {
        "authorId": "2238951251",
        "name": "Guoliang Xing"
      },
      {
        "authorId": "2302458577",
        "name": "Hongkai Chen"
      },
      {
        "authorId": "2302502130",
        "name": "Xiaofan Jiang"
      },
      {
        "authorId": "2261616783",
        "name": "Zhenyu Yan"
      }
    ],
    "source": "semantic_scholar",
    "score": 100.1415686865115
  },
  {
    "paperId": "c37c2eda0b1126f81dc4bda23cdcc240d44f39d9",
    "url": "https://www.semanticscholar.org/paper/c37c2eda0b1126f81dc4bda23cdcc240d44f39d9",
    "title": "Fine-Grained Self-Endorsement Improves Factuality and Reasoning",
    "abstract": "This work studies improving large language model (LLM) generations at inference time by mitigating fact-conflicting hallucinations. Particularly, we propose a self-endorsement framework that leverages the fine-grained fact-level comparisons across multiple sampled responses. Compared with prior ensemble methods (Wang et al., 2022;Chen et al., 2023)) that perform response-level selection, our approach can better alleviate hallucinations, especially for longform generation tasks. Our approach can broadly benefit smaller and open-source LLMs as it mainly conducts simple content-based comparisons. Experiments on Biographies show that our method can effectively improve the factuality of generations with simple and intuitive prompts across different scales of LLMs. Besides, comprehensive analyses on TriviaQA and GSM8K demonstrate the potential of self-endorsement for broader application.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-23",
    "authors": [
      {
        "authorId": "1754106063",
        "name": "Ante Wang"
      },
      {
        "authorId": "2273672063",
        "name": "Linfeng Song"
      },
      {
        "authorId": "2284066568",
        "name": "Baolin Peng"
      },
      {
        "authorId": "2243391254",
        "name": "Ye Tian"
      },
      {
        "authorId": "50496698",
        "name": "Lifeng Jin"
      },
      {
        "authorId": "2238955130",
        "name": "Haitao Mi"
      },
      {
        "authorId": "2274092718",
        "name": "Jinsong Su"
      },
      {
        "authorId": "2239076081",
        "name": "Dong Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "6e4ba9f89a56ae9e16be5379dda7decd45c538ca",
    "url": "https://www.semanticscholar.org/paper/6e4ba9f89a56ae9e16be5379dda7decd45c538ca",
    "title": "Arithmetic Reasoning with LLM: Prolog Generation & Permutation",
    "abstract": "Instructing large language models (LLMs) to solve elementary school math problems has shown great success using Chain of Thought (CoT). However, the CoT approach relies on an LLM to generate a sequence of arithmetic calculations which can be prone to cascaded calculation errors. We hypothesize that an LLM should focus on extracting predicates and generating symbolic formulas from the math problem description so that the underlying calculation can be done via an external code interpreter. We investigate using LLM to generate Prolog programs to solve mathematical questions. Experimental results show that our Prolog-based arithmetic problem-solving outperforms CoT generation in the GSM8K benchmark across three distinct LLMs. In addition, given the insensitive ordering of predicates and symbolic formulas in Prolog, we propose to permute the ground truth predicates for more robust LLM training via data augmentation.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-28",
    "authors": [
      {
        "authorId": "2238383265",
        "name": "Xiaocheng Yang"
      },
      {
        "authorId": "2304145951",
        "name": "Bingsen Chen"
      },
      {
        "authorId": "1789138",
        "name": "Yik-Cheung Tam"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "c5e787caef8da20a8537fefd587531335c03ce72",
    "url": "https://www.semanticscholar.org/paper/c5e787caef8da20a8537fefd587531335c03ce72",
    "title": "MC-GPT: Empowering Vision-and-Language Navigation with Memory Map and Reasoning Chains",
    "abstract": "In the Vision-and-Language Navigation (VLN) task, the agent is required to navigate to a destination following a natural language instruction. While learning-based approaches have been a major solution to the task, they suffer from high training costs and lack of interpretability. Recently, Large Language Models (LLMs) have emerged as a promising tool for VLN due to their strong generalization capabilities. However, existing LLM-based methods face limitations in memory construction and diversity of navigation strategies. To address these challenges, we propose a suite of techniques. Firstly, we introduce a method to maintain a topological map that stores navigation history, retaining information about viewpoints, objects, and their spatial relationships. This map also serves as a global action space. Additionally, we present a Navigation Chain of Thoughts module, leveraging human navigation examples to enrich navigation strategy diversity. Finally, we establish a pipeline that integrates navigational memory and strategies with perception and action prediction modules. Experimental results on the REVERIE and R2R datasets show that our method effectively enhances the navigation ability of the LLM and improves the interpretability of navigation reasoning.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-17",
    "authors": [
      {
        "authorId": "2301932789",
        "name": "Zhaohuan Zhan"
      },
      {
        "authorId": "2302279883",
        "name": "Lisha Yu"
      },
      {
        "authorId": "2302283535",
        "name": "Sijie Yu"
      },
      {
        "authorId": "2302153396",
        "name": "Guang Tan"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "2c9b4a60c21afaaf3bbddff074aae6605b205acf",
    "url": "https://www.semanticscholar.org/paper/2c9b4a60c21afaaf3bbddff074aae6605b205acf",
    "title": "Causal Agent based on Large Language Model",
    "abstract": "Large language models (LLMs) have achieved significant success across various domains. However, the inherent complexity of causal problems and causal theory poses challenges in accurately describing them in natural language, making it difficult for LLMs to comprehend and use them effectively. Causal methods are not easily conveyed through natural language, which hinders LLMs' ability to apply them accurately. Additionally, causal datasets are typically tabular, while LLMs excel in handling natural language data, creating a structural mismatch that impedes effective reasoning with tabular data. This lack of causal reasoning capability limits the development of LLMs. To address these challenges, we have equipped the LLM with causal tools within an agent framework, named the Causal Agent, enabling it to tackle causal problems. The causal agent comprises tools, memory, and reasoning modules. In the tools module, the causal agent applies causal methods to align tabular data with natural language. In the reasoning module, the causal agent employs the ReAct framework to perform reasoning through multiple iterations with the tools. In the memory module, the causal agent maintains a dictionary instance where the keys are unique names and the values are causal graphs. To verify the causal ability of the causal agent, we established a benchmark consisting of four levels of causal problems: variable level, edge level, causal graph level, and causal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for these four levels of issues and tested the causal agent on the datasets. Our methodology demonstrates remarkable efficacy on the four-level causal problems, with accuracy rates all above 80%. For further insights and implementation details, our code is accessible via the GitHub repository https://github.com/Kairong-Han/Causal_Agent.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-13",
    "authors": [
      {
        "authorId": "2315988838",
        "name": "Kairong Han"
      },
      {
        "authorId": "33870528",
        "name": "Kun Kuang"
      },
      {
        "authorId": "2279090109",
        "name": "Ziyu Zhao"
      },
      {
        "authorId": "2316167378",
        "name": "Junjian Ye"
      },
      {
        "authorId": "2316159866",
        "name": "Fei Wu"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "e850d4c0dad3aee3b8b40be5e5d5e5c31354d8cc",
    "url": "https://www.semanticscholar.org/paper/e850d4c0dad3aee3b8b40be5e5d5e5c31354d8cc",
    "title": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change",
    "abstract": "Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks-where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.",
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "citationCount": 147,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-06-21",
    "authors": [
      {
        "authorId": "144982263",
        "name": "Karthik Valmeekam"
      },
      {
        "authorId": "2060820816",
        "name": "Alberto Olmo"
      },
      {
        "authorId": "2400282",
        "name": "S. Sreedharan"
      },
      {
        "authorId": "2047340230",
        "name": "Subbarao Kambhampati"
      }
    ],
    "source": "semantic_scholar",
    "score": 151.95818410646172
  },
  {
    "paperId": "fe7f2a0535c82b10f82623683257696685030cfb",
    "url": "https://www.semanticscholar.org/paper/fe7f2a0535c82b10f82623683257696685030cfb",
    "title": "MalAlgoQA: A Pedagogical Approach for Evaluating Counterfactual Reasoning Abilities",
    "abstract": "This paper introduces MalAlgoQA, a novel dataset designed to evaluate the counterfactual reasoning capabilities of Large Language Models (LLMs) through a pedagogical approach. The dataset comprises mathematics and reading comprehension questions, each accompanied by four answer choices and their corresponding rationales. We focus on the incorrect answer rationales, termed \"malgorithms\", which highlights flawed reasoning steps leading to incorrect answers and offers valuable insights into erroneous thought processes. We also propose the Malgorithm Identification task, where LLMs are assessed based on their ability to identify corresponding malgorithm given an incorrect answer choice. To evaluate the model performance, we introduce two metrics: Algorithm Identification Accuracy (AIA) for correct answer rationale identification, and Malgorithm Identification Accuracy (MIA) for incorrect answer rationale identification. The task is challenging since state-of-the-art LLMs exhibit significant drops in MIA as compared to AIA. Moreover, we find that the chain-of-thought prompting technique not only fails to consistently enhance MIA, but can also lead to underperformance compared to simple prompting. These findings hold significant implications for the development of more cognitively-inspired LLMs to improve their counterfactual reasoning abilities, particularly through a pedagogical perspective where understanding and rectifying student misconceptions are crucial. The data and code can be found at https://github.com/luffycodes/MalAlgoQA-Dataset.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2143066732",
        "name": "Naiming Liu"
      },
      {
        "authorId": "1720691070",
        "name": "Shashank Sonkar"
      },
      {
        "authorId": "2243334456",
        "name": "Myco Le"
      },
      {
        "authorId": "2249763597",
        "name": "R. Baraniuk"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "4fc87dc2ac306a1527c60f3cd55534d412de078b",
    "url": "https://www.semanticscholar.org/paper/4fc87dc2ac306a1527c60f3cd55534d412de078b",
    "title": "Tree-of-Reasoning Question Decomposition for Complex Question Answering with Large Language Models",
    "abstract": "Large language models (LLMs) have recently demonstrated remarkable performance across various Natual Language Processing tasks. In the field of multi-hop reasoning, the Chain-of-thought (CoT) prompt method has emerged as a paradigm, using curated stepwise reasoning demonstrations to enhance LLM's ability to reason and produce coherent rational pathways. To ensure the accuracy, reliability, and traceability of the generated answers, many studies have incorporated information retrieval (IR) to provide LLMs with external knowledge. However, existing CoT with IR methods decomposes questions into sub-questions based on a single compositionality type, which limits their effectiveness for questions involving multiple compositionality types. Additionally, these methods suffer from inefficient retrieval, as complex questions often contain abundant information, leading to the retrieval of irrelevant information inconsistent with the query's intent. In this work, we propose a novel question decomposition framework called TRQA for multi-hop question answering, which addresses these limitations. Our framework introduces a reasoning tree (RT) to represent the structure of complex questions. It consists of four components: the Reasoning Tree Constructor (RTC), the Question Generator (QG), the Retrieval and LLM Interaction Module (RAIL), and the Answer Aggregation Module (AAM). Specifically, the RTC predicts diverse sub-question structures to construct the reasoning tree, allowing a more comprehensive representation of complex questions. The QG generates sub-questions for leaf-node in the reasoning tree, and we explore two methods for QG: prompt-based and T5-based approaches. The IR module retrieves documents aligned with sub-questions, while the LLM formulates answers based on the retrieved information. Finally, the AAM aggregates answers along the reason tree, producing a definitive response from bottom to top.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "https://ojs.aaai.org/index.php/AAAI/article/download/29928/31621",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-24",
    "authors": [
      {
        "authorId": "2273821894",
        "name": "Kun Zhang"
      },
      {
        "authorId": "2279915145",
        "name": "Jiali Zeng"
      },
      {
        "authorId": "33427918",
        "name": "Fandong Meng"
      },
      {
        "authorId": "2273941833",
        "name": "Yuanzhuo Wang"
      },
      {
        "authorId": "2275709795",
        "name": "Shiqi Sun"
      },
      {
        "authorId": "2293477689",
        "name": "Long Bai"
      },
      {
        "authorId": "2293781045",
        "name": "Huawei Shen"
      },
      {
        "authorId": "2265517237",
        "name": "Jie Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "124b40ec883eb59e89be76e5da5489dbed89275c",
    "url": "https://www.semanticscholar.org/paper/124b40ec883eb59e89be76e5da5489dbed89275c",
    "title": "RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models",
    "abstract": "This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), a versatile extension to the mutual reasoning framework (rStar), aimed at enhancing reasoning accuracy and factual integrity across large language models (LLMs) for complex, knowledge-intensive tasks such as commonsense and medical reasoning. RARE incorporates two innovative actions within the Monte Carlo Tree Search (MCTS) framework: A6, which generates search queries based on the initial problem statement, performs information retrieval using those queries, and augments reasoning with the retrieved data to formulate the final answer; and A7, which leverages information retrieval specifically for generated sub-questions and re-answers these sub-questions with the relevant contextual information. Additionally, a Retrieval-Augmented Factuality Scorer is proposed to replace the original discriminator, prioritizing reasoning paths that meet high standards of factuality. Experimental results with LLaMA 3.1 show that RARE enables open-source LLMs to achieve competitive performance with top open-source models like GPT-4 and GPT-4o. This research establishes RARE as a scalable solution for improving LLMs in domains where logical coherence and factual integrity are critical.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-12-03",
    "authors": [
      {
        "authorId": "2263121822",
        "name": "Hieu Tran"
      },
      {
        "authorId": "1576489304",
        "name": "Zonghai Yao"
      },
      {
        "authorId": "2261394407",
        "name": "Junda Wang"
      },
      {
        "authorId": "2333589853",
        "name": "Yifan Zhang"
      },
      {
        "authorId": "2261462978",
        "name": "Zhichao Yang"
      },
      {
        "authorId": "2261455807",
        "name": "Hong Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "b023471edcedf34f9ec9732b8f553bd52880bbfa",
    "url": "https://www.semanticscholar.org/paper/b023471edcedf34f9ec9732b8f553bd52880bbfa",
    "title": "IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across textual and visual domains but often generate outputs that violate physical laws, revealing a gap in their understanding of the physical world. Inspired by human cognition, where perception is fundamental to reasoning, we explore augmenting LLMs with enhanced perception abilities using Internet of Things (IoT) sensor data and pertinent knowledge for IoT task reasoning in the physical world. In this work, we systematically study LLMs capability to address real-world IoT tasks by augmenting their perception and knowledge base, and then propose a unified framework, IoT-LLM, to enhance such capability. In IoT-LLM, we customize three steps for LLMs: preprocessing IoT data into formats amenable to LLMs, activating their commonsense knowledge through chain-of-thought prompting and specialized role definitions, and expanding their understanding via IoT-oriented retrieval-augmented generation based on in-context learning. To evaluate the performance, We design a new benchmark with five real-world IoT tasks with different data types and reasoning difficulties and provide the benchmarking results on six open-source and close-source LLMs. Experimental results demonstrate the limitations of existing LLMs with naive textual inputs that cannot perform these tasks effectively. We show that IoT-LLM significantly enhances the performance of IoT tasks reasoning of LLM, such as GPT-4, achieving an average improvement of 65% across various tasks against previous methods. The results also showcase LLMs ability to comprehend IoT data and the physical law behind data by providing a reasoning process. Limitations of our work are claimed to inspire future research in this new era.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-03",
    "authors": [
      {
        "authorId": "2324048327",
        "name": "Tuo An"
      },
      {
        "authorId": "2118117287",
        "name": "Yunjiao Zhou"
      },
      {
        "authorId": "2324001224",
        "name": "Han Zou"
      },
      {
        "authorId": "2324068577",
        "name": "Jianfei Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "c0489aee8617519d0e1d012ae24c5f7e46f3ae03",
    "url": "https://www.semanticscholar.org/paper/c0489aee8617519d0e1d012ae24c5f7e46f3ae03",
    "title": "0x.Yuan at SemEval-2024 Task 5: Enhancing Legal Argument Reasoning with Structured Prompts",
    "abstract": "The intersection of legal reasoning and Natural Language Processing (NLP) technologies, particularly Large Language Models (LLMs), offers groundbreaking potential for augmenting human capabilities in the legal domain. This paper presents our approach and findings from participating in SemEval-2024 Task 5, focusing on the effect of argument reasoning in civil procedures using legal reasoning prompts. We investigated the impact of structured legal reasoning methodologies, including TREACC, IRAC, IRAAC, and MIRAC, on guiding LLMs to analyze and evaluate legal arguments systematically. Our experimental setup involved crafting specific prompts based on these methodologies to instruct the LLM to dissect and scrutinize legal cases, aiming to discern the cogency of argumentative solutions within a zero-shot learning framework. The performance of our approach, as measured by F1 score and accuracy, demonstrated the efficacy of integrating structured legal reasoning into LLMs for legal analysis. The findings underscore the promise of LLMs, when equipped with legal reasoning prompts, in enhancing their ability to process and reason through complex legal texts, thus contributing to the broader application of AI in legal studies and practice.",
    "venue": "International Workshop on Semantic Evaluation",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2308917090",
        "name": "Yu-An Lu"
      },
      {
        "authorId": "2308473921",
        "name": "Hung-yu Kao"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "3ad07bcd566a2307ff02ee7547c714763451de14",
    "url": "https://www.semanticscholar.org/paper/3ad07bcd566a2307ff02ee7547c714763451de14",
    "title": "Chain-of-History Reasoning for Temporal Knowledge Graph Forecasting",
    "abstract": "Temporal Knowledge Graph (TKG) forecasting aims to predict future facts based on given histories. Most recent graph-based models excel at capturing structural information within TKGs but lack semantic comprehension abilities. Nowadays, with the surge of LLMs, the LLM-based TKG prediction model has emerged. However, the existing LLM-based model exhibits three shortcomings: (1) It only focuses on the first-order history for prediction while ignoring high-order historical information, resulting in the provided information for LLMs being extremely limited. (2) LLMs struggle with optimal reasoning performance under heavy historical information loads. (3) For TKG prediction, the temporal reasoning capability of LLM alone is limited. To address the first two challenges, we propose Chain-of-History (CoH) reasoning which explores high-order histories step-by-step, achieving effective utilization of high-order historical information for LLMs on TKG prediction. To address the third issue, we design CoH as a plug-and-play module to enhance the performance of graph-based models for TKG prediction. Extensive experiments on three datasets and backbones demonstrate the effectiveness of CoH.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2402.14382",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-22",
    "authors": [
      {
        "authorId": "1625825549",
        "name": "Yuwei Xia"
      },
      {
        "authorId": "2284756529",
        "name": "Ding Wang"
      },
      {
        "authorId": "48873756",
        "name": "Q. Liu"
      },
      {
        "authorId": "2258184369",
        "name": "Liang Wang"
      },
      {
        "authorId": "50425438",
        "name": "Shu Wu"
      },
      {
        "authorId": "2115796365",
        "name": "Xiaoyu Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "2cf24022c8bd374909ff05a1d88aea2b021c91e6",
    "url": "https://www.semanticscholar.org/paper/2cf24022c8bd374909ff05a1d88aea2b021c91e6",
    "title": "GenDec: A robust generative Question-decomposition method for Multi-hop reasoning",
    "abstract": "Multi-hop QA (MHQA) involves step-by-step reasoning to answer complex questions and find multiple relevant supporting facts. However, Existing large language models'(LLMs) reasoning ability in multi-hop question answering remains exploration, which is inadequate in answering multi-hop questions. Moreover, it is unclear whether LLMs follow a desired reasoning chain to reach the right final answer. In this paper, we propose a \\textbf{gen}erative question \\textbf{dec}omposition method (GenDec) from the perspective of explainable QA by generating independent and complete sub-questions based on incorporating additional extracted evidence for enhancing LLMs' reasoning ability in RAG. To demonstrate the impact, generalization, and robustness of Gendec, we conduct two experiments, the first is combining GenDec with small QA systems on paragraph retrieval and QA tasks. We secondly examine the reasoning capabilities of various state-of-the-art LLMs including GPT-4 and GPT-3.5 combined with GenDec. We experiment on the HotpotQA, 2WikihopMultiHopQA, MuSiQue, and PokeMQA datasets.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-17",
    "authors": [
      {
        "authorId": "2266715262",
        "name": "Jian Wu"
      },
      {
        "authorId": "2284931437",
        "name": "Linyi Yang"
      },
      {
        "authorId": "2284731471",
        "name": "Yuliang Ji"
      },
      {
        "authorId": "2284758952",
        "name": "Wenhao Huang"
      },
      {
        "authorId": "2047947436",
        "name": "Börje F. Karlsson"
      },
      {
        "authorId": "2266466660",
        "name": "Manabu Okumura"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "3bbf541c2a3ce0ffaed3703cd486f0e1b2febb27",
    "url": "https://www.semanticscholar.org/paper/3bbf541c2a3ce0ffaed3703cd486f0e1b2febb27",
    "title": "Token-Budget-Aware LLM Reasoning",
    "abstract": "Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework, which dynamically estimates token budgets for different problems based on reasoning complexity and uses the estimated token budgets to guide the reasoning process. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-12-24",
    "authors": [
      {
        "authorId": "2170360833",
        "name": "Tingxu Han"
      },
      {
        "authorId": "2154723145",
        "name": "Zhenting Wang"
      },
      {
        "authorId": "2239197945",
        "name": "Chunrong Fang"
      },
      {
        "authorId": "2110773055",
        "name": "Shiyun Zhao"
      },
      {
        "authorId": "2333472479",
        "name": "Shiqing Ma"
      },
      {
        "authorId": "2238950128",
        "name": "Zhenyu Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "7b699f364fbbbb1c5fb1d4b5e73345f56c2fa722",
    "url": "https://www.semanticscholar.org/paper/7b699f364fbbbb1c5fb1d4b5e73345f56c2fa722",
    "title": "PACAR: Automated Fact-Checking with Planning and Customized Action Reasoning Using Large Language Models",
    "abstract": "In an era characterized by the rapid proliferation of information, the pervasive issues of misinformation and disinformation have significantly impacted numerous individuals. Consequently, the evaluation of information’s truthfulness and accuracy has garnered substantial attention among researchers. In this work, we present a novel fact-checking framework called PACAR, fact-checking based on planning and customized action reasoning using LLMs. It comprises four modules: a claim decomposer with self-reflection, an LLM-centric planner module, an executor for carrying out planned actions, and a verifier module that assesses veracity and generates explanations based on the overall reasoning process. Unlike previous work that employs single-path decision-making and single-step verdict prediction, PACAR focuses on the use of LLMs in dynamic planning and execution of actions. Furthermore, in contrast to previous work that relied primarily on general reasoning, we introduce tailored actions such as numerical reasoning and entity disambiguation to effectively address potential challenges in fact-checking. Our PACAR framework, incorporating LLM-centric planning along with customized action reasoning, significantly outperforms baseline methods across three datasets from different domains and with varying complexity levels. Additional experiments, including multidimensional and sliced observations, demonstrate the effectiveness of PACAR and offer valuable insights for the advancement of automated fact-checking.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2282306215",
        "name": "Xiaoyan Zhao"
      },
      {
        "authorId": "2282353702",
        "name": "Lingzhi Wang"
      },
      {
        "authorId": "2301628717",
        "name": "Zhanghao Wang"
      },
      {
        "authorId": "2301759628",
        "name": "Hong Cheng"
      },
      {
        "authorId": "2274191373",
        "name": "Rui Zhang"
      },
      {
        "authorId": "2264107863",
        "name": "Kam-Fai Wong"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "9d4624e0d7ff80712e6d82ebff025d108bb2dcc2",
    "url": "https://www.semanticscholar.org/paper/9d4624e0d7ff80712e6d82ebff025d108bb2dcc2",
    "title": "ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion Classification",
    "abstract": "Improving the accessibility of psychotherapy with the aid of Large Language Models (LLMs) is garnering a significant attention in recent years. Recognizing cognitive distortions from the interviewee’s utterances can be an essential part of psychotherapy, especially for cognitive behavioral therapy. In this paper, we propose ERD, which improves LLM-based cognitive distortion classification performance with the aid of additional modules of (1) extracting the parts related to cognitive distortion, and (2) debating the reasoning steps by multiple agents. Our experimental results on a public dataset show that ERD improves the multi-class F1 score as well as binary specificity score. Regarding the latter score, it turns out that our method is effective in debiasing the baseline method which has high false positive rate, especially when the summary of multi-agent debate is provided to LLMs.",
    "venue": "Clinical Natural Language Processing Workshop",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-21",
    "authors": [
      {
        "authorId": "2292666978",
        "name": "Sehee Lim"
      },
      {
        "authorId": "2292612628",
        "name": "Yejin Kim"
      },
      {
        "authorId": "2293135908",
        "name": "Chi-Hyun Choi"
      },
      {
        "authorId": "8414722",
        "name": "Jy-yong Sohn"
      },
      {
        "authorId": "2292416594",
        "name": "Byung-Hoon Kim"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "eda16ec7223c0e96cd92e720a408c6222d815af1",
    "url": "https://www.semanticscholar.org/paper/eda16ec7223c0e96cd92e720a408c6222d815af1",
    "title": "Persona is a Double-edged Sword: Enhancing the Zero-shot Reasoning by Ensembling the Role-playing and Neutral Prompts",
    "abstract": "Recent studies demonstrate that prompting an appropriate role-playing persona to an LLM improves its reasoning capability. However, assigning a proper persona is difficult since an LLM’s performance is extremely sensitive to assigned prompts; therefore, personas some-times hinder LLMs and degrade their reasoning capabilities. In this paper, we propose a novel framework, Jekyll & Hyde, which ensembles the results of role-playing and neutral prompts to eradicate performance degradation via uni-lateral use of role-playing prompted LLM and enhance the robustness of an LLM’s reasoning ability. Specifically, Jekyll & Hyde collects two potential solutions from both role-playing and neutral prompts and selects a better solu-tion after cross-checking via an LLM evaluator. However, LLM-based evaluators tend to be affected by the order of those potential solutions within the prompt when selecting the proper solution; thus, we also propose a robust LLM evaluator to mitigate the position bias. The experimental analysis demonstrates that role-playing prompts distract LLMs and degrade their reasoning abilities in 4 out of 12 datasets, even when using GPT-4. In addition, we reveal that Jekyll & Hyde improves reasoning capabilities by selecting better choices among the potential solutions on twelve widely-used reasoning datasets. We further show that our proposed LLM evaluator outperforms other baselines, proving the LLMs’ position bias is successfully mitigated.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2297330711",
        "name": "Junseok Kim"
      },
      {
        "authorId": "2153401901",
        "name": "Nakyeong Yang"
      },
      {
        "authorId": "2249538747",
        "name": "Kyomin Jung"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "b9c304cd9449d98014d21e44ce77add601e5ca04",
    "url": "https://www.semanticscholar.org/paper/b9c304cd9449d98014d21e44ce77add601e5ca04",
    "title": "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models",
    "abstract": "Large Language Models (LLMs) demonstrate ever-increasing abilities in mathematical and algorithmic tasks, yet their geometric reasoning skills are underexplored. We investigate LLMs' abilities in constructive geometric problem-solving one of the most fundamental steps in the development of human mathematical reasoning. Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas. LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements. To this end, we introduce a framework that formulates an LLMs-based multi-agents system that enhances their existing reasoning potential by conducting an internal dialogue. This work underscores LLMs' current limitations in geometric reasoning and improves geometric reasoning capabilities through self-correction, collaboration, and diverse role specializations.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-06",
    "authors": [
      {
        "authorId": "148099243",
        "name": "Spyridon Mouselinos"
      },
      {
        "authorId": "47407464",
        "name": "H. Michalewski"
      },
      {
        "authorId": "2282977129",
        "name": "Mateusz Malinowski"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "dd40699c4a42cf5f61087cd2b068442750375b1b",
    "url": "https://www.semanticscholar.org/paper/dd40699c4a42cf5f61087cd2b068442750375b1b",
    "title": "Large Language Models Need Consultants for Reasoning: Becoming an Expert in a Complex Human System Through Behavior Simulation",
    "abstract": "Large language models (LLMs), in conjunction with various reasoning reinforcement methodologies, have demonstrated remarkable capabilities comparable to humans in fields such as mathematics, law, coding, common sense, and world knowledge. In this paper, we delve into the reasoning abilities of LLMs within complex human systems. We propose a novel reasoning framework, termed ``Mosaic Expert Observation Wall'' (MEOW) exploiting generative-agents-based simulation technique. In the MEOW framework, simulated data are utilized to train an expert model concentrating ``experience'' about a specific task in each independent time of simulation. It is the accumulated ``experience'' through the simulation that makes for an expert on a task in a complex human system. We conduct the experiments within a communication game that mirrors real-world security scenarios. The results indicate that our proposed methodology can cooperate with existing methodologies to enhance the reasoning abilities of LLMs in complex human systems.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-27",
    "authors": [
      {
        "authorId": "2289863541",
        "name": "Chuwen Wang"
      },
      {
        "authorId": "2279791694",
        "name": "Shirong Zeng"
      },
      {
        "authorId": "2289946585",
        "name": "Cheng Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "4f8608bb4b5943032bd23995a5b44d04d7faba3c",
    "url": "https://www.semanticscholar.org/paper/4f8608bb4b5943032bd23995a5b44d04d7faba3c",
    "title": "Probe Then Retrieve and Reason: Distilling Probing and Reasoning Capabilities into Smaller Language Models",
    "abstract": "Step-by-step reasoning methods, such as the Chain-of-Thought (CoT), have been demonstrated to be highly effective in harnessing the reasoning capabilities of Large Language Models (LLMs). Recent research efforts have sought to distill LLMs into Small Language Models (SLMs), with a significant focus on transferring the reasoning capabilities of LLMs to SLMs via CoT. However, the outcomes of CoT distillation are inadequate for knowledge-intensive reasoning tasks. This is because generating accurate rationales requires crucial factual knowledge, which SLMs struggle to retain due to their parameter constraints. We propose a retrieval-based CoT distillation framework, named Probe then Retrieve and Reason (PRR), which distills the question probing and reasoning capabilities from LLMs into SLMs. We train two complementary distilled SLMs, a probing model and a reasoning model, in tandem. When presented with a new question, the probing model first identifies the necessary knowledge to answer it, generating queries for retrieval. Subsequently, the reasoning model uses the retrieved knowledge to construct a step-by-step rationale for the answer. In knowledge-intensive reasoning tasks, such as StrategyQA and OpenbookQA, our distillation framework yields superior performance for SLMs compared to conventional methods, including simple CoT distillation and knowledge-augmented distillation using raw questions.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2109745506",
        "name": "Yichun Zhao"
      },
      {
        "authorId": "2301799603",
        "name": "Shuheng Zhou"
      },
      {
        "authorId": "2301637080",
        "name": "Huijia Zhu"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "2ed0504ec0a97c3419a97e888abbf50f2b6976ab",
    "url": "https://www.semanticscholar.org/paper/2ed0504ec0a97c3419a97e888abbf50f2b6976ab",
    "title": "AuRoRA: A One-for-all Platform for Augmented Reasoning and Refining with Task-Adaptive Chain-of-Thought Prompting",
    "abstract": "Large language models (LLMs) empowered by chain-of-thought (CoT) prompting have yielded remarkable prowess in reasoning tasks. Nevertheless, current methods predominantly lean on handcrafted or task-specific demonstrations, lack reliable knowledge basis and thus struggle for trustworthy responses in an automated pattern. While recent works endeavor to improve upon one certain aspect, they ignore the importance and necessity of establishing an integrated and interpretable reasoning system. To address these drawbacks and provide a universal solution, we propose AuRoRA: a one-for-all platform for augmented reasoning and refining based on CoT prompting that excels in adaptability, reliability, integrity, and interpretability. The system exhibits superior performances across six reasoning tasks and offers real-time visual analysis, which has pivotal academic and application value in the era of LLMs. The AuRoRA platform is available at https://huggingface.co/spaces/Anni123/AuRoRA.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2187586628",
        "name": "Anni Zou"
      },
      {
        "authorId": "3322871",
        "name": "Zhuosheng Zhang"
      },
      {
        "authorId": "2257373879",
        "name": "Hai Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "431bec13188dfecf66b2520e896b705d4c967fe0",
    "url": "https://www.semanticscholar.org/paper/431bec13188dfecf66b2520e896b705d4c967fe0",
    "title": "Automated Theorem Provers Help Improve Large Language Model Reasoning",
    "abstract": "In this paper we demonstrate how logic programming systems and Automated first- order logic Theorem Provers (ATPs) can improve the accuracy of Large Language Models (LLMs) for logical reasoning tasks where the baseline performance is given by direct LLM solutions. We first evaluate LLM reasoning on steamroller problems using the PRON- TOQA benchmark. We show how accuracy can be improved with a neuro-symbolic ar- chitecture where the LLM acts solely as a front-end for translating a given problem into a formal logic language and an automated reasoning engine is called for solving it. How- ever, this approach critically hinges on the correctness of the LLM translation. To assess this translation correctness, we secondly define a framework of syntactic and semantic er- ror categories. We implemented the framework and used it to identify errors that LLMs make in the benchmark domain. Based on these findings, we thirdly extended our method with capabilities for automatically correcting syntactic and semantic errors. For semantic error correction we integrate first-order logic ATPs, which is our main and novel contribu- tion. We demonstrate that this approach reduces semantic errors significantly and further increases the accurracy of LLM logical reasoning.",
    "venue": "Logic Programming and Automated Reasoning",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "https://easychair.org/publications/open/vzpW",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-08-07",
    "authors": [
      {
        "authorId": "31727676",
        "name": "Lachlan McGinness"
      },
      {
        "authorId": "2303557186",
        "name": "Peter Baumgartner"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "2d362392fb0e13baf77e8ee35b5543e08207e97e",
    "url": "https://www.semanticscholar.org/paper/2d362392fb0e13baf77e8ee35b5543e08207e97e",
    "title": "Causal Reasoning in Large Language Models using Causal Graph Retrieval Augmented Generation",
    "abstract": "Large Language Models (LLMs) are leading the Generative Artificial Intelligence transformation in natural language understanding. Beyond language understanding, LLMs have demonstrated capabilities in reasoning tasks, including commonsense, logical, and mathematical reasoning. However, their proficiency in causal understanding has been limited due to the complex nature of causal reasoning. Several recent studies have discussed the role of external causal models for improved causal understanding. Building on the success of Retrieval-Augmented Generation (RAG) for factual reasoning in LLMs, this paper introduces a novel approach that utilizes Causal Graphs as external sources for establishing causal relationships between complex vectors. This method is empirically evaluated using two benchmark datasets across the metrics of Context Relevance, Answer Relevance, and Grounding, in its ability to retrieve relevant context with causal alignment. The retrieval effectiveness is further compared with traditional RAG methods that are based on semantic proximity.",
    "venue": "International Conference on Human System Interaction",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-08",
    "authors": [
      {
        "authorId": "51437913",
        "name": "Chamod Samarajeewa"
      },
      {
        "authorId": "144286739",
        "name": "Daswin De Silva"
      },
      {
        "authorId": "2276845387",
        "name": "Evgeny Osipov"
      },
      {
        "authorId": "143775049",
        "name": "D. Alahakoon"
      },
      {
        "authorId": "2185595070",
        "name": "Milos Manic"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "881b763caaa228f0725ae1cd93e0ebd5d137a1d0",
    "url": "https://www.semanticscholar.org/paper/881b763caaa228f0725ae1cd93e0ebd5d137a1d0",
    "title": "Towards Analysis and Interpretation of Large Language Models for Arithmetic Reasoning",
    "abstract": "Large Language Models (LLMs) have recently conquered the research scene, with particular regards to the Transformer architecture in the context of arithmetic reasoning. In this so-delineated scenario, this paper puts the basis for a causal mediation analysis about the approach of Transformer-based LLMs to complex arithmetic problems. In particular, we try to discover which parameters are crucial for complex reasoning tasks such as model activations. Our preliminary results state that, for complex arithmetic operations, information is channeled from mid-layer activations to the final token through enhanced attention mechanisms. Preliminary experiments are reported.",
    "venue": "Swiss Conference on Data Science",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-30",
    "authors": [
      {
        "authorId": "2127959940",
        "name": "Mst. Shapna Akter"
      },
      {
        "authorId": "2277554652",
        "name": "Hossain Shahriar"
      },
      {
        "authorId": "2293984775",
        "name": "Alfredo Cuzzocrea"
      }
    ],
    "source": "semantic_scholar",
    "score": 85.39720770839918
  },
  {
    "paperId": "df591b3d26e94f1101e29647ee6703d56ae2f714",
    "url": "https://www.semanticscholar.org/paper/df591b3d26e94f1101e29647ee6703d56ae2f714",
    "title": "ArgMed-Agents: Explainable Clinical Decision Reasoning with LLM Disscusion via Argumentation Schemes",
    "abstract": "There are two main barriers to using large language models (LLMs) in clinical reasoning. Firstly, while LLMs exhibit significant promise in Natural Language Processing (NLP) tasks, their performance in complex reasoning and planning falls short of expectations. Secondly, LLMs use uninterpretable methods to make clinical decisions that are fundamentally different from the clinician’s cognitive processes. This leads to user distrust. In this paper, we present a multi-agent framework called ArgMed-Agents, which aims to enable LLM-based agents to make explainable clinical decision reasoning through interaction. ArgMed-Agents performs self-argumentation iterations via Argumentation Scheme for Clinical Discussion (a reasoning mechanism for modeling cognitive processes in clinical reasoning), and then constructs the argumentation process as a directed graph representing conflicting relationships. Ultimately, use symbolic solver to identify a series of rational and coherent arguments to support decision. We construct a formal model of ArgMed-Agents and present conjectures for theoretical guarantees. ArgMed-Agents enables LLMs to mimic the process of clinical argumentative reasoning by generating explanations of reasoning in a self-directed manner. The setup experiments show that ArgMed-Agents not only improves accuracy in complex clinical decision reasoning problems compared to other prompt methods, but more importantly, it provides users with decision explanations that increase their confidence.",
    "venue": "IEEE International Conference on Bioinformatics and Biomedicine",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2403.06294",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2024-03-10",
    "authors": [
      {
        "authorId": "2279932265",
        "name": "Shengxin Hong"
      },
      {
        "authorId": "2280081126",
        "name": "Liang Xiao"
      },
      {
        "authorId": "2290897459",
        "name": "Xin Zhang"
      },
      {
        "authorId": "2242983688",
        "name": "Jian-Xing Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.39720770839918
  },
  {
    "paperId": "63d0e5a8f195b1453006781d4d8a4eb7262652d9",
    "url": "https://www.semanticscholar.org/paper/63d0e5a8f195b1453006781d4d8a4eb7262652d9",
    "title": "Logical Reasoning over Natural Language as Knowledge Representation: A Survey",
    "abstract": "Logical reasoning is central to human cognition and intelligence. It includes deductive, inductive, and abductive reasoning. Past research of logical reasoning within AI uses formal language as knowledge representation and symbolic reasoners. However, reasoning with formal language has proved challenging (e.g., brittleness and knowledge-acquisition bottleneck). This paper provides a comprehensive overview on a new paradigm of logical reasoning, which uses natural language as knowledge representation and pretrained language models as reasoners, including philosophical definition and categorization of logical reasoning, advantages of the new paradigm, benchmarks and methods, challenges of the new paradigm, possible future directions, and relation to related NLP fields. This new paradigm is promising since it not only alleviates many challenges of formal representation but also has advantages over end-to-end neural methods. This survey focus on transformer-based LLMs explicitly working on deductive, inductive, and abductive reasoning over English representation.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 22,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2303.12023",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-03-21",
    "authors": [
      {
        "authorId": "2124477940",
        "name": "Zonglin Yang"
      },
      {
        "authorId": "13728923",
        "name": "Xinya Du"
      },
      {
        "authorId": "2106694812",
        "name": "Rui Mao"
      },
      {
        "authorId": "1486121968",
        "name": "Jinjie Ni"
      },
      {
        "authorId": "49943757",
        "name": "E. Cambria"
      }
    ],
    "source": "semantic_scholar",
    "score": 117.03241323893724
  },
  {
    "paperId": "8f070e301979732e0dd73f6aa6170309cf73aa7d",
    "url": "https://www.semanticscholar.org/paper/8f070e301979732e0dd73f6aa6170309cf73aa7d",
    "title": "Large Language Model based Multi-Agents: A Survey of Progress and Challenges",
    "abstract": "Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to their notable capabilities in planning and reasoning, LLMs have been utilized as autonomous agents for the automatic execution of various tasks. Recently, LLM-based agent systems have rapidly evolved from single-agent planning or decision-making to operating as multi-agent systems, enhancing their ability in complex problem-solving and world simulation. To offer an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects and challenges of LLM-based multi-agent (LLM-MA) systems. Our objective is to provide readers with an in-depth understanding of these key points: the domains and settings where LLM-MA systems operate or simulate; the profiling and communication methods of these agents; and the means by which these agents develop their skills. For those interested in delving into this field, we also summarize the commonly used datasets or benchmarks. To keep researchers updated on the latest studies, we maintain an open-source GitHub repository (github.com/taichengguo/LLM_MultiAgents_Survey_Papers), dedicated to outlining the research of LLM-MA research.",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2024,
    "citationCount": 137,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference",
      "Review"
    ],
    "publicationDate": "2024-01-21",
    "authors": [
      {
        "authorId": "2179710545",
        "name": "Taicheng Guo"
      },
      {
        "authorId": "2257124794",
        "name": "Xiuying Chen"
      },
      {
        "authorId": "2282544971",
        "name": "Yaqi Wang"
      },
      {
        "authorId": "2282542602",
        "name": "Ruidi Chang"
      },
      {
        "authorId": "2546496",
        "name": "Shichao Pei"
      },
      {
        "authorId": "144539424",
        "name": "N. Chawla"
      },
      {
        "authorId": "2249763890",
        "name": "Olaf Wiest"
      },
      {
        "authorId": "2258447205",
        "name": "Xiangliang Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 143.90880527735806
  },
  {
    "paperId": "18664b47516ba5424ba5efa79d3f816224245325",
    "url": "https://www.semanticscholar.org/paper/18664b47516ba5424ba5efa79d3f816224245325",
    "title": "ChatRule: Mining Logical Rules with Large Language Models for Knowledge Graph Reasoning",
    "abstract": "Logical rules are essential for uncovering the logical connections between relations, which could improve reasoning performance and provide interpretable results on knowledge graphs (KGs). Although there have been many efforts to mine meaningful logical rules over KGs, existing methods suffer from computationally intensive searches over the rule space and a lack of scalability for large-scale KGs. Besides, they often ignore the semantics of relations which is crucial for uncovering logical connections. Recently, large language models (LLMs) have shown impressive performance in the field of natural language processing and various applications, owing to their emergent ability and generalizability. In this paper, we propose a novel framework, ChatRule, unleashing the power of large language models for mining logical rules over knowledge graphs. Specifically, the framework is initiated with an LLM-based rule generator, leveraging both the semantic and structural information of KGs to prompt LLMs to generate logical rules. To refine the generated rules, a rule ranking module estimates the rule quality by incorporating facts from existing KGs. Last, the ranked rules can be used to conduct reasoning over KGs. ChatRule is evaluated on four large-scale KGs, w.r.t. different rule quality metrics and downstream tasks, showing the effectiveness and scalability of our method.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 19,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.01538",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-04",
    "authors": [
      {
        "authorId": "2238130759",
        "name": "Linhao Luo"
      },
      {
        "authorId": "2237806895",
        "name": "Jiaxin Ju"
      },
      {
        "authorId": "2065364720",
        "name": "Bo Xiong"
      },
      {
        "authorId": "4495301",
        "name": "Yuan-Fang Li"
      },
      {
        "authorId": "2561045",
        "name": "Gholamreza Haffari"
      },
      {
        "authorId": "2585415",
        "name": "Shirui Pan"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.93598410330986
  },
  {
    "paperId": "00a67af3b7dc785b4813b61d232cc76b4fb2b189",
    "url": "https://www.semanticscholar.org/paper/00a67af3b7dc785b4813b61d232cc76b4fb2b189",
    "title": "TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning",
    "abstract": "Table reasoning tasks have shown remarkable progress with the development of large language models (LLMs), which involve interpreting and drawing conclusions from tabular data based on natural language (NL) questions. Existing solutions mainly tested on smaller tables face scalability issues and struggle with complex queries due to incomplete or dispersed data across different table sections. To alleviate these challenges, we propose TAP4LLM as a versatile pre-processor suite for leveraging LLMs in table-based tasks effectively. It covers several distinct components: (1) table sampling to decompose large tables into manageable sub-tables based on query semantics, (2) table augmentation to enhance tables with additional knowledge from external sources or models, and (3) table packing&serialization to convert tables into various formats suitable for LLMs' understanding. In each module, we design and compare several common methods under various usage scenarios, aiming to shed light on the best practices for leveraging LLMs for table-reasoning tasks. Our experiments show that our method improves LLMs' reasoning capabilities in various tabular tasks and enhances the interaction between LLMs and tabular data by employing effective pre-processing.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-14",
    "authors": [
      {
        "authorId": "2145641810",
        "name": "Yuan Sui"
      },
      {
        "authorId": "2288273199",
        "name": "Jiaru Zou"
      },
      {
        "authorId": "144203509",
        "name": "Mengyu Zhou"
      },
      {
        "authorId": "2116554804",
        "name": "Xinyi He"
      },
      {
        "authorId": "12723949",
        "name": "Lun Du"
      },
      {
        "authorId": "2109750123",
        "name": "Shi Han"
      },
      {
        "authorId": "2140415600",
        "name": "Dongmei Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.49820016084324
  },
  {
    "paperId": "1d7f414983eb847c4618489baa44e99b01162f98",
    "url": "https://www.semanticscholar.org/paper/1d7f414983eb847c4618489baa44e99b01162f98",
    "title": "DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks",
    "abstract": "Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns are raised about potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a general and flexible protocol for dynamic evaluation of LLMs. Based on our framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to GPT-3.5-Turbo and GPT-4. Experiments show that LLMs perform worse in DyVal-generated evaluation samples with different complexities, highlighting the significance of dynamic evaluation. We also analyze the failure cases and results of different prompting methods. Moreover, DyVal-generated samples are not only evaluation sets, but also helpful data for fine-tuning to improve the performance of LLMs on existing benchmarks. We hope that DyVal can shed light on future evaluation research of LLMs. Code is available at: https://github.com/microsoft/promptbench.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 17,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-29",
    "authors": [
      {
        "authorId": "2543684",
        "name": "Kaijie Zhu"
      },
      {
        "authorId": "47739850",
        "name": "Jiaao Chen"
      },
      {
        "authorId": "2145270616",
        "name": "Jindong Wang"
      },
      {
        "authorId": "2249536787",
        "name": "Neil Zhenqiang Gong"
      },
      {
        "authorId": "2022168",
        "name": "Diyi Yang"
      },
      {
        "authorId": "2249681654",
        "name": "Xing Xie"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.35557636844247
  },
  {
    "paperId": "33ac04c55ebcfa7a6dbc47514922bfb5cfeecbab",
    "url": "https://www.semanticscholar.org/paper/33ac04c55ebcfa7a6dbc47514922bfb5cfeecbab",
    "title": "Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales",
    "abstract": "Machine reasoning has made great progress in recent years owing to large language models (LLMs). In the clinical domain, however, most NLP-driven projects mainly focus on clinical classification or reading comprehension, and under-explore clinical reasoning for disease diagnosis due to the expensive rationale annotation with clinicians. In this work, we present a \"reasoning-aware\" diagnosis framework that rationalizes the diagnostic process via prompt-based learning in a time- and labor-efficient manner, and learns to reason over the prompt-generated rationales. Specifically, we address the clinical reasoning for disease diagnosis, where the LLM generates diagnostic rationales providing its insight on presented patient data and the reasoning path towards the diagnosis, namely Clinical Chain-of-Thought (Clinical CoT). We empirically demonstrate LLMs/LMs' ability of clinical reasoning via extensive experiments and analyses on both rationale generation and disease diagnosis in various settings. We further propose a novel set of criteria for evaluating machine-generated rationales' potential for real-world clinical settings, facilitating and benefiting future research in this area.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2023,
    "citationCount": 17,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-12",
    "authors": [
      {
        "authorId": "2258722263",
        "name": "Taeyoon Kwon"
      },
      {
        "authorId": "2210267033",
        "name": "Kai Tzu-iunn Ong"
      },
      {
        "authorId": "2266420525",
        "name": "Dongjin Kang"
      },
      {
        "authorId": "2266717741",
        "name": "Seungjun Moon"
      },
      {
        "authorId": "2118365169",
        "name": "J. Lee"
      },
      {
        "authorId": "2273646755",
        "name": "Dosik Hwang"
      },
      {
        "authorId": "1404043981",
        "name": "Yongsik Sim"
      },
      {
        "authorId": "8655860",
        "name": "B. Sohn"
      },
      {
        "authorId": "2258907627",
        "name": "Dongha Lee"
      },
      {
        "authorId": "2258712913",
        "name": "Jinyoung Yeo"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.35557636844247
  },
  {
    "paperId": "801d7ba75fc833aa76ce4863dc1f79e30ee0c23f",
    "url": "https://www.semanticscholar.org/paper/801d7ba75fc833aa76ce4863dc1f79e30ee0c23f",
    "title": "Forward-Backward Reasoning in Large Language Models for Mathematical Verification",
    "abstract": "Self-Consistency samples diverse reasoning chains with answers and chooses the final answer by majority voting. It is based on forward reasoning and cannot further improve performance by sampling more reasoning chains when saturated. To further boost performance, we introduce backward reasoning to verify candidate answers. Specifically, for mathematical tasks, we mask a number in the question and ask the LLM to answer a backward question created by a simple template, i.e., to predict the masked number when a candidate answer is provided. Instead of using forward or backward reasoning alone, we propose FOBAR to combine FOrward and BAckward Reasoning for verification. Extensive experiments on six standard mathematical data sets and three LLMs show that FOBAR achieves state-of-the-art performance. In particular, FOBAR outperforms Self-Consistency, which uses forward reasoning alone, demonstrating that combining forward and forward reasoning is better. In addition, FOBAR performs better than existing verification methods, showing the effectiveness of the simple template used in backward reasoning and the proposed combination. Extensions to non-mathematical problems are also discussed and validated empirically.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 14,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.07758",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-08-15",
    "authors": [
      {
        "authorId": "2152123946",
        "name": "Weisen Jiang"
      },
      {
        "authorId": "152751416",
        "name": "Han Shi"
      },
      {
        "authorId": "2112584251",
        "name": "L. Yu"
      },
      {
        "authorId": "2216103422",
        "name": "Zheng Liu"
      },
      {
        "authorId": "2153638098",
        "name": "Yu Zhang"
      },
      {
        "authorId": "2249755860",
        "name": "Zhenguo Li"
      },
      {
        "authorId": "2243335442",
        "name": "James T. Kwok"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "974f0e1a85c1ece2555718342ff2abb6bcb6a825",
    "url": "https://www.semanticscholar.org/paper/974f0e1a85c1ece2555718342ff2abb6bcb6a825",
    "title": "KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models",
    "abstract": "While large language models (LLMs) have made considerable advancements in understanding and generating unstructured text, their application in structured data remains underexplored. Particularly, using LLMs for complex reasoning tasks on knowledge graphs (KGs) remains largely untouched. To address this, we propose KG-GPT, a multi-purpose framework leveraging LLMs for tasks employing KGs. KG-GPT comprises three steps: Sentence Segmentation, Graph Retrieval, and Inference, each aimed at partitioning sentences, retrieving relevant graph components, and deriving logical conclusions, respectively. We evaluate KG-GPT using KG-based fact verification and KGQA benchmarks, with the model showing competitive and robust performance, even outperforming several fully-supervised models. Our work, therefore, marks a significant step in unifying structured and unstructured data processing within the realm of LLMs.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 14,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-17",
    "authors": [
      {
        "authorId": "2154586028",
        "name": "Jiho Kim"
      },
      {
        "authorId": "2201732484",
        "name": "Yeonsu Kwon"
      },
      {
        "authorId": "39947629",
        "name": "Yohan Jo"
      },
      {
        "authorId": "2258962475",
        "name": "Edward Choi"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "557182159154a0478b50f19838767ebb1749db0d",
    "url": "https://www.semanticscholar.org/paper/557182159154a0478b50f19838767ebb1749db0d",
    "title": "Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models",
    "abstract": "The burgeoning interest in Multimodal Large Language Models (MLLMs), such as OpenAI's GPT-4V(ision), has significantly impacted both academic and industrial realms. These models enhance Large Language Models (LLMs) with advanced visual understanding capabilities, facilitating their application in a variety of multimodal tasks. Recently, Google introduced Gemini, a cutting-edge MLLM designed specifically for multimodal integration. Despite its advancements, preliminary benchmarks indicate that Gemini lags behind GPT models in commonsense reasoning tasks. However, this assessment, based on a limited dataset (i.e., HellaSWAG), does not fully capture Gemini's authentic commonsense reasoning potential. To address this gap, our study undertakes a thorough evaluation of Gemini's performance in complex reasoning tasks that necessitate the integration of commonsense knowledge across modalities. We carry out a comprehensive analysis of 12 commonsense reasoning datasets, ranging from general to domain-specific tasks. This includes 11 datasets focused solely on language, as well as one that incorporates multimodal elements. Our experiments across four LLMs and two MLLMs demonstrate Gemini's competitive commonsense reasoning capabilities. Additionally, we identify common challenges faced by current LLMs and MLLMs in addressing commonsense problems, underscoring the need for further advancements in enhancing the commonsense reasoning abilities of these models.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-29",
    "authors": [
      {
        "authorId": "2108035197",
        "name": "Yuqing Wang"
      },
      {
        "authorId": "47827165",
        "name": "Yun Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "5d5dbba58aaf5b3fa4044cc3ffc71a3fe2b8c654",
    "url": "https://www.semanticscholar.org/paper/5d5dbba58aaf5b3fa4044cc3ffc71a3fe2b8c654",
    "title": "SCREWS: A Modular Framework for Reasoning with Revisions",
    "abstract": "Large language models (LLMs) can improve their accuracy on various tasks through iteratively refining and revising their output based on feedback. We observe that these revisions can introduce errors, in which case it is better to roll back to a previous result. Further, revisions are typically homogeneous: they use the same reasoning method that produced the initial answer, which may not correct errors. To enable exploration in this space, we present SCREWS, a modular framework for reasoning with revisions. It is comprised of three main modules: Sampling, Conditional Resampling, and Selection, each consisting of sub-modules that can be hand-selected per task. We show that SCREWS not only unifies several previous approaches under a common framework, but also reveals several novel strategies for identifying improved reasoning chains. We evaluate our framework with state-of-the-art LLMs (ChatGPT and GPT-4) on a diverse set of reasoning tasks and uncover useful new reasoning strategies for each: arithmetic word problems, multi-hop question answering, and code debugging. Heterogeneous revision strategies prove to be important, as does selection between original and revised candidates.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 12,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.13075",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-20",
    "authors": [
      {
        "authorId": "2065212009",
        "name": "K. Shridhar"
      },
      {
        "authorId": "2006291",
        "name": "Harsh Jhamtani"
      },
      {
        "authorId": "145204655",
        "name": "Hao Fang"
      },
      {
        "authorId": "7536576",
        "name": "Benjamin Van Durme"
      },
      {
        "authorId": "145043214",
        "name": "Jason Eisner"
      },
      {
        "authorId": "2465658",
        "name": "Patrick Xia"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "bd1ef429956cea3542c14006df67e890cb57a80b",
    "url": "https://www.semanticscholar.org/paper/bd1ef429956cea3542c14006df67e890cb57a80b",
    "title": "IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions",
    "abstract": "Retrieval-Augmented Generation (RAG), by incorporating external knowledge with parametric memory of language models, has become the state-of-the-art architecture for open-domain QA tasks. However, common knowledge bases are inherently constrained by limited coverage and noisy information, making retrieval-based approaches inadequate to answer implicit reasoning questions. In this paper, we propose an Induction-Augmented Generation (IAG) framework that utilizes inductive knowledge along with the retrieved documents for implicit reasoning. We leverage large language models (LLMs) for deriving such knowledge via a novel prompting method based on inductive reasoning patterns. On top of this, we implement two versions of IAG named IAG-GPT and IAG-Student, respectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for answer prediction, while IAG-Student gets rid of dependencies on GPT service at inference time by incorporating a student inductor model. The inductor is firstly trained via knowledge distillation and further optimized by back-propagating the generator feedback via differentiable beam scores. Experimental results show that IAG outperforms RAG baselines as well as ChatGPT on two Open-Domain QA tasks. Notably, our best models have won the first place in the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA (since Jan 8, 2023).",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-30",
    "authors": [
      {
        "authorId": "2268800181",
        "name": "Zhebin Zhang"
      },
      {
        "authorId": "2268798179",
        "name": "Xinyu Zhang"
      },
      {
        "authorId": "2269027747",
        "name": "Yuanhang Ren"
      },
      {
        "authorId": "2269117649",
        "name": "Saijiang Shi"
      },
      {
        "authorId": "2223174680",
        "name": "Meng Han"
      },
      {
        "authorId": "2115859566",
        "name": "Yongkang Wu"
      },
      {
        "authorId": "2128243415",
        "name": "Ruofei Lai"
      },
      {
        "authorId": "2269030109",
        "name": "Zhao Cao"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "457896eebc1ddd780efb98a7b5276ed75e76f802",
    "url": "https://www.semanticscholar.org/paper/457896eebc1ddd780efb98a7b5276ed75e76f802",
    "title": "Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?",
    "abstract": "Despite the high performances of large language models (LLMs) across numerous benchmarks, recent research has unveiled their suffering from hallucinations and unfaithful reasoning. This work studies a type of hallucination induced by semantic associations. We investigate to what extent LLMs take shortcuts from certain keyword/entity biases in the prompt instead of following correct reasoning paths. To quantify this phenomenon, we propose a novel probing method and benchmark called EUREQA. EUREQA is an entity-searching task where a model finds a missing entity based on described multi-hop relations with other entities. These deliberately designed multi-hop relations create deceptive semantic associations, and models must stick to the correct reasoning path instead of incorrect shortcuts to find the correct answer.Experiments show that existing LLMs cannot follow correct reasoning paths and resist the attempt of greedy shortcuts, with GPT-4 only achieving 62% accuracy. Analyses provide further evidence that LLMs rely on semantic biases to solve the task instead of proper reasoning, questioning the validity and generalizability of current LLMs’ high performances.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2311.09702",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-16",
    "authors": [
      {
        "authorId": "2261387782",
        "name": "Bangzheng Li"
      },
      {
        "authorId": "2267020892",
        "name": "Ben Zhou"
      },
      {
        "authorId": "47939052",
        "name": "Fei Wang"
      },
      {
        "authorId": "2078360",
        "name": "Xingyu Fu"
      },
      {
        "authorId": "2240532649",
        "name": "Dan Roth"
      },
      {
        "authorId": "1998918",
        "name": "Muhao Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "ecfe2becc1040d3810c10f006d4be43b4eef3c41",
    "url": "https://www.semanticscholar.org/paper/ecfe2becc1040d3810c10f006d4be43b4eef3c41",
    "title": "Tree-of-Mixed-Thought: Combining Fast and Slow Thinking for Multi-hop Visual Reasoning",
    "abstract": "There emerges a promising trend of using large language models (LLMs) to generate code-like plans for complex inference tasks such as visual reasoning. This paradigm, known as LLM-based planning, provides flexibility in problem solving and endows better interpretability. However, current research is mostly limited to basic scenarios of simple questions that can be straightforward answered in a few inference steps. Planning for the more challenging multi-hop visual reasoning tasks remains under-explored. Specifically, under multi-hop reasoning situations, the trade-off between accuracy and the complexity of plan-searching becomes prominent. The prevailing algorithms either address the efficiency issue by employing the fast one-stop generation or adopt a complex iterative generation method to improve accuracy. Both fail to balance the need for efficiency and performance. Drawing inspiration from the dual system of cognition in the human brain, the fast and the slow think processes, we propose a hierarchical plan-searching algorithm that integrates the one-stop reasoning (fast) and the Tree-of-thought (slow). Our approach succeeds in performance while significantly saving inference steps. Moreover, we repurpose the PTR and the CLEVER datasets, developing a systematic framework for evaluating the performance and efficiency of LLMs-based plan-search algorithms under reasoning tasks at different levels of difficulty. Extensive experiments demonstrate the superiority of our proposed algorithm in terms of performance and efficiency. The dataset and code will be release soon.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.09658",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-08-18",
    "authors": [
      {
        "authorId": "2067772264",
        "name": "Pengbo Hu"
      },
      {
        "authorId": "47825925",
        "name": "Jingxian Qi"
      },
      {
        "authorId": "2155447865",
        "name": "Xingyu Li"
      },
      {
        "authorId": "2213528169",
        "name": "Hong Li"
      },
      {
        "authorId": "2119918264",
        "name": "Xinqi Wang"
      },
      {
        "authorId": "2232610435",
        "name": "Bing Quan"
      },
      {
        "authorId": "2232724332",
        "name": "Ruiyu Wang"
      },
      {
        "authorId": "2118764703",
        "name": "Yi Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "fdcef72d381d1ad3c374b602a2c1de7558250331",
    "url": "https://www.semanticscholar.org/paper/fdcef72d381d1ad3c374b602a2c1de7558250331",
    "title": "Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions",
    "abstract": "Large language models (LLMs) are capable of answering knowledge-intensive complex questions with chain-of-thought (CoT) reasoning. However, they tend to generate factually incorrect reasoning steps when the required knowledge is not available or up-to-date in models' parameters. Recent works turn to retrieving external knowledge to augment CoT reasoning. Despite being promising, these chain-based methods suffer from: 1) Negative retrieval. Unnecessary or incorrect retrieval may mislead the reasoning; 2) Limited sight. Lacking the ability to look backward or forward, a local error in one step will propagate along the chain. In this paper, we propose a novel approach: Probabilistic Tree-of-thought Reasoning (ProbTree). First, LLMs translate a complex question into a query tree, in which each non-root node denotes a sub-question of its parent node. Then, probabilistic reasoning is conducted over the tree, by solving questions from leaf to root considering the confidence of both question decomposing and answering. During reasoning, for leaf nodes, LLMs choose a more confident answer from Closed-book QA that employs parametric knowledge and Open-book QA that employs retrieved external knowledge, thus eliminating the negative retrieval problem. For non-leaf nodes, with the hierarchical structure, LLMs have broader sights and are able to globally reason with the information from child nodes, thus recovering from local errors. The experiments on three Complex QA datasets under the open-domain setting show that our approach outperforms SOTA methods significantly, demonstrating the effect of probabilistic tree-of-thought reasoning.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-23",
    "authors": [
      {
        "authorId": "1712738522",
        "name": "S. Cao"
      },
      {
        "authorId": "2107983722",
        "name": "Jiajie Zhang"
      },
      {
        "authorId": "2256650328",
        "name": "Jiaxin Shi"
      },
      {
        "authorId": "2268485303",
        "name": "Xin Lv"
      },
      {
        "authorId": "1423719712",
        "name": "Zijun Yao"
      },
      {
        "authorId": "2149898858",
        "name": "Qingwen Tian"
      },
      {
        "authorId": "2133353675",
        "name": "Juanzi Li"
      },
      {
        "authorId": "2055765060",
        "name": "Lei Hou"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "5d77cd554909a12e8ae6660b24e5903074c56ba5",
    "url": "https://www.semanticscholar.org/paper/5d77cd554909a12e8ae6660b24e5903074c56ba5",
    "title": "Getting Sick After Seeing a Doctor? Diagnosing and Mitigating Knowledge Conflicts in Event Temporal Reasoning",
    "abstract": "Event temporal reasoning aims at identifying the temporal relations between two or more events from narratives. However, knowledge conflicts arise when there is a mismatch between the actual temporal relations of events in the context and the prior knowledge or biases learned by the model. In this paper, we propose to detect knowledge-conflict examples in event temporal reasoning using bias indicators, which include event relation prior bias, tense bias, narrative bias, and dependency bias. We define conflict examples as those where event relations are opposite to biased or prior relations. To mitigate event-related knowledge conflicts, we introduce a Counterfactual Data Augmentation (CDA) based method that can be applied to both Pre-trained Language Models (PLMs) and Large Language Models (LLMs) either as additional training data or demonstrations for In-Context Learning. Experiments suggest both PLMs and LLMs suffer from knowledge conflicts in event temporal reasoning, and CDA has the potential for reducing hallucination and improving model performance.",
    "venue": "NAACL-HLT",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.14970",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-24",
    "authors": [
      {
        "authorId": "2044202073",
        "name": "Tianqing Fang"
      },
      {
        "authorId": "2187830802",
        "name": "Zhaowei Wang"
      },
      {
        "authorId": "2203076",
        "name": "Wenxuan Zhou"
      },
      {
        "authorId": "48212577",
        "name": "Hongming Zhang"
      },
      {
        "authorId": "1809614",
        "name": "Yangqiu Song"
      },
      {
        "authorId": "1998918",
        "name": "Muhao Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "ff4acf33aeafcbe7d12afdc6bb9ca26537219658",
    "url": "https://www.semanticscholar.org/paper/ff4acf33aeafcbe7d12afdc6bb9ca26537219658",
    "title": "Studying and improving reasoning in humans and machines",
    "abstract": null,
    "venue": "Communications psychology",
    "year": 2023,
    "citationCount": 11,
    "openAccessPdf": {
      "url": "https://www.nature.com/articles/s44271-024-00091-8.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-21",
    "authors": [
      {
        "authorId": "2238327629",
        "name": "Nicolas Yax"
      },
      {
        "authorId": "2244622476",
        "name": "Hernan Anll'o"
      },
      {
        "authorId": "6286029",
        "name": "Stefano Palminteri"
      }
    ],
    "source": "semantic_scholar",
    "score": 87.27359974682
  },
  {
    "paperId": "3f49bd6b95e254d82ce1ed1bd556a7a8f81a47db",
    "url": "https://www.semanticscholar.org/paper/3f49bd6b95e254d82ce1ed1bd556a7a8f81a47db",
    "title": "Chain of Empathy: Enhancing Empathetic Response of Large Language Models Based on Psychotherapy Models",
    "abstract": "We present a novel method, the Chain of Empathy (CoE) prompting, that utilizes insights from psychotherapy to induce Large Language Models (LLMs) to reason about human emotional states. This method is inspired by various psychotherapy approaches including Cognitive Behavioral Therapy (CBT), Dialectical Behavior Therapy (DBT), Person Centered Therapy (PCT), and Reality Therapy (RT), each leading to different patterns of interpreting clients' mental states. LLMs without reasoning generated predominantly exploratory responses. However, when LLMs used CoE reasoning, we found a more comprehensive range of empathetic responses aligned with the different reasoning patterns of each psychotherapy model. The CBT based CoE resulted in the most balanced generation of empathetic responses. The findings underscore the importance of understanding the emotional context and how it affects human and AI communication. Our research contributes to understanding how psychotherapeutic models can be incorporated into LLMs, facilitating the development of context-specific, safer, and empathetic AI.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-02",
    "authors": [
      {
        "authorId": "2110392883",
        "name": "Y. Lee"
      },
      {
        "authorId": "2152635683",
        "name": "Inju Lee"
      },
      {
        "authorId": "2265753046",
        "name": "Minjung Shin"
      },
      {
        "authorId": "2212998141",
        "name": "Seoyeon Bae"
      },
      {
        "authorId": "2265753028",
        "name": "Sowon Hahn"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "e3e6fafc30a425d161ee69652219a8d31e845fca",
    "url": "https://www.semanticscholar.org/paper/e3e6fafc30a425d161ee69652219a8d31e845fca",
    "title": "STYLECAP: Automatic Speaking-Style Captioning from Speech Based on Speech and Language Self-Supervised Learning Models",
    "abstract": "We propose StyleCap, a method to generate natural language descriptions of speaking styles appearing in speech. Although most of conventional techniques for para-/non-linguistic information recognition focus on the category classification or the intensity estimation of pre-defined labels, they cannot provide the reasoning of the recognition result in an interpretable manner. StyleCap is a first step towards an end-to-end method for generating speaking-style prompts from speech, i.e., automatic speaking-style captioning. StyleCap is trained with paired data of speech and natural language descriptions. We train neural networks that convert a speech representation vector into prefix vectors that are fed into a large language model (LLM)-based text decoder. We explore an appropriate text decoder and speech feature representation suitable for this new task. The experimental results demonstrate that our StyleCap leveraging richer LLMs for the text decoder, speech self-supervised learning (SSL) features, and sentence rephrasing augmentation improves the accuracy and diversity of generated speaking-style captions. Samples of speaking-style captions generated by our StyleCap are publicly available 1.",
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year": 2023,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2311.16509",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-28",
    "authors": [
      {
        "authorId": "2268494185",
        "name": "Kazuki Yamauchi"
      },
      {
        "authorId": "2036253",
        "name": "Yusuke Ijima"
      },
      {
        "authorId": "2268512816",
        "name": "Yuki Saito"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.87639203842082
  },
  {
    "paperId": "4dbae943822dcb21d3d1dd757b70740615bb4172",
    "url": "https://www.semanticscholar.org/paper/4dbae943822dcb21d3d1dd757b70740615bb4172",
    "title": "Moderating New Waves of Online Hate with Chain-of-Thought Reasoning in Large Language Models",
    "abstract": "Online hate is an escalating problem that negatively impacts the lives of Internet users, and is also subject to rapid changes due to evolving events, resulting in new waves of online hate that pose a critical threat. Detecting and mitigating these new waves present two key challenges: it demands reasoning-based complex decision-making to determine the presence of hateful content, and the limited availability of training samples hinders updating the detection model. To address this critical issue, we present a novel framework called HateGuard for effectively moderating new waves of online hate. HateGuard employs a reasoning-based approach that leverages the recently introduced chain-of-thought (CoT) prompting technique, harnessing the capabilities of large language models (LLMs). HateGuard further achieves prompt-based zero-shot detection by automatically generating and updating detection prompts with new derogatory terms and targets in new wave samples to effectively address new waves of online hate. To demonstrate the effectiveness of our approach, we compile a new dataset consisting of tweets related to three recently witnessed new waves: the 2022 Russian invasion of Ukraine, the 2021 insurrection of the US Capitol, and the COVID-19 pandemic. Our studies reveal crucial longitudinal patterns in these new waves concerning the evolution of events and the pressing need for techniques to rapidly update existing moderation tools to counteract them. Comparative evaluations against state-of-the-art approaches illustrate the superiority of our framework, showcasing a substantial 10.59% to 88% improvement in detecting the three new waves of online hate. Our work highlights the severe threat posed by the emergence of new waves of online hate and represents a paradigm shift in addressing this threat practically.",
    "venue": "IEEE Symposium on Security and Privacy",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2312.15099",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-22",
    "authors": [
      {
        "authorId": "8842079",
        "name": "Nishant Vishwamitra"
      },
      {
        "authorId": "2165583728",
        "name": "Keyan Guo"
      },
      {
        "authorId": "2276431200",
        "name": "Farhan Tajwar Romit"
      },
      {
        "authorId": "2276431198",
        "name": "Isabelle Ondracek"
      },
      {
        "authorId": "2276447950",
        "name": "Long Cheng"
      },
      {
        "authorId": "2276488043",
        "name": "Ziming Zhao"
      },
      {
        "authorId": "2241790138",
        "name": "Hongxin Hu"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.95836866004329
  },
  {
    "paperId": "518c3eae36d00ef612a623f335a595e8577655aa",
    "url": "https://www.semanticscholar.org/paper/518c3eae36d00ef612a623f335a595e8577655aa",
    "title": "Novice Learner and Expert Tutor: Evaluating Math Reasoning Abilities of Large Language Models with Misconceptions",
    "abstract": "We propose novel evaluations for mathematical reasoning capabilities of Large Language Models (LLMs) based on mathematical misconceptions. Our primary approach is to simulate LLMs as a novice learner and an expert tutor, aiming to identify the incorrect answer to math question resulted from a specific misconception and to recognize the misconception(s) behind an incorrect answer, respectively. Contrary to traditional LLMs-based mathematical evaluations that focus on answering math questions correctly, our approach takes inspirations from principles in educational learning sciences. We explicitly ask LLMs to mimic a novice learner by answering questions in a specific incorrect manner based on incomplete knowledge; and to mimic an expert tutor by identifying misconception(s) corresponding to an incorrect answer to a question. Using simple grade-school math problems, our experiments reveal that, while LLMs can easily answer these questions correctly, they struggle to identify 1) the incorrect answer corresponding to specific incomplete knowledge (misconceptions); 2) the misconceptions that explain particular incorrect answers. Our study indicates new opportunities for enhancing LLMs' math reasoning capabilities, especially on developing robust student simulation and expert tutoring models in the educational applications such as intelligent tutoring systems.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.02439",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-03",
    "authors": [
      {
        "authorId": "2143066732",
        "name": "Naiming Liu"
      },
      {
        "authorId": "1720691070",
        "name": "Shashank Sonkar"
      },
      {
        "authorId": "2117419133",
        "name": "Zichao Wang"
      },
      {
        "authorId": "2066899529",
        "name": "Simon Woodhead"
      },
      {
        "authorId": "2249763597",
        "name": "R. Baraniuk"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "63b1b40781faf00a0d948bba653e0be02895e587",
    "url": "https://www.semanticscholar.org/paper/63b1b40781faf00a0d948bba653e0be02895e587",
    "title": "Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models",
    "abstract": "The age of social media is rife with memes. Understanding and detecting harmful memes pose a significant challenge due to their implicit meaning that is not explicitly conveyed through the surface text and image. However, existing harmful meme detection approaches only recognize superficial harm-indicative signals in an end-to-end classification manner but ignore in-depth cognition of the meme text and image. In this paper, we attempt to detect harmful memes based on advanced reasoning over the interplay of multimodal information in memes. Inspired by the success of Large Language Models (LLMs) on complex reasoning, we first conduct abductive reasoning with LLMs. Then we propose a novel generative framework to learn reasonable thoughts from LLMs for better multimodal fusion and lightweight fine-tuning, which consists of two training stages: 1) Distill multimodal reasoning knowledge from LLMs; and 2) Fine-tune the generative framework to infer harmfulness. Extensive experiments conducted on three meme datasets demonstrate that our proposed approach achieves superior performance than state-of-the-art methods on the harmful meme detection task.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.findings-emnlp.611.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-09",
    "authors": [
      {
        "authorId": "2109380683",
        "name": "Hongzhan Lin"
      },
      {
        "authorId": "23523733",
        "name": "Ziyang Luo"
      },
      {
        "authorId": "2157404600",
        "name": "Jing Ma"
      },
      {
        "authorId": "143891667",
        "name": "Long Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "10828be2eaa52ba7fd78356980afd0669e2f2879",
    "url": "https://www.semanticscholar.org/paper/10828be2eaa52ba7fd78356980afd0669e2f2879",
    "title": "Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge",
    "abstract": "We attempt to solve the Abstraction and Reasoning Corpus (ARC) Challenge using Large Language Models (LLMs) as a system of multiple expert agents. Using the flexibility of LLMs to be prompted to do various novel tasks using zero-shot, few-shot, context-grounded prompting, we explore the feasibility of using LLMs to solve the ARC Challenge. We firstly convert the input image into multiple suitable text-based abstraction spaces. We then utilise the associative power of LLMs to derive the input-output relationship and map this to actions in the form of a working program, similar to Voyager / Ghost in the MineCraft. In addition, we use iterative environmental feedback in order to guide LLMs to solve the task. Our proposed approach achieves 50 solves out of 111 training set problems (45%) with just three abstraction spaces - grid, object and pixel - and we believe that with more abstraction spaces and learnable actions, we will be able to solve more.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.05146",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-08",
    "authors": [
      {
        "authorId": "2219388173",
        "name": "J. Tan"
      },
      {
        "authorId": "1770486",
        "name": "M. Motani"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "4ed896f45e143b31dca465b2153fc7fc93ca3fdf",
    "url": "https://www.semanticscholar.org/paper/4ed896f45e143b31dca465b2153fc7fc93ca3fdf",
    "title": "Temporal Knowledge Question Answering via Abstract Reasoning Induction",
    "abstract": "In this study, we address the challenge of enhancing temporal knowledge reasoning in Large Language Models (LLMs). LLMs often struggle with this task, leading to the generation of inaccurate or misleading responses. This issue mainly arises from their limited ability to handle evolving factual knowledge and complex temporal logic. To overcome these limitations, we propose Abstract Reasoning Induction (ARI) framework, which divides temporal reasoning into two distinct phases: Knowledge-agnostic and Knowledge-based. This framework offers factual knowledge support to LLMs while minimizing the incorporation of extraneous noisy data. Concurrently, informed by the principles of constructivism, ARI provides LLMs the capability to engage in proactive, self-directed learning from both correct and incorrect historical reasoning samples. By teaching LLMs to actively construct knowledge and methods, it can significantly boosting their temporal reasoning abilities. Our approach achieves remarkable improvements, with relative gains of 29.7% and 9.27% on two temporal QA datasets, underscoring its efficacy in advancing temporal reasoning in LLMs. The code can be found at https://github.com/czy1999/ARI-QA",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-15",
    "authors": [
      {
        "authorId": "2117098298",
        "name": "Ziyang Chen"
      },
      {
        "authorId": "2265618386",
        "name": "Dongfang Li"
      },
      {
        "authorId": "2327289650",
        "name": "Xiang Zhao"
      },
      {
        "authorId": "33968873",
        "name": "Baotian Hu"
      },
      {
        "authorId": "2258690227",
        "name": "Min Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "5076bbbf831a92174c9cc1b347bd0584560435fc",
    "url": "https://www.semanticscholar.org/paper/5076bbbf831a92174c9cc1b347bd0584560435fc",
    "title": "Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning",
    "abstract": "Large Language Models (LLMs) demonstrate impressive ability in handling reasoning tasks. However, unlike humans who can instinctively adapt their problem-solving strategies to the complexity of task, most LLM-based methods adopt a one-size-fits-all approach. These methods employ consistent models, sample sizes, prompting methods and levels of problem decomposition, regardless of the problem complexity. The inflexibility of these methods can bring unnecessary computational overhead or sub-optimal performance. To address this limitation, we introduce an Adaptive-Solver (AS) framework tha dynamically adapts solving strategies to suit various problems, enabling the flexible allocation of test-time computational resources. The framework functions with two primary modules. The initial evaluation module assesses the reliability of the current solution using answer consistency. If the solution is deemed unreliable, the subsequent adaptation module comes into play. Within this module, various types of adaptation strategies are employed collaboratively. Through such dynamic and multi-faceted adaptations, our framework can help reduce computational consumption and improve performance. Experimental results from complex reasoning benchmarks reveal that our method can significantly reduce API costs (up to 85%) while maintaining original performance. Alternatively, it achieves up to 4.5% higher accuracy compared to the baselines at the same cost. The code and dataset are available at https://github.com/john1226966735/Adaptive-Solver.",
    "venue": "Information Processing & Management",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.01446",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-01",
    "authors": [
      {
        "authorId": "2256299370",
        "name": "Jianpeng Zhou"
      },
      {
        "authorId": "81970097",
        "name": "Wanjun Zhong"
      },
      {
        "authorId": "2214155529",
        "name": "Yanlin Wang"
      },
      {
        "authorId": "2110182921",
        "name": "Jiahai Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "811f451f1991ec5508e67d00375ca4f5d05e0eeb",
    "url": "https://www.semanticscholar.org/paper/811f451f1991ec5508e67d00375ca4f5d05e0eeb",
    "title": "TRAM: Benchmarking Temporal Reasoning for Large Language Models",
    "abstract": "Reasoning about time is essential for understanding the nuances of events described in natural language. Previous research on this topic has been limited in scope, characterized by a lack of standardized benchmarks that would allow for consistent evaluations across different studies. In this paper, we introduce TRAM, a temporal reasoning benchmark composed of ten datasets, encompassing various temporal aspects of events such as order, arithmetic, frequency, and duration, designed to facilitate a comprehensive evaluation of the TeR capabilities of large language models (LLMs). We evaluate popular LLMs like GPT-4 and Llama2 in zero-shot and few-shot scenarios, and establish baselines with BERT-based and domain-specific models. Our findings indicate that the best-performing model lags significantly behind human performance. It is our aspiration that TRAM will spur further progress in enhancing the TeR capabilities of LLMs.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.00835",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-02",
    "authors": [
      {
        "authorId": "2108035197",
        "name": "Yuqing Wang"
      },
      {
        "authorId": "46317573",
        "name": "Yun Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "34802b1f153d436d5ddb428642b8ae415485269e",
    "url": "https://www.semanticscholar.org/paper/34802b1f153d436d5ddb428642b8ae415485269e",
    "title": "Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large Language Models",
    "abstract": "The advancement of Large Language Models (LLMs) has brought substantial attention to the Chain of Thought (CoT) approach, primarily due to its ability to enhance the capability of LLMs on complex reasoning tasks. Moreover, the significance of CoT approaches extends to the application of LLMs for multi-modal tasks. However, the selection of optimal CoT demonstration examples in multi-modal reasoning remains less explored for LLMs due to the inherent complexity of multi-modal examples. In this paper, we introduce a novel approach that addresses this challenge by using retrieval mechanisms to dynamically and automatically select demonstration examples based on cross-modal and intra-modal similarities. Furthermore, we employ a Stratified Sampling method of categorising demonstration examples into groups based on their types and then retrieving examples from different groups respectively to promote the diversity of demonstration examples. Through a series of experiments on two popular benchmark datasets: ScienceQA and MathVista, we demonstrate that our approach significantly improves the performance of GPT-4 by 6% on ScienceQA and 12.9% on MathVista, and enhances the performance of GPT-4V on two datasets by 2.7%, substantially improving the performance of the most advanced LLMs and LMMs for complex multi-modal reasoning tasks.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-04",
    "authors": [
      {
        "authorId": "2269484821",
        "name": "Bingshuai Liu"
      },
      {
        "authorId": "2266387313",
        "name": "Chenyang Lyu"
      },
      {
        "authorId": "2192607321",
        "name": "Zijun Min"
      },
      {
        "authorId": "2268638120",
        "name": "Zhanyu Wang"
      },
      {
        "authorId": "2269468248",
        "name": "Jinsong Su"
      },
      {
        "authorId": "2269690569",
        "name": "Longyue Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "537335d9aad0ddbaef93e7f88b0db096671ef6ec",
    "url": "https://www.semanticscholar.org/paper/537335d9aad0ddbaef93e7f88b0db096671ef6ec",
    "title": "No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function",
    "abstract": "Large language models (LLMs) demonstrate impressive language understanding and contextual learning abilities, making them suitable for natural language processing (NLP) tasks and complex mathematical reasoning. However, when applied to mathematical reasoning tasks, LLMs often struggle to generate correct reasoning steps and answers despite having high probabilities for the solutions. To overcome this limitation and enhance the mathematical reasoning capabilities of fine-tuned LLMs without additional fine-tuning steps, we propose a method that incorporates Monte Carlo Tree Search (MCTS) and a lightweight energy function to rank decision steps and enable immediate reaction and precise reasoning. Specifically, we re-formulate the fine-tuned LLMs into a Residual-based Energy Model (Residual-EBM) and employ noise contrastive estimation to estimate the energy function's parameters. We then utilize MCTS with the energy function as a path verifier to search the output space and evaluate the reasoning path. Through extensive experiments on two mathematical reasoning benchmarks, GSM8k and AQUA-RAT, we demonstrate the exceptional capabilities of our method, which significantly improves the pass@1 metric of the fine-tuned model without requiring additional fine-tuning or reinforcement learning with human feedback alignment.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.03224",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-01",
    "authors": [
      {
        "authorId": "2238243665",
        "name": "Haotian Xu"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "3f8f428d3249d53dd0452283163a4a4124f9bc9c",
    "url": "https://www.semanticscholar.org/paper/3f8f428d3249d53dd0452283163a4a4124f9bc9c",
    "title": "VerityMath: Advancing Mathematical Reasoning by Self-Verification Through Unit Consistency",
    "abstract": "Large Language Models (LLMs), combined with program-based solving techniques, are increasingly demonstrating proficiency in mathematical reasoning. For example, closed-source models such as OpenAI GPT-4 and Claude show excellent results in solving math word problems. However, progress in math word problem-solving for open-source LLMs is limited, and the challenges these models face are not well-studied. In this paper, we study the performance of strong open-source LLMs, including Llama 2 (7B), Code Llama (7B), and Mistral (7B) on math word problems using program-based solving techniques. Specifically, we analyze the outputs of these models when applied to math word problems and identify a category of problems that pose a significant challenge, particularly those involving quantities spanning multiple units. To address this issue, we propose a systematic approach by defining the units for each quantity and ensuring the consistency of these units during mathematical operations. We developed Unit Consistency Programs (UCPs), an annotated dataset of math word problems, each paired with programs containing unit specifications and unit verification routines. We fine-tuned Llama 2 (7B), Code Llama (7B), and Mistral (7B) models with UCPs to produce theirVerityMath variants. Our findings indicate that our approach, which incorporates unit consistency, currently slightly underperforms compared to an approach that does not. To understand the reasons behind this, we conduct an in-depth error analysis and suggest options for future improvements. Our code and dataset are available at https://github.com/vernontoh/VerityMath.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-13",
    "authors": [
      {
        "authorId": "2266396284",
        "name": "Vernon Toh"
      },
      {
        "authorId": "2990638",
        "name": "Ratish Puduppully"
      },
      {
        "authorId": "2266426897",
        "name": "Nancy Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "2129c6edc2593bf4adb5bc2772fdb042bdf14070",
    "url": "https://www.semanticscholar.org/paper/2129c6edc2593bf4adb5bc2772fdb042bdf14070",
    "title": "Monte Carlo Thought Search: Large Language Model Querying for Complex Scientific Reasoning in Catalyst Design",
    "abstract": "Discovering novel catalysts requires complex reasoning involving multiple chemical properties and resultant trade-offs, leading to a combinatorial growth in the search space. While large language models (LLM) have demonstrated novel capabilities for chemistry through complex instruction following capabilities and high quality reasoning, a goal-driven combinatorial search using LLMs has not been explored in detail. In this work, we present a Monte Carlo Tree Search-based approach that improves beyond state-of-the-art chain-of-thought prompting variants to augment scientific reasoning. We introduce two new reasoning datasets: 1) a curation of computational chemistry simulations, and 2) diverse questions written by catalysis researchers for reasoning about novel chemical conversion processes. We improve over the best baseline by 25.8\\% and find that our approach can augment scientist's reasoning and discovery process with novel insights.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-22",
    "authors": [
      {
        "authorId": "2190281477",
        "name": "Henry W Sprueill"
      },
      {
        "authorId": "48870109",
        "name": "Carl N. Edwards"
      },
      {
        "authorId": "15751638",
        "name": "Mariefel V. Olarte"
      },
      {
        "authorId": "37794737",
        "name": "Udishnu Sanyal"
      },
      {
        "authorId": "2264201811",
        "name": "Heng Ji"
      },
      {
        "authorId": "7617146",
        "name": "Sutanay Choudhury"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "1990b5d48b7ba3d5ddd75e171d1a4632042fbbc0",
    "url": "https://www.semanticscholar.org/paper/1990b5d48b7ba3d5ddd75e171d1a4632042fbbc0",
    "title": "Coupling Symbolic Reasoning with Language Modeling for Efficient Longitudinal Understanding of Unstructured Electronic Medical Records",
    "abstract": "The application of Artificial Intelligence (AI) in healthcare has been revolutionary, especially with the recent advancements in transformer-based Large Language Models (LLMs). However, the task of understanding unstructured electronic medical records remains a challenge given the nature of the records (e.g., disorganization, inconsistency, and redundancy) and the inability of LLMs to derive reasoning paradigms that allow for comprehensive understanding of medical variables. In this work, we examine the power of coupling symbolic reasoning with language modeling toward improved understanding of unstructured clinical texts. We show that such a combination improves the extraction of several medical variables from unstructured records. In addition, we show that the state-of-the-art commercially-free LLMs enjoy retrieval capabilities comparable to those provided by their commercial counterparts. Finally, we elaborate on the need for LLM steering through the application of symbolic reasoning as the exclusive use of LLMs results in the lowest performance.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.03360",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-08-07",
    "authors": [
      {
        "authorId": "2228149800",
        "name": "Shivani Shekhar"
      },
      {
        "authorId": "1508843118",
        "name": "Simran Tiwari"
      },
      {
        "authorId": "49322629",
        "name": "T. Rensink"
      },
      {
        "authorId": "37558091",
        "name": "R. Eskander"
      },
      {
        "authorId": "2946757",
        "name": "Wael Salloum"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "152e0185bbae8a58e7e03aa4a630c0e7e6e19513",
    "url": "https://www.semanticscholar.org/paper/152e0185bbae8a58e7e03aa4a630c0e7e6e19513",
    "title": "Exploring and Improving the Spatial Reasoning Abilities of Large Language Models",
    "abstract": "Large Language Models (LLMs) represent formidable tools for sequence modeling, boasting an innate capacity for general pattern recognition. Nevertheless, their broader spatial reasoning capabilities, especially applied to numerical trajectory data, remain insufficiently explored. In this paper, we investigate the out-of-the-box performance of ChatGPT-3.5, ChatGPT-4 and Llama 2 7B models when confronted with 3D robotic trajectory data from the CALVIN baseline and associated tasks, including 2D directional and shape labeling. Additionally, we introduce a novel prefix-based prompting mechanism, which yields a 33% improvement on the 3D trajectory data and an increase of up to 10% on SpartQA tasks over zero-shot prompting (with gains for other prompting types as well). The experimentation with 3D trajectory data offers an intriguing glimpse into the manner in which LLMs engage with numerical and spatial information, thus laying a solid foundation for the identification of target areas for future enhancements.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-02",
    "authors": [
      {
        "authorId": "2269731192",
        "name": "Manasi Sharma"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "4200b8253f56de8948d20fd69e7731b92c3ac3a4",
    "url": "https://www.semanticscholar.org/paper/4200b8253f56de8948d20fd69e7731b92c3ac3a4",
    "title": "Controlling Equational Reasoning in Large Language Models with Prompt Interventions",
    "abstract": "This paper investigates how hallucination rates in Large Language Models (LLMs) may be controlled via a symbolic data generation framework, exploring a fundamental relationship between the rate of certain mathematical errors and types of input intervention. Specifically, we systematically generate data for a derivation generation task using a symbolic engine, applying targeted interventions to prompts to perturb features of mathematical derivations such as the surface forms of symbols, equational tree structures, and mathematical context. We then evaluate the effect of prompt interventions across a range of LLMs including fine-tuned T5 models, GPT, and LLaMa-based models. Our experiments suggest that T5-Large can outperform the few-shot performance of GPT-4 on various evaluation sets generated via the framework. However, an extensive evaluation based on human analysis, template-based error detection, and text generation metrics reveals model weaknesses beyond what the reference-based metrics singularly describe. We use these results to tie characteristic distributional footprints of interventions to the human evaluation of LLM derivation quality, potentially leading to significant control over fine-grained mathematical capabilities of language models with respect to specific types of errors.",
    "venue": "",
    "year": 2023,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-07-19",
    "authors": [
      {
        "authorId": "151056786",
        "name": "Jordan Meadows"
      },
      {
        "authorId": "34102057",
        "name": "Marco Valentino"
      },
      {
        "authorId": "2057619238",
        "name": "André Freitas"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "18cad1aa7a3f6e783183a5588e85a9dc3ba6ede6",
    "url": "https://www.semanticscholar.org/paper/18cad1aa7a3f6e783183a5588e85a9dc3ba6ede6",
    "title": "Enhancing Quantitative Reasoning Skills of Large Language Models through Dimension Perception",
    "abstract": "Quantities are distinct and critical components of texts that characterize the magnitude properties of entities, providing a precise perspective for the understanding of natural language, especially for reasoning tasks. In recent years, there has been a flurry of research on reasoning tasks based on large language models (LLMs), most of which solely focus on numerical values, neglecting the dimensional concept of quantities with units despite its importance. We argue that the concept of dimension is essential for precisely understanding quantities and of great significance for LLMs to perform quantitative reasoning. However, the lack of dimension knowledge and quantity-related benchmarks has resulted in low performance of LLMs. Hence, we present a framework to enhance the quantitative reasoning ability of language models based on dimension perception. We first construct a dimensional unit knowledge base (DimUnitKB) to address the knowledge gap in this area. We propose a benchmark DimEval consisting of seven tasks of three categories to probe and enhance the dimension perception skills of LLMs. To evaluate the effectiveness of our methods, we propose a quantitative reasoning task and conduct experiments. The experimental results show that our dimension perception method dramatically improves accuracy (43.55%→50.67%) on quantitative reasoning tasks compared to GPT-4.",
    "venue": "IEEE International Conference on Data Engineering",
    "year": 2023,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2312.17532",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-29",
    "authors": [
      {
        "authorId": "2220023225",
        "name": "Yuncheng Huang"
      },
      {
        "authorId": "2152880833",
        "name": "Qi He"
      },
      {
        "authorId": "3366523",
        "name": "Jiaqing Liang"
      },
      {
        "authorId": "1999030240",
        "name": "Sihang Jiang"
      },
      {
        "authorId": "2267005966",
        "name": "Yanghua Xiao"
      },
      {
        "authorId": "2277239339",
        "name": "Yunwen Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 92.47918433002164
  },
  {
    "paperId": "857297f749838d7d07ffb4dc54fae141379e38c0",
    "url": "https://www.semanticscholar.org/paper/857297f749838d7d07ffb4dc54fae141379e38c0",
    "title": "Parrot Mind: Towards Explaining the Complex Task Reasoning of Pretrained Large Language Models with Template-Content Structure",
    "abstract": "The pre-trained large language models (LLMs) have shown their extraordinary capacity to solve reasoning tasks, even on tasks that require a complex process involving multiple sub-steps. However, given the vast possible generation space of all the tasks, how the pretrained model learns the reasoning ability remains an open question. We firstly propose that an intrinsic structural constraint on the generated sequence of language-based reasoning -- we called it template-content structure (T-C structure) -- is the key to explain why LLMs can solve a large number of complex reasoning problems with limited training data by showing this structure can reduce the possible space from exponential level to linear level. Furthermore, by generalizing this structure to the hierarchical case, we demonstrate that models can achieve task composition, further reducing the space needed to learn from linear to logarithmic, thereby effectively learning on complex reasoning involving multiple steps. We provide both examples and formal theory of our T-C structure. We also experimentally validate the existence of the T-C structure in some current LLMs and its effectiveness for reasoning.",
    "venue": "",
    "year": 2023,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-10-09",
    "authors": [
      {
        "authorId": "2185454298",
        "name": "Haotong Yang"
      },
      {
        "authorId": "2256983328",
        "name": "Fanxu Meng"
      },
      {
        "authorId": "2257038975",
        "name": "Zhouchen Lin"
      },
      {
        "authorId": "2257092769",
        "name": "Muhan Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "961a1772f3b90d9dffd2b571c6996007a1d0ccd1",
    "url": "https://www.semanticscholar.org/paper/961a1772f3b90d9dffd2b571c6996007a1d0ccd1",
    "title": "JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents",
    "abstract": "Building a conversational embodied agent to execute real-life tasks has been a long-standing yet quite challenging research goal, as it requires effective human-agent communication, multi-modal understanding, long-range sequential decision making, etc. Traditional symbolic methods have scaling and generalization issues, while end-to-end deep learning models suffer from data scarcity and high task complexity, and are often hard to explain. To benefit from both worlds, we propose JARVIS, a neuro-symbolic commonsense reasoning framework for modular, generalizable, and interpretable conversational embodied agents. First, it acquires symbolic representations by prompting large language models (LLMs) for language understanding and sub-goal planning, and by constructing semantic maps from visual observations. Then the symbolic module reasons for sub-goal planning and action generation based on task- and action-level common sense. Extensive experiments on the TEACh dataset validate the efficacy and efficiency of our JARVIS framework, which achieves state-of-the-art (SOTA) results on all three dialog-based embodied tasks, including Execution from Dialog History (EDH), Trajectory from Dialog (TfD), and Two-Agent Task Completion (TATC) (e.g., our method boosts the unseen Success Rate on EDH from 6.1\\% to 15.8\\%). Moreover, we systematically analyze the essential factors that affect the task performance and also demonstrate the superiority of our method in few-shot settings. Our JARVIS model ranks first in the Alexa Prize SimBot Public Benchmark Challenge.",
    "venue": "arXiv.org",
    "year": 2022,
    "citationCount": 30,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2208.13266",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-08-28",
    "authors": [
      {
        "authorId": "145487536",
        "name": "Kai Zheng"
      },
      {
        "authorId": "9368148",
        "name": "KAI-QING Zhou"
      },
      {
        "authorId": "145612104",
        "name": "Jing Gu"
      },
      {
        "authorId": "2118165871",
        "name": "Yue Fan"
      },
      {
        "authorId": "2109655937",
        "name": "Jialu Wang"
      },
      {
        "authorId": "2142692796",
        "name": "Zong-xiao Li"
      },
      {
        "authorId": "2149253467",
        "name": "Xuehai He"
      },
      {
        "authorId": "47120131",
        "name": "X. Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.50980806727719
  },
  {
    "paperId": "239b5649b12f28fd610de036afba41b9246db6c9",
    "url": "https://www.semanticscholar.org/paper/239b5649b12f28fd610de036afba41b9246db6c9",
    "title": "Parsel: A Unified Natural Language Framework for Algorithmic Reasoning",
    "abstract": "Despite recent success in large language model (LLM) reasoning, LLMs still struggle with hierarchical multi-step reasoning like generating complex programs. In these cases, humans often start with a high-level algorithmic design and implement each part gradually. We introduce Parsel 2 , a framework enabling automatic implementation and validation of complex algorithms with code LLMs, based on hierarchical function descriptions in natural language. Parsel can be used across domains requiring hierarchical reasoning, e.g. code synthesis, theorem proving, and robotic planning. We demonstrate Parsel’s capabilities by using it to generate complex programs that cannot currently be automatically implemented from one description and backtranslating Python programs in the APPS dataset. Beyond modeling capabilities, Parsel allows problem-solving with high-level algorithmic designs, beneﬁting both students and professional programmers.",
    "venue": "arXiv.org",
    "year": 2022,
    "citationCount": 15,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2212.10561",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "49456763",
        "name": "E. Zelikman"
      },
      {
        "authorId": "144862341",
        "name": "Qian Huang"
      },
      {
        "authorId": "2113249490",
        "name": "Gabriel Poesia"
      },
      {
        "authorId": "144002017",
        "name": "Noah D. Goodman"
      },
      {
        "authorId": "32551479",
        "name": "Nick Haber"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.58883083359672
  },
  {
    "paperId": "1dbd58bd8768ba0dada2e7c84aa2fe0b9f418ebc",
    "url": "https://www.semanticscholar.org/paper/1dbd58bd8768ba0dada2e7c84aa2fe0b9f418ebc",
    "title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification",
    "abstract": "Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has brought significant advancements in addressing math reasoning problems. In particular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter, shows remarkable performance on challenging math datasets. In this paper, we explore the effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the \\textit{Code Usage Frequency} of GPT-4 Code Interpreter. We found that its success can be largely attributed to its powerful skills in generating and executing code, evaluating the output of code execution, and rectifying its solution when receiving unreasonable outputs. Based on this insight, we propose a novel and effective prompting method, explicit \\uline{c}ode-based \\uline{s}elf-\\uline{v}erification~(CSV), to further boost the mathematical reasoning potential of GPT-4 Code Interpreter. This method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to use code to self-verify its answers. In instances where the verification state registers as ``False'', the model shall automatically amend its solution, analogous to our approach of rectifying errors during a mathematics examination. Furthermore, we recognize that the states of the verification result indicate the confidence of a solution, which can improve the effectiveness of majority voting. With GPT-4 Code Interpreter and CSV, we achieve an impressive zero-shot accuracy on MATH dataset \\textbf{(53.9\\% $\\to$ 84.3\\%)}.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 126,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.07921",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-08-15",
    "authors": [
      {
        "authorId": "9548994",
        "name": "Aojun Zhou"
      },
      {
        "authorId": "2254440245",
        "name": "Ke Wang"
      },
      {
        "authorId": "2230110008",
        "name": "Zimu Lu"
      },
      {
        "authorId": "15243971",
        "name": "Weikang Shi"
      },
      {
        "authorId": "2182235729",
        "name": "Sichun Luo"
      },
      {
        "authorId": "2099587433",
        "name": "Zipeng Qin"
      },
      {
        "authorId": "1418276662",
        "name": "Shaoqing Lu"
      },
      {
        "authorId": "2229208409",
        "name": "Anya Jia"
      },
      {
        "authorId": "2167114336",
        "name": "Linqi Song"
      },
      {
        "authorId": "2060755203",
        "name": "Mingjie Zhan"
      },
      {
        "authorId": "2229147559",
        "name": "Hongsheng Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 142.66280629687887
  },
  {
    "paperId": "a9b3c9e4efeabf9dc30693189ac7ee051fcc781d",
    "url": "https://www.semanticscholar.org/paper/a9b3c9e4efeabf9dc30693189ac7ee051fcc781d",
    "title": "Guidelines For Rigorous Evaluation of Clinical LLMs For Conversational Reasoning",
    "abstract": "The integration of Large Language Models (LLMs) like GPT-4 and GPT-3.5 into clinical diagnostics has the potential to transform patient-doctor interactions. However, the readiness of these models for real-world clinical application remains inadequately tested. This paper introduces the Conversational Reasoning Assessment Framework for Testing in Medicine (CRAFT-MD), a novel approach for evaluating clinical LLMs. Unlike traditional methods that rely on structured medical exams, CRAFT-MD focuses on natural dialogues, using simulated AI agents to interact with LLMs in a controlled, ethical environment. We applied CRAFT-MD to assess the diagnostic capabilities of GPT-4 and GPT-3.5 in the context of skin diseases. Our experiments revealed critical insights into the limitations of current LLMs in terms of clinical conversational reasoning, history taking, and diagnostic accuracy. Based on these findings, we propose a comprehensive set of guidelines for future evaluations of clinical LLMs. These guidelines emphasize realistic doctor-patient conversations, comprehensive history taking, open-ended questioning, and a combination of automated and expert evaluations. The introduction of CRAFT-MD marks a significant advancement in LLM testing, aiming to ensure that these models augment medical practice effectively and ethically.",
    "venue": "",
    "year": null,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1388622178",
        "name": "S. Johri"
      },
      {
        "authorId": "2239167151",
        "name": "Jaehwan Jeong"
      },
      {
        "authorId": "2239076163",
        "name": "Benjamin A. Tran"
      },
      {
        "authorId": "50624195",
        "name": "D. Schlessinger"
      },
      {
        "authorId": "2239076288",
        "name": "S. Md"
      },
      {
        "authorId": "2239164584",
        "name": "Zhuo Ran Cai"
      },
      {
        "authorId": "2239076389",
        "name": "Roxana Daneshjou"
      },
      {
        "authorId": "2239076381",
        "name": "†. PranavRajpurkarPhD"
      },
      {
        "authorId": "2239164584",
        "name": "Zhuo Ran Cai"
      }
    ],
    "source": "semantic_scholar",
    "score": 81.19162312519754
  },
  {
    "paperId": "1562390dd212516cd857009cbd4f857a902d1f3d",
    "url": "https://www.semanticscholar.org/paper/1562390dd212516cd857009cbd4f857a902d1f3d",
    "title": "MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents",
    "abstract": "Significant advancements have occurred in the application of Large Language Models (LLMs) for various tasks and social simulations. Despite this, their capacities to coordinate within task-oriented social contexts are under-explored. Such capabilities are crucial if LLMs are to effectively mimic human-like social behavior and produce meaningful results. To bridge this gap, we introduce collaborative generative agents, endowing LLM-based Agents with consistent behavior patterns and task-solving abilities. We situate these agents in a simulated job fair environment as a case study to scrutinize their coordination skills. We propose a novel framework that equips collaborative generative agents with human-like reasoning abilities and specialized skills. Our evaluation demonstrates that these agents show promising performance. However, we also uncover limitations that hinder their effectiveness in more complex coordination tasks. Our work provides valuable insights into the role and evolution of LLMs in task-oriented social simulations.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 74,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.06500",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-10",
    "authors": [
      {
        "authorId": "2257259410",
        "name": "Yuan Li"
      },
      {
        "authorId": "2257107248",
        "name": "Yixuan Zhang"
      },
      {
        "authorId": "2243348413",
        "name": "Lichao Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 134.76232170304465
  },
  {
    "paperId": "176793264b4344a598d301dcfb8d43e13f323ef0",
    "url": "https://www.semanticscholar.org/paper/176793264b4344a598d301dcfb8d43e13f323ef0",
    "title": "Vul-RAG: Enhancing LLM-based Vulnerability Detection via Knowledge-level RAG",
    "abstract": "Vulnerability detection is essential for software quality assurance. In recent years, deep learning models (especially large language models) have shown promise in vulnerability detection. In this work, we propose a novel LLM-based vulnerability detection technique Vul-RAG, which leverages knowledge-level retrieval-augmented generation (RAG) framework to detect vulnerability for the given code in three phases. First, Vul-RAG constructs a vulnerability knowledge base by extracting multi-dimension knowledge via LLMs from existing CVE instances; second, for a given code snippet, Vul-RAG} retrieves the relevant vulnerability knowledge from the constructed knowledge base based on functional semantics; third, Vul-RAG leverages LLMs to check the vulnerability of the given code snippet by reasoning the presence of vulnerability causes and fixing solutions of the retrieved vulnerability knowledge. Our evaluation of Vul-RAG on our constructed benchmark PairVul shows that Vul-RAG substantially outperforms all baselines by 12.96\\%/110\\% relative improvement in accuracy/pairwise-accuracy. In addition, our user study shows that the vulnerability knowledge generated by Vul-RAG can serve as high-quality explanations which can improve the manual detection accuracy from 0.60 to 0.77.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-17",
    "authors": [
      {
        "authorId": "2133930029",
        "name": "Xueying Du"
      },
      {
        "authorId": "2307070440",
        "name": "Geng Zheng"
      },
      {
        "authorId": "2296494155",
        "name": "Kaixin Wang"
      },
      {
        "authorId": "2296601233",
        "name": "Jiayi Feng"
      },
      {
        "authorId": "2307018188",
        "name": "Wentai Deng"
      },
      {
        "authorId": "2007711208",
        "name": "Mingwei Liu"
      },
      {
        "authorId": "144943089",
        "name": "Bihuan Chen"
      },
      {
        "authorId": "2216694669",
        "name": "Xin Peng"
      },
      {
        "authorId": "2308628801",
        "name": "Tao Ma"
      },
      {
        "authorId": "2265400203",
        "name": "Yiling Lou"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "32524aa3ae8522542753ed7e6f4cca3970e4acab",
    "url": "https://www.semanticscholar.org/paper/32524aa3ae8522542753ed7e6f4cca3970e4acab",
    "title": "Can an Embodied Agent Find Your “Cat-shaped Mug”? LLM-Based Zero-Shot Object Navigation",
    "abstract": "We present language-guided exploration (LGX), a novel algorithm for Language-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied agent navigates to an uniquely described target object in a previously unseen environment. Our approach makes use of large language models (LLMs) for this task by leveraging the LLM's commonsense-reasoning capabilities for making sequential navigational decisions. Simultaneously, we perform generalized target object detection using a pre-trained Vision-Language grounding model. We achieve state-of-the-art zero-shot object navigation results on RoboTHOR with a success rate (SR) improvement of over 27% over the current baseline of the OWL-ViT CLIP on Wheels (OWL CoW). Furthermore, we study the usage of LLMs for robot navigation and present an analysis of various prompting strategies affecting the model output. Finally, we showcase the benefits of our approach via real-world experiments that indicate the superior performance of LGX in detecting and navigating to visually unique objects.",
    "venue": "IEEE Robotics and Automation Letters",
    "year": 2023,
    "citationCount": 69,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-03-06",
    "authors": [
      {
        "authorId": "1395945114",
        "name": "Vishnu Sashank Dorbala"
      },
      {
        "authorId": "2057888241",
        "name": "James F. Mullen"
      },
      {
        "authorId": "1699159",
        "name": "Dinesh Manocha"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.7274286307404
  },
  {
    "paperId": "fc45b0c7249c9e48de2cd35fc3d9984490229392",
    "url": "https://www.semanticscholar.org/paper/fc45b0c7249c9e48de2cd35fc3d9984490229392",
    "title": "Case-Based or Rule-Based: How Do Transformers Do the Math?",
    "abstract": "Despite the impressive performance in a variety of complex tasks, modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition. While we can easily learn basic rules of addition and apply them to new problems of any length, LLMs struggle to do the same. Instead, they may rely on similar cases seen in the training corpus for help. We define these two different reasoning mechanisms as\"rule-based reasoning\"and\"case-based reasoning\". Since rule-based reasoning is essential for acquiring systematic generalization ability, we aim to explore exactly whether transformers use rule-based or case-based reasoning for math problems. Through carefully designed intervention experiments on five math tasks, we confirm that transformers are performing case-based reasoning, no matter whether scratchpad is used, which aligns with the previous observations that transformers use subgraph matching/shortcut learning to reason. To mitigate such problems, we propose a Rule-Following Fine-Tuning (RFFT) technique to teach transformers to perform rule-based reasoning. Specifically, we provide explicit rules in the input and then instruct transformers to recite and follow the rules step by step. Through RFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to generalize to up to 12-digit addition with over 95% accuracy, which is over 40% higher than scratchpad. The significant improvement demonstrates that teaching LLMs to use rules explicitly helps them learn rule-based reasoning and generalize better in length.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-27",
    "authors": [
      {
        "authorId": "2288333334",
        "name": "Yi Hu"
      },
      {
        "authorId": "2189014540",
        "name": "Xiaojuan Tang"
      },
      {
        "authorId": "2185454298",
        "name": "Haotong Yang"
      },
      {
        "authorId": "2257092769",
        "name": "Muhan Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "95166d7b62c864dd5f10eda5f1ad0a32fa12f004",
    "url": "https://www.semanticscholar.org/paper/95166d7b62c864dd5f10eda5f1ad0a32fa12f004",
    "title": "TransLLaMa: LLM-based Simultaneous Translation System",
    "abstract": "Decoder-only large language models (LLMs) have recently demonstrated impressive capabilities in text generation and reasoning. Nonetheless, they have limited applications in simultaneous machine translation (SiMT), currently dominated by encoder-decoder transformers. This study demonstrates that, after fine-tuning on a small dataset comprising causally aligned source and target sentence pairs, a pre-trained open-source LLM can control input segmentation directly by generating a special\"wait\"token. This obviates the need for a separate policy and enables the LLM to perform English-German and English-Russian SiMT tasks with BLEU scores that are comparable to those of specific state-of-the-art baselines. We also evaluated closed-source models such as GPT-4, which displayed encouraging results in performing the SiMT task without prior training (zero-shot), indicating a promising avenue for enhancing future SiMT systems.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-07",
    "authors": [
      {
        "authorId": "2283139737",
        "name": "Roman Koshkin"
      },
      {
        "authorId": "1790811",
        "name": "Katsuhito Sudoh"
      },
      {
        "authorId": "2148246160",
        "name": "Satoshi Nakamura"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "95dad62c52800600e571b4197314578fd441ca28",
    "url": "https://www.semanticscholar.org/paper/95dad62c52800600e571b4197314578fd441ca28",
    "title": "KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents",
    "abstract": "Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone models demonstrate that KnowAgent can achieve comparable or superior performance to existing baselines. Further analysis indicates the effectiveness of KnowAgent in terms of planning hallucinations mitigation. Code is available in https://github.com/zjunlp/KnowAgent.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-05",
    "authors": [
      {
        "authorId": "2283182145",
        "name": "Yuqi Zhu"
      },
      {
        "authorId": "2190751119",
        "name": "Shuofei Qiao"
      },
      {
        "authorId": "2196928874",
        "name": "Yixin Ou"
      },
      {
        "authorId": "152931849",
        "name": "Shumin Deng"
      },
      {
        "authorId": "2153010067",
        "name": "Ningyu Zhang"
      },
      {
        "authorId": "116540347",
        "name": "Shiwei Lyu"
      },
      {
        "authorId": "2290138094",
        "name": "Yue Shen"
      },
      {
        "authorId": "2285135033",
        "name": "Lei Liang"
      },
      {
        "authorId": "2269769748",
        "name": "Jinjie Gu"
      },
      {
        "authorId": "2144200945",
        "name": "Huajun Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "ec783473ce69f5a92104587aec23189206096e6c",
    "url": "https://www.semanticscholar.org/paper/ec783473ce69f5a92104587aec23189206096e6c",
    "title": "AutoGPT+P: Affordance-based Task Planning with Large Language Models",
    "abstract": "Recent advances in task planning leverage Large Language Models (LLMs) to improve generalizability by combining such models with classical planning algorithms to address their inherent limitations in reasoning capabilities. However, these approaches face the challenge of dynamically capturing the initial state of the task planning problem. To alleviate this issue, we propose AutoGPT+P, a system that combines an affordance-based scene representation with a planning system. Affordances encompass the action possibilities of an agent on the environment and objects present in it. Thus, deriving the planning domain from an affordance-based scene representation allows symbolic planning with arbitrary objects. AutoGPT+P leverages this representation to derive and execute a plan for a task specified by the user in natural language. In addition to solving planning tasks under a closed-world assumption, AutoGPT+P can also handle planning with incomplete information, e. g., tasks with missing objects by exploring the scene, suggesting alternatives, or providing a partial plan. The affordance-based scene representation combines object detection with an automatically generated object-affordance-mapping using ChatGPT. The core planning tool extends existing work by automatically correcting semantic and syntactic errors. Our approach achieves a success rate of 98%, surpassing the current 81% success rate of the current state-of-the-art LLM-based planning method SayCan on the SayCan instruction set. Furthermore, we evaluated our approach on our newly created dataset with 150 scenarios covering a wide range of complex tasks with missing objects, achieving a success rate of 79% on our dataset. The dataset and the code are publicly available at https://git.h2t.iar.kit.edu/birr/autogpt-p-standalone.",
    "venue": "Robotics: Science and Systems",
    "year": 2024,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-16",
    "authors": [
      {
        "authorId": "2284598372",
        "name": "Timo Birr"
      },
      {
        "authorId": "2281826992",
        "name": "Christoph Pohl"
      },
      {
        "authorId": "2284595305",
        "name": "Abdelrahman Younes"
      },
      {
        "authorId": "2248801384",
        "name": "Tamim Asfour"
      }
    ],
    "source": "semantic_scholar",
    "score": 117.27359974682
  },
  {
    "paperId": "de87c9522ce0c70dc39ce66874b06b34a9fe74eb",
    "url": "https://www.semanticscholar.org/paper/de87c9522ce0c70dc39ce66874b06b34a9fe74eb",
    "title": "Mindstorms in Natural Language-Based Societies of Mind",
    "abstract": "Both Minsky's\"society of mind\"and Schmidhuber's\"learning to think\"inspire diverse societies of large multimodal neural networks (NNs) that solve problems by interviewing each other in a\"mindstorm.\"Recent implementations of NN-based societies of minds consist of large language models (LLMs) and other NN-based experts communicating through a natural language interface. In doing so, they overcome the limitations of single LLMs, improving multimodal zero-shot reasoning. In these natural language-based societies of mind (NLSOMs), new agents -- all communicating through the same universal symbolic language -- are easily added in a modular fashion. To demonstrate the power of NLSOMs, we assemble and experiment with several of them (having up to 129 members), leveraging mindstorms in them to solve some practical AI tasks: visual question answering, image captioning, text-to-image synthesis, 3D generation, egocentric retrieval, embodied AI, and general language-based task solving. We view this as a starting point towards much larger NLSOMs with billions of agents-some of which may be humans. And with this emergence of great societies of heterogeneous minds, many new research questions have suddenly become paramount to the future of artificial intelligence. What should be the social structure of an NLSOM? What would be the (dis)advantages of having a monarchical rather than a democratic structure? How can principles of NN economies be used to maximize the total reward of a reinforcement learning NLSOM? In this work, we identify, discuss, and try to answer some of these questions.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 57,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.17066",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-26",
    "authors": [
      {
        "authorId": "1722264111",
        "name": "Mingchen Zhuge"
      },
      {
        "authorId": "46935952",
        "name": "Haozhe Liu"
      },
      {
        "authorId": "79787170",
        "name": "Francesco Faccio"
      },
      {
        "authorId": "35497986",
        "name": "Dylan R. Ashley"
      },
      {
        "authorId": "2258963332",
        "name": "R'obert Csord'as"
      },
      {
        "authorId": "31534667",
        "name": "Anand Gopalakrishnan"
      },
      {
        "authorId": "24029368",
        "name": "Abdullah Hamdi"
      },
      {
        "authorId": "1500194870",
        "name": "Hasan Hammoud"
      },
      {
        "authorId": "2074116613",
        "name": "Vincent Herrmann"
      },
      {
        "authorId": "2350348",
        "name": "Kazuki Irie"
      },
      {
        "authorId": "3031520",
        "name": "Louis Kirsch"
      },
      {
        "authorId": "114486625",
        "name": "Bing-chuan Li"
      },
      {
        "authorId": "49461641",
        "name": "G. Li"
      },
      {
        "authorId": "2107935382",
        "name": "Shuming Liu"
      },
      {
        "authorId": "2005711751",
        "name": "Jinjie Mai"
      },
      {
        "authorId": "2107705802",
        "name": "Piotr Pikekos"
      },
      {
        "authorId": "1992922591",
        "name": "A. Ramesh"
      },
      {
        "authorId": "35328044",
        "name": "Imanol Schlag"
      },
      {
        "authorId": "2154581258",
        "name": "Weimin Shi"
      },
      {
        "authorId": "2218538693",
        "name": "Aleksandar Stani'c"
      },
      {
        "authorId": "2218962876",
        "name": "Wenyi Wang"
      },
      {
        "authorId": "2208969427",
        "name": "Yu‐Han Wang"
      },
      {
        "authorId": "97375393",
        "name": "Mengmeng Xu"
      },
      {
        "authorId": "23999143",
        "name": "Deng-Ping Fan"
      },
      {
        "authorId": "2931652",
        "name": "Bernard Ghanem"
      },
      {
        "authorId": "145341374",
        "name": "J. Schmidhuber"
      }
    ],
    "source": "semantic_scholar",
    "score": 130.90664515819628
  },
  {
    "paperId": "612cb53584b23e2066631d143e09b471852afaeb",
    "url": "https://www.semanticscholar.org/paper/612cb53584b23e2066631d143e09b471852afaeb",
    "title": "Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting",
    "abstract": "Incorporating factual knowledge in knowledge graph is regarded as a promising approach for mitigating the hallucination of large language models (LLMs). Existing methods usually only use the user's input to query the knowledge graph, thus failing to address the factual hallucination generated by LLMs during its reasoning process. To address this problem, this paper proposes Knowledge Graph-based Retrofitting (KGR), a new framework that incorporates LLMs with KGs to mitigate factual hallucination during the reasoning process by retrofitting the initial draft responses of LLMs based on the factual knowledge stored in KGs. Specifically, KGR leverages LLMs to extract, select, validate, and retrofit factual statements within the model-generated responses, which enables an autonomous knowledge verifying and refining procedure without any additional manual efforts. Experiments show that KGR can significantly improve the performance of LLMs on factual QA benchmarks especially when involving complex reasoning processes, which demonstrates the necessity and effectiveness of KGR in mitigating hallucination and enhancing the reliability of LLMs.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2023,
    "citationCount": 43,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-22",
    "authors": [
      {
        "authorId": "2189374700",
        "name": "Xinyan Guan"
      },
      {
        "authorId": "49420585",
        "name": "Yanjiang Liu"
      },
      {
        "authorId": "2116455765",
        "name": "Hongyu Lin"
      },
      {
        "authorId": "1831434",
        "name": "Yaojie Lu"
      },
      {
        "authorId": "2046814249",
        "name": "Ben He"
      },
      {
        "authorId": "2118233348",
        "name": "Xianpei Han"
      },
      {
        "authorId": "2110832778",
        "name": "Le Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 126.76284450877392
  },
  {
    "paperId": "910fcfd759b0eba01b5744e84d5d2355ed618531",
    "url": "https://www.semanticscholar.org/paper/910fcfd759b0eba01b5744e84d5d2355ed618531",
    "title": "GeckOpt: LLM System Efficiency via Intent-Based Tool Selection",
    "abstract": "In this preliminary study, we investigate a GPT-driven intent-based reasoning approach to streamline tool selection for large language models (LLMs) aimed at system efficiency. By identifying the intent behind user prompts at runtime, we narrow down the API toolset required for task execution, reducing token consumption by up to 24.6%. Early results on a real-world, massively parallel Copilot platform with over 100 GPT-4-Turbo nodes show cost reductions and potential towards improving LLM-based system efficiency.",
    "venue": "ACM Great Lakes Symposium on VLSI",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2404.15804",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle"
    ],
    "publicationDate": "2024-04-24",
    "authors": [
      {
        "authorId": "2298042345",
        "name": "Michael Fore"
      },
      {
        "authorId": "2267505607",
        "name": "Simranjit Singh"
      },
      {
        "authorId": "2298040094",
        "name": "Dimitrios Stamoulis"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.53877639491068
  },
  {
    "paperId": "b02e8ce41cab6656af38dc0d65b61f56bbb91072",
    "url": "https://www.semanticscholar.org/paper/b02e8ce41cab6656af38dc0d65b61f56bbb91072",
    "title": "SymbolicAI: A framework for logic-based approaches combining generative models and solvers",
    "abstract": "We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for multi-modal data that connects multi-step generative processes and aligns their outputs with user objectives in complex workflows. As a result, we can transition between the capabilities of various foundation models with in-context learning capabilities and specialized, fine-tuned models or solvers proficient in addressing specific problems. Through these operations based on in-context learning our framework enables the creation and evaluation of explainable computational graphs. Finally, we introduce a quality measure and its empirical score for evaluating these computational graphs, and propose a benchmark that compares various state-of-the-art LLMs across a set of complex workflows. We refer to the empirical score as the\"Vector Embedding for Relational Trajectory Evaluation through Cross-similarity\", or VERTEX score for short. The framework codebase and benchmark are linked below.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-01",
    "authors": [
      {
        "authorId": "1974372841",
        "name": "Marius-Constantin Dinu"
      },
      {
        "authorId": "2282138287",
        "name": "Claudiu Leoveanu-Condrei"
      },
      {
        "authorId": "103040573",
        "name": "Markus Holzleitner"
      },
      {
        "authorId": "2282139788",
        "name": "Werner Zellinger"
      },
      {
        "authorId": "3308557",
        "name": "Sepp Hochreiter"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "a970be54c4df5f04c3fe65b7414e0c2879c55909",
    "url": "https://www.semanticscholar.org/paper/a970be54c4df5f04c3fe65b7414e0c2879c55909",
    "title": "HoneyComb: A Flexible LLM-Based Agent System for Materials Science",
    "abstract": "The emergence of specialized large language models (LLMs) has shown promise in addressing complex tasks for materials science. Many LLMs, however, often struggle with distinct complexities of material science tasks, such as materials science computational tasks, and often rely heavily on outdated implicit knowledge, leading to inaccuracies and hallucinations. To address these challenges, we introduce HoneyComb, the first LLM-based agent system specifically designed for materials science. HoneyComb leverages a novel, high-quality materials science knowledge base (MatSciKB) and a sophisticated tool hub (ToolHub) to enhance its reasoning and computational capabilities tailored to materials science. MatSciKB is a curated, structured knowledge collection based on reliable literature, while ToolHub employs an Inductive Tool Construction method to generate, decompose, and refine API tools for materials science. Additionally, HoneyComb leverages a retriever module that adaptively selects the appropriate knowledge source or tools for specific tasks, thereby ensuring accuracy and relevance. Our results demonstrate that HoneyComb significantly outperforms baseline models across various tasks in materials science, effectively bridging the gap between current LLM capabilities and the specialized needs of this domain. Furthermore, our adaptable framework can be easily extended to other scientific domains, highlighting its potential for broad applicability in advancing scientific research and applications.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-08-29",
    "authors": [
      {
        "authorId": "2257922310",
        "name": "Huan Zhang"
      },
      {
        "authorId": "2257367927",
        "name": "Yu Song"
      },
      {
        "authorId": "2319897065",
        "name": "Ziyu Hou"
      },
      {
        "authorId": "2336267014",
        "name": "Santiago Miret"
      },
      {
        "authorId": "2257467973",
        "name": "Bang Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "d0a074d5da6dc44177b7a9533f700b0d8fda23be",
    "url": "https://www.semanticscholar.org/paper/d0a074d5da6dc44177b7a9533f700b0d8fda23be",
    "title": "LLM-Based Multi-Agent Systems for Software Engineering: Vision and the Road Ahead",
    "abstract": "Integrating Large Language Models(LLMs) intoautonomousagents marks a signiﬁcant shift in the research landscape by oﬀering cognitive abilities competitive to human planning and reasoning. This paper envisions the evolution of LLM-based Multi-Agent (LMA) systems in addressing complex and multi-faceted software engineering challenges. LMA systems introduce numerous beneﬁts, including enhanced robustness throughcollaborativecross-examination, autonomous problem-solving, and scalable solutions to complex software projects. By examining the role of LMA systems in future software engineering practices, this vision paper highlights the potential applications and emerging challenges. We further point to speciﬁc opportunities for research and conclude with a research agenda with a set of research questions to guide future research directions",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2158107537",
        "name": "Junda He"
      },
      {
        "authorId": "2257011464",
        "name": "Christoph Treude"
      },
      {
        "authorId": "2275193225",
        "name": "David Lo"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "2985af7568176c4c229e552cb0acaaf3502432df",
    "url": "https://www.semanticscholar.org/paper/2985af7568176c4c229e552cb0acaaf3502432df",
    "title": "Computational Experiments Meet Large Language Model Based Agents: A Survey and Perspective",
    "abstract": "Computational experiments have emerged as a valuable method for studying complex systems, involving the algorithmization of counterfactuals. However, accurately representing real social systems in Agent-based Modeling (ABM) is challenging due to the diverse and intricate characteristics of humans, including bounded rationality and heterogeneity. To address this limitation, the integration of Large Language Models (LLMs) has been proposed, enabling agents to possess anthropomorphic abilities such as complex reasoning and autonomous learning. These agents, known as LLM-based Agent, offer the potential to enhance the anthropomorphism lacking in ABM. Nonetheless, the absence of explicit explainability in LLMs significantly hinders their application in the social sciences. Conversely, computational experiments excel in providing causal analysis of individual behaviors and complex phenomena. Thus, combining computational experiments with LLM-based Agent holds substantial research potential. This paper aims to present a comprehensive exploration of this fusion. Primarily, it outlines the historical development of agent structures and their evolution into artificial societies, emphasizing their importance in computational experiments. Then it elucidates the advantages that computational experiments and LLM-based Agents offer each other, considering the perspectives of LLM-based Agent for computational experiments and vice versa. Finally, this paper addresses the challenges and future trends in this research domain, offering guidance for subsequent related studies.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-02-01",
    "authors": [
      {
        "authorId": "2282275759",
        "name": "Qun Ma"
      },
      {
        "authorId": "2276115373",
        "name": "Xiao Xue"
      },
      {
        "authorId": "2112224453",
        "name": "Deyu Zhou"
      },
      {
        "authorId": "3233596",
        "name": "Xiangning Yu"
      },
      {
        "authorId": "2152510458",
        "name": "Donghua Liu"
      },
      {
        "authorId": "2282234785",
        "name": "Xuwen Zhang"
      },
      {
        "authorId": "2282239078",
        "name": "Zihan Zhao"
      },
      {
        "authorId": "2261891593",
        "name": "Yifan Shen"
      },
      {
        "authorId": "2282139368",
        "name": "Peilin Ji"
      },
      {
        "authorId": "2282192155",
        "name": "Juanjuan Li"
      },
      {
        "authorId": "2257792734",
        "name": "Gang Wang"
      },
      {
        "authorId": "2276183489",
        "name": "Wanpeng Ma"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "ead701babc67e1d03777339e31d163f1d3f26362",
    "url": "https://www.semanticscholar.org/paper/ead701babc67e1d03777339e31d163f1d3f26362",
    "title": "Efficient Prompting for LLM-Based Generative Internet of Things",
    "abstract": "Large language models (LLMs) have demonstrated remarkable capacities on various tasks, and integrating the capacities of LLMs into the Internet of Things (IoT) applications has drawn much research attention recently. Due to security concerns, many institutions avoid accessing state-of-the-art commercial LLM services, requiring the deployment and utilization of open-source LLMs in a local network setting. However, open-source LLMs usually have more limitations regarding their performance, such as their arithmetic calculation and reasoning capacities, and practical systems of applying LLMs to IoT have yet to be well-explored. Therefore, we propose an LLM-based Generative IoT (GIoT) system deployed in the local network setting in this study. To alleviate the limitations of LLMs and provide service with competitive performance, we apply prompt engineering methods to enhance the capacities of the open-source LLMs, design a Prompt Management Module and a Postprocessing Module to manage the tailored prompts for different tasks and process the results generated by the LLMs. To demonstrate the effectiveness of the proposed system, we discuss a challenging table question answering (Table-QA) task as a case study of the proposed system, as tabular data is usually more challenging than plaintext because of their complex structures, heterogeneous data types and sometimes huge sizes. We conduct comprehensive experiments on the two popular Table-QA data sets, and the results show that our proposal can achieve competitive performance compared with state-of-the-art LLMs, demonstrating that the proposed LLM-based GIoT system can provide competitive performance with tailored prompting methods and is easily extensible to new tasks without training.",
    "venue": "IEEE Internet of Things Journal",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2406.10382",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-14",
    "authors": [
      {
        "authorId": "2261437346",
        "name": "Bin Xiao"
      },
      {
        "authorId": "2497479",
        "name": "B. Kantarci"
      },
      {
        "authorId": "2261731446",
        "name": "Jiawen Kang"
      },
      {
        "authorId": "2266084696",
        "name": "Dusist Niyato"
      },
      {
        "authorId": "2238050222",
        "name": "Mohsen Guizani"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.1886522358297
  },
  {
    "paperId": "52350cfb03f4cccdbe141727334a5083bd613222",
    "url": "https://www.semanticscholar.org/paper/52350cfb03f4cccdbe141727334a5083bd613222",
    "title": "Coverage-based Example Selection for In-Context Learning",
    "abstract": "In-context learning (ICL), the ability of large language models to perform novel tasks by conditioning on a prompt with a few task examples, requires these examples to be informative about the test instance. The standard approach of independently ranking and selecting the most similar examples selects redundant examples while omitting important information. In this work, we show that BERTScore-Recall (BSR) selects better examples that demonstrate more of the salient aspects, e.g. reasoning patterns, of the test input. We further extend BSR and many standard metrics to easily optimizable set-level metrics, giving still better coverage of those salient aspects. On 15 datasets spanning 6 tasks and with 7 diverse LLMs, we show that (1) BSR is the superior metric for in-context example selection across the board, and (2) for compositional tasks, set selection using Set-BSR outperforms independent ranking by up to 17 points on average and, despite being training-free, surpasses methods that leverage task or LLM-specific training.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 29,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.14907",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-24",
    "authors": [
      {
        "authorId": "1698760333",
        "name": "Shivanshu Gupta"
      },
      {
        "authorId": "34650964",
        "name": "Sameer Singh"
      },
      {
        "authorId": "40642935",
        "name": "Matt Gardner"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.01796072493232
  },
  {
    "paperId": "55b5b020d906420ff724a810f9db92a8d215ae43",
    "url": "https://www.semanticscholar.org/paper/55b5b020d906420ff724a810f9db92a8d215ae43",
    "title": "Bayesian Optimization with LLM-Based Acquisition Functions for Natural Language Preference Elicitation",
    "abstract": "Designing preference elicitation (PE) methodologies that can quickly ascertain a user's top item preferences in a cold-start setting is a key challenge for building effective and personalized conversational recommendation (ConvRec) systems. While large language models (LLMs) enable fully natural language (NL) PE dialogues, we hypothesize that monolithic LLM NL-PE approaches lack the multi-turn, decision-theoretic reasoning required to effectively balance the exploration and exploitation of user preferences towards an arbitrary item set. In contrast, traditional Bayesian optimization PE methods define theoretically optimal PE strategies, but cannot generate arbitrary NL queries or reason over content in NL item descriptions -- requiring users to express preferences via ratings or comparisons of unfamiliar items. To overcome the limitations of both approaches, we formulate NL-PE in a Bayesian Optimization (BO) framework that seeks to actively elicit NL feedback to identify the best recommendation. Key challenges in generalizing BO to deal with natural language feedback include determining: (a) how to leverage LLMs to model the likelihood of NL preference feedback as a function of item utilities, and (b) how to design an acquisition function for NL BO that can elicit preferences in the infinite space of language. We demonstrate our framework in a novel NL-PE algorithm, PEBOL, which uses: 1) Natural Language Inference (NLI) between user preference utterances and NL item descriptions to maintain Bayesian preference beliefs, and 2) BO strategies such as Thompson Sampling (TS) and Upper Confidence Bound (UCB) to steer LLM query generation. We numerically evaluate our methods in controlled simulations, finding that after 10 turns of dialogue, PEBOL can achieve an MRR@10 of up to 0.27 compared to the best monolithic LLM baseline's MRR@10 of 0.17, despite relying on earlier and smaller LLMs.",
    "venue": "ACM Conference on Recommender Systems",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3640457.3688142",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-02",
    "authors": [
      {
        "authorId": "2299329319",
        "name": "David Eric Austin"
      },
      {
        "authorId": "2123355559",
        "name": "A. Korikov"
      },
      {
        "authorId": "1646622849",
        "name": "Armin Toroghi"
      },
      {
        "authorId": "2273670268",
        "name": "Scott Sanner"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.19162312519754
  },
  {
    "paperId": "78eaced1b4008b4227831adcccd39da4322e969e",
    "url": "https://www.semanticscholar.org/paper/78eaced1b4008b4227831adcccd39da4322e969e",
    "title": "Stance Detection with Collaborative Role-Infused LLM-Based Agents",
    "abstract": "Stance detection automatically detects the stance in a text towards a target, vital for content analysis in web and social media research. Despite their promising capabilities, LLMs encounter challenges when directly applied to stance detection. First, stance detection demands multi-aspect knowledge, from deciphering event-related terminologies to understanding the expression styles in social media platforms. Second, stance detection requires advanced reasoning to infer authors' implicit viewpoints, as stances are often subtly embedded rather than overtly stated in the text. To address these challenges, we design a three-stage framework COLA (short for Collaborative rOle-infused LLM-based Agents) in which LLMs are designated distinct roles, creating a collaborative system where each role contributes uniquely. Initially, in the multidimensional text analysis stage, we configure the LLMs to act as a linguistic expert, a domain specialist, and a social media veteran to get a multifaceted analysis of texts, thus overcoming the first challenge. Next, in the reasoning-enhanced debating stage, for each potential stance, we designate a specific LLM-based agent to advocate for it, guiding the LLM to detect logical connections between text features and stance, tackling the second challenge. Finally, in the stance conclusion stage, a final decision maker agent consolidates prior insights to determine the stance. Our approach avoids extra annotated data and model training and is highly usable. We achieve state-of-the-art performance across multiple datasets. Ablation studies validate the effectiveness of each role design in handling stance detection. Further experiments have demonstrated the explainability and the versatility of our approach. Our approach excels in usability, accuracy, effectiveness, explainability and versatility, highlighting its value.",
    "venue": "International Conference on Web and Social Media",
    "year": 2023,
    "citationCount": 24,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-16",
    "authors": [
      {
        "authorId": "2156918252",
        "name": "Xiaochong Lan"
      },
      {
        "authorId": "49281242",
        "name": "Chen Gao"
      },
      {
        "authorId": "49953590",
        "name": "Depeng Jin"
      },
      {
        "authorId": "2258761514",
        "name": "Yong Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.28313737302301
  },
  {
    "paperId": "1f9822022f586e375461660db792f23e891c7123",
    "url": "https://www.semanticscholar.org/paper/1f9822022f586e375461660db792f23e891c7123",
    "title": "Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems",
    "abstract": "The complexity of managing multiagent systems (MASs) in autonomic computing can be mitigated using a self-adaptation approach, where systems are equipped to monitor and adjust themselves based on specific concerns. Communication in these systems is key given that in scenarios involving agent interaction, it enhances cooperation and reduces coordination challenges by enabling direct, clear information exchange. However, the tasks of boosting communication expressiveness within MASs and logically processing a multitude of variables in dynamic environments are still challenging. This paper presents a novel strategy: integrating large language models (LLMs) like GPT-based technologies into MASs to boost communication and agent autonomy. Our proposal encompasses the development of a novel LLM/GPT-based agent architecture, focusing not only on advanced conversation features but also on the reasoning and decision-making capacities of these models. This is grounded in the MAPE-K model, known for supporting system adaptability in dynamic environments. We illustrate our approach through a marketplace scenario. This work represents a paradigm shift in MAS self-adaptation, utilizing LLMs' capabilities and indicating further research opportunities to assess LLMs' applicability in more complex MAS scenarios. This could pave the way for more potent problem-solving capabilities and refined communication within MASs.",
    "venue": "2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion (ACSOS-C)",
    "year": 2023,
    "citationCount": 28,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2307.06187",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-07-12",
    "authors": [
      {
        "authorId": "2658311",
        "name": "N. Nascimento"
      },
      {
        "authorId": "40761174",
        "name": "Paulo Alencar"
      },
      {
        "authorId": "2149928782",
        "name": "Donald D. Cowan"
      }
    ],
    "source": "semantic_scholar",
    "score": 126.5094374497971
  },
  {
    "paperId": "16f8bbfb04b1d9ac350e15732bc021e778a2cf68",
    "url": "https://www.semanticscholar.org/paper/16f8bbfb04b1d9ac350e15732bc021e778a2cf68",
    "title": "DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation",
    "abstract": "The rise of Large Language Models (LLMs) has sparked interest in their application to sequential recommendation tasks as they can provide supportive item information. However, due to the inherent complexities of sequential recommendation, such as sequential patterns across datasets, noise within sequences, and the temporal evolution of user preferences, existing LLM reasoning strategies, such as in-context learning and chain-of-thought are not fully effective. To address these challenges, we introduce a novel reasoning principle: Dynamic Reflection with Divergent Thinking within a retriever-reranker framework. Our approach starts with a collaborative in-context demonstration retriever, which collects sequences exhibiting collaborative behaviors as in-context examples. Following this, we abstract high-level user preferences across multiple aspects, providing a more nuanced understanding of user interests and circumventing the noise within the raw sequences. The cornerstone of our methodology is dynamic reflection, a process that emulates human learning through probing, critiquing, and reflecting, using user feedback to tailor the analysis more effectively to the target user in a temporal manner. We evaluate our approach on three datasets using six pre-trained LLMs. The superior performance observed across these models demonstrates the efficacy of our reasoning strategy, notably achieved without the need to fine-tune the LLMs. With our principle, we managed to outperform GPT-Turbo-3.5 on three datasets using 7b models e.g., Vicuna-7b and Openchat-7b on NDCG@10. This research not only highlights the potential of LLMs in enhancing sequential recommendation systems but also underscores the importance of developing tailored reasoning strategies to fully harness their capabilities.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 25,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-18",
    "authors": [
      {
        "authorId": "2153606201",
        "name": "Yu Wang"
      },
      {
        "authorId": "2223887365",
        "name": "Zhiwei Liu"
      },
      {
        "authorId": "2108313930",
        "name": "Jianguo Zhang"
      },
      {
        "authorId": "2275161304",
        "name": "Weiran Yao"
      },
      {
        "authorId": "71926704",
        "name": "Shelby Heinecke"
      },
      {
        "authorId": "2258846722",
        "name": "Philip S. Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.87144807032223
  },
  {
    "paperId": "bc8d248fb86a3b6a285e8b9a6fe2c09e7f0b19c9",
    "url": "https://www.semanticscholar.org/paper/bc8d248fb86a3b6a285e8b9a6fe2c09e7f0b19c9",
    "title": "Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach",
    "abstract": "The remarkable progress in Large Language Models (LLMs) opens up new avenues for addressing planning and decision-making problems in Multi-Agent Systems (MAS). However, as the number of agents increases, the issues of hallucination in LLMs and coordination in MAS have become increasingly prominent. Additionally, the efficient utilization of tokens emerges as a critical consideration when employing LLMs to facilitate the interactions among a substantial number of agents. In this paper, we develop a modular framework called LLaMAC to mitigate these challenges. LLaMAC implements a value distribution encoding similar to that found in the human brain, utilizing internal and external feedback mechanisms to facilitate collaboration and iterative reasoning among its modules. Through evaluations involving system resource allocation and robot grid transportation, we demonstrate the considerable advantages afforded by our proposed approach.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 24,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-23",
    "authors": [
      {
        "authorId": "2119453124",
        "name": "Bin Zhang"
      },
      {
        "authorId": "2262446566",
        "name": "Hangyu Mao"
      },
      {
        "authorId": "2135060971",
        "name": "Jingqing Ruan"
      },
      {
        "authorId": "2155576705",
        "name": "Ying Wen"
      },
      {
        "authorId": "2321328048",
        "name": "Yang Li"
      },
      {
        "authorId": "2116577679",
        "name": "Shao Zhang"
      },
      {
        "authorId": "153008118",
        "name": "Zhiwei Xu"
      },
      {
        "authorId": "2115499783",
        "name": "Dapeng Li"
      },
      {
        "authorId": "2262543561",
        "name": "Ziyue Li"
      },
      {
        "authorId": "2263456785",
        "name": "Rui Zhao"
      },
      {
        "authorId": "2214835116",
        "name": "Lijuan Li"
      },
      {
        "authorId": "2067827377",
        "name": "Guoliang Fan"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.28313737302301
  },
  {
    "paperId": "0504c5d96ebf7be01e2b622ff3a4bf155f2b0a41",
    "url": "https://www.semanticscholar.org/paper/0504c5d96ebf7be01e2b622ff3a4bf155f2b0a41",
    "title": "Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models",
    "abstract": "Causal graph recovery is traditionally done using statistical estimation-based methods or based on individual's knowledge about variables of interests. They often suffer from data collection biases and limitations of individuals' knowledge. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that leverages LLMs to deduce causal relationships in general causal graph recovery tasks. This method leverages knowledge compressed in LLMs and knowledge LLMs extracted from scientific publication database as well as experiment data about factors of interest to achieve this goal. Our method gives a prompting strategy to extract associational relationships among those factors and a mechanism to perform causality verification for these associations. Comparing to other LLM-based methods that directly instruct LLMs to do the highly complex causal reasoning, our method shows clear advantage on causal graph quality on benchmark datasets. More importantly, as causality among some factors may change as new research results emerge, our method show sensitivity to new evidence in the literature and can provide useful information for updating causal graphs accordingly.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-23",
    "authors": [
      {
        "authorId": "2258602359",
        "name": "Yuzhe Zhang"
      },
      {
        "authorId": "2286591780",
        "name": "Yipeng Zhang"
      },
      {
        "authorId": "2286307576",
        "name": "Yidong Gan"
      },
      {
        "authorId": "2258412350",
        "name": "Lina Yao"
      },
      {
        "authorId": "2286410001",
        "name": "Chen Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "93e71ad3f5e2b203a3f5fa7adbea5c31d8c5f24f",
    "url": "https://www.semanticscholar.org/paper/93e71ad3f5e2b203a3f5fa7adbea5c31d8c5f24f",
    "title": "KnowHalu: Hallucination Detection via Multi-Form Knowledge Based Factual Checking",
    "abstract": "This paper introduces KnowHalu, a novel approach for detecting hallucinations in text generated by large language models (LLMs), utilizing step-wise reasoning, multi-formulation query, multi-form knowledge for factual checking, and fusion-based detection mechanism. As LLMs are increasingly applied across various domains, ensuring that their outputs are not hallucinated is critical. Recognizing the limitations of existing approaches that either rely on the self-consistency check of LLMs or perform post-hoc fact-checking without considering the complexity of queries or the form of knowledge, KnowHalu proposes a two-phase process for hallucination detection. In the first phase, it identifies non-fabrication hallucinations--responses that, while factually correct, are irrelevant or non-specific to the query. The second phase, multi-form based factual checking, contains five key steps: reasoning and query decomposition, knowledge retrieval, knowledge optimization, judgment generation, and judgment aggregation. Our extensive evaluations demonstrate that KnowHalu significantly outperforms SOTA baselines in detecting hallucinations across diverse tasks, e.g., improving by 15.65% in QA tasks and 5.50% in summarization tasks, highlighting its effectiveness and versatility in detecting hallucinations in LLM-generated content.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-03",
    "authors": [
      {
        "authorId": "2280198133",
        "name": "Jiawei Zhang"
      },
      {
        "authorId": "2153079868",
        "name": "Chejian Xu"
      },
      {
        "authorId": "2294871940",
        "name": "Yu Gai"
      },
      {
        "authorId": "2281746065",
        "name": "Freddy Lecue"
      },
      {
        "authorId": "2293597685",
        "name": "Dawn Song"
      },
      {
        "authorId": "2295053351",
        "name": "Bo Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "d9dadfb199bb68abb40b3e9d38abab16fd52056f",
    "url": "https://www.semanticscholar.org/paper/d9dadfb199bb68abb40b3e9d38abab16fd52056f",
    "title": "Large Language Models for Constrained-Based Causal Discovery",
    "abstract": "Causality is essential for understanding complex systems, such as the economy, the brain, and the climate. Constructing causal graphs often relies on either data-driven or expert-driven approaches, both fraught with challenges. The former methods, like the celebrated PC algorithm, face issues with data requirements and assumptions of causal sufficiency, while the latter demand substantial time and domain knowledge. This work explores the capabilities of Large Language Models (LLMs) as an alternative to domain experts for causal graph generation. We frame conditional independence queries as prompts to LLMs and employ the PC algorithm with the answers. The performance of the LLM-based conditional independence oracle on systems with known causal graphs shows a high degree of variability. We improve the performance through a proposed statistical-inspired voting schema that allows some control over false-positive and false-negative rates. Inspecting the chain-of-thought argumentation, we find causal reasoning to justify its answer to a probabilistic query. We show evidence that knowledge-based CIT could eventually become a complementary tool for data-driven causal discovery.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-11",
    "authors": [
      {
        "authorId": "2284868016",
        "name": "Kai-Hendrik Cohrs"
      },
      {
        "authorId": "2820201",
        "name": "Gherardo Varando"
      },
      {
        "authorId": "2051299779",
        "name": "Emiliano Díaz"
      },
      {
        "authorId": "51170231",
        "name": "Vasileios Sitokonstantinou"
      },
      {
        "authorId": "2256862557",
        "name": "G. Camps-Valls"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "fc8ce12d6186ddaa797e2b36d5e8eb7921425308",
    "url": "https://www.semanticscholar.org/paper/fc8ce12d6186ddaa797e2b36d5e8eb7921425308",
    "title": "A survey on LLM-based multi-agent systems: workflow, infrastructure, and challenges",
    "abstract": null,
    "venue": "Vicinagearth",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "https://link.springer.com/content/pdf/10.1007/s44336-024-00009-2.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-10-08",
    "authors": [
      {
        "authorId": "2324981454",
        "name": "Xinyi Li"
      },
      {
        "authorId": "2325107355",
        "name": "Sai Wang"
      },
      {
        "authorId": "2325009078",
        "name": "Siqi Zeng"
      },
      {
        "authorId": "2325107611",
        "name": "Yu Wu"
      },
      {
        "authorId": "2325127007",
        "name": "Yi Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 76.87639203842082
  },
  {
    "paperId": "088ab579bf490691eea7ac92e122ee11c9b9d131",
    "url": "https://www.semanticscholar.org/paper/088ab579bf490691eea7ac92e122ee11c9b9d131",
    "title": "JudgeBench: A Benchmark for Evaluating LLM-based Judges",
    "abstract": "LLM-based judges have emerged as a scalable alternative to human evaluation and are increasingly used to assess, compare, and improve models. However, the reliability of LLM-based judges themselves is rarely scrutinized. As LLMs become more advanced, their responses grow more sophisticated, requiring stronger judges to evaluate them. Existing benchmarks primarily focus on a judge's alignment with human preferences, but often fail to account for more challenging tasks where crowdsourced human preference is a poor indicator of factual and logical correctness. To address this, we propose a novel evaluation framework to objectively evaluate LLM-based judges. Based on this framework, we propose JudgeBench, a benchmark for evaluating LLM-based judges on challenging response pairs spanning knowledge, reasoning, math, and coding. JudgeBench leverages a novel pipeline for converting existing difficult datasets into challenging response pairs with preference labels reflecting objective correctness. Our comprehensive evaluation on a collection of prompted judges, fine-tuned judges, multi-agent judges, and reward models shows that JudgeBench poses a significantly greater challenge than previous benchmarks, with many strong models (e.g., GPT-4o) performing just slightly better than random guessing. Overall, JudgeBench offers a reliable platform for assessing increasingly advanced LLM-based judges. Data and code are available at https://github.com/ScalerLab/JudgeBench .",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-16",
    "authors": [
      {
        "authorId": "2296003107",
        "name": "Sijun Tan"
      },
      {
        "authorId": "92721493",
        "name": "Siyuan Zhuang"
      },
      {
        "authorId": "2254275867",
        "name": "Kyle Montgomery"
      },
      {
        "authorId": "2326253583",
        "name": "William Y. Tang"
      },
      {
        "authorId": "2326113379",
        "name": "Alejandro Cuadron"
      },
      {
        "authorId": "2255299714",
        "name": "Chenguang Wang"
      },
      {
        "authorId": "2315123815",
        "name": "R. Popa"
      },
      {
        "authorId": "2316152835",
        "name": "Ion Stoica"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "4ebb04b8b1496f2dd3a0d56ec32f84fa8e198b06",
    "url": "https://www.semanticscholar.org/paper/4ebb04b8b1496f2dd3a0d56ec32f84fa8e198b06",
    "title": "LLM-Based Operating Systems for Automated Vehicles: A New Perspective",
    "abstract": "The deployment of large language models (LLMs) brings challenges to intelligent systems because its capability of integrating large-scale training data facilitates contextual reasoning. This paper envisions a revolution of the LLM based (Artificial) Intelligent Operating Systems (IOS, or AIOS) to support the core of automated vehicles. We explain the structure of this LLM-OS and discuss the resulting benefits and implementation difficulties.",
    "venue": "IEEE Transactions on Intelligent Vehicles",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-01",
    "authors": [
      {
        "authorId": "2149634848",
        "name": "Jingwei Ge"
      },
      {
        "authorId": "87920848",
        "name": "Chen Chang"
      },
      {
        "authorId": "2144138656",
        "name": "Jiawei Zhang"
      },
      {
        "authorId": "2240418173",
        "name": "Lingxi Li"
      },
      {
        "authorId": "2275875782",
        "name": "Xiaoxiang Na"
      },
      {
        "authorId": "3396678",
        "name": "Yilun Lin"
      },
      {
        "authorId": "2283638790",
        "name": "Li Li"
      },
      {
        "authorId": "2301311129",
        "name": "Fei-Yue Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 95.1415686865115
  },
  {
    "paperId": "6cc60d3aafadf164b6acf2bbe02244bd3a02679e",
    "url": "https://www.semanticscholar.org/paper/6cc60d3aafadf164b6acf2bbe02244bd3a02679e",
    "title": "RDRec: Rationale Distillation for LLM-based Recommendation",
    "abstract": "Large language model (LLM)-based recommender models that bridge users and items through textual prompts for effective semantic reasoning have gained considerable attention. However, few methods consider the underlying rationales behind interactions, such as user preferences and item attributes, limiting the reasoning capability of LLMs for recommendations. This paper proposes a rationale distillation recommender (RDRec), a compact model designed to learn rationales generated by a larger language model (LM). By leveraging rationales from reviews related to users and items, RDRec remarkably specifies their profiles for recommendations. Experiments show that RDRec achieves state-of-the-art (SOTA) performance in both top-N and sequential recommendations. Our source code is released at https://github.com/WangXFng/RDRec.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference",
      "Review"
    ],
    "publicationDate": "2024-05-17",
    "authors": [
      {
        "authorId": "2273814002",
        "name": "Xinfeng Wang"
      },
      {
        "authorId": "2223796723",
        "name": "Jin Cui"
      },
      {
        "authorId": "1518601697",
        "name": "Yoshimi Suzuki"
      },
      {
        "authorId": "3029852",
        "name": "Fumiyo Fukumoto"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "e02c313dc3292b67fe4a3a2420bbc06f6861ca5d",
    "url": "https://www.semanticscholar.org/paper/e02c313dc3292b67fe4a3a2420bbc06f6861ca5d",
    "title": "Leveraging Prompt-Based Large Language Models: Predicting Pandemic Health Decisions and Outcomes Through Social Media Language",
    "abstract": "We introduce a multi-step reasoning framework using prompt-based LLMs to examine the relationship between social media language patterns and trends in national health outcomes. Grounded in fuzzy-trace theory, which emphasizes the importance of “gists” of causal coherence in effective health communication, we introduce Role-Based Incremental Coaching (RBIC), a prompt-based LLM framework, to identify gists at-scale. Using RBIC, we systematically extract gists from subreddit discussions opposing COVID-19 health measures (Study 1). We then track how these gists evolve across key events (Study 2) and assess their influence on online engagement (Study 3). Finally, we investigate how the volume of gists is associated with national health trends like vaccine uptake and hospitalizations (Study 4). Our work is the first to empirically link social media linguistic patterns to real-world public health trends, highlighting the potential of prompt-based LLMs in identifying critical online discussion patterns that can form the basis of public health communication strategies.",
    "venue": "International Conference on Human Factors in Computing Systems",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3613904.3642117",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2024-03-01",
    "authors": [
      {
        "authorId": "144703574",
        "name": "Xi Ding"
      },
      {
        "authorId": "2124421636",
        "name": "Buse Çarik"
      },
      {
        "authorId": "2078502250",
        "name": "U. Gunturi"
      },
      {
        "authorId": "2273938737",
        "name": "Valerie F. Reyna"
      },
      {
        "authorId": "2289842117",
        "name": "Eugenia H. Rho"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "b6b5ffb9a1faee7b488eb46f2d68f62beaecb8c7",
    "url": "https://www.semanticscholar.org/paper/b6b5ffb9a1faee7b488eb46f2d68f62beaecb8c7",
    "title": "Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering",
    "abstract": "Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs). While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with LLMs for KVQA (MAIL). It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning. Specifically, (i) we propose a two-stage prompting strategy with LLMs to densely embody the image into a scene graph with detailed visual features; (ii) We construct a coupled concept graph by linking the mentioned entities with external facts. (iii) A tailored pseudo-siamese graph medium fusion is designed for sufficient multimodal fusion. We utilize the shared mentioned entities in two graphs as mediums to bridge a tight inter-modal exchange, while maximally preserving insightful intra-modal learning by constraining the fusion within mediums. Extensive experiments on two benchmark datasets show the superiority of MAIL with 24x less resources.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-20",
    "authors": [
      {
        "authorId": "2171902940",
        "name": "Junnan Dong"
      },
      {
        "authorId": "2172162773",
        "name": "Qinggang Zhang"
      },
      {
        "authorId": "1962337917",
        "name": "Huachi Zhou"
      },
      {
        "authorId": "1759658",
        "name": "D. Zha"
      },
      {
        "authorId": "2284768151",
        "name": "Pai Zheng"
      },
      {
        "authorId": "2171663135",
        "name": "Xiao Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "c6c9d94ee10275c71dfd309044226a363dcc7e33",
    "url": "https://www.semanticscholar.org/paper/c6c9d94ee10275c71dfd309044226a363dcc7e33",
    "title": "Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation",
    "abstract": "Large Language Models (LLMs) are emerging as promising approaches to enhance session-based recommendation (SBR), where both prompt-based and fine-tuning-based methods have been widely investigated to align LLMs with SBR. However, the former methods struggle with optimal prompts to elicit the correct reasoning of LLMs due to the lack of task-specific feedback, leading to unsatisfactory recommendations. Although the latter methods attempt to fine-tune LLMs with domain-specific knowledge, they face limitations such as high computational costs and reliance on open-source backbones. To address such issues, we propose a Reflective Reinforcement Large Language Model (Re2LLM) for SBR, guiding LLMs to focus on specialized knowledge essential for more accurate recommendations effectively and efficiently. In particular, we first design the Reflective Exploration Module to effectively extract knowledge that is readily understandable and digestible by LLMs. To be specific, we direct LLMs to examine recommendation errors through self-reflection and construct a knowledge base (KB) comprising hints capable of rectifying these errors. To efficiently elicit the correct reasoning of LLMs, we further devise the Reinforcement Utilization Module to train a lightweight retrieval agent. It learns to select hints from the constructed KB based on the task-specific feedback, where the hints can serve as guidance to help correct LLMs reasoning for better recommendations. Extensive experiments on multiple real-world datasets demonstrate that our method consistently outperforms state-of-the-art methods.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-25",
    "authors": [
      {
        "authorId": "2142663126",
        "name": "Ziyan Wang"
      },
      {
        "authorId": "2293409485",
        "name": "Yingpeng Du"
      },
      {
        "authorId": "2284074007",
        "name": "Zhu Sun"
      },
      {
        "authorId": "2284062835",
        "name": "Haoyan Chua"
      },
      {
        "authorId": "2273938691",
        "name": "Kaidong Feng"
      },
      {
        "authorId": "2293318034",
        "name": "Wenya Wang"
      },
      {
        "authorId": "2257228461",
        "name": "Jie Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "5b81062b7d071be54d3603fe3bc95539b1423505",
    "url": "https://www.semanticscholar.org/paper/5b81062b7d071be54d3603fe3bc95539b1423505",
    "title": "To Help or Not to Help: LLM-based Attentive Support for Human-Robot Group Interactions",
    "abstract": "How can a robot provide unobtrusive physical support within a group of humans? We present Attentive Support, a novel interaction concept for robots to support a group of humans. It combines scene perception, dialogue acquisition, situation understanding, and behavior generation with the common-sense reasoning capabilities of Large Language Models (LLMs). In addition to following user instructions, Attentive Support is capable of deciding when and how to support the humans, and when to remain silent to not disturb the group. With a diverse set of scenarios, we show and evaluate the robot’s attentive behavior, which supports and helps the humans when required, while not disturbing if no help is needed.",
    "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2403.12533",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-19",
    "authors": [
      {
        "authorId": "4338399",
        "name": "Daniel Tanneberg"
      },
      {
        "authorId": "2257038646",
        "name": "Felix Ocker"
      },
      {
        "authorId": "2257039476",
        "name": "Stephan Hasler"
      },
      {
        "authorId": "2187928830",
        "name": "Joerg Deigmoeller"
      },
      {
        "authorId": "2901441",
        "name": "Anna Belardinelli"
      },
      {
        "authorId": "2144448837",
        "name": "Chao Wang"
      },
      {
        "authorId": "2285133661",
        "name": "H. Wersing"
      },
      {
        "authorId": "2292199966",
        "name": "Bernhard Sendhoff"
      },
      {
        "authorId": "1713430",
        "name": "M. Gienger"
      }
    ],
    "source": "semantic_scholar",
    "score": 100.1415686865115
  },
  {
    "paperId": "f86bdd77c14398900a9b4a86eca111e9e6f9c148",
    "url": "https://www.semanticscholar.org/paper/f86bdd77c14398900a9b4a86eca111e9e6f9c148",
    "title": "Ruffle&Riley: Insights from Designing and Evaluating a Large Language Model-Based Conversational Tutoring System",
    "abstract": "Conversational tutoring systems (CTSs) offer learning experiences through interactions based on natural language. They are recognized for promoting cognitive engagement and improving learning outcomes, especially in reasoning tasks. Nonetheless, the cost associated with authoring CTS content is a major obstacle to widespread adoption and to research on effective instructional design. In this paper, we discuss and evaluate a novel type of CTS that leverages recent advances in large language models (LLMs) in two ways: First, the system enables AI-assisted content authoring by inducing an easily editable tutoring script automatically from a lesson text. Second, the system automates the script orchestration in a learning-by-teaching format via two LLM-based agents (Ruffle&Riley) acting as a student and a professor. The system allows for free-form conversations that follow the ITS-typical inner and outer loop structure. We evaluate Ruffle&Riley's ability to support biology lessons in two between-subject online user studies (N = 200) comparing the system to simpler QA chatbots and reading activity. Analyzing system usage patterns, pre/post-test scores and user experience surveys, we find that Ruffle&Riley users report high levels of engagement, understanding and perceive the offered support as helpful. Even though Ruffle&Riley users require more time to complete the activity, we did not find significant differences in short-term learning gains over the reading activity. Our system architecture and user study provide various insights for designers of future CTSs. We further open-source our system to support ongoing research on effective instructional design of LLM-based learning technologies.",
    "venue": "International Conference on Artificial Intelligence in Education",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference",
      "Review"
    ],
    "publicationDate": "2024-04-26",
    "authors": [
      {
        "authorId": "2238003671",
        "name": "Robin Schmucker"
      },
      {
        "authorId": "2253471421",
        "name": "Meng Xia"
      },
      {
        "authorId": "1746466",
        "name": "A. Azaria"
      },
      {
        "authorId": "2253469788",
        "name": "Tom Mitchell"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "32bee919a779c601881e8257988e6dabe10383c1",
    "url": "https://www.semanticscholar.org/paper/32bee919a779c601881e8257988e6dabe10383c1",
    "title": "From News to Forecast: Integrating Event Analysis in LLM-Based Time Series Forecasting with Reflection",
    "abstract": "This paper introduces a novel approach that leverages Large Language Models (LLMs) and Generative Agents to enhance time series forecasting by reasoning across both text and time series data. With language as a medium, our method adaptively integrates social events into forecasting models, aligning news content with time series fluctuations to provide richer insights. Specifically, we utilize LLM-based agents to iteratively filter out irrelevant news and employ human-like reasoning to evaluate predictions. This enables the model to analyze complex events, such as unexpected incidents and shifts in social behavior, and continuously refine the selection logic of news and the robustness of the agent's output. By integrating selected news events with time series data, we fine-tune a pre-trained LLM to predict sequences of digits in time series. The results demonstrate significant improvements in forecasting accuracy, suggesting a potential paradigm shift in time series forecasting through the effective utilization of unstructured news data.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-26",
    "authors": [
      {
        "authorId": "2325204201",
        "name": "Xinlei Wang"
      },
      {
        "authorId": "2325154129",
        "name": "Maike Feng"
      },
      {
        "authorId": "2327174861",
        "name": "Jing Qiu"
      },
      {
        "authorId": "2325724708",
        "name": "Jinjin Gu"
      },
      {
        "authorId": "2145804583",
        "name": "Junhua Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "16b459de55727171aff6ea674535bea499e58261",
    "url": "https://www.semanticscholar.org/paper/16b459de55727171aff6ea674535bea499e58261",
    "title": "Simple is Effective: The Roles of Graphs and Large Language Models in Knowledge-Graph-Based Retrieval-Augmented Generation",
    "abstract": "Large Language Models (LLMs) demonstrate strong reasoning abilities but face limitations such as hallucinations and outdated knowledge. Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by grounding LLM outputs in structured external knowledge from KGs. However, current KG-based RAG frameworks still struggle to optimize the trade-off between retrieval effectiveness and efficiency in identifying a suitable amount of relevant graph information for the LLM to digest. We introduce SubgraphRAG, extending the KG-based RAG framework that retrieves subgraphs and leverages LLMs for reasoning and answer prediction. Our approach innovatively integrates a lightweight multilayer perceptron with a parallel triple-scoring mechanism for efficient and flexible subgraph retrieval while encoding directional structural distances to enhance retrieval effectiveness. The size of retrieved subgraphs can be flexibly adjusted to match the query's need and the downstream LLM's capabilities. This design strikes a balance between model complexity and reasoning power, enabling scalable and generalizable retrieval processes. Notably, based on our retrieved subgraphs, smaller LLMs like Llama3.1-8B-Instruct deliver competitive results with explainable reasoning, while larger models like GPT-4o achieve state-of-the-art accuracy compared with previous baselines -- all without fine-tuning. Extensive evaluations on the WebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency, accuracy, and reliability by reducing hallucinations and improving response grounding.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-28",
    "authors": [
      {
        "authorId": "2112144150",
        "name": "Mufei Li"
      },
      {
        "authorId": "2151793768",
        "name": "Siqi Miao"
      },
      {
        "authorId": "2328306177",
        "name": "Pan Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "5271e1c561b6c82aff1b4aaab712797d33910dc3",
    "url": "https://www.semanticscholar.org/paper/5271e1c561b6c82aff1b4aaab712797d33910dc3",
    "title": "LLMChain: Blockchain-Based Reputation System for Sharing and Evaluating Large Language Models",
    "abstract": "Large Language Models (LLMs) have witnessed a rapid growth in emerging challenges and capabilities of language understanding, generation, and reasoning. Despite their remarkable performance in natural language processing-based applications, LLMs are susceptible to undesirable and erratic behaviors, including hallucinations, unreliable reasoning, and the generation of harmful content. These flawed behaviors under-mine trust in LLMs and pose significant hurdles to their adoption in real-world applications, such as legal assistance and medical diagnosis, where precision, reliability, and ethical considerations are paramount. These could also lead to user dissatisfaction, which is currently inadequately assessed and captured. Therefore, to effectively and transparently assess users' satisfaction and trust in their interactions with LLMs, we design and develop LLMChain, a decentralized blockchain-based reputation system that combines automatic evaluation with human feedback to assign contextual reputation scores that accurately reflect LLM's behavior. LLMChain helps users and entities identify the most trustworthy LLM for their specific needs and provides LLM developers with valuable information to refine and improve their models. To our knowledge, this is the first time that a blockchain-based distributed framework for sharing and evaluating LLMs has been introduced. Implemented using emerging tools, LLMChain is evaluated across two benchmark datasets, showcasing its effectiveness and scalability in assessing seven different LLMs.",
    "venue": "Annual International Computer Software and Applications Conference",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2404.13236",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-20",
    "authors": [
      {
        "authorId": "2224976290",
        "name": "Mouhamed Amine Bouchiha"
      },
      {
        "authorId": "2267791844",
        "name": "Quentin Telnoff"
      },
      {
        "authorId": "1404347694",
        "name": "Souhail Bakkali"
      },
      {
        "authorId": "2799495",
        "name": "R. Champagnat"
      },
      {
        "authorId": "144250915",
        "name": "Mourad Rabah"
      },
      {
        "authorId": "90570839",
        "name": "Mickael Coustaty"
      },
      {
        "authorId": "1397038647",
        "name": "Y. Ghamri-Doudane"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "9dddbaecb7279a813db8b13c34498da0210e3ddc",
    "url": "https://www.semanticscholar.org/paper/9dddbaecb7279a813db8b13c34498da0210e3ddc",
    "title": "Hybrid LLM-DDQN based Joint Optimization of V2I Communication and Autonomous Driving",
    "abstract": "Large language models (LLMs) have received considerable interest recently due to their outstanding reasoning and comprehension capabilities. This work explores applying LLMs to vehicular networks, aiming to jointly optimize vehicle-to-infrastructure (V2I) communications and autonomous driving (AD) policies. We deploy LLMs for AD decision-making to maximize traffic flow and avoid collisions for road safety, and a double deep Q-learning algorithm (DDQN) is used for V2I optimization to maximize the received data rate and reduce frequent handovers. In particular, for LLM-enabled AD, we employ the Euclidean distance to identify previously explored AD experiences, and then LLMs can learn from past good and bad decisions for further improvement. Then, LLM-based AD decisions will become part of states in V2I problems, and DDQN will optimize the V2I decisions accordingly. After that, the AD and V2I decisions are iteratively optimized until convergence. Such an iterative optimization approach can better explore the interactions between LLMs and conventional reinforcement learning techniques, revealing the potential of using LLMs for network optimization and management. Finally, the simulations demonstrate that our proposed hybrid LLM-DDQN approach outperforms the conventional DDQN algorithm, showing faster convergence and higher average rewards.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-11",
    "authors": [
      {
        "authorId": "2180656487",
        "name": "Zijiang Yan"
      },
      {
        "authorId": "2302316320",
        "name": "Hao Zhou"
      },
      {
        "authorId": "2277601902",
        "name": "Hina Tabassum"
      },
      {
        "authorId": "2303713550",
        "name": "Xue Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "47aacaab789e80388d22598b4810213655e62888",
    "url": "https://www.semanticscholar.org/paper/47aacaab789e80388d22598b4810213655e62888",
    "title": "Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents",
    "abstract": "With the remarkable advancements of large language models (LLMs), LLM-based agents have become a research hotspot in human-computer interaction. However, there is a scarcity of benchmarks available for LLM-based mobile agents. Benchmarking these agents generally faces three main challenges: (1) The inefficiency of UI-only operations imposes limitations to task evaluation. (2) Specific instructions within a singular application lack adequacy for assessing the multi-dimensional reasoning and decision-making capacities of LLM mobile agents. (3) Current evaluation metrics are insufficient to accurately assess the process of sequential actions. To this end, we propose Mobile-Bench, a novel benchmark for evaluating the capabilities of LLM-based mobile agents. First, we expand conventional UI operations by incorporating 103 collected APIs to accelerate the efficiency of task completion. Subsequently, we collect evaluation data by combining real user queries with augmentation from LLMs. To better evaluate different levels of planning capabilities for mobile agents, our data is categorized into three distinct groups: SAST, SAMT, and MAMT, reflecting varying levels of task complexity. Mobile-Bench comprises 832 data entries, with more than 200 tasks specifically designed to evaluate multi-APP collaboration scenarios. Furthermore, we introduce a more accurate evaluation metric, named CheckPoint, to assess whether LLM-based mobile agents reach essential points during their planning and reasoning steps.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-01",
    "authors": [
      {
        "authorId": "2309690070",
        "name": "Shihan Deng"
      },
      {
        "authorId": "2262867071",
        "name": "Weikai Xu"
      },
      {
        "authorId": "2257127695",
        "name": "Hongda Sun"
      },
      {
        "authorId": "2257333016",
        "name": "Wei Liu"
      },
      {
        "authorId": "2309299004",
        "name": "Tao Tan"
      },
      {
        "authorId": "2309199381",
        "name": "Jianfeng Liu"
      },
      {
        "authorId": "2309681549",
        "name": "Ang Li"
      },
      {
        "authorId": "2257013742",
        "name": "Jian Luan"
      },
      {
        "authorId": "2257388949",
        "name": "Bin Wang"
      },
      {
        "authorId": "2257014132",
        "name": "Rui Yan"
      },
      {
        "authorId": "2232780079",
        "name": "Shuo Shang"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "b3d86acd1bae52b92b14e5253a3a5bb8a3a3f8da",
    "url": "https://www.semanticscholar.org/paper/b3d86acd1bae52b92b14e5253a3a5bb8a3a3f8da",
    "title": "A Solution-based LLM API-using Methodology for Academic Information Seeking",
    "abstract": "Applying large language models (LLMs) for academic API usage shows promise in reducing researchers' academic information seeking efforts. However, current LLM API-using methods struggle with complex API coupling commonly encountered in academic queries. To address this, we introduce SoAy, a solution-based LLM API-using methodology for academic information seeking. It uses code with a solution as the reasoning method, where a solution is a pre-constructed API calling sequence. The addition of the solution reduces the difficulty for the model to understand the complex relationships between APIs. Code improves the efficiency of reasoning. To evaluate SoAy, we introduce SoAyBench, an evaluation benchmark accompanied by SoAyEval, built upon a cloned environment of APIs from AMiner. Experimental results demonstrate a 34.58-75.99\\% performance improvement compared to state-of-the-art LLM API-based baselines. All datasets, codes, tuned models, and deployed online services are publicly accessible at https://github.com/RUCKBReasoning/SoAy.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-24",
    "authors": [
      {
        "authorId": "2303359307",
        "name": "Yuanchun Wang"
      },
      {
        "authorId": "2116034394",
        "name": "Jifan Yu"
      },
      {
        "authorId": "2273946831",
        "name": "Zijun Yao"
      },
      {
        "authorId": "2273814561",
        "name": "Jing Zhang"
      },
      {
        "authorId": "2303331850",
        "name": "Yuyang Xie"
      },
      {
        "authorId": "2116520118",
        "name": "Shangqing Tu"
      },
      {
        "authorId": "2303312077",
        "name": "Yiyang Fu"
      },
      {
        "authorId": "2303322860",
        "name": "Youhe Feng"
      },
      {
        "authorId": "2303325357",
        "name": "Jinkai Zhang"
      },
      {
        "authorId": "2303326120",
        "name": "Jingyao Zhang"
      },
      {
        "authorId": "2303429684",
        "name": "Bowen Huang"
      },
      {
        "authorId": "2303364479",
        "name": "Yuanyao Li"
      },
      {
        "authorId": "2287001632",
        "name": "Huihui Yuan"
      },
      {
        "authorId": "2284777109",
        "name": "Lei Hou"
      },
      {
        "authorId": "2273824956",
        "name": "Juan-Zi Li"
      },
      {
        "authorId": "2266873369",
        "name": "Jie Tang"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "044557354021be61a268c8f5b6da8fdf92270bc3",
    "url": "https://www.semanticscholar.org/paper/044557354021be61a268c8f5b6da8fdf92270bc3",
    "title": "Training Task Experts through Retrieval Based Distillation",
    "abstract": "One of the most reliable ways to create deployable models for specialized tasks is to obtain an adequate amount of high-quality task-specific data. However, for specialized tasks, often such datasets do not exist. Existing methods address this by creating such data from large language models (LLMs) and then distilling such knowledge into smaller models. However, these methods are limited by the quality of the LLMs output, and tend to generate repetitive or incorrect data. In this work, we present Retrieval Based Distillation (ReBase), a method that first retrieves data from rich online sources and then transforms them into domain-specific data. This method greatly enhances data diversity. Moreover, ReBase generates Chain-of-Thought reasoning and distills the reasoning capacity of LLMs. We test our method on 4 benchmarks and results show that our method significantly improves performance by up to 7.8% on SQuAD, 1.37% on MNLI, and 1.94% on BigBench-Hard.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-07",
    "authors": [
      {
        "authorId": "2214584825",
        "name": "Jiaxin Ge"
      },
      {
        "authorId": "2310863037",
        "name": "Xueying Jia"
      },
      {
        "authorId": "2061499362",
        "name": "Vijay Viswanathan"
      },
      {
        "authorId": "2310574886",
        "name": "Hongyin Luo"
      },
      {
        "authorId": "2265547593",
        "name": "Graham Neubig"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "4deeb8ea9fd60858bde847775170168e1ede883e",
    "url": "https://www.semanticscholar.org/paper/4deeb8ea9fd60858bde847775170168e1ede883e",
    "title": "keqing: knowledge-based question answering is a nature chain-of-thought mentor of LLM",
    "abstract": "Large language models (LLMs) have exhibited remarkable performance on various natural language processing (NLP) tasks, especially for question answering. However, in the face of problems beyond the scope of knowledge, these LLMs tend to talk nonsense with a straight face, where the potential solution could be incorporating an Information Retrieval (IR) module and generating response based on these retrieved knowledge. In this paper, we present a novel framework to assist LLMs, such as ChatGPT, to retrieve question-related structured information on the knowledge graph, and demonstrate that Knowledge-based question answering (Keqing) could be a nature Chain-of-Thought (CoT) mentor to guide the LLM to sequentially find the answer entities of a complex question through interpretable logical chains. Specifically, the workflow of Keqing will execute decomposing a complex question according to predefined templates, retrieving candidate entities on knowledge graph, reasoning answers of sub-questions, and finally generating response with reasoning paths, which greatly improves the reliability of LLM's response. The experimental results on KBQA datasets show that Keqing can achieve competitive performance and illustrate the logic of answering each question.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 17,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-31",
    "authors": [
      {
        "authorId": "2136111348",
        "name": "Chaojie Wang"
      },
      {
        "authorId": "2110598147",
        "name": "Yishi Xu"
      },
      {
        "authorId": "2277351921",
        "name": "Zhong Peng"
      },
      {
        "authorId": "2299302176",
        "name": "Chenxi Zhang"
      },
      {
        "authorId": "2277429390",
        "name": "Bo Chen"
      },
      {
        "authorId": "10737491",
        "name": "Xinrun Wang"
      },
      {
        "authorId": "2257385968",
        "name": "Lei Feng"
      },
      {
        "authorId": "2277251460",
        "name": "Bo An"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.35557636844247
  },
  {
    "paperId": "e82348d329bedb4545ac04d405cce07a2a4ede9b",
    "url": "https://www.semanticscholar.org/paper/e82348d329bedb4545ac04d405cce07a2a4ede9b",
    "title": "Enhancing Job Recommendation through LLM-based Generative Adversarial Networks",
    "abstract": "Recommending suitable jobs to users is a critical task in online recruitment platforms. While existing job recommendation methods encounter challenges such as the low quality of users' resumes, which hampers their accuracy and practical effectiveness.With the rapid development of large language models (LLMs), utilizing the rich external knowledge encapsulated within them, as well as their powerful reasoning capabilities, is a promising way to complete users' resumes for more accurate recommendations. However, directly leveraging LLMs to enhance recommendation results is not a one-size-fits-all solution, as LLMs may suffer from fabricated generation and few-shot problems, which degrade the quality of resume completion.\n\nIn this paper, we propose a novel LLM-based approach for job recommendation. To alleviate the limitation of fabricated generation for LLMs, we extract accurate and valuable information beyond users' self-description, which helps the LLMs better profile users for resume completion. Specifically, we not only extract users' explicit properties (e.g., skills, interests) from their self-description but also infer users' implicit characteristics from their behaviors for more accurate and meaningful resume completion. Nevertheless, some users still suffer from few-shot problems, which arise due to scarce interaction records, leading to limited guidance for high-quality resume generation. To address this issue, we propose aligning unpaired low-quality with high-quality generated resumes by Generative Adversarial Networks (GANs), which can refine the resume representations for better recommendation results. Extensive experiments on three large real-world recruitment datasets demonstrate the effectiveness of our proposed method.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2023,
    "citationCount": 21,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.10747",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-07-20",
    "authors": [
      {
        "authorId": "51123128",
        "name": "Yingpeng Du"
      },
      {
        "authorId": "2215612529",
        "name": "Di Luo"
      },
      {
        "authorId": "2055864893",
        "name": "Rui Yan"
      },
      {
        "authorId": "2109503105",
        "name": "Hongzhi Liu"
      },
      {
        "authorId": "144404428",
        "name": "Yang Song"
      },
      {
        "authorId": "1968806",
        "name": "Hengshu Zhu"
      },
      {
        "authorId": "2210549980",
        "name": "Jiehan Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 116.36563680037474
  },
  {
    "paperId": "6bf49d4ffd67f86b18d3decd4d513f52ec87a3b5",
    "url": "https://www.semanticscholar.org/paper/6bf49d4ffd67f86b18d3decd4d513f52ec87a3b5",
    "title": "CK12: A Rounded K12 Knowledge Graph Based Benchmark for Chinese Holistic Cognition Evaluation",
    "abstract": "New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present a meticulously designed evaluation benchmark that leverages the knowledge graph. This evaluation comprises 584 level-1 knowledge points and 1,989 level-2 knowledge points, thereby encompassing a comprehensive spectrum of the K12 education domain knowledge. The primary objective is to comprehensively assess the high-level comprehension aptitude and reasoning capabilities of LLMs operating within the Chinese context. Our evaluation incorporates five distinct question types with 39,452 questions. We test the current mainstream LLMs by three distinct modes. Firstly, four prompt evaluation modes were employed to assess the fundamental capacity. Additionally, for choice questions, a result-oriented evaluation approach was designed through data augmentation to assess the model's proficiency in advanced knowledge and reasoning. Moreover, a subset with reasoning process is derived, and the process-oriented testing method is used to test the model's interpretability and higher-order reasoning capacity. We further show models' capability in our knowledge points, and anticipate the evaluation can assist in the assessment of the strengths and deficiencies of LLMs on knowledge points, thus fostering their development within the Chinese context. Our Dataset will be publicly available in https://github.com/tal-tech/chinese-k12-evaluation.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "https://ojs.aaai.org/index.php/AAAI/article/download/29914/31598",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-24",
    "authors": [
      {
        "authorId": "2293459203",
        "name": "Weihao You"
      },
      {
        "authorId": "2293553536",
        "name": "Pengcheng Wang"
      },
      {
        "authorId": "2293659433",
        "name": "Changlong Li"
      },
      {
        "authorId": "15383651",
        "name": "Zhilong Ji"
      },
      {
        "authorId": "2113830377",
        "name": "Jinfeng Bai"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "aeea4c7b2bd963a1c80422bc09c41ba2ca0040d2",
    "url": "https://www.semanticscholar.org/paper/aeea4c7b2bd963a1c80422bc09c41ba2ca0040d2",
    "title": "Early Exploration of Using ChatGPT for Log-based Anomaly Detection on Parallel File Systems Logs",
    "abstract": "Log-based anomaly detection has been extensively studied to help detect complex runtime anomalies in production systems. However, existing techniques exhibit several common issues. First, they rely heavily on expert-labeled logs to discern anomalous behavior patterns. But labelling enough log data manually to effectively train deep neural networks may take too long. Second, they rely on numeric model prediction based on numeric vector input which causes model decisions to be largely non-interpretable by humans which further rules out targeted error correction. In recent years, we have witnessed groundbreaking advancements in large language models (LLMs) such as ChatGPT. These models have proven their ability to retain context and formulate insightful responses over entire conversations. They also present the ability to conduct few-shot and in-context learning with reasoning ability. In light of these abilities, it is only natural to explore their applicability in understanding log content and conducting anomaly classification among parallel file system logs.",
    "venue": "IEEE International Symposium on High-Performance Parallel Distributed Computing",
    "year": 2023,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2023-08-07",
    "authors": [
      {
        "authorId": "2192059264",
        "name": "Chris Egersdoerfer"
      },
      {
        "authorId": "2141082909",
        "name": "Di Zhang"
      },
      {
        "authorId": "2052160398",
        "name": "Dong Dai"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.49820016084324
  },
  {
    "paperId": "fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd",
    "url": "https://www.semanticscholar.org/paper/fa4ea8b773ffd6706ba5cf427f9671f81b04dcdd",
    "title": "ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning",
    "abstract": "From grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we refine the performance of the evaluator LLM, ultimately reducing reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data, such as medical summarization, education, and and productivity.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 14,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.13701",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-24",
    "authors": [
      {
        "authorId": "2245381886",
        "name": "Hosein Hasanbeig"
      },
      {
        "authorId": "20013278",
        "name": "Hiteshi Sharma"
      },
      {
        "authorId": "15449757",
        "name": "Leo Betthauser"
      },
      {
        "authorId": "1844283112",
        "name": "F. Frujeri"
      },
      {
        "authorId": "1990422",
        "name": "I. Momennejad"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "fdd95548b2a9183cff92aeb89fed18a5e4457d4e",
    "url": "https://www.semanticscholar.org/paper/fdd95548b2a9183cff92aeb89fed18a5e4457d4e",
    "title": "Intelligent Virtual Assistants with LLM-based Process Automation",
    "abstract": "While intelligent virtual assistants like Siri, Alexa, and Google Assistant have become ubiquitous in modern life, they still face limitations in their ability to follow multi-step instructions and accomplish complex goals articulated in natural language. However, recent breakthroughs in large language models (LLMs) show promise for overcoming existing barriers by enhancing natural language processing and reasoning capabilities. Though promising, applying LLMs to create more advanced virtual assistants still faces challenges like ensuring robust performance and handling variability in real-world user commands. This paper proposes a novel LLM-based virtual assistant that can automatically perform multi-step operations within mobile apps based on high-level user requests. The system represents an advance in assistants by providing an end-to-end solution for parsing instructions, reasoning about goals, and executing actions. LLM-based Process Automation (LLMPA) has modules for decomposing instructions, generating descriptions, detecting interface elements, predicting next actions, and error checking. Experiments demonstrate the system completing complex mobile operation tasks in Alipay based on natural language instructions. This showcases how large language models can enable automated assistants to accomplish real-world tasks. The main contributions are the novel LLMPA architecture optimized for app process automation, the methodology for applying LLMs to mobile apps, and demonstrations of multi-step task completion in a real-world environment. Notably, this work represents the first real-world deployment and extensive evaluation of a large language model-based virtual assistant in a widely used mobile application with an enormous user base numbering in the hundreds of millions.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-04",
    "authors": [
      {
        "authorId": "2228805404",
        "name": "Yanchu Guan"
      },
      {
        "authorId": "2274077975",
        "name": "Dong Wang"
      },
      {
        "authorId": "2237992280",
        "name": "Zhixuan Chu"
      },
      {
        "authorId": "2313672294",
        "name": "Shiyu Wang"
      },
      {
        "authorId": "2175653791",
        "name": "Feiyue Ni"
      },
      {
        "authorId": "2273660008",
        "name": "Ruihua Song"
      },
      {
        "authorId": "2238177474",
        "name": "Longfei Li"
      },
      {
        "authorId": "2266811973",
        "name": "Jinjie Gu"
      },
      {
        "authorId": "2240539453",
        "name": "Chenyi Zhuang"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "6182e5801cfb38904f6270ac912db5fb04daa6cc",
    "url": "https://www.semanticscholar.org/paper/6182e5801cfb38904f6270ac912db5fb04daa6cc",
    "title": "Automating the Generation of Prompts for LLM-based Action Choice in PDDL Planning",
    "abstract": "Large language models (LLMs) have revolutionized a large variety of NLP tasks. An active debate is to what extent they can do reasoning and planning. Prior work has assessed the latter in the specific context of PDDL planning, based on manually converting three PDDL domains into natural language (NL) prompts. Here we automate this conversion step, showing how to leverage an LLM to automatically generate NL prompts from PDDL input. Our automatically generated NL prompts result in similar LLM-planning performance as the previous manually generated ones. Beyond this, the automation enables us to run much larger experiments, providing for the first time a broad evaluation of LLM planning performance in PDDL.",
    "venue": "",
    "year": 2023,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-11-16",
    "authors": [
      {
        "authorId": "2265097424",
        "name": "Katharina Stein"
      },
      {
        "authorId": "2261362941",
        "name": "Alexander Koller"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "860bafe8a3aa0e3a981951f0757996272276b54e",
    "url": "https://www.semanticscholar.org/paper/860bafe8a3aa0e3a981951f0757996272276b54e",
    "title": "ALLURE: A Systematic Protocol for Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning",
    "abstract": "From grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we aim to refine the performance of the evaluator LLM, ultimately reducing the reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data and productivity in these fields.",
    "venue": "",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2245381886",
        "name": "Hosein Hasanbeig"
      },
      {
        "authorId": "2258705787",
        "name": "Microsoft Usa HITESHI SHARMA"
      },
      {
        "authorId": "2258705783",
        "name": "Microsoft Usa LEO BETTHAUSER"
      },
      {
        "authorId": "2258705381",
        "name": "Microsoft Usa FELIPE VIEIRA FRUJERI"
      },
      {
        "authorId": "2248289111",
        "name": "Ida Momennejad"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "89c3f28292c0f68960c6b1f3c8639768addc491a",
    "url": "https://www.semanticscholar.org/paper/89c3f28292c0f68960c6b1f3c8639768addc491a",
    "title": "Retrieval-based Video Language Model for Efficient Long Video Question Answering",
    "abstract": "The remarkable natural language understanding, reasoning, and generation capabilities of large language models (LLMs) have made them attractive for application to video question answering (Video QA) tasks, utilizing video tokens as contextual input. However, employing LLMs for long video understanding presents significant challenges and remains under-explored. The extensive number of video tokens leads to considerable computational costs for LLMs while using aggregated tokens results in loss of vision details. Moreover, the presence of abundant question-irrelevant tokens introduces noise to the video QA process. To address these issues, we introduce a simple yet effective retrieval-based video language model (R-VLM) for efficient and interpretable long video QA. Specifically, given a question (query) and a long video, our model identifies and selects the most relevant $K$ video chunks and uses their associated visual tokens to serve as context for the LLM inference. This effectively reduces the number of video tokens, eliminates noise interference, and enhances system performance. Our experimental results validate the effectiveness of our framework for comprehending long videos. Furthermore, based on the retrieved chunks, our model is interpretable that provides the justifications on where we get the answers.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-08",
    "authors": [
      {
        "authorId": "2272945123",
        "name": "Jiaqi Xu"
      },
      {
        "authorId": "2272888106",
        "name": "Cuiling Lan"
      },
      {
        "authorId": "2257012595",
        "name": "Wenxuan Xie"
      },
      {
        "authorId": "2272450620",
        "name": "Xuejin Chen"
      },
      {
        "authorId": "2198169719",
        "name": "Yan Lu"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "39a9beda08f4bf33d800fa6bf2e9fdde12d0a118",
    "url": "https://www.semanticscholar.org/paper/39a9beda08f4bf33d800fa6bf2e9fdde12d0a118",
    "title": "MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering",
    "abstract": "Recent advances in few-shot question answering (QA) mostly rely on the power of pre-trained large language models (LLMs) and fine-tuning in specific settings. Although the pre-training stage has already equipped LLMs with powerful reasoning capabilities, LLMs still need to be fine-tuned to adapt to specific domains to achieve the best results. In this paper, we propose to select the most informative data for fine-tuning, thereby improving the efficiency of the fine-tuning process with comparative or even better accuracy on the open-domain QA task. We present MinPrompt, a minimal data augmentation framework for open-domain QA based on an approximate graph algorithm and unsupervised question generation. We transform the raw text into a graph structure to build connections between different factual sentences, then apply graph algorithms to identify the minimal set of sentences needed to cover the most information in the raw text. We then generate QA pairs based on the identified sentence subset and train the model on the selected sentences to obtain the final model. Empirical results on several benchmark datasets and theoretical analysis show that MinPrompt is able to achieve comparable or better results than baselines with a high degree of efficiency, bringing consistent improvements in F-1 scores.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.05007",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-08",
    "authors": [
      {
        "authorId": "29963551",
        "name": "Xiusi Chen"
      },
      {
        "authorId": "3165179",
        "name": "Jyun-Yu Jiang"
      },
      {
        "authorId": "1702500",
        "name": "Wei-Cheng Chang"
      },
      {
        "authorId": "2256992918",
        "name": "Cho-Jui Hsieh"
      },
      {
        "authorId": "2257105171",
        "name": "Hsiang-Fu Yu"
      },
      {
        "authorId": "2283212563",
        "name": "Wei Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "769a924d0af014acec326f50c15c5d70d258a969",
    "url": "https://www.semanticscholar.org/paper/769a924d0af014acec326f50c15c5d70d258a969",
    "title": "LLMGA: Multimodal Large Language Model based Generation Assistant",
    "abstract": "In this paper, we introduce a Multimodal Large Language Model-based Generation Assistant (LLMGA), leveraging the vast reservoir of knowledge and proficiency in reasoning, comprehension, and response inherent in Large Language Models (LLMs) to assist users in image generation and editing. Diverging from existing approaches where Multimodal Large Language Models (MLLMs) generate fixed-size embeddings to control Stable Diffusion (SD), our LLMGA provides a detailed language generation prompt for precise control over SD. This not only augments LLM context understanding but also reduces noise in generation prompts, yields images with more intricate and precise content, and elevates the interpretability of the network. To this end, we curate a comprehensive dataset comprising prompt refinement, similar image generation, inpainting \\&outpainting, and instruction-based editing. Moreover, we propose a two-stage training scheme. In the first stage, we train the MLLM to grasp the properties of image generation and editing, enabling it to generate detailed prompts. In the second stage, we optimize SD to align with the MLLM's generation prompts. Additionally, we propose a reference-based restoration network to alleviate texture, brightness, and contrast disparities between generated and preserved regions during inpainting and outpainting. Extensive results show that LLMGA has promising generation and editing capabilities and can enable more flexible and expansive applications in an interactive manner.",
    "venue": "European Conference on Computer Vision",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-27",
    "authors": [
      {
        "authorId": "2268492513",
        "name": "Bin Xia"
      },
      {
        "authorId": "2268798428",
        "name": "Shiyin Wang"
      },
      {
        "authorId": "2269126763",
        "name": "Yingfan Tao"
      },
      {
        "authorId": "2268627199",
        "name": "Yitong Wang"
      },
      {
        "authorId": "2268680608",
        "name": "Jiaya Jia"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "6e50162ff04f315e49e722a61f24f9a9c8ced67d",
    "url": "https://www.semanticscholar.org/paper/6e50162ff04f315e49e722a61f24f9a9c8ced67d",
    "title": "Towards Responsible Generative AI: A Reference Architecture for Designing Foundation Model Based Agents",
    "abstract": "Foundation models, such as large language models (LLMs), have been widely recognised as transformative AI technologies due to their capabilities to understand and generate content, including plans with reasoning capabilities. Foundation model based agents derive their autonomy from the capabilities of foundation models, which enable them to autonomously break down a given goal into a set of manageable tasks and orchestrate task execution to meet the goal. Despite the huge efforts put into building foundation model based agents, the architecture design of the agents has not yet been systematically explored. Also, while there are significant benefits of using agents for planning and execution, there are serious considerations regarding responsible AI related software quality attributes, such as security and accountability. Therefore, this paper presents a pattern-oriented reference architecture that serves as guidance when designing foundation model based agents. We evaluate the completeness and utility of the proposed reference architecture by mapping it to the architecture of two real-world agents.",
    "venue": "2024 IEEE 21st International Conference on Software Architecture Companion (ICSA-C)",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2311.13148",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-22",
    "authors": [
      {
        "authorId": "2151674574",
        "name": "Qinghua Lu"
      },
      {
        "authorId": "2125737414",
        "name": "Liming Zhu"
      },
      {
        "authorId": "2266422366",
        "name": "Xiwei Xu"
      },
      {
        "authorId": "2247838339",
        "name": "Zhenchang Xing"
      },
      {
        "authorId": "2267725392",
        "name": "Stefan Harrer"
      },
      {
        "authorId": "2158859108",
        "name": "Jon Whittle"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.95836866004329
  },
  {
    "paperId": "a20b3a32fc4b7d46ee4dfb30c1217d5e87d2dd63",
    "url": "https://www.semanticscholar.org/paper/a20b3a32fc4b7d46ee4dfb30c1217d5e87d2dd63",
    "title": "TableQAKit: A Comprehensive and Practical Toolkit for Table-based Question Answering",
    "abstract": "Table-based question answering (TableQA) is an important task in natural language processing, which requires comprehending tables and employing various reasoning ways to answer the questions. This paper introduces TableQAKit, the first comprehensive toolkit designed specifically for TableQA. The toolkit designs a unified platform that includes plentiful TableQA datasets and integrates popular methods of this task as well as large language models (LLMs). Users can add their datasets and methods according to the friendly interface. Also, pleasantly surprised using the modules in this toolkit achieves new SOTA on some datasets. Finally, \\tableqakit{} also provides an LLM-based TableQA Benchmark for evaluating the role of LLMs in TableQA. TableQAKit is open-source with an interactive interface that includes visual operations, and comprehensive data for ease of use.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-23",
    "authors": [
      {
        "authorId": "2257004176",
        "name": "Fangyu Lei"
      },
      {
        "authorId": "2280137715",
        "name": "Tongxu Luo"
      },
      {
        "authorId": "2264261088",
        "name": "Pengqi Yang"
      },
      {
        "authorId": "2290973054",
        "name": "Weihao Liu"
      },
      {
        "authorId": "2262577094",
        "name": "Hanwen Liu"
      },
      {
        "authorId": "2238951064",
        "name": "Jiahe Lei"
      },
      {
        "authorId": "2261394934",
        "name": "Yiming Huang"
      },
      {
        "authorId": "2216484616",
        "name": "Yifan Wei"
      },
      {
        "authorId": "1954845",
        "name": "Shizhu He"
      },
      {
        "authorId": "2257315249",
        "name": "Jun Zhao"
      },
      {
        "authorId": "77397868",
        "name": "Kang Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787",
    "url": "https://www.semanticscholar.org/paper/a0a79dad89857a96f8f71b14238e5237cbfc4787",
    "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
    "abstract": "Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 2775,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-09",
    "authors": [
      {
        "authorId": "2149970173",
        "name": "Lianmin Zheng"
      },
      {
        "authorId": "2537924",
        "name": "Wei-Lin Chiang"
      },
      {
        "authorId": "2209360681",
        "name": "Ying Sheng"
      },
      {
        "authorId": "92721493",
        "name": "Siyuan Zhuang"
      },
      {
        "authorId": "1390573666",
        "name": "Zhanghao Wu"
      },
      {
        "authorId": "2152482391",
        "name": "Yonghao Zhuang"
      },
      {
        "authorId": "143872641",
        "name": "Zi Lin"
      },
      {
        "authorId": "2141335450",
        "name": "Zhuohan Li"
      },
      {
        "authorId": "2117961435",
        "name": "Dacheng Li"
      },
      {
        "authorId": "143977260",
        "name": "E. Xing"
      },
      {
        "authorId": "145140331",
        "name": "Haotong Zhang"
      },
      {
        "authorId": "49988044",
        "name": "Joseph E. Gonzalez"
      },
      {
        "authorId": "2055174324",
        "name": "Ion Stoica"
      }
    ],
    "source": "semantic_scholar",
    "score": 195.93149482440043
  },
  {
    "paperId": "d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43",
    "url": "https://www.semanticscholar.org/paper/d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43",
    "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face",
    "abstract": "Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 713,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2303.17580",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1471660296",
        "name": "Yongliang Shen"
      },
      {
        "authorId": "50982078",
        "name": "Kaitao Song"
      },
      {
        "authorId": "48391466",
        "name": "Xu Tan"
      },
      {
        "authorId": "2330364101",
        "name": "Dongsheng Li"
      },
      {
        "authorId": "1776903",
        "name": "Weiming Lu"
      },
      {
        "authorId": "2056432541",
        "name": "Y. Zhuang"
      }
    ],
    "source": "semantic_scholar",
    "score": 175.56324443509376
  },
  {
    "paperId": "873a581320d928249609d3c07229d5af182a379c",
    "url": "https://www.semanticscholar.org/paper/873a581320d928249609d3c07229d5af182a379c",
    "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
    "abstract": "Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 603,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.emnlp-main.85.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-02-08",
    "authors": [
      {
        "authorId": "2084609980",
        "name": "Chengwei Qin"
      },
      {
        "authorId": "2085709",
        "name": "Aston Zhang"
      },
      {
        "authorId": "3322871",
        "name": "Zhuosheng Zhang"
      },
      {
        "authorId": "47739850",
        "name": "Jiaao Chen"
      },
      {
        "authorId": "19168196",
        "name": "Michihiro Yasunaga"
      },
      {
        "authorId": "2143919864",
        "name": "Diyi Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 166.05361296902223
  },
  {
    "paperId": "6c7ba2af4b3e472bd8a5717367b88dcdd4abbd31",
    "url": "https://www.semanticscholar.org/paper/6c7ba2af4b3e472bd8a5717367b88dcdd4abbd31",
    "title": "Evaluating the Feasibility of ChatGPT in Healthcare: An Analysis of Multiple Clinical and Research Scenarios",
    "abstract": null,
    "venue": "Journal of medical systems",
    "year": 2023,
    "citationCount": 629,
    "openAccessPdf": {
      "url": "https://link.springer.com/content/pdf/10.1007/s10916-023-01925-4.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Medicine",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-03-04",
    "authors": [
      {
        "authorId": "7547313",
        "name": "M. Cascella"
      },
      {
        "authorId": "4239573",
        "name": "J. Montomoli"
      },
      {
        "authorId": "8663537",
        "name": "Valentina Bellini"
      },
      {
        "authorId": "4155267",
        "name": "E. Bignami"
      }
    ],
    "source": "semantic_scholar",
    "score": 146.68579729078368
  },
  {
    "paperId": "6052486bc9144dc1730c12bf35323af3792a1fd0",
    "url": "https://www.semanticscholar.org/paper/6052486bc9144dc1730c12bf35323af3792a1fd0",
    "title": "Large language models encode clinical knowledge",
    "abstract": null,
    "venue": "Nature",
    "year": 2022,
    "citationCount": 1740,
    "openAccessPdf": {
      "url": "https://www.nature.com/articles/s41586-023-06291-2.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-12-26",
    "authors": [
      {
        "authorId": "122902793",
        "name": "K. Singhal"
      },
      {
        "authorId": "40151244",
        "name": "Shekoofeh Azizi"
      },
      {
        "authorId": "1661884341",
        "name": "T. Tu"
      },
      {
        "authorId": "145439417",
        "name": "S. Mahdavi"
      },
      {
        "authorId": "119640649",
        "name": "Jason Wei"
      },
      {
        "authorId": "3351938",
        "name": "Hyung Won Chung"
      },
      {
        "authorId": "1471909492",
        "name": "Nathan Scales"
      },
      {
        "authorId": "39132551",
        "name": "A. Tanwani"
      },
      {
        "authorId": "1396212229",
        "name": "H. Cole-Lewis"
      },
      {
        "authorId": "51231678",
        "name": "S. Pfohl"
      },
      {
        "authorId": "2067024894",
        "name": "P. Payne"
      },
      {
        "authorId": "6454443",
        "name": "Martin G. Seneviratne"
      },
      {
        "authorId": "2067332323",
        "name": "P. Gamble"
      },
      {
        "authorId": "144244303",
        "name": "C. Kelly"
      },
      {
        "authorId": "2198273063",
        "name": "Nathaneal Scharli"
      },
      {
        "authorId": "2841893",
        "name": "Aakanksha Chowdhery"
      },
      {
        "authorId": "40390773",
        "name": "P. A. Mansfield"
      },
      {
        "authorId": "2661025",
        "name": "B. A. Y. Arcas"
      },
      {
        "authorId": "47191829",
        "name": "D. Webster"
      },
      {
        "authorId": "2084098271",
        "name": "Greg S. Corrado"
      },
      {
        "authorId": "1745572",
        "name": "Yossi Matias"
      },
      {
        "authorId": "2111722061",
        "name": "K. Chou"
      },
      {
        "authorId": "2051681709",
        "name": "Juraj Gottweis"
      },
      {
        "authorId": "2213266",
        "name": "Nenad Tomašev"
      },
      {
        "authorId": "2118113314",
        "name": "Yun Liu"
      },
      {
        "authorId": "8638650",
        "name": "A. Rajkomar"
      },
      {
        "authorId": "3658427",
        "name": "J. Barral"
      },
      {
        "authorId": "52326632",
        "name": "Christopher Semturs"
      },
      {
        "authorId": "6413143",
        "name": "A. Karthikesalingam"
      },
      {
        "authorId": "144223091",
        "name": "Vivek Natarajan"
      }
    ],
    "source": "semantic_scholar",
    "score": 171.93322409652285
  },
  {
    "paperId": "95898b1f82cf7ad7d96fcc85b4def7f086325af5",
    "url": "https://www.semanticscholar.org/paper/95898b1f82cf7ad7d96fcc85b4def7f086325af5",
    "title": "LESS: Selecting Influential Data for Targeted Instruction Tuning",
    "abstract": "Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that training on a LESS-selected 5% of the data can often outperform training on the full dataset across diverse downstream tasks. Furthermore, the selected data is highly transferable: smaller models can be leveraged to select useful data for larger models and models from different families. Our qualitative analysis shows that our method goes beyond surface form cues to identify data that exemplifies the necessary reasoning skills for the intended downstream application.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 111,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-06",
    "authors": [
      {
        "authorId": "67284811",
        "name": "Mengzhou Xia"
      },
      {
        "authorId": "49288855",
        "name": "Sadhika Malladi"
      },
      {
        "authorId": "40895369",
        "name": "Suchin Gururangan"
      },
      {
        "authorId": "2283134097",
        "name": "Sanjeev Arora"
      },
      {
        "authorId": "50536468",
        "name": "Danqi Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.7774830694264
  },
  {
    "paperId": "31d2ccff82e313eb5c1620c44bb8322da4a38513",
    "url": "https://www.semanticscholar.org/paper/31d2ccff82e313eb5c1620c44bb8322da4a38513",
    "title": "A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications",
    "abstract": "Prompt engineering has emerged as an indispensable technique for extending the capabilities of large language models (LLMs) and vision-language models (VLMs). This approach leverages task-specific instructions, known as prompts, to enhance model efficacy without modifying the core model parameters. Rather than updating the model parameters, prompts allow seamless integration of pre-trained models into downstream tasks by eliciting desired model behaviors solely based on the given prompt. Prompts can be natural language instructions that provide context to guide the model or learned vector representations that activate relevant knowledge. This burgeoning field has enabled success across various applications, from question-answering to commonsense reasoning. However, there remains a lack of systematic organization and understanding of the diverse prompt engineering methods and techniques. This survey paper addresses the gap by providing a structured overview of recent advancements in prompt engineering, categorized by application area. For each prompting approach, we provide a summary detailing the prompting methodology, its applications, the models involved, and the datasets utilized. We also delve into the strengths and limitations of each approach and include a taxonomy diagram and table summarizing datasets, models, and critical points of each prompting technique. This systematic analysis enables a better understanding of this rapidly developing field and facilitates future research by illuminating open challenges and opportunities for prompt engineering.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 147,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-02-05",
    "authors": [
      {
        "authorId": "2244661284",
        "name": "Pranab Sahoo"
      },
      {
        "authorId": "2284033818",
        "name": "Ayush Kumar Singh"
      },
      {
        "authorId": "2244669276",
        "name": "Sriparna Saha"
      },
      {
        "authorId": "2212131028",
        "name": "Vinija Jain"
      },
      {
        "authorId": "144818157",
        "name": "S. Mondal"
      },
      {
        "authorId": "2275226689",
        "name": "Aman Chadha"
      }
    ],
    "source": "semantic_scholar",
    "score": 144.95818410646172
  },
  {
    "paperId": "f82f49c20c6acc69f884f05e3a9f1ceea91061ce",
    "url": "https://www.semanticscholar.org/paper/f82f49c20c6acc69f884f05e3a9f1ceea91061ce",
    "title": "Detecting hallucinations in large language models using semantic entropy",
    "abstract": null,
    "venue": "The Naturalist",
    "year": 2024,
    "citationCount": 100,
    "openAccessPdf": {
      "url": "https://www.nature.com/articles/s41586-024-07421-0.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Medicine",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-01",
    "authors": [
      {
        "authorId": "33859827",
        "name": "Sebastian Farquhar"
      },
      {
        "authorId": "2064853201",
        "name": "Jannik Kossen"
      },
      {
        "authorId": "39879848",
        "name": "Lorenz Kuhn"
      },
      {
        "authorId": "2303846295",
        "name": "Yarin Gal"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.2268077526189
  },
  {
    "paperId": "06d860a5bbb99a4eafdbbb2d5f6aa8dd5fd32cf4",
    "url": "https://www.semanticscholar.org/paper/06d860a5bbb99a4eafdbbb2d5f6aa8dd5fd32cf4",
    "title": "Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security",
    "abstract": "Since the advent of personal computing devices, intelligent personal assistants (IPAs) have been one of the key technologies that researchers and engineers have focused on, aiming to help users efficiently obtain information and execute tasks, and provide users with more intelligent, convenient, and rich interaction experiences. With the development of smartphones and IoT, computing and sensing devices have become ubiquitous, greatly expanding the boundaries of IPAs. However, due to the lack of capabilities such as user intent understanding, task planning, tool using, and personal data management etc., existing IPAs still have limited practicality and scalability. Recently, the emergence of foundation models, represented by large language models (LLMs), brings new opportunities for the development of IPAs. With the powerful semantic understanding and reasoning capabilities, LLM can enable intelligent agents to solve complex problems autonomously. In this paper, we focus on Personal LLM Agents, which are LLM-based agents that are deeply integrated with personal data and personal devices and used for personal assistance. We envision that Personal LLM Agents will become a major software paradigm for end-users in the upcoming era. To realize this vision, we take the first step to discuss several important questions about Personal LLM Agents, including their architecture, capability, efficiency and security. We start by summarizing the key components and design choices in the architecture of Personal LLM Agents, followed by an in-depth analysis of the opinions collected from domain experts. Next, we discuss several key challenges to achieve intelligent, efficient and secure Personal LLM Agents, followed by a comprehensive survey of representative solutions to address these challenges.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 95,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-01-10",
    "authors": [
      {
        "authorId": "2144416765",
        "name": "Yuanchun Li"
      },
      {
        "authorId": "2188861830",
        "name": "Hao Wen"
      },
      {
        "authorId": "2235256501",
        "name": "Weijun Wang"
      },
      {
        "authorId": "2303907684",
        "name": "Xiangyu Li"
      },
      {
        "authorId": "2220186156",
        "name": "Yizhen Yuan"
      },
      {
        "authorId": "2235520970",
        "name": "Guohong Liu"
      },
      {
        "authorId": "2306055502",
        "name": "Jiacheng Liu"
      },
      {
        "authorId": "2236181887",
        "name": "Wenxing Xu"
      },
      {
        "authorId": "2335737961",
        "name": "Xiang Wang"
      },
      {
        "authorId": "2279192869",
        "name": "Yi Sun"
      },
      {
        "authorId": "2279022002",
        "name": "Rui Kong"
      },
      {
        "authorId": "2279098701",
        "name": "Yile Wang"
      },
      {
        "authorId": "2279022354",
        "name": "Hanfei Geng"
      },
      {
        "authorId": "2257013742",
        "name": "Jian Luan"
      },
      {
        "authorId": "2279327977",
        "name": "Xuefeng Jin"
      },
      {
        "authorId": "2202520157",
        "name": "Zi-Liang Ye"
      },
      {
        "authorId": "2279022490",
        "name": "Guanjing Xiong"
      },
      {
        "authorId": "2279267483",
        "name": "Fan Zhang"
      },
      {
        "authorId": "2307049398",
        "name": "Xiang Li"
      },
      {
        "authorId": "2239403838",
        "name": "Mengwei Xu"
      },
      {
        "authorId": "2274446834",
        "name": "Zhijun Li"
      },
      {
        "authorId": "144326610",
        "name": "Peng Li"
      },
      {
        "authorId": "2268734755",
        "name": "Yang Liu"
      },
      {
        "authorId": "2220028284",
        "name": "Yaqiong Zhang"
      },
      {
        "authorId": "2276471113",
        "name": "Yunxin Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 138.46522287201753
  },
  {
    "paperId": "7d8905a1fd288068f12c8347caeabefd36d0dd6c",
    "url": "https://www.semanticscholar.org/paper/7d8905a1fd288068f12c8347caeabefd36d0dd6c",
    "title": "Gorilla: Large Language Model Connected with Massive APIs",
    "abstract": "Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 391,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-24",
    "authors": [
      {
        "authorId": "80887461",
        "name": "Shishir G. Patil"
      },
      {
        "authorId": "1993655237",
        "name": "Tianjun Zhang"
      },
      {
        "authorId": "2153692009",
        "name": "Xin Wang"
      },
      {
        "authorId": "49988044",
        "name": "Joseph E. Gonzalez"
      }
    ],
    "source": "semantic_scholar",
    "score": 159.56892759685695
  },
  {
    "paperId": "1b5db3170c195508ff24fee8eda0d4987e806f0b",
    "url": "https://www.semanticscholar.org/paper/1b5db3170c195508ff24fee8eda0d4987e806f0b",
    "title": "EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty",
    "abstract": "Autoregressive decoding makes the inference of Large Language Models (LLMs) time-consuming. In this paper, we reconsider speculative sampling and derive two key observations. Firstly, autoregression at the feature (second-to-top-layer) level is more straightforward than at the token level. Secondly, the inherent uncertainty in feature (second-to-top-layer) level autoregression constrains its performance. Based on these insights, we introduce EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency), a simple yet highly efficient speculative sampling framework. By incorporating a token sequence advanced by one time step, EAGLE effectively resolves the uncertainty, enabling precise second-to-top-layer feature prediction with minimal overhead. We conducted comprehensive evaluations of EAGLE, including all models from the Vicuna and LLaMA2-Chat series, the MoE model Mixtral 8x7B Instruct, and tasks in dialogue, code generation, mathematical reasoning, and instruction following. For LLaMA2-Chat 70B, EAGLE achieved a latency speedup ratio of 2.7x-3.5x, doubled throughput, while maintaining the distribution of the generated text.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 70,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-01-26",
    "authors": [
      {
        "authorId": "2192674200",
        "name": "Yuhui Li"
      },
      {
        "authorId": "2239197291",
        "name": "Fangyun Wei"
      },
      {
        "authorId": "2256776221",
        "name": "Chao Zhang"
      },
      {
        "authorId": "2281685288",
        "name": "Hongyang Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.94019815561973
  },
  {
    "paperId": "f05e84702562cb693dd68d3d1c88072519a7bd71",
    "url": "https://www.semanticscholar.org/paper/f05e84702562cb693dd68d3d1c88072519a7bd71",
    "title": "∞Bench: Extending Long Context Evaluation Beyond 100K Tokens",
    "abstract": "Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose $\\infty$Bench, the first LLM benchmark featuring an average data length surpassing 100K tokens. $\\infty$Bench comprises synthetic and realistic tasks spanning diverse domains, presented in both English and Chinese. The tasks in $\\infty$Bench are designed to require well understanding of long dependencies in contexts, and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. In our experiments, based on $\\infty$Bench, we evaluate the state-of-the-art proprietary and open-source LLMs tailored for processing long contexts. The results indicate that existing long context LLMs still require significant advancements to effectively process 100K+ context. We further present three intriguing analyses regarding the behavior of LLMs processing long context.",
    "venue": "Volume 1",
    "year": 2024,
    "citationCount": 80,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-21",
    "authors": [
      {
        "authorId": "2254576790",
        "name": "Xinrong Zhang"
      },
      {
        "authorId": "2109274417",
        "name": "Yingfa Chen"
      },
      {
        "authorId": "1576223501",
        "name": "Shengding Hu"
      },
      {
        "authorId": "2284948588",
        "name": "Zihang Xu"
      },
      {
        "authorId": "2284931475",
        "name": "Junhao Chen"
      },
      {
        "authorId": "2284863107",
        "name": "Moo Khai Hao"
      },
      {
        "authorId": "48506411",
        "name": "Xu Han"
      },
      {
        "authorId": "2284862784",
        "name": "Zhen Leng Thai"
      },
      {
        "authorId": "2267033597",
        "name": "Shuo Wang"
      },
      {
        "authorId": "2141313179",
        "name": "Zhiyuan Liu"
      },
      {
        "authorId": "2273551430",
        "name": "Maosong Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 135.91673732008658
  },
  {
    "paperId": "1cd8373490efc2d74c2796f4b2aa27c7d4415ec9",
    "url": "https://www.semanticscholar.org/paper/1cd8373490efc2d74c2796f4b2aa27c7d4415ec9",
    "title": "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models",
    "abstract": "Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Videos and code at https://voxposer.github.io",
    "venue": "Conference on Robot Learning",
    "year": 2023,
    "citationCount": 377,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.05973",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-12",
    "authors": [
      {
        "authorId": "2158105356",
        "name": "Wenlong Huang"
      },
      {
        "authorId": null,
        "name": "Chen Wang"
      },
      {
        "authorId": "2657185",
        "name": "Ruohan Zhang"
      },
      {
        "authorId": "3422021",
        "name": "Yunzhu Li"
      },
      {
        "authorId": "3045089",
        "name": "Jiajun Wu"
      },
      {
        "authorId": "48004138",
        "name": "Li Fei-Fei"
      }
    ],
    "source": "semantic_scholar",
    "score": 159.02341293429382
  },
  {
    "paperId": "e26888285436bc7998e5c95102a9beb60144be5e",
    "url": "https://www.semanticscholar.org/paper/e26888285436bc7998e5c95102a9beb60144be5e",
    "title": "Textbooks Are All You Need II: phi-1.5 technical report",
    "abstract": "We continue the investigation into the power of smaller Transformer-based language models as initiated by \\textbf{TinyStories} -- a 10 million parameter model that can produce coherent English -- and the follow-up work on \\textbf{phi-1}, a 1.3 billion parameter model with Python coding performance close to the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to generate ``textbook quality\"data as a way to enhance the learning process compared to traditional web data. We follow the ``Textbooks Are All You Need\"approach, focusing this time on common sense reasoning in natural language, and create a new 1.3 billion parameter model named \\textbf{phi-1.5}, with performance on natural language tasks comparable to models 5x larger, and surpassing most non-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic coding. More generally, \\textbf{phi-1.5} exhibits many of the traits of much larger LLMs, both good -- such as the ability to ``think step by step\"or perform some rudimentary in-context learning -- and bad, including hallucinations and the potential for toxic and biased generations -- encouragingly though, we are seeing improvement on that front thanks to the absence of web data. We open-source \\textbf{phi-1.5} to promote further research on these urgent topics.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 357,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.05463",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-11",
    "authors": [
      {
        "authorId": "152244300",
        "name": "Yuan-Fang Li"
      },
      {
        "authorId": "1815542",
        "name": "Sébastien Bubeck"
      },
      {
        "authorId": "2315830",
        "name": "Ronen Eldan"
      },
      {
        "authorId": "50672277",
        "name": "Allison Del Giorno"
      },
      {
        "authorId": "3317356",
        "name": "Suriya Gunasekar"
      },
      {
        "authorId": "2239163839",
        "name": "Yin Tat Lee"
      }
    ],
    "source": "semantic_scholar",
    "score": 158.2079947960105
  },
  {
    "paperId": "ebedc4d7a2356090904baba4104ef0832bc236df",
    "url": "https://www.semanticscholar.org/paper/ebedc4d7a2356090904baba4104ef0832bc236df",
    "title": "A survey on multimodal large language models",
    "abstract": "ABSTRACT Recently, the multimodal large language model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful large language models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of the MLLM, such as writing stories based on images and optical character recognition–free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even outperform GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First, we present the basic formulation of the MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages and scenarios. We continue with multimodal hallucination and extended techniques, including multimodal in-context learning, multimodal chain of thought and LLM-aided visual reasoning. To conclude the paper, we discuss existing challenges and point out promising research directions.",
    "venue": "National Science Review",
    "year": 2023,
    "citationCount": 395,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine",
      "Computer Science"
    ],
    "publicationTypes": [
      "Review",
      "JournalArticle"
    ],
    "publicationDate": "2023-06-23",
    "authors": [
      {
        "authorId": "2187312485",
        "name": "Shukang Yin"
      },
      {
        "authorId": "2258705773",
        "name": "Chaoyou Fu"
      },
      {
        "authorId": "2111005730",
        "name": "Sirui Zhao"
      },
      {
        "authorId": "2149141063",
        "name": "Ke Li"
      },
      {
        "authorId": "143900241",
        "name": "Xing Sun"
      },
      {
        "authorId": "50383766",
        "name": "Tong Xu"
      },
      {
        "authorId": "2173129111",
        "name": "Enhong Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 169.7212131688172
  },
  {
    "paperId": "f5df0667365764a970fc6abfa0a68b7d1d0ae413",
    "url": "https://www.semanticscholar.org/paper/f5df0667365764a970fc6abfa0a68b7d1d0ae413",
    "title": "Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models",
    "abstract": "Understanding the internal representations of large language models (LLMs) can help explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of questions about an LLM's computation. We show that many prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and multihop reasoning error correction.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 57,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-01-11",
    "authors": [
      {
        "authorId": "2270681948",
        "name": "Asma Ghandeharioun"
      },
      {
        "authorId": "27743758",
        "name": "Avi Caciularu"
      },
      {
        "authorId": "2279021731",
        "name": "Adam Pearce"
      },
      {
        "authorId": "2270074412",
        "name": "Lucas Dixon"
      },
      {
        "authorId": "22245981",
        "name": "Mor Geva"
      }
    ],
    "source": "semantic_scholar",
    "score": 130.90664515819628
  },
  {
    "paperId": "ccb1ccc4deacc4fb18000f0e1ce24329548963ae",
    "url": "https://www.semanticscholar.org/paper/ccb1ccc4deacc4fb18000f0e1ce24329548963ae",
    "title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents",
    "abstract": "We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose\"$\\underline{D}$escribe, $\\underline{E}$xplain, $\\underline{P}$lan and $\\underline{S}$elect\"($\\textbf{DEPS}$), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated $\\textit{plan}$ by integrating $\\textit{description}$ of the plan execution process and providing self-$\\textit{explanation}$ of feedback when encountering failures during the extended planning phases. Furthermore, it includes a goal $\\textit{selector}$, which is a trainable module that ranks parallel candidate sub-goals based on the estimated steps of completion, consequently refining the initial plan. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method's general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the $\\texttt{ObtainDiamond}$ grand challenge with our approach. The code is released at https://github.com/CraftJarvis/MC-Planner.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 266,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2302.01560",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-02-03",
    "authors": [
      {
        "authorId": "47196237",
        "name": "Zihao Wang"
      },
      {
        "authorId": "1993661033",
        "name": "Shaofei Cai"
      },
      {
        "authorId": "70097297",
        "name": "Anji Liu"
      },
      {
        "authorId": "121875989",
        "name": "Xiaojian Ma"
      },
      {
        "authorId": "2397352",
        "name": "Yitao Liang"
      }
    ],
    "source": "semantic_scholar",
    "score": 153.80872987600372
  },
  {
    "paperId": "e0f27336698c84709bd60b6b7f4ce588cbae66bf",
    "url": "https://www.semanticscholar.org/paper/e0f27336698c84709bd60b6b7f4ce588cbae66bf",
    "title": "StructGPT: A General Framework for Large Language Model to Reason over Structured Data",
    "abstract": "In this paper, we study how to improve the zero-shot reasoning ability of large language models~(LLMs) over structured data in a unified way. Inspired by the study on tool augmentation for LLMs, we develop an \\emph{Iterative Reading-then-Reasoning~(IRR)} approach for solving question answering tasks based on structured data, called \\textbf{StructGPT}. In our approach, we construct the specialized function to collect relevant evidence from structured data (\\ie \\emph{reading}), and let LLMs concentrate the reasoning task based on the collected information (\\ie \\emph{reasoning}). Specially, we propose an \\emph{invoking-linearization-generation} procedure to support LLMs in reasoning on the structured data with the help of the external interfaces. By iterating this procedures with provided interfaces, our approach can gradually approach the target answer to a given query. Extensive experiments conducted on three types of structured data demonstrate the effectiveness of our approach, which can significantly boost the performance of ChatGPT and achieve comparable performance against the full-data supervised-tuning baselines. Our codes and data are publicly available at~\\url{https://github.com/RUCAIBox/StructGPT}.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 203,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.09645",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-16",
    "authors": [
      {
        "authorId": "2118240359",
        "name": "Jinhao Jiang"
      },
      {
        "authorId": "1423651904",
        "name": "Kun Zhou"
      },
      {
        "authorId": "2198280871",
        "name": "Zican Dong"
      },
      {
        "authorId": "1657563745",
        "name": "Keming Ye"
      },
      {
        "authorId": "2542603",
        "name": "Wayne Xin Zhao"
      },
      {
        "authorId": "153693432",
        "name": "Ji-rong Wen"
      }
    ],
    "source": "semantic_scholar",
    "score": 149.77179990766325
  },
  {
    "paperId": "76f54657eb0893a0b203da57dcf0b4fffeebfc2c",
    "url": "https://www.semanticscholar.org/paper/76f54657eb0893a0b203da57dcf0b4fffeebfc2c",
    "title": "CoRRPUS: Code-based Structured Prompting for Neurosymbolic Story Understanding",
    "abstract": "Story generation and understanding -- as with all NLG/NLU tasks -- has seen a surge in neurosymbolic work. Researchers have recognized that, while large language models (LLMs) have tremendous utility, they can be augmented with symbolic means to be even better and to make up for any flaws that the neural networks might have. However, symbolic methods are extremely costly in terms of the amount of time and expertise needed to create them. In this work, we capitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use of symbolic methods for tracking the state of stories and aiding in story understanding. We show that our CoRRPUS system and abstracted prompting procedures can beat current state-of-the-art structured LLM techniques on pre-existing story understanding tasks (bAbI Task 2 and Re^3) with minimal hand engineering. We hope that this work can help highlight the importance of symbolic representations and specialized prompting for LLMs as these models require some guidance for performing reasoning tasks properly.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.findings-acl.832.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-12-21",
    "authors": [
      {
        "authorId": "2156155948",
        "name": "Yi Dong"
      },
      {
        "authorId": "145262322",
        "name": "Lara J. Martin"
      },
      {
        "authorId": "1763608",
        "name": "Chris Callison-Burch"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "ccd6f8b6544f112de632e49bfbe592a0a654537d",
    "url": "https://www.semanticscholar.org/paper/ccd6f8b6544f112de632e49bfbe592a0a654537d",
    "title": "DriveGPT4: Interpretable End-to-End Autonomous Driving Via Large Language Model",
    "abstract": "Multimodallarge language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non-textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end-to-end autonomous driving system based on LLMs. Capable of processing multi-frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end-to-end fashion. These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy. DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain-specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V.",
    "venue": "IEEE Robotics and Automation Letters",
    "year": 2023,
    "citationCount": 163,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2310.01412",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-02",
    "authors": [
      {
        "authorId": "2256935831",
        "name": "Zhenhua Xu"
      },
      {
        "authorId": "2254056238",
        "name": "Yujia Zhang"
      },
      {
        "authorId": "2247612880",
        "name": "Enze Xie"
      },
      {
        "authorId": "145737114",
        "name": "Zhen Zhao"
      },
      {
        "authorId": "2253711797",
        "name": "Yong Guo"
      },
      {
        "authorId": "2315785954",
        "name": "Kwan-Yee. K. Wong"
      },
      {
        "authorId": "2253395155",
        "name": "Zhenguo Li"
      },
      {
        "authorId": "2253834598",
        "name": "Hengshuang Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 152.49799641736297
  },
  {
    "paperId": "8296eef3797afd1515021ff568a694412c38101b",
    "url": "https://www.semanticscholar.org/paper/8296eef3797afd1515021ff568a694412c38101b",
    "title": "Large Language Models for Robotics: Opportunities, Challenges, and Perspectives",
    "abstract": "Large language models (LLMs) have undergone significant expansion and have been increasingly integrated across various domains. Notably, in the realm of robot task planning, LLMs harness their advanced reasoning and language comprehension capabilities to formulate precise and efficient action plans based on natural language instructions. However, for embodied tasks, where robots interact with complex environments, text-only LLMs often face challenges due to a lack of compatibility with robotic visual perception. This study provides a comprehensive overview of the emerging integration of LLMs and multimodal LLMs into various robotic tasks. Additionally, we propose a framework that utilizes multimodal GPT-4V to enhance embodied task planning through the combination of natural language instructions and robot visual perceptions. Our results, based on diverse datasets, indicate that GPT-4V effectively enhances robot performance in embodied tasks. This extensive survey and evaluation of LLMs and multimodal LLMs across a variety of robotic tasks enriches the understanding of LLM-centric embodied intelligence and provides forward-looking insights toward bridging the gap in Human-Robot-Environment interaction.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 43,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-01-09",
    "authors": [
      {
        "authorId": "2221022947",
        "name": "Jiaqi Wang"
      },
      {
        "authorId": "2263593041",
        "name": "Zihao Wu"
      },
      {
        "authorId": "2257102397",
        "name": "Yiwei Li"
      },
      {
        "authorId": "2273631049",
        "name": "Hanqi Jiang"
      },
      {
        "authorId": "2220096705",
        "name": "Peng Shu"
      },
      {
        "authorId": "2131108859",
        "name": "Enze Shi"
      },
      {
        "authorId": "2215809245",
        "name": "Huawen Hu"
      },
      {
        "authorId": "120688117",
        "name": "Chong-Yi Ma"
      },
      {
        "authorId": "2116426849",
        "name": "Yi-Hsueh Liu"
      },
      {
        "authorId": "2278782711",
        "name": "Xuhui Wang"
      },
      {
        "authorId": "2278797305",
        "name": "Yincheng Yao"
      },
      {
        "authorId": "2278801747",
        "name": "Xuan Liu"
      },
      {
        "authorId": "2276747984",
        "name": "Huaqin Zhao"
      },
      {
        "authorId": "2145977326",
        "name": "Zheng Liu"
      },
      {
        "authorId": "29944950",
        "name": "Haixing Dai"
      },
      {
        "authorId": "2111641126",
        "name": "Lin Zhao"
      },
      {
        "authorId": "144691205",
        "name": "Bao Ge"
      },
      {
        "authorId": "2250835054",
        "name": "Xiang Li"
      },
      {
        "authorId": "2254792886",
        "name": "Tianming Liu"
      },
      {
        "authorId": "2256586513",
        "name": "Shu Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 126.76284450877392
  },
  {
    "paperId": "d1500f1dbd62e26ef0753f31e845078f58479968",
    "url": "https://www.semanticscholar.org/paper/d1500f1dbd62e26ef0753f31e845078f58479968",
    "title": "Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners",
    "abstract": "Large language models (LLMs) exhibit a wide range of promising capabilities -- from step-by-step planning to commonsense reasoning -- that may provide utility for robots, but remain prone to confidently hallucinated predictions. In this work, we present KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed. KnowNo builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help in complex multi-step planning settings. Experiments across a variety of simulated and real robot setups that involve tasks with different modes of ambiguity (e.g., from spatial to numeric uncertainties, from human preferences to Winograd schemas) show that KnowNo performs favorably over modern baselines (which may involve ensembles or extensive prompt tuning) in terms of improving efficiency and autonomy, while providing formal assurances. KnowNo can be used with LLMs out of the box without model-finetuning, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models. Website: https://robot-help.github.io",
    "venue": "Conference on Robot Learning",
    "year": 2023,
    "citationCount": 176,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.01928",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-04",
    "authors": [
      {
        "authorId": "1861425270",
        "name": "Allen Z. Ren"
      },
      {
        "authorId": "19243019",
        "name": "Anushri Dixit"
      },
      {
        "authorId": "2221129178",
        "name": "Alexandra Bodrova"
      },
      {
        "authorId": "2109040371",
        "name": "Sumeet Singh"
      },
      {
        "authorId": "1985955",
        "name": "Stephen Tu"
      },
      {
        "authorId": "2161343011",
        "name": "Noah Brown"
      },
      {
        "authorId": "2153917744",
        "name": "Peng Xu"
      },
      {
        "authorId": "1753156",
        "name": "L. Takayama"
      },
      {
        "authorId": "144956443",
        "name": "F. Xia"
      },
      {
        "authorId": "31568090",
        "name": "Jacob Varley"
      },
      {
        "authorId": "74498275",
        "name": "Zhenjia Xu"
      },
      {
        "authorId": "1779671",
        "name": "Dorsa Sadigh"
      },
      {
        "authorId": "38591293",
        "name": "Andy Zeng"
      },
      {
        "authorId": "1780468",
        "name": "Anirudha Majumdar"
      }
    ],
    "source": "semantic_scholar",
    "score": 147.64224598860744
  },
  {
    "paperId": "d32ba88571141ed0ebe7aeefbaa4ccaf8cda7be3",
    "url": "https://www.semanticscholar.org/paper/d32ba88571141ed0ebe7aeefbaa4ccaf8cda7be3",
    "title": "Mathematical discoveries from program search with large language models",
    "abstract": null,
    "venue": "The Naturalist",
    "year": 2023,
    "citationCount": 202,
    "openAccessPdf": {
      "url": "https://www.nature.com/articles/s41586-023-06924-6_reference.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-14",
    "authors": [
      {
        "authorId": "1403031665",
        "name": "Bernardino Romera-Paredes"
      },
      {
        "authorId": "39393520",
        "name": "M. Barekatain"
      },
      {
        "authorId": "2274227521",
        "name": "Alexander Novikov"
      },
      {
        "authorId": "2265578914",
        "name": "Matej Balog"
      },
      {
        "authorId": "2275617322",
        "name": "M. P. Kumar"
      },
      {
        "authorId": "2274228249",
        "name": "Emilien Dupont"
      },
      {
        "authorId": "2274226620",
        "name": "Francisco J. R. Ruiz"
      },
      {
        "authorId": "39820624",
        "name": "J. Ellenberg"
      },
      {
        "authorId": "2274928828",
        "name": "Pengming Wang"
      },
      {
        "authorId": "2257153583",
        "name": "Omar Fawzi"
      },
      {
        "authorId": "143967473",
        "name": "Pushmeet Kohli"
      },
      {
        "authorId": "33054064",
        "name": "Alhussein Fawzi"
      },
      {
        "authorId": "2279896188",
        "name": "Josh Grochow"
      },
      {
        "authorId": "2279875053",
        "name": "Andrea Lodi"
      },
      {
        "authorId": "2139567894",
        "name": "Jean-Baptiste Mouret"
      },
      {
        "authorId": "2279874998",
        "name": "Talia Ringer"
      },
      {
        "authorId": "2279931189",
        "name": "Tao Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 129.69808968562683
  },
  {
    "paperId": "ee19d5c943f1ebcd1a9e52a7bf494a88255b8e04",
    "url": "https://www.semanticscholar.org/paper/ee19d5c943f1ebcd1a9e52a7bf494a88255b8e04",
    "title": "Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies",
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 173,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.03188",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-08-06",
    "authors": [
      {
        "authorId": "3470231",
        "name": "Liangming Pan"
      },
      {
        "authorId": "48227633",
        "name": "Michael Stephen Saxon"
      },
      {
        "authorId": "145738382",
        "name": "Wenda Xu"
      },
      {
        "authorId": "51130642",
        "name": "Deepak Nathani"
      },
      {
        "authorId": "2115553132",
        "name": "Xinyi Wang"
      },
      {
        "authorId": "1682479",
        "name": "William Yang Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 147.38582948821795
  },
  {
    "paperId": "df2beaae63e4d68ef8e762bcd4704c9f11f856d9",
    "url": "https://www.semanticscholar.org/paper/df2beaae63e4d68ef8e762bcd4704c9f11f856d9",
    "title": "Can Language Models Solve Graph Problems in Natural Language?",
    "abstract": "Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language models do demonstrate preliminary graph reasoning abilities, 2) the benefit of advanced prompting and in-context learning diminishes on more complex graph problems, while 3) LLMs are also (un)surprisingly brittle in the face of spurious correlations in graph and problem settings. We then propose Build-a-Graph Prompting and Algorithmic Prompting, two instruction-based approaches to enhance LLMs in solving natural language graph problems. Build-a-Graph and Algorithmic prompting improve the performance of LLMs on NLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to solve the most complicated graph reasoning tasks in our setup with language models remains an open research question. The NLGraph benchmark and evaluation code are available at https://github.com/Arthur-Heng/NLGraph.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 132,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.10037",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-17",
    "authors": [
      {
        "authorId": "2256778370",
        "name": "Heng Wang"
      },
      {
        "authorId": "2114887261",
        "name": "Shangbin Feng"
      },
      {
        "authorId": "3083253",
        "name": "Tianxing He"
      },
      {
        "authorId": "2093186816",
        "name": "Zhaoxuan Tan"
      },
      {
        "authorId": "40500540",
        "name": "Xiaochuang Han"
      },
      {
        "authorId": "2073587169",
        "name": "Yulia Tsvetkov"
      }
    ],
    "source": "semantic_scholar",
    "score": 150.3552369233263
  },
  {
    "paperId": "bda605928d6ebe4db906e69ab5d343df75918727",
    "url": "https://www.semanticscholar.org/paper/bda605928d6ebe4db906e69ab5d343df75918727",
    "title": "Large Language Model Guided Tree-of-Thought",
    "abstract": "In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub: \\url{https://github.com/jieyilong/tree-of-thought-puzzle-solver}.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 140,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.08291",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-15",
    "authors": [
      {
        "authorId": "2210091344",
        "name": "Jieyi Long"
      }
    ],
    "source": "semantic_scholar",
    "score": 144.23139835567252
  },
  {
    "paperId": "3707939a856655fcabf0acd5cba1a1009987b439",
    "url": "https://www.semanticscholar.org/paper/3707939a856655fcabf0acd5cba1a1009987b439",
    "title": "Generative Verifiers: Reward Modeling as Next-Token Prediction",
    "abstract": "Verifiers or reward models are often used to enhance the reasoning performance of large language models (LLMs). A common approach is the Best-of-N method, where N candidate solutions generated by the LLM are ranked by a verifier, and the best one is selected. While LLM-based verifiers are typically trained as discriminative classifiers to score solutions, they do not utilize the text generation capabilities of pretrained LLMs. To overcome this limitation, we instead propose training verifiers using the ubiquitous next-token prediction objective, jointly on verification and solution generation. Compared to standard verifiers, such generative verifiers (GenRM) can benefit from several advantages of LLMs: they integrate seamlessly with instruction tuning, enable chain-of-thought reasoning, and can utilize additional test-time compute via majority voting for better verification. We demonstrate that GenRM outperforms discriminative, DPO verifiers, and LLM-as-a-Judge, resulting in a 16-40% improvement in the number of problems solved with Best-of-N on algorithmic and math reasoning tasks. Furthermore, we find that training GenRM with synthetic verification rationales is sufficient to pick out subtle errors on math problems. Finally, we demonstrate that generative verifiers scale favorably with model size and inference-time compute.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 38,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-27",
    "authors": [
      {
        "authorId": "2318804131",
        "name": "Lunjun Zhang"
      },
      {
        "authorId": "2090537547",
        "name": "Arian Hosseini"
      },
      {
        "authorId": "2317010356",
        "name": "Hritik Bansal"
      },
      {
        "authorId": "2317010095",
        "name": "Mehran Kazemi"
      },
      {
        "authorId": "2317038858",
        "name": "Aviral Kumar"
      },
      {
        "authorId": "2317013277",
        "name": "Rishabh Agarwal"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.95342469194469
  },
  {
    "paperId": "e4be613cc875e61b8c1c6c60d958f1c20d12d6c0",
    "url": "https://www.semanticscholar.org/paper/e4be613cc875e61b8c1c6c60d958f1c20d12d6c0",
    "title": "Task and Motion Planning with Large Language Models for Object Rearrangement",
    "abstract": "Multi-object rearrangement is a crucial skill for service robots, and commonsense reasoning is frequently needed in this process. However, achieving commonsense arrangements requires knowledge about objects, which is hard to transfer to robots. Large language models (LLMs) are one potential source of this knowledge, but they do not naively capture information about plausible physical arrangements of the world. We propose LLM-GROP, which uses prompting to extract commonsense knowledge about semantically valid object configurations from an LLM and instantiates them with a task and motion planner in order to generalize to varying scene geometry. LLM-GROP allows us to go from natural-language commands to human-aligned object rearrangement in varied environments. Based on human evaluations, our approach achieves the highest rating while outperforming competitive baselines in terms of success rate while maintaining comparable cumulative action costs. Finally, we demonstrate a practical implementation of LLM-GROP on a mobile manipulator in real-world scenarios. Supplementary materials are available at: https://sites.google.com/view/llm-grop",
    "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
    "year": 2023,
    "citationCount": 146,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2303.06247",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-03-10",
    "authors": [
      {
        "authorId": "2110668031",
        "name": "Yan Ding"
      },
      {
        "authorId": "2682457",
        "name": "Xiaohan Zhang"
      },
      {
        "authorId": "1977464",
        "name": "Chris Paxton"
      },
      {
        "authorId": "40295359",
        "name": "Shiqi Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 150.85648880168105
  },
  {
    "paperId": "f79a228535c9372b5f835dd13a8f9a3b4250480a",
    "url": "https://www.semanticscholar.org/paper/f79a228535c9372b5f835dd13a8f9a3b4250480a",
    "title": "Assessing the Utility of ChatGPT Throughout the Entire Clinical Workflow: Development and Usability Study",
    "abstract": "Background Large language model (LLM)–based artificial intelligence chatbots direct the power of large training data sets toward successive, related tasks as opposed to single-ask tasks, for which artificial intelligence already achieves impressive performance. The capacity of LLMs to assist in the full scope of iterative clinical reasoning via successive prompting, in effect acting as artificial physicians, has not yet been evaluated. Objective This study aimed to evaluate ChatGPT’s capacity for ongoing clinical decision support via its performance on standardized clinical vignettes. Methods We inputted all 36 published clinical vignettes from the Merck Sharpe & Dohme (MSD) Clinical Manual into ChatGPT and compared its accuracy on differential diagnoses, diagnostic testing, final diagnosis, and management based on patient age, gender, and case acuity. Accuracy was measured by the proportion of correct responses to the questions posed within the clinical vignettes tested, as calculated by human scorers. We further conducted linear regression to assess the contributing factors toward ChatGPT’s performance on clinical tasks. Results ChatGPT achieved an overall accuracy of 71.7% (95% CI 69.3%-74.1%) across all 36 clinical vignettes. The LLM demonstrated the highest performance in making a final diagnosis with an accuracy of 76.9% (95% CI 67.8%-86.1%) and the lowest performance in generating an initial differential diagnosis with an accuracy of 60.3% (95% CI 54.2%-66.6%). Compared to answering questions about general medical knowledge, ChatGPT demonstrated inferior performance on differential diagnosis (β=–15.8%; P<.001) and clinical management (β=–7.4%; P=.02) question types. Conclusions ChatGPT achieves impressive accuracy in clinical decision-making, with increasing strength as it gains more clinical information at its disposal. In particular, ChatGPT demonstrates the greatest accuracy in tasks of final diagnosis as compared to initial diagnosis. Limitations include possible model hallucinations and the unclear composition of ChatGPT’s training data set.",
    "venue": "Journal of Medical Internet Research",
    "year": 2023,
    "citationCount": 124,
    "openAccessPdf": {
      "url": "https://www.jmir.org/2023/1/e48659/PDF",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-08-22",
    "authors": [
      {
        "authorId": "2210587830",
        "name": "Arya Rao"
      },
      {
        "authorId": "40342534",
        "name": "Michael Pang"
      },
      {
        "authorId": "2210777816",
        "name": "John Kim"
      },
      {
        "authorId": "72458178",
        "name": "M. Kamineni"
      },
      {
        "authorId": "2210471154",
        "name": "Winston Lie"
      },
      {
        "authorId": "2210476712",
        "name": "Anoop K Prasad"
      },
      {
        "authorId": "2516236",
        "name": "A. Landman"
      },
      {
        "authorId": "2232934354",
        "name": "Keith Dreyer"
      },
      {
        "authorId": "14920559",
        "name": "M. Succi"
      }
    ],
    "source": "semantic_scholar",
    "score": 142.42470605953451
  },
  {
    "paperId": "2fe4d4af4af051fe90a472efad66a2dc692e1c21",
    "url": "https://www.semanticscholar.org/paper/2fe4d4af4af051fe90a472efad66a2dc692e1c21",
    "title": "(A)I Am Not a Lawyer, But...: Engaging Legal Experts towards Responsible LLM Policies for Legal Advice",
    "abstract": "Large language models (LLMs) are increasingly capable of providing users with advice in a wide range of professional domains, including legal advice. However, relying on LLMs for legal queries raises concerns due to the significant expertise required and the potential real-world consequences of the advice. To explore when and why LLMs should or should not provide advice to users, we conducted workshops with 20 legal experts using methods inspired by case-based reasoning. The provided realistic queries (“cases”) allowed experts to examine granular, situation-specific concerns and overarching technical and legal constraints, producing a concrete set of contextual considerations for LLM developers. By synthesizing the factors that impacted LLM response appropriateness, we present a 4-dimension framework: (1) User attributes and behaviors, (2) Nature of queries, (3) AI capabilities, and (4) Social impacts. We share experts’ recommendations for LLM response strategies, which center around helping users identify ‘right questions to ask’ and relevant information rather than providing definitive legal judgments. Our findings reveal novel legal considerations, such as unauthorized practice of law, confidentiality, and liability for inaccurate advice, that have been overlooked in the literature. The case-based deliberation method enabled us to elicit fine-grained, practice-informed insights that surpass those from de-contextualized surveys or speculative principles. These findings underscore the applicability of our method for translating domain-specific professional knowledge and practices into policies that can guide LLM behavior in a more responsible direction.",
    "venue": "Conference on Fairness, Accountability and Transparency",
    "year": 2024,
    "citationCount": 35,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3630106.3659048",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-02-02",
    "authors": [
      {
        "authorId": "2267350119",
        "name": "Inyoung Cheong"
      },
      {
        "authorId": "2267351231",
        "name": "King Xia"
      },
      {
        "authorId": "2267338491",
        "name": "K. Feng"
      },
      {
        "authorId": "2257107303",
        "name": "Quan Ze Chen"
      },
      {
        "authorId": "144518215",
        "name": "Amy X. Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 123.75278407684165
  },
  {
    "paperId": "c946888e2f81b1db84ba4addf2a11e87f0568fe9",
    "url": "https://www.semanticscholar.org/paper/c946888e2f81b1db84ba4addf2a11e87f0568fe9",
    "title": "Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies",
    "abstract": "While large language models (LLMs) have shown remarkable effectiveness in various NLP tasks, they are still prone to issues such as hallucination, unfaithful reasoning, and toxicity. A promising approach to rectify these flaws is correcting LLMs with feedback, where the LLM itself is prompted or guided with feedback to fix problems in its own output. Techniques leveraging automated feedback—either produced by the LLM itself (self-correction) or some external system—are of particular interest as they make LLM-based solutions more practical and deployable with minimal human intervention. This paper provides an exhaustive review of the recent advances in correcting LLMs with automated feedback, categorizing them into training-time, generation-time, and post-hoc approaches. We also identify potential challenges and future directions in this emerging field.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 35,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-05-01",
    "authors": [
      {
        "authorId": "2256983134",
        "name": "Liangming Pan"
      },
      {
        "authorId": "48227633",
        "name": "Michael Stephen Saxon"
      },
      {
        "authorId": "145738382",
        "name": "Wenda Xu"
      },
      {
        "authorId": "51130642",
        "name": "Deepak Nathani"
      },
      {
        "authorId": "2237389696",
        "name": "Xinyi Wang"
      },
      {
        "authorId": "2257130314",
        "name": "W. Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 123.75278407684165
  },
  {
    "paperId": "19933dd9e03058e686ef412262eef7696cce3e8f",
    "url": "https://www.semanticscholar.org/paper/19933dd9e03058e686ef412262eef7696cce3e8f",
    "title": "LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving",
    "abstract": "Existing learning-based autonomous driving (AD) systems face challenges in comprehending high-level information, generalizing to rare events, and providing interpretability. To address these problems, this work employs Large Language Models (LLMs) as a decision-making component for complex AD scenarios that require human commonsense understanding. We devise cognitive pathways to enable comprehensive reasoning with LLMs, and develop algorithms for translating LLM decisions into actionable driving commands. Through this approach, LLM decisions are seamlessly integrated with low-level controllers by guided parameter matrix adaptation. Extensive experiments demonstrate that our proposed method not only consistently surpasses baseline approaches in single-vehicle tasks, but also helps handle complex driving behaviors even multi-vehicle coordination, thanks to the commonsense reasoning capabilities of LLMs. This paper presents an initial step toward leveraging LLMs as effective decision-makers for intricate AD scenarios in terms of safety, efficiency, generalizability, and interoperability. We aspire for it to serve as inspiration for future research in this field. Project page: https://sites.google.com/view/llm-mpc",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 121,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.03026",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-04",
    "authors": [
      {
        "authorId": "2253656556",
        "name": "Hao Sha"
      },
      {
        "authorId": "2253674656",
        "name": "Yao Mu"
      },
      {
        "authorId": "2212387204",
        "name": "Yuxuan Jiang"
      },
      {
        "authorId": "2254272547",
        "name": "Li Chen"
      },
      {
        "authorId": "1490695028",
        "name": "Chenfeng Xu"
      },
      {
        "authorId": "2253674868",
        "name": "Ping Luo"
      },
      {
        "authorId": "2153703665",
        "name": "S. Li"
      },
      {
        "authorId": "2245825283",
        "name": "Masayoshi Tomizuka"
      },
      {
        "authorId": "144267500",
        "name": "Wei Zhan"
      },
      {
        "authorId": "2253455336",
        "name": "Mingyu Ding"
      }
    ],
    "source": "semantic_scholar",
    "score": 142.06031567099885
  },
  {
    "paperId": "19c63eade265d8a47d160098d97194b3b83d3770",
    "url": "https://www.semanticscholar.org/paper/19c63eade265d8a47d160098d97194b3b83d3770",
    "title": "In-Context Impersonation Reveals Large Language Models' Strengths and Biases",
    "abstract": "In everyday conversations, humans can take on different roles and adapt their vocabulary to their chosen roles. We explore whether LLMs can take on, that is impersonate, different roles when they generate text in-context. We ask LLMs to assume different personas before solving vision and language tasks. We do this by prefixing the prompt with a persona that is associated either with a social identity or domain expertise. In a multi-armed bandit task, we find that LLMs pretending to be children of different ages recover human-like developmental stages of exploration. In a language-based reasoning task, we find that LLMs impersonating domain experts perform better than LLMs impersonating non-domain experts. Finally, we test whether LLMs' impersonations are complementary to visual information when describing different categories. We find that impersonation can improve performance: an LLM prompted to be a bird expert describes birds better than one prompted to be a car expert. However, impersonation can also uncover LLMs' biases: an LLM prompted to be a man describes cars better than one prompted to be a woman. These findings demonstrate that LLMs are capable of taking on diverse roles and that this in-context impersonation can be used to uncover their hidden strengths and biases.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 122,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.14930",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-24",
    "authors": [
      {
        "authorId": "151097297",
        "name": "Leonard Salewski"
      },
      {
        "authorId": "40894329",
        "name": "Stephan Alaniz"
      },
      {
        "authorId": "2237800855",
        "name": "Isabel Rio-Torto"
      },
      {
        "authorId": "49427184",
        "name": "Eric Schulz"
      },
      {
        "authorId": "2893664",
        "name": "Zeynep Akata"
      }
    ],
    "source": "semantic_scholar",
    "score": 149.18276533058625
  },
  {
    "paperId": "3fc3460c4554a28e489a0ea6ef067b79b7d301d9",
    "url": "https://www.semanticscholar.org/paper/3fc3460c4554a28e489a0ea6ef067b79b7d301d9",
    "title": "Active Prompting with Chain-of-Thought for Large Language Models",
    "abstract": "The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available at https://github.com/shizhediao/active-prompt.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 100,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2302.12246",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-02-23",
    "authors": [
      {
        "authorId": "50826757",
        "name": "Shizhe Diao"
      },
      {
        "authorId": "2108818768",
        "name": "Pengcheng Wang"
      },
      {
        "authorId": "2154487280",
        "name": "Yong Lin"
      },
      {
        "authorId": "2265743820",
        "name": "Xiang Liu"
      },
      {
        "authorId": "2146324423",
        "name": "Tong Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.2268077526189
  },
  {
    "paperId": "587352c3b95c90de6d37f061c8e117f42be0b575",
    "url": "https://www.semanticscholar.org/paper/587352c3b95c90de6d37f061c8e117f42be0b575",
    "title": "Building Cooperative Embodied Agents Modularly with Large Language Models",
    "abstract": "In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA-2 still underperform, we fine-tune a CoELA with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 118,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.02485",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-05",
    "authors": [
      {
        "authorId": "2118083343",
        "name": "Hongxin Zhang"
      },
      {
        "authorId": "2214830561",
        "name": "Weihua Du"
      },
      {
        "authorId": "2193050211",
        "name": "Jiaming Shan"
      },
      {
        "authorId": "2107604346",
        "name": "Qinhong Zhou"
      },
      {
        "authorId": "15394275",
        "name": "Yilun Du"
      },
      {
        "authorId": "1763295",
        "name": "J. Tenenbaum"
      },
      {
        "authorId": "1844358",
        "name": "Tianmin Shu"
      },
      {
        "authorId": "2056157586",
        "name": "Chuang Gan"
      }
    ],
    "source": "semantic_scholar",
    "score": 141.68685239667295
  },
  {
    "paperId": "9ec42d155e2014e86ab49adcf76fd40a41a867ea",
    "url": "https://www.semanticscholar.org/paper/9ec42d155e2014e86ab49adcf76fd40a41a867ea",
    "title": "Evaluating large language models on a highly-specialized topic, radiation oncology physics",
    "abstract": "Purpose We present the first study to investigate Large Language Models (LLMs) in answering radiation oncology physics questions. Because popular exams like AP Physics, LSAT, and GRE have large test-taker populations and ample test preparation resources in circulation, they may not allow for accurately assessing the true potential of LLMs. This paper proposes evaluating LLMs on a highly-specialized topic, radiation oncology physics, which may be more pertinent to scientific and medical communities in addition to being a valuable benchmark of LLMs. Methods We developed an exam consisting of 100 radiation oncology physics questions based on our expertise. Four LLMs, ChatGPT (GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against medical physicists and non-experts. The performance of ChatGPT (GPT-4) was further explored by being asked to explain first, then answer. The deductive reasoning capability of ChatGPT (GPT-4) was evaluated using a novel approach (substituting the correct answer with “None of the above choices is the correct answer.”). A majority vote analysis was used to approximate how well each group could score when working together. Results ChatGPT GPT-4 outperformed all other LLMs and medical physicists, on average, with improved accuracy when prompted to explain before answering. ChatGPT (GPT-3.5 and GPT-4) showed a high level of consistency in its answer choices across a number of trials, whether correct or incorrect, a characteristic that was not observed in the human test groups or Bard (LaMDA). In evaluating deductive reasoning ability, ChatGPT (GPT-4) demonstrated surprising accuracy, suggesting the potential presence of an emergent ability. Finally, although ChatGPT (GPT-4) performed well overall, its intrinsic properties did not allow for further improvement when scoring based on a majority vote across trials. In contrast, a team of medical physicists were able to greatly outperform ChatGPT (GPT-4) using a majority vote. Conclusion This study suggests a great potential for LLMs to work alongside radiation oncology experts as highly knowledgeable assistants.",
    "venue": "Frontiers in Oncology",
    "year": 2023,
    "citationCount": 109,
    "openAccessPdf": {
      "url": "https://www.frontiersin.org/articles/10.3389/fonc.2023.1219326/pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Physics",
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-04-01",
    "authors": [
      {
        "authorId": "152888058",
        "name": "J. Holmes"
      },
      {
        "authorId": "2145977326",
        "name": "Zheng Liu"
      },
      {
        "authorId": "2146645863",
        "name": "Lian-Cheng Zhang"
      },
      {
        "authorId": "2111069623",
        "name": "Yuzhen Ding"
      },
      {
        "authorId": "5503509",
        "name": "T. Sio"
      },
      {
        "authorId": "40300642",
        "name": "L. Mcgee"
      },
      {
        "authorId": "6361772",
        "name": "J. Ashman"
      },
      {
        "authorId": "2144438902",
        "name": "Xiang Li"
      },
      {
        "authorId": "2115345993",
        "name": "Tianming Liu"
      },
      {
        "authorId": "5250637",
        "name": "Jiajian Shen"
      },
      {
        "authorId": "46641573",
        "name": "W. Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.50720548688625
  },
  {
    "paperId": "3cbfe152220de84ecf8059fa50c47587a3134c86",
    "url": "https://www.semanticscholar.org/paper/3cbfe152220de84ecf8059fa50c47587a3134c86",
    "title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models",
    "abstract": "Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models (LLMs) with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu's capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets which highlights its potential to be deployed on practical autonomous driving systems. To the best of our knowledge, we are the first to leverage knowledge-driven capability in decision-making for autonomous vehicles. Through the proposed DiLu framework, LLM is strengthened to apply knowledge and to reason causally in the autonomous driving domain. Project page: https://pjlab-adg.github.io/DiLu/",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 104,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.16292",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-28",
    "authors": [
      {
        "authorId": "153109152",
        "name": "Licheng Wen"
      },
      {
        "authorId": "150967272",
        "name": "Daocheng Fu"
      },
      {
        "authorId": "2153898327",
        "name": "Xin Li"
      },
      {
        "authorId": "2239169372",
        "name": "Xinyu Cai"
      },
      {
        "authorId": "1901958",
        "name": "Tengyu Ma"
      },
      {
        "authorId": "26978261",
        "name": "Pinlong Cai"
      },
      {
        "authorId": "2197075911",
        "name": "Min Dou"
      },
      {
        "authorId": "119700639",
        "name": "Botian Shi"
      },
      {
        "authorId": "2220657993",
        "name": "Liang He"
      },
      {
        "authorId": "145858545",
        "name": "Y. Qiao"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.80940525236286
  },
  {
    "paperId": "6001dce1c8f63350263e013e0e6ff69816f0a9af",
    "url": "https://www.semanticscholar.org/paper/6001dce1c8f63350263e013e0e6ff69816f0a9af",
    "title": "Text Classification via Large Language Models",
    "abstract": "Despite the remarkable success of large-scale Language Models (LLMs) such as GPT-3, their performances still significantly underperform fine-tuned models in the task of text classification. This is due to (1) the lack of reasoning ability in addressing complex linguistic phenomena (e.g., intensification, contrast, irony etc); (2) limited number of tokens allowed in in-context learning. In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adopts a progressive reasoning strategy tailored to addressing the complex linguistic phenomena involved in text classification: CARP first prompts LLMs to find superficial clues (e.g., keywords, tones, semantic relations, references, etc), based on which a diagnostic reasoning process is induced for final decisions. To further address the limited-token issue, CARP uses a fine-tuned model on the supervised dataset for $k$NN demonstration search in the in-context learning, allowing the model to take the advantage of both LLM's generalization ability and the task-specific evidence provided by the full labeled dataset. Remarkably, CARP yields new SOTA performances on 4 out of 5 widely-used text-classification benchmarks, 97.39 (+1.24) on SST-2, 96.40 (+0.72) on AGNews, 98.78 (+0.25) on R8 and 96.95 (+0.6) on R52, and a performance comparable to SOTA on MR (92.39 v.s. 93.3). More importantly, we find that CARP delivers impressive abilities on low-resource and domain-adaptation setups. Specifically, using 16 examples per class, CARP achieves comparable performances to supervised models with 1,024 examples per class.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 108,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2305.08377",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-15",
    "authors": [
      {
        "authorId": "2109329406",
        "name": "Xiaofei Sun"
      },
      {
        "authorId": "2845020",
        "name": "Xiaoya Li"
      },
      {
        "authorId": "2172372802",
        "name": "Jiwei Li"
      },
      {
        "authorId": "144894837",
        "name": "Fei Wu"
      },
      {
        "authorId": "16042895",
        "name": "Shangwei Guo"
      },
      {
        "authorId": "2146331573",
        "name": "Tianwei Zhang"
      },
      {
        "authorId": "2107926840",
        "name": "Guoyin Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.37021823343716
  },
  {
    "paperId": "d0c48d3b80efb9c170e7100cb9d78d1e7f7710bf",
    "url": "https://www.semanticscholar.org/paper/d0c48d3b80efb9c170e7100cb9d78d1e7f7710bf",
    "title": "Leveraging Generative AI and Large Language Models: A Comprehensive Roadmap for Healthcare Integration",
    "abstract": "Generative artificial intelligence (AI) and large language models (LLMs), exemplified by ChatGPT, are promising for revolutionizing data and information management in healthcare and medicine. However, there is scant literature guiding their integration for non-AI professionals. This study conducts a scoping literature review to address the critical need for guidance on integrating generative AI and LLMs into healthcare and medical practices. It elucidates the distinct mechanisms underpinning these technologies, such as Reinforcement Learning from Human Feedback (RLFH), including few-shot learning and chain-of-thought reasoning, which differentiates them from traditional, rule-based AI systems. It requires an inclusive, collaborative co-design process that engages all pertinent stakeholders, including clinicians and consumers, to achieve these benefits. Although global research is examining both opportunities and challenges, including ethical and legal dimensions, LLMs offer promising advancements in healthcare by enhancing data management, information retrieval, and decision-making processes. Continued innovation in data acquisition, model fine-tuning, prompt strategy development, evaluation, and system implementation is imperative for realizing the full potential of these technologies. Organizations should proactively engage with these technologies to improve healthcare quality, safety, and efficiency, adhering to ethical and legal guidelines for responsible application.",
    "venue": "Healthcare",
    "year": 2023,
    "citationCount": 100,
    "openAccessPdf": {
      "url": "https://www.mdpi.com/2227-9032/11/20/2776/pdf?version=1697783532",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "Review",
      "JournalArticle"
    ],
    "publicationDate": "2023-10-01",
    "authors": [
      {
        "authorId": "16119240",
        "name": "Ping Yu"
      },
      {
        "authorId": "2260843110",
        "name": "Hua Xu"
      },
      {
        "authorId": "2260832222",
        "name": "Xia Hu"
      },
      {
        "authorId": "144551284",
        "name": "Chao Deng"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.2268077526189
  },
  {
    "paperId": "1358f90705b05cdb20ebe6799b02196205e7e9f0",
    "url": "https://www.semanticscholar.org/paper/1358f90705b05cdb20ebe6799b02196205e7e9f0",
    "title": "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data",
    "abstract": "Chain-of-thought (CoT) advances the reasoning abilities of large language models (LLMs) and achieves superior performance in complex reasoning tasks. However, most CoT studies rely on carefully designed human-annotated rational chains to prompt LLMs, posing challenges for real-world applications where labeled data is available without rational chains. This paper proposes a new strategy, Automate-CoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought), that can bypass human engineering of CoT by automatically augmenting rational chains from a small labeled dataset, and then pruning low-quality chains to construct a candidate pool of machine-generated rationale chains based on the labels. Finally, it selects the optimal combination of several rationale chains from the pool for CoT prompting by employing a variance-reduced policy gradient strategy to estimate the significance of each example. Automate-CoT enables a quick adaptation of the CoT technique to different tasks. Experimental results demonstrate the effectiveness of our method, where competitive results are achieved on arithmetic reasoning (+2.7%), commonsense reasoning (+3.4%), symbolic reasoning (+3.2%), and non-reasoning tasks (+2.5%). The code is available at https://github.com/SHUMKASHUN/Automate-CoT.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 110,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2302.12822",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-02-24",
    "authors": [
      {
        "authorId": "2121340452",
        "name": "Kashun Shum"
      },
      {
        "authorId": "50826757",
        "name": "Shizhe Diao"
      },
      {
        "authorId": "2146324423",
        "name": "Tong Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.64295301968502
  },
  {
    "paperId": "19ec24c4f923b3eda61f9aa8ea657bb2105abff6",
    "url": "https://www.semanticscholar.org/paper/19ec24c4f923b3eda61f9aa8ea657bb2105abff6",
    "title": "Potential of ChatGPT and GPT-4 for Data Mining of Free-Text CT Reports on Lung Cancer.",
    "abstract": "Background The latest large language models (LLMs) solve unseen problems via user-defined text prompts without the need for retraining, offering potentially more efficient information extraction from free-text medical records than manual annotation. Purpose To compare the performance of the LLMs ChatGPT and GPT-4 in data mining and labeling oncologic phenotypes from free-text CT reports on lung cancer by using user-defined prompts. Materials and Methods This retrospective study included patients who underwent lung cancer follow-up CT between September 2021 and March 2023. A subset of 25 reports was reserved for prompt engineering to instruct the LLMs in extracting lesion diameters, labeling metastatic disease, and assessing oncologic progression. This output was fed into a rule-based natural language processing pipeline to match ground truth annotations from four radiologists and derive performance metrics. The oncologic reasoning of LLMs was rated on a five-point Likert scale for factual correctness and accuracy. The occurrence of confabulations was recorded. Statistical analyses included Wilcoxon signed rank and McNemar tests. Results On 424 CT reports from 424 patients (mean age, 65 years ± 11 [SD]; 265 male), GPT-4 outperformed ChatGPT in extracting lesion parameters (98.6% vs 84.0%, P < .001), resulting in 96% correctly mined reports (vs 67% for ChatGPT, P < .001). GPT-4 achieved higher accuracy in identification of metastatic disease (98.1% [95% CI: 97.7, 98.5] vs 90.3% [95% CI: 89.4, 91.0]) and higher performance in generating correct labels for oncologic progression (F1 score, 0.96 [95% CI: 0.94, 0.98] vs 0.91 [95% CI: 0.89, 0.94]) (both P < .001). In oncologic reasoning, GPT-4 had higher Likert scale scores for factual correctness (4.3 vs 3.9) and accuracy (4.4 vs 3.3), with a lower rate of confabulation (1.7% vs 13.7%) than ChatGPT (all P < .001). Conclusion When using user-defined prompts, GPT-4 outperformed ChatGPT in extracting oncologic phenotypes from free-text CT reports on lung cancer and demonstrated better oncologic reasoning with fewer confabulations. © RSNA, 2023 Supplemental material is available for this article. See also the editorial by Hafezi-Nejad and Trivedi in this issue.",
    "venue": "Radiology",
    "year": 2023,
    "citationCount": 109,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-01",
    "authors": [
      {
        "authorId": "2053604827",
        "name": "M. Fink"
      },
      {
        "authorId": "37770208",
        "name": "A. Bischoff"
      },
      {
        "authorId": "46880996",
        "name": "C. Fink"
      },
      {
        "authorId": "2243298554",
        "name": "Martin Moll"
      },
      {
        "authorId": "1396670221",
        "name": "Jonas Kroschke"
      },
      {
        "authorId": "2243305906",
        "name": "Luca Dulz"
      },
      {
        "authorId": "2205691011",
        "name": "C. Heussel"
      },
      {
        "authorId": "2238415762",
        "name": "H. Kauczor"
      },
      {
        "authorId": "2789246",
        "name": "T. Weber"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.50720548688625
  },
  {
    "paperId": "5aa3b1009955ce2c8f896e0d5e94e06155ef1e43",
    "url": "https://www.semanticscholar.org/paper/5aa3b1009955ce2c8f896e0d5e94e06155ef1e43",
    "title": "LLMRec: Large Language Models with Graph Augmentation for Recommendation",
    "abstract": "The problem of data sparsity has long been a challenge in recommendation systems, and previous studies have attempted to address this issue by incorporating side information. However, this approach often introduces side effects such as noise, availability issues, and low data quality, which in turn hinder the accurate modeling of user preferences and adversely impact recommendation performance. In light of the recent advancements in large language models (LLMs), which possess extensive knowledge bases and strong reasoning capabilities, we propose a novel framework called LLMRec that enhances recommender systems by employing three simple yet effective LLM-based graph augmentation strategies. Our approach leverages the rich content available within online platforms (e.g., Netflix, MovieLens) to augment the interaction graph in three ways: (i) reinforcing user-item interaction egde, (ii) enhancing the understanding of item node attributes, and (iii) conducting user node profiling, intuitively from the natural language perspective. By employing these strategies, we address the challenges posed by sparse implicit feedback and low-quality side information in recommenders. Besides, to ensure the quality of the augmentation, we develop a denoised data robustification mechanism that includes techniques of noisy implicit feedback pruning and MAE-based feature enhancement that help refine the augmented data and improve its reliability. Furthermore, we provide theoretical analysis to support the effectiveness of LLMRec and clarify the benefits of our method in facilitating model optimization. Experimental results on benchmark datasets demonstrate the superiority of our LLM-based augmentation approach over state-of-the-art techniques. To ensure reproducibility, we have made our code and augmented data publicly available at: https://github.com/HKUDS/LLMRec.git.",
    "venue": "Web Search and Data Mining",
    "year": 2023,
    "citationCount": 109,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2311.00423",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2023-11-01",
    "authors": [
      {
        "authorId": "2261084776",
        "name": "Wei Wei"
      },
      {
        "authorId": "2163180478",
        "name": "Xubin Ren"
      },
      {
        "authorId": "2261087798",
        "name": "Jiabin Tang"
      },
      {
        "authorId": "2263735306",
        "name": "Qinyong Wang"
      },
      {
        "authorId": "2261149457",
        "name": "Lixin Su"
      },
      {
        "authorId": "2111102795",
        "name": "Suqi Cheng"
      },
      {
        "authorId": "2261393578",
        "name": "Junfeng Wang"
      },
      {
        "authorId": "2261086768",
        "name": "Dawei Yin"
      },
      {
        "authorId": "2261248317",
        "name": "Chao Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.50720548688625
  },
  {
    "paperId": "221a72a3631ebf8b555c27bc864338390611feb1",
    "url": "https://www.semanticscholar.org/paper/221a72a3631ebf8b555c27bc864338390611feb1",
    "title": "S3: Social-network Simulation System with Large Language Model-Empowered Agents",
    "abstract": "Social network simulation plays a crucial role in addressing various challenges within social science. It offers extensive applications such as state prediction, phenomena explanation, and policy-making support, among others. In this work, we harness the formidable human-like capabilities exhibited by large language models (LLMs) in sensing, reasoning, and behaving, and utilize these qualities to construct the S$^3$ system (short for $\\textbf{S}$ocial network $\\textbf{S}$imulation $\\textbf{S}$ystem). Adhering to the widely employed agent-based simulation paradigm, we employ prompt engineering and prompt tuning techniques to ensure that the agent's behavior closely emulates that of a genuine human within the social network. Specifically, we simulate three pivotal aspects: emotion, attitude, and interaction behaviors. By endowing the agent in the system with the ability to perceive the informational environment and emulate human actions, we observe the emergence of population-level phenomena, including the propagation of information, attitudes, and emotions. We conduct an evaluation encompassing two levels of simulation, employing real-world social network data. Encouragingly, the results demonstrate promising accuracy. This work represents an initial step in the realm of social network simulation empowered by LLM-based agents. We anticipate that our endeavors will serve as a source of inspiration for the development of simulation systems within, but not limited to, social science.",
    "venue": "Social Science Research Network",
    "year": 2023,
    "citationCount": 95,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.14984",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-27",
    "authors": [
      {
        "authorId": "49281242",
        "name": "Chen Gao"
      },
      {
        "authorId": "2156918252",
        "name": "Xiaochong Lan"
      },
      {
        "authorId": "2110327058",
        "name": "Zhi-jie Lu"
      },
      {
        "authorId": "2069541466",
        "name": "Jinzhu Mao"
      },
      {
        "authorId": "10241302",
        "name": "J. Piao"
      },
      {
        "authorId": "2255821",
        "name": "Huandong Wang"
      },
      {
        "authorId": "49953590",
        "name": "Depeng Jin"
      },
      {
        "authorId": "2154403926",
        "name": "Yong Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 148.46522287201753
  },
  {
    "paperId": "3b0792f6d7f6aa6aadd316e73943116afef2979b",
    "url": "https://www.semanticscholar.org/paper/3b0792f6d7f6aa6aadd316e73943116afef2979b",
    "title": "Med-HALT: Medical Domain Hallucination Test for Large Language Models",
    "abstract": "This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs’ problem-solving and information retrieval abilities. Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting transparency and reproducibility. Through this work, we aim to contribute to the development of safer and more reliable language models in healthcare. Our benchmark can be found at medhalt.github.io",
    "venue": "Conference on Computational Natural Language Learning",
    "year": 2023,
    "citationCount": 88,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.15343",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-28",
    "authors": [
      {
        "authorId": "2160540450",
        "name": "Logesh Kumar Umapathi"
      },
      {
        "authorId": "2057980220",
        "name": "Ankit Pal"
      },
      {
        "authorId": "51117577",
        "name": "Malaikannan Sankarasubbu"
      }
    ],
    "source": "semantic_scholar",
    "score": 137.3295455459821
  },
  {
    "paperId": "07955e96cbd778d0ae2a68f09d073b866dd84c2a",
    "url": "https://www.semanticscholar.org/paper/07955e96cbd778d0ae2a68f09d073b866dd84c2a",
    "title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks",
    "abstract": "Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired. We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even simpler solvable sub-tasks. When the complexity comes from the input length, we can recursively decompose the task into the same task but with smaller inputs. We also evaluate our approach on textual multi-step reasoning tasks: on long-context multi-hop QA task, we can more effectively teach the sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA, we can incorporate a symbolic information retrieval within our decomposition framework, leading to improved performance on both tasks. Datasets, Code and Prompts available at https://github.com/allenai/DecomP.",
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "citationCount": 332,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2210.02406",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-10-05",
    "authors": [
      {
        "authorId": "2236429",
        "name": "Tushar Khot"
      },
      {
        "authorId": "6365809",
        "name": "H. Trivedi"
      },
      {
        "authorId": "1580418311",
        "name": "Matthew Finlayson"
      },
      {
        "authorId": "46956602",
        "name": "Yao Fu"
      },
      {
        "authorId": "46666605",
        "name": "Kyle Richardson"
      },
      {
        "authorId": "48323507",
        "name": "Peter Clark"
      },
      {
        "authorId": "48229640",
        "name": "Ashish Sabharwal"
      }
    ],
    "source": "semantic_scholar",
    "score": 157.12213734970666
  },
  {
    "paperId": "d9823ffa34f865fb1d0adef95d64a0c352ae125f",
    "url": "https://www.semanticscholar.org/paper/d9823ffa34f865fb1d0adef95d64a0c352ae125f",
    "title": "REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction",
    "abstract": "The ability to detect and analyze failed executions automatically is crucial for an explainable and robust robotic system. Recently, Large Language Models (LLMs) have demonstrated strong reasoning abilities on textual inputs. To leverage the power of LLMs for robot failure explanation, we introduce REFLECT, a framework which queries LLM for failure reasoning based on a hierarchical summary of robot past experiences generated from multisensory observations. The failure explanation can further guide a language-based planner to correct the failure and complete the task. To systematically evaluate the framework, we create the RoboFail dataset with a variety of tasks and failure scenarios. We demonstrate that the LLM-based framework is able to generate informative failure explanations that assist successful correction planning.",
    "venue": "Conference on Robot Learning",
    "year": 2023,
    "citationCount": 86,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2306.15724",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-27",
    "authors": [
      {
        "authorId": "2176845464",
        "name": "Zeyi Liu"
      },
      {
        "authorId": "113548550",
        "name": "Arpit Bahety"
      },
      {
        "authorId": "3340170",
        "name": "Shuran Song"
      }
    ],
    "source": "semantic_scholar",
    "score": 136.98862177981874
  },
  {
    "paperId": "42016f91e5b1da63174d45acb96bc89b64aa124d",
    "url": "https://www.semanticscholar.org/paper/42016f91e5b1da63174d45acb96bc89b64aa124d",
    "title": "Knowledge Editing for Large Language Models: A Survey",
    "abstract": "\n Large Language Models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently,\n Knowledge-based Model Editing\n (KME), also known as\n Knowledge Editing\n or\n Model Editing\n , has attracted increasing attention, which aims to precisely modify the LLMs to incorporate specific knowledge, without negatively influencing other irrelevant knowledge. In this survey, we aim to provide a comprehensive and in-depth overview of recent advances in the field of KME. We first introduce a general formulation of KME to encompass different KME strategies. Afterward, we provide an innovative taxonomy of KME techniques based on how the new knowledge is introduced into pre-trained LLMs, and investigate existing KME strategies while analyzing key insights, advantages, and limitations of methods from each category. Moreover, representative metrics, datasets, and applications of KME are introduced accordingly. Finally, we provide an in-depth analysis regarding the practicality and remaining challenges of KME and suggest promising research directions for further advancement in this field.\n",
    "venue": "ACM Computing Surveys",
    "year": 2023,
    "citationCount": 89,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-10-24",
    "authors": [
      {
        "authorId": "2117075272",
        "name": "Song Wang"
      },
      {
        "authorId": "2261804201",
        "name": "Yaochen Zhu"
      },
      {
        "authorId": "2261678191",
        "name": "Haochen Liu"
      },
      {
        "authorId": "2262086932",
        "name": "Zaiyi Zheng"
      },
      {
        "authorId": "2127380428",
        "name": "Chen Chen"
      },
      {
        "authorId": "2261788139",
        "name": "Jundong Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 143.497145054954
  },
  {
    "paperId": "67239d6e9c2c5f8a6d19cb35154e5aa7eaa00f51",
    "url": "https://www.semanticscholar.org/paper/67239d6e9c2c5f8a6d19cb35154e5aa7eaa00f51",
    "title": "Large Language Models on Graphs: A Comprehensive Survey",
    "abstract": "Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-paired graphs. We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models. Furthermore, we discuss the real-world applications of such methods and summarize open-source codes and benchmark datasets. Finally, we conclude with potential future research directions in this fast-growing field.",
    "venue": "IEEE Transactions on Knowledge and Data Engineering",
    "year": 2023,
    "citationCount": 94,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2312.02783",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-12-05",
    "authors": [
      {
        "authorId": "2057050247",
        "name": "Bowen Jin"
      },
      {
        "authorId": "2146562364",
        "name": "Gang Liu"
      },
      {
        "authorId": "2118642562",
        "name": "Chi Han"
      },
      {
        "authorId": "2237095609",
        "name": "Meng Jiang"
      },
      {
        "authorId": "2271097936",
        "name": "Heng Ji"
      },
      {
        "authorId": "2259869648",
        "name": "Jiawei Han"
      }
    ],
    "source": "semantic_scholar",
    "score": 144.3081533740081
  },
  {
    "paperId": "4e5d86ea8eacd341d13123852d50d1ef62738744",
    "url": "https://www.semanticscholar.org/paper/4e5d86ea8eacd341d13123852d50d1ef62738744",
    "title": "LiveBench: A Challenging, Contamination-Free LLM Benchmark",
    "abstract": "Test set contamination, wherein test data from a benchmark ends up in a newer model's training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions. In this work, we introduce a new benchmark for LLMs designed to be immune to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. We release LiveBench, the first benchmark that (1) contains frequently-updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-free versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 110B in size. LiveBench is difficult, with top models achieving below 65% accuracy. We release all questions, code, and model answers. Questions will be added and updated on a monthly basis, and we will release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. We welcome community engagement and collaboration for expanding the benchmark tasks and models.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 25,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-27",
    "authors": [
      {
        "authorId": "2087034921",
        "name": "Colin White"
      },
      {
        "authorId": "2066631240",
        "name": "Samuel Dooley"
      },
      {
        "authorId": "2306973774",
        "name": "∗. ManleyRoberts"
      },
      {
        "authorId": "2284763248",
        "name": "Arka Pal"
      },
      {
        "authorId": "120557092",
        "name": "Ben Feuer"
      },
      {
        "authorId": "2307560471",
        "name": "Siddhartha Jain"
      },
      {
        "authorId": "1411405900",
        "name": "Ravid Shwartz-Ziv"
      },
      {
        "authorId": "2204647912",
        "name": "Neel Jain"
      },
      {
        "authorId": "2203810783",
        "name": "Khalid Saifullah"
      },
      {
        "authorId": "2060450397",
        "name": "Siddartha Naidu"
      },
      {
        "authorId": "2262216898",
        "name": "Chinmay Hegde"
      },
      {
        "authorId": "2265899558",
        "name": "Yann LeCun"
      },
      {
        "authorId": "2306981546",
        "name": "Tom Goldstein"
      },
      {
        "authorId": "2934259",
        "name": "W. Neiswanger"
      },
      {
        "authorId": "121592562",
        "name": "Micah Goldblum"
      },
      {
        "authorId": "2306978347",
        "name": "Abacus.AI"
      },
      {
        "authorId": "2306978343",
        "name": "Nyu"
      },
      {
        "authorId": "2276243439",
        "name": "Nvidia"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.87144807032223
  },
  {
    "paperId": "82460f7995f66f3f035f34ecbd2c82b024282529",
    "url": "https://www.semanticscholar.org/paper/82460f7995f66f3f035f34ecbd2c82b024282529",
    "title": "Make Your LLM Fully Utilize the Context",
    "abstract": "While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the lost-in-the-middle challenge. We hypothesize that it stems from insufficient explicit supervision during the long-context training, which fails to emphasize that any position in a long context can hold crucial information. Based on this intuition, our study presents information-intensive (IN2) training, a purely data-driven solution to overcome lost-in-the-middle. Specifically, IN2 training leverages a synthesized long-context question-answer dataset, where the answer requires (1) fine-grained information awareness on a short segment (~128 tokens) within a synthesized long context (4K-32K tokens), and (2) the integration and reasoning of information from two or more short segments. Through applying this information-intensive training on Mistral-7B, we present FILM-7B (FILl-in-the-Middle). To thoroughly assess the ability of FILM-7B for utilizing long contexts, we design three probing tasks that encompass various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval). The probing results demonstrate that FILM-7B can robustly retrieve information from different positions in its 32K context window. Beyond these probing tasks, FILM-7B significantly improves the performance on real-world long-context tasks (e.g., 23.5->26.9 F1 score on NarrativeQA), while maintaining a comparable performance on short-context tasks (e.g., 59.3->59.2 accuracy on MMLU). Github Link: https://github.com/microsoft/FILM.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 28,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-25",
    "authors": [
      {
        "authorId": "2119217081",
        "name": "Shengnan An"
      },
      {
        "authorId": "2264290103",
        "name": "Zexiong Ma"
      },
      {
        "authorId": "2277149729",
        "name": "Zeqi Lin"
      },
      {
        "authorId": "2263567225",
        "name": "Nanning Zheng"
      },
      {
        "authorId": "153249455",
        "name": "Jian-Guang Lou"
      }
    ],
    "source": "semantic_scholar",
    "score": 120.5094374497971
  },
  {
    "paperId": "1c66c41ff22adf66bb9b06d87d5a2ab7b8ee8de7",
    "url": "https://www.semanticscholar.org/paper/1c66c41ff22adf66bb9b06d87d5a2ab7b8ee8de7",
    "title": "Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities",
    "abstract": "Large language models (LLMs) have received considerable attention recently due to their outstanding comprehension and reasoning capabilities, leading to great progress in many fields. The advancement of LLM techniques also offers promising opportunities to automate many tasks in the telecommunication (telecom) field. After pre-training and fine-tuning, LLMs can perform diverse downstream tasks based on human instructions, paving the way to artificial general intelligence (AGI)-enabled 6G. Given the great potential of LLM technologies, this work aims to provide a comprehensive overview of LLM-enabled telecom networks. In particular, we first present LLM fundamentals, including model architecture, pre-training, fine-tuning, inference and utilization, model evaluation, and telecom deployment. Then, we introduce LLM-enabled key techniques and telecom applications in terms of generation, classification, optimization, and prediction problems. Specifically, the LLM-enabled generation applications include telecom domain knowledge, code, and network configuration generation. After that, the LLM-based classification applications involve network security, text, image, and traffic classification problems. Moreover, multiple LLM-enabled optimization techniques are introduced, such as automated reward function design for reinforcement learning and verbal reinforcement learning. Furthermore, for LLM-aided prediction problems, we discussed time-series prediction models and multi-modality prediction problems for telecom. Finally, we highlight the challenges and identify the future directions of LLM-enabled telecom networks.",
    "venue": "IEEE Communications Surveys &amp; Tutorials",
    "year": 2024,
    "citationCount": 26,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-05-17",
    "authors": [
      {
        "authorId": "2302316320",
        "name": "Hao Zhou"
      },
      {
        "authorId": "11577774",
        "name": "Chengming Hu"
      },
      {
        "authorId": "2283264350",
        "name": "Ye Yuan"
      },
      {
        "authorId": "2301404967",
        "name": "Yufei Cui"
      },
      {
        "authorId": "2302296620",
        "name": "Yili Jin"
      },
      {
        "authorId": "2243412535",
        "name": "Can Chen"
      },
      {
        "authorId": "107747459",
        "name": "Haolun Wu"
      },
      {
        "authorId": "2212193383",
        "name": "Dun Yuan"
      },
      {
        "authorId": "2302565436",
        "name": "Li Jiang"
      },
      {
        "authorId": "2302271437",
        "name": "Di Wu"
      },
      {
        "authorId": "2243903453",
        "name": "Xue Liu"
      },
      {
        "authorId": "2288317098",
        "name": "Charlie Zhang"
      },
      {
        "authorId": "2302210920",
        "name": "Xianbin Wang"
      },
      {
        "authorId": "2302284624",
        "name": "Jiangchuan Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 125.43755299006494
  },
  {
    "paperId": "4a48d628e53f554eb6ef09a457ca855188b96171",
    "url": "https://www.semanticscholar.org/paper/4a48d628e53f554eb6ef09a457ca855188b96171",
    "title": "DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models",
    "abstract": "Recent LLM-driven visual agents mainly focus on solving image-based tasks, which limits their ability to understand dynamic scenes, making it far from real-life applications like guiding students in laboratory experiments and identifying their mistakes. Hence, this paper explores DoraemonGPT, a comprehensive and conceptually elegant system driven by LLMs to understand dynamic scenes. Considering the video modality better reflects the ever-changing nature of real-world scenarios, we exemplify DoraemonGPT as a video agent. Given a video with a question/task, DoraemonGPT begins by converting the input video into a symbolic memory that stores task-related attributes. This structured representation allows for spatial-temporal querying and reasoning by well-designed sub-task tools, resulting in concise intermediate results. Recognizing that LLMs have limited internal knowledge when it comes to specialized domains (e.g., analyzing the scientific principles underlying experiments), we incorporate plug-and-play tools to assess external knowledge and address tasks across different domains. Moreover, a novel LLM-driven planner based on Monte Carlo Tree Search is introduced to explore the large planning space for scheduling various tools. The planner iteratively finds feasible solutions by backpropagating the result's reward, and multiple solutions can be summarized into an improved final answer. We extensively evaluate DoraemonGPT's effectiveness on three benchmarks and several in-the-wild scenarios. The code will be released at https://github.com/z-x-yang/DoraemonGPT.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 24,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-01-16",
    "authors": [
      {
        "authorId": "15556978",
        "name": "Zongxin Yang"
      },
      {
        "authorId": "2278580328",
        "name": "Guikun Chen"
      },
      {
        "authorId": "2273819554",
        "name": "Xiaodi Li"
      },
      {
        "authorId": "2257367339",
        "name": "Wenguan Wang"
      },
      {
        "authorId": "2273914134",
        "name": "Yi Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.28313737302301
  },
  {
    "paperId": "bab39663b1f31eb1ac79e6e0b1ed560b086803da",
    "url": "https://www.semanticscholar.org/paper/bab39663b1f31eb1ac79e6e0b1ed560b086803da",
    "title": "Assessing the Utility of ChatGPT Throughout the Entire Clinical Workflow",
    "abstract": "IMPORTANCE: Large language model (LLM) artificial intelligence (AI) chatbots direct the power of large training datasets towards successive, related tasks, as opposed to single-ask tasks, for which AI already achieves impressive performance. The capacity of LLMs to assist in the full scope of iterative clinical reasoning via successive prompting, in effect acting as virtual physicians, has not yet been evaluated. OBJECTIVE: To evaluate ChatGPT's capacity for ongoing clinical decision support via its performance on standardized clinical vignettes. DESIGN: We inputted all 36 published clinical vignettes from the Merck Sharpe & Dohme (MSD) Clinical Manual into ChatGPT and compared accuracy on differential diagnoses, diagnostic testing, final diagnosis, and management based on patient age, gender, and case acuity. SETTING: ChatGPT, a publicly available LLM PARTICIPANTS: Clinical vignettes featured hypothetical patients with a variety of age and gender identities, and a range of Emergency Severity Indices (ESIs) based on initial clinical presentation. EXPOSURES: MSD Clinical Manual vignettes MAIN OUTCOMES AND MEASURES: We measured the proportion of correct responses to the questions posed within the clinical vignettes tested. RESULTS: ChatGPT achieved 71.7% (95% CI, 69.3% to 74.1%) accuracy overall across all 36 clinical vignettes. The LLM demonstrated the highest performance in making a final diagnosis with an accuracy of 76.9% (95% CI, 67.8% to 86.1%), and the lowest performance in generating an initial differential diagnosis with an accuracy of 60.3% (95% CI, 54.2% to 66.6%). Compared to answering questions about general medical knowledge, ChatGPT demonstrated inferior performance on differential diagnosis ({beta}=-15.8%, p<0.001) and clinical management ({beta}=-7.4%, p=0.02) type questions. CONCLUSIONS AND RELEVANCE: ChatGPT achieves impressive accuracy in clinical decision making, with particular strengths emerging as it has more clinical information at its disposal.",
    "venue": "medRxiv",
    "year": 2023,
    "citationCount": 84,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-02-26",
    "authors": [
      {
        "authorId": "2210587830",
        "name": "Arya Rao"
      },
      {
        "authorId": "40342534",
        "name": "Michael Pang"
      },
      {
        "authorId": "2210777816",
        "name": "John Kim"
      },
      {
        "authorId": "72458178",
        "name": "M. Kamineni"
      },
      {
        "authorId": "2210471154",
        "name": "Winston Lie"
      },
      {
        "authorId": "2210476712",
        "name": "Anoop K Prasad"
      },
      {
        "authorId": "2516236",
        "name": "A. Landman"
      },
      {
        "authorId": "26654844",
        "name": "K. Dreyer"
      },
      {
        "authorId": "14920559",
        "name": "M. Succi"
      }
    ],
    "source": "semantic_scholar",
    "score": 136.63976884735473
  },
  {
    "paperId": "434b9f9bc71c935e4a46a1aff36a8cc4c22d9afa",
    "url": "https://www.semanticscholar.org/paper/434b9f9bc71c935e4a46a1aff36a8cc4c22d9afa",
    "title": "Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration",
    "abstract": "Human intelligence thrives on cognitive synergy, where collaboration among different minds yield superior outcomes compared to isolated individuals. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist is an intelligent agent that collaboratively combines multiple minds’ strengths and knowledge to enhance problem-solving in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis shows that assigning multiple fine-grained personas in LLMs improves problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, experimental results demonstrate that SPP effectively reduces factual hallucination, and maintains strong reasoning capabilities. Additionally, comparative experiments show that cognitive synergy only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo and Llama2-13b-chat, which draws an interesting analogy to human development. Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 81,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-07-11",
    "authors": [
      {
        "authorId": "2052036545",
        "name": "Zhenhailong Wang"
      },
      {
        "authorId": "35374367",
        "name": "Shaoguang Mao"
      },
      {
        "authorId": "51198241",
        "name": "Wenshan Wu"
      },
      {
        "authorId": "50251691",
        "name": "Tao Ge"
      },
      {
        "authorId": "49807919",
        "name": "Furu Wei"
      },
      {
        "authorId": "2072975661",
        "name": "Heng Ji"
      }
    ],
    "source": "semantic_scholar",
    "score": 136.10078870896382
  },
  {
    "paperId": "f2f9c02a7eb484dd7b7ac46892856e3f278eed77",
    "url": "https://www.semanticscholar.org/paper/f2f9c02a7eb484dd7b7ac46892856e3f278eed77",
    "title": "AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model",
    "abstract": "We present Any-Modality Augmented Language Model (AnyMAL), a unified model that reasons over diverse input modality signals (i.e. text, image, video, audio, IMU motion sensor), and generates textual responses. AnyMAL inherits the powerful text-based reasoning abilities of the state-of-the-art LLMs including Llama-3 (70B), and converts modality-specific signals to the joint textual space through a pre-trained aligner module.In this paper, we provide details on the optimizations implemented to efficiently scale the training pipeline, and present a comprehensive recipe for model and training configurations. We conduct comprehensive empirical analysis comprising both human and automatic evaluations, and demonstrate state-of-the-art performance on various multimodal tasks compared to industry-leading models – albeit with a relatively small number of trainable parameters.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 79,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-09-27",
    "authors": [
      {
        "authorId": "2256132624",
        "name": "Seungwhan Moon"
      },
      {
        "authorId": "2111680936",
        "name": "Andrea Madotto"
      },
      {
        "authorId": "2146396528",
        "name": "Zhaojiang Lin"
      },
      {
        "authorId": "2248184174",
        "name": "Tushar Nagarajan"
      },
      {
        "authorId": "2249027674",
        "name": "Matt Smith"
      },
      {
        "authorId": "2249741629",
        "name": "Shashank Jain"
      },
      {
        "authorId": "2248041474",
        "name": "Chun-Fu Yeh"
      },
      {
        "authorId": "2248184913",
        "name": "Prakash Murugesan"
      },
      {
        "authorId": "2248176677",
        "name": "Peyman Heidari"
      },
      {
        "authorId": "2247965931",
        "name": "Yue Liu"
      },
      {
        "authorId": "27693639",
        "name": "Kavya Srinet"
      },
      {
        "authorId": "3057557",
        "name": "Babak Damavandi"
      },
      {
        "authorId": "2247977368",
        "name": "Anuj Kumar"
      }
    ],
    "source": "semantic_scholar",
    "score": 135.73039952010822
  },
  {
    "paperId": "b0435af3063195e8ae880489e64ccde64e6d7563",
    "url": "https://www.semanticscholar.org/paper/b0435af3063195e8ae880489e64ccde64e6d7563",
    "title": "Guiding Large Language Models via Directional Stimulus Prompting",
    "abstract": "We introduce Directional Stimulus Prompting, a novel framework for guiding black-box large language models (LLMs) toward specific desired outputs. Instead of directly adjusting LLMs, our method employs a small tunable policy model (e.g., T5) to generate an auxiliary directional stimulus prompt for each input instance. These directional stimulus prompts act as nuanced, instance-specific hints and clues to guide LLMs in generating desired outcomes, such as including specific keywords in the generated summary. Our approach sidesteps the challenges of direct LLM tuning by optimizing the policy model to explore directional stimulus prompts that align LLMs with desired behaviors. The policy model can be optimized through 1) supervised fine-tuning using labeled data and 2) reinforcement learning from offline or online rewards based on the LLM's output. We assess our method across summarization, dialogue response generation, and chain-of-thought reasoning tasks. Our experiments demonstrate that the framework consistently improves LLMs' (e.g., ChatGPT, Codex, InstructGPT) performance on these supervised tasks using minimal labeled data. Notably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances ChatGPT's performance by an impressive 41.4%, matching or surpassing some fully supervised start-of-the-art models. Additionally, the instance-specific chain-of-thought prompt generated by our approach improves InstructGPT's reasoning accuracy compared to human-crafted or automatically generated prompts. The code and data are publicly available at \\url{https://github.com/Leezekun/Directional-Stimulus-Prompting}.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 76,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2302.11520",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-02-22",
    "authors": [
      {
        "authorId": "2109964198",
        "name": "Zekun Li"
      },
      {
        "authorId": "1780690",
        "name": "Baolin Peng"
      },
      {
        "authorId": "50462546",
        "name": "Pengcheng He"
      },
      {
        "authorId": "1947267",
        "name": "Michel Galley"
      },
      {
        "authorId": "48441311",
        "name": "Jianfeng Gao"
      },
      {
        "authorId": "145026971",
        "name": "Xi Yan"
      }
    ],
    "source": "semantic_scholar",
    "score": 142.15708132780526
  },
  {
    "paperId": "2cea424c7dce71042c24d43317521abdc4c0ffb4",
    "url": "https://www.semanticscholar.org/paper/2cea424c7dce71042c24d43317521abdc4c0ffb4",
    "title": "Large Multimodal Agents: A Survey",
    "abstract": "Large language models (LLMs) have achieved superior performance in powering text-based AI agents, endowing them with decision-making and reasoning abilities akin to humans. Concurrently, there is an emerging research trend focused on extending these LLM-powered AI agents into the multimodal domain. This extension enables AI agents to interpret and respond to diverse multimodal user queries, thereby handling more intricate and nuanced tasks. In this paper, we conduct a systematic review of LLM-driven multimodal agents, which we refer to as large multimodal agents ( LMAs for short). First, we introduce the essential components involved in developing LMAs and categorize the current body of research into four distinct types. Subsequently, we review the collaborative frameworks integrating multiple LMAs , enhancing collective efficacy. One of the critical challenges in this field is the diverse evaluation methods used across existing studies, hindering effective comparison among different LMAs . Therefore, we compile these evaluation methodologies and establish a comprehensive framework to bridge the gaps. This framework aims to standardize evaluations, facilitating more meaningful comparisons. Concluding our review, we highlight the extensive applications of LMAs and propose possible future research directions. Our discussion aims to provide valuable insights and guidelines for future research in this rapidly evolving field. An up-to-date resource list is available at https://github.com/jun0wanan/awesome-large-multimodal-agents.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 23,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-02-23",
    "authors": [
      {
        "authorId": "2286630428",
        "name": "Junlin Xie"
      },
      {
        "authorId": "46843171",
        "name": "Zhihong Chen"
      },
      {
        "authorId": "2109962816",
        "name": "Ruifei Zhang"
      },
      {
        "authorId": "2237799248",
        "name": "Xiang Wan"
      },
      {
        "authorId": "2255423041",
        "name": "Guanbin Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 117.67080745521918
  },
  {
    "paperId": "59084df7203c6be33838ba3e3854eb9bda053ed2",
    "url": "https://www.semanticscholar.org/paper/59084df7203c6be33838ba3e3854eb9bda053ed2",
    "title": "Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint",
    "abstract": "Reinforcement learning (RL) has been widely used in training large language models (LLMs) for preventing unexpected outputs, eg reducing harmfulness and errors. However, existing RL methods mostly adopt the instance-level reward, which is unable to provide fine-grained supervision for complex reasoning tasks, and can not focus on the few key tokens that lead to the incorrectness. To address it, we propose a new RL method named RLMEC that incorporates a generative model as the reward model, which is trained by the erroneous solution rewriting task under the minimum editing constraint, and can produce token-level rewards for RL training. Based on the generative reward model, we design the token-level RL objective for training and an imitation-based regularization for stabilizing RL process. And the both objectives focus on the learning of the key tokens for the erroneous solution, reducing the effect of other unimportant tokens. The experiment results on mathematical tasks and question-answering tasks have demonstrated the effectiveness of our approach. Our code and data are available at https://github.com/RUCAIBox/RLMEC.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 22,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-01-11",
    "authors": [
      {
        "authorId": "2256558402",
        "name": "Zhipeng Chen"
      },
      {
        "authorId": "2265383494",
        "name": "Kun Zhou"
      },
      {
        "authorId": "2257376413",
        "name": "Wayne Xin Zhao"
      },
      {
        "authorId": "2257029126",
        "name": "Junchen Wan"
      },
      {
        "authorId": "2257136363",
        "name": "Fuzheng Zhang"
      },
      {
        "authorId": "2257381268",
        "name": "Di Zhang"
      },
      {
        "authorId": "2274218622",
        "name": "Ji-Rong Wen"
      }
    ],
    "source": "semantic_scholar",
    "score": 117.03241323893724
  },
  {
    "paperId": "7c1adf2bcff5d7ed1cf330680a100060f278e2a3",
    "url": "https://www.semanticscholar.org/paper/7c1adf2bcff5d7ed1cf330680a100060f278e2a3",
    "title": "Embodied LLM Agents Learn to Cooperate in Organized Teams",
    "abstract": "Large Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs to propose enhanced organizational prompts, via a Criticize-Reflect process, resulting in novel organization structures that reduce communication costs and enhance team efficiency.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 20,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-19",
    "authors": [
      {
        "authorId": "2124466167",
        "name": "Xudong Guo"
      },
      {
        "authorId": "2242535459",
        "name": "Kaixuan Huang"
      },
      {
        "authorId": "2253854104",
        "name": "Jiale Liu"
      },
      {
        "authorId": "2113532692",
        "name": "Wenhui Fan"
      },
      {
        "authorId": "2292199710",
        "name": "Natalia V'elez"
      },
      {
        "authorId": "2257162610",
        "name": "Qingyun Wu"
      },
      {
        "authorId": "2257095799",
        "name": "Huazheng Wang"
      },
      {
        "authorId": "2292201338",
        "name": "Thomas L. Griffiths"
      },
      {
        "authorId": "2257417692",
        "name": "Mengdi Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 115.66783656585135
  },
  {
    "paperId": "d9db99fa712edf900873b62143a08f5a14c22986",
    "url": "https://www.semanticscholar.org/paper/d9db99fa712edf900873b62143a08f5a14c22986",
    "title": "Advancing entity recognition in biomedicine via instruction tuning of large language models",
    "abstract": "Abstract Motivation Large Language Models (LLMs) have the potential to revolutionize the field of Natural Language Processing, excelling not only in text generation and reasoning tasks but also in their ability for zero/few-shot learning, swiftly adapting to new tasks with minimal fine-tuning. LLMs have also demonstrated great promise in biomedical and healthcare applications. However, when it comes to Named Entity Recognition (NER), particularly within the biomedical domain, LLMs fall short of the effectiveness exhibited by fine-tuned domain-specific models. One key reason is that NER is typically conceptualized as a sequence labeling task, whereas LLMs are optimized for text generation and reasoning tasks. Results We developed an instruction-based learning paradigm that transforms biomedical NER from a sequence labeling task into a generation task. This paradigm is end-to-end and streamlines the training and evaluation process by automatically repurposing pre-existing biomedical NER datasets. We further developed BioNER-LLaMA using the proposed paradigm with LLaMA-7B as the foundational LLM. We conducted extensive testing on BioNER-LLaMA across three widely recognized biomedical NER datasets, consisting of entities related to diseases, chemicals, and genes. The results revealed that BioNER-LLaMA consistently achieved higher F1-scores ranging from 5% to 30% compared to the few-shot learning capabilities of GPT-4 on datasets with different biomedical entities. We show that a general-domain LLM can match the performance of rigorously fine-tuned PubMedBERT models and PMC-LLaMA, biomedical-specific language model. Our findings underscore the potential of our proposed paradigm in developing general-domain LLMs that can rival SOTA performances in multi-task, multi-domain scenarios in biomedical and health applications. Availability and implementation Datasets and other resources are available at https://github.com/BIDS-Xu-Lab/BioNER-LLaMA.",
    "venue": "Bioinformatics",
    "year": 2024,
    "citationCount": 22,
    "openAccessPdf": {
      "url": "https://academic.oup.com/bioinformatics/advance-article-pdf/doi/10.1093/bioinformatics/btae163/57056267/btae163.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-21",
    "authors": [
      {
        "authorId": "65893930",
        "name": "V. Keloth"
      },
      {
        "authorId": "2113665949",
        "name": "Yan Hu"
      },
      {
        "authorId": "2292723737",
        "name": "Qianqian Xie"
      },
      {
        "authorId": "2274126492",
        "name": "Xueqing Peng"
      },
      {
        "authorId": "2293045413",
        "name": "Yan Wang"
      },
      {
        "authorId": "2292717231",
        "name": "Andrew Zheng"
      },
      {
        "authorId": "2292716940",
        "name": "Melih Selek"
      },
      {
        "authorId": "2237967839",
        "name": "Kalpana Raja"
      },
      {
        "authorId": "3252035",
        "name": "Chih-Hsuan Wei"
      },
      {
        "authorId": "2261406713",
        "name": "Qiao Jin"
      },
      {
        "authorId": "2246834558",
        "name": "Zhiyong Lu"
      },
      {
        "authorId": "2256591935",
        "name": "Qingyu Chen"
      },
      {
        "authorId": "2274744074",
        "name": "Hua Xu"
      }
    ],
    "source": "semantic_scholar",
    "score": 117.03241323893724
  },
  {
    "paperId": "966852963a88a28786b798c91b6662d6e501e590",
    "url": "https://www.semanticscholar.org/paper/966852963a88a28786b798c91b6662d6e501e590",
    "title": "AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn",
    "abstract": "Recent research on Large Language Models (LLMs) has led to remarkable advancements in general NLP AI assistants. Some studies have further explored the use of LLMs for planning and invoking models or APIs to address more general multi-modal user queries. Despite this progress, complex visual-based tasks still remain challenging due to the diverse nature of visual tasks. This diversity is reflected in two aspects: 1) Reasoning paths. For many real-life applications, it is hard to accurately decompose a query simply by examining the query itself. Planning based on the specific visual content and the results of each step is usually required. 2) Flexible inputs and intermediate results. Input forms could be flexible for in-the-wild cases, and involves not only a single image or video but a mixture of videos and images, e.g., a user-view image with some reference videos. Besides, a complex reasoning process will also generate diverse multimodal intermediate results, e.g., video narrations, segmented video clips, etc. To address such general cases, we propose a multi-modal AI assistant, AssistGPT, with an interleaved code and language reasoning approach called Plan, Execute, Inspect, and Learn (PEIL) to integrate LLMs with various tools. Specifically, the Planner is capable of using natural language to plan which tool in Executor should do next based on the current reasoning progress. Inspector is an efficient memory manager to assist the Planner to feed proper visual information into a specific tool. Finally, since the entire reasoning process is complex and flexible, a Learner is designed to enable the model to autonomously explore and discover the optimal solution. We conducted experiments on A-OKVQA and NExT-QA benchmarks, achieving state-of-the-art results. Moreover, showcases demonstrate the ability of our system to handle questions far more complex than those found in the benchmarks.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 62,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.08640",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-14",
    "authors": [
      {
        "authorId": "2420608",
        "name": "Difei Gao"
      },
      {
        "authorId": "144906579",
        "name": "Lei Ji"
      },
      {
        "authorId": "2116644664",
        "name": "Luowei Zhou"
      },
      {
        "authorId": "48085802",
        "name": "Kevin Lin"
      },
      {
        "authorId": "1391202077",
        "name": "Joya Chen"
      },
      {
        "authorId": "2219035569",
        "name": "Zihan Fan"
      },
      {
        "authorId": "2258957578",
        "name": "Mike Zheng Shou"
      }
    ],
    "source": "semantic_scholar",
    "score": 132.147020895873
  },
  {
    "paperId": "85996f9fc312777f487dd51bf9e96bb3704c2fb7",
    "url": "https://www.semanticscholar.org/paper/85996f9fc312777f487dd51bf9e96bb3704c2fb7",
    "title": "On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark)",
    "abstract": "Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) how good LLMs are by themselves in generating and validating simple plans in commonsense planning tasks (of the type that humans are generally quite good at) and (2) how good LLMs are in being a source of heuristic guidance for other agents--either AI planners or human planners--in their planning tasks. To investigate these questions in a systematic rather than anecdotal manner, we start by developing a benchmark suite based on the kinds of domains employed in the International Planning Competition. On this benchmark, we evaluate LLMs in three modes: autonomous, heuristic and human-in-the-loop. Our results show that LLM's ability to autonomously generate executable plans is quite meager, averaging only about 3% success rate. The heuristic and human-in-the-loop modes show slightly more promise. In addition to these results, we also make our benchmark and evaluation tools available to support investigations by research community.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 62,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2302.06706",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-02-13",
    "authors": [
      {
        "authorId": "144982263",
        "name": "Karthik Valmeekam"
      },
      {
        "authorId": "2400282",
        "name": "S. Sreedharan"
      },
      {
        "authorId": "2188993062",
        "name": "Matthew Marquez"
      },
      {
        "authorId": "46538164",
        "name": "Alberto Olmo Hernandez"
      },
      {
        "authorId": "2047340230",
        "name": "Subbarao Kambhampati"
      }
    ],
    "source": "semantic_scholar",
    "score": 132.147020895873
  },
  {
    "paperId": "574da0a8844c0e553b864386d31998516525cfe8",
    "url": "https://www.semanticscholar.org/paper/574da0a8844c0e553b864386d31998516525cfe8",
    "title": "DeepEdit: Knowledge Editing as Decoding with Constraints",
    "abstract": "How to edit the knowledge in multi-step reasoning has become the major challenge in the knowledge editing (KE) of large language models (LLMs). The difficulty arises because the hallucinations of LLMs during multi-step reasoning often lead to incorrect use of new knowledge and incorrect answers. To address this issue, we design decoding constraints to\"regulate\"LLMs' reasoning, enhancing logical coherence when incorporating new knowledge. We propose a new KE framework: DEEPEDIT (Depth-first Search-based Constrained Decoding for Knowledge Editing), which enhances LLMs's ability to generate coherent reasoning chains with new knowledge through depth-first search. Our search selects the most important knowledge that satisfies our constraints as the reasoning step to efficiently increase the reasoning depth. In addition to DEEPEDIT, we propose two new KE benchmarks: MQUAKE-2002 and MQUAKE-HARD, which provide more precise and challenging assessments of KE approaches. Qualitatively, DEEPEDIT enables LLMs to produce succinct and coherent reasoning chains involving new knowledge. Quantitatively, it yields significant improvements on multiple KE benchmarks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 19,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-19",
    "authors": [
      {
        "authorId": "2280103482",
        "name": "Yiwei Wang"
      },
      {
        "authorId": "1998918",
        "name": "Muhao Chen"
      },
      {
        "authorId": "2256996328",
        "name": "Nanyun Peng"
      },
      {
        "authorId": "2257127887",
        "name": "Kai-Wei Chang"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.93598410330986
  },
  {
    "paperId": "8bb527c332b133e52bb7b6c5da84375888f41c05",
    "url": "https://www.semanticscholar.org/paper/8bb527c332b133e52bb7b6c5da84375888f41c05",
    "title": "Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models",
    "abstract": "Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents. How to integrate agent ability into general LLMs becomes a crucial and urgent problem. This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) LLMs exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations. Based on the above findings, we propose Agent-FLAN to effectively Fine-tune LANguage models for Agents. Through careful decomposition and redesign of the training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by 3.5\\% across various agent evaluation datasets. With comprehensively constructed negative samples, Agent-FLAN greatly alleviates the hallucination issues based on our established evaluation benchmark. Besides, it consistently improves the agent capability of LLMs when scaling model sizes while slightly enhancing the general capability of LLMs. The code will be available at https://github.com/InternLM/Agent-FLAN.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 19,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-19",
    "authors": [
      {
        "authorId": "2275197422",
        "name": "Zehui Chen"
      },
      {
        "authorId": "2029335061",
        "name": "Kuikun Liu"
      },
      {
        "authorId": "2287804145",
        "name": "Qiuchen Wang"
      },
      {
        "authorId": "2266359401",
        "name": "Wenwei Zhang"
      },
      {
        "authorId": "2275801272",
        "name": "Jiangning Liu"
      },
      {
        "authorId": "2261095726",
        "name": "Dahua Lin"
      },
      {
        "authorId": "2275790072",
        "name": "Kai Chen"
      },
      {
        "authorId": "2275819548",
        "name": "Feng Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.93598410330986
  },
  {
    "paperId": "8a4498bdc31ca56803db14c72a562542b79b2bc0",
    "url": "https://www.semanticscholar.org/paper/8a4498bdc31ca56803db14c72a562542b79b2bc0",
    "title": "How to Prompt Your Robot: A PromptBook for Manipulation Skills with Code as Policies",
    "abstract": "Large Language Models (LLMs) have demonstrated the ability to perform semantic reasoning, planning and write code for robotics tasks. However, most methods rely on pre-existing primitives (i.e. pick, open drawer) or similar examples of robot code alone, which heavily limits their scalability to new scenarios. We present PromptBook, a collection of different prompting paradigms to generate code for successfully executing new manipulation skills. We demonstrate example-based, instruction-based and chain-of-thought to write robot code; as well as a method to build the prompt leveraging LLMs and human feedback. We show PromptBook enables LLMs to write code for new low-level manipulation skills in a zero-shot manner: from picking diverse objects, opening/closing drawers, to whisking, and waving hello. We evaluate the new skills on a mobile manipulator with 83% success rate at picking, 50-71% at opening drawers and 100% at closing them. Notably, the LLM is able to infer gripper orientation for grasping a drawer handle (z-axis aligned) vs. a top-down grasp (x-axis aligned).",
    "venue": "IEEE International Conference on Robotics and Automation",
    "year": 2024,
    "citationCount": 19,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-13",
    "authors": [
      {
        "authorId": "153134021",
        "name": "Montse Gonzalez Arenas"
      },
      {
        "authorId": "9961095",
        "name": "Ted Xiao"
      },
      {
        "authorId": "2109040371",
        "name": "Sumeet Singh"
      },
      {
        "authorId": "2253472236",
        "name": "Vidhi Jain"
      },
      {
        "authorId": "2284694289",
        "name": "Allen Ren"
      },
      {
        "authorId": "2288210223",
        "name": "Quan Vuong"
      },
      {
        "authorId": "2257289716",
        "name": "Jake Varley"
      },
      {
        "authorId": "2253642204",
        "name": "Alex Herzog"
      },
      {
        "authorId": "2057988112",
        "name": "Isabel Leal"
      },
      {
        "authorId": "51881277",
        "name": "Sean Kirmani"
      },
      {
        "authorId": "2315511778",
        "name": "Mario Prats"
      },
      {
        "authorId": "1779671",
        "name": "Dorsa Sadigh"
      },
      {
        "authorId": "1808676",
        "name": "Vikas Sindhwani"
      },
      {
        "authorId": "2256967164",
        "name": "Kanishka Rao"
      },
      {
        "authorId": "2271707306",
        "name": "Jacky Liang"
      },
      {
        "authorId": "2294878239",
        "name": "Andy Zeng"
      }
    ],
    "source": "semantic_scholar",
    "score": 120.93598410330986
  },
  {
    "paperId": "440d8b87e158a352efa58d2630d8626640afefe6",
    "url": "https://www.semanticscholar.org/paper/440d8b87e158a352efa58d2630d8626640afefe6",
    "title": "When is Tree Search Useful for LLM Planning? It Depends on the Discriminator",
    "abstract": "In this paper, we examine how large language models (LLMs) solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method. We investigate the practical utility of two advanced planning methods, iterative correction and tree search. We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90% accuracy to achieve significant improvements over re-ranking; (2) current LLMs' discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency. For example, compared to the other two methods, tree search is at least 10--20 times slower but leads to negligible performance gains, which hinders its real-world applications. Code and data are available at https://github.com/OSU-NLP-Group/llm-planning-eval.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 19,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-16",
    "authors": [
      {
        "authorId": "11832104",
        "name": "Ziru Chen"
      },
      {
        "authorId": "2284730462",
        "name": "Michael White"
      },
      {
        "authorId": "2284591621",
        "name": "Raymond Mooney"
      },
      {
        "authorId": "2286242531",
        "name": "Ali Payani"
      },
      {
        "authorId": "1758652",
        "name": "Yu Su"
      },
      {
        "authorId": "2284825007",
        "name": "Huan Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.93598410330986
  },
  {
    "paperId": "450d97c7f456eafbb69dd70322d341058028b171",
    "url": "https://www.semanticscholar.org/paper/450d97c7f456eafbb69dd70322d341058028b171",
    "title": "In-Context Principle Learning from Mistakes",
    "abstract": "In-context learning (ICL, also known as few-shot prompting) has been the standard method of adapting LLMs to downstream tasks, by learning from a few input-output examples. Nonetheless, all ICL-based approaches only learn from correct input-output pairs. In this paper, we revisit this paradigm, by learning more from the few given input-output examples. We introduce Learning Principles (LEAP): First, we intentionally induce the model to make mistakes on these few examples; then we reflect on these mistakes, and learn explicit task-specific\"principles\"from them, which help solve similar problems and avoid common mistakes; finally, we prompt the model to answer unseen test questions using the original few-shot examples and these learned general principles. We evaluate LEAP on a wide range of benchmarks, including multi-hop question answering (Hotpot QA), textual QA (DROP), Big-Bench Hard reasoning, and math problems (GSM8K and MATH); in all these benchmarks, LEAP improves the strongest available LLMs such as GPT-3.5-turbo, GPT-4, GPT-4 turbo and Claude-2.1. For example, LEAP improves over the standard few-shot prompting using GPT-4 by 7.5% in DROP, and by 3.3% in HotpotQA. Importantly, LEAP does not require any more input or examples than the standard few-shot prompting settings.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 19,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-08",
    "authors": [
      {
        "authorId": "1993655237",
        "name": "Tianjun Zhang"
      },
      {
        "authorId": "21626987",
        "name": "Aman Madaan"
      },
      {
        "authorId": "2267242298",
        "name": "Luyu Gao"
      },
      {
        "authorId": "2283438176",
        "name": "Steven Zheng"
      },
      {
        "authorId": "1817207",
        "name": "Swaroop Mishra"
      },
      {
        "authorId": "2267307049",
        "name": "Yiming Yang"
      },
      {
        "authorId": "2261389843",
        "name": "Niket Tandon"
      },
      {
        "authorId": "47051926",
        "name": "Uri Alon"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.93598410330986
  },
  {
    "paperId": "3713112311efbcf785de17fa86e5bf42e4360f77",
    "url": "https://www.semanticscholar.org/paper/3713112311efbcf785de17fa86e5bf42e4360f77",
    "title": "G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model",
    "abstract": "Large language models (LLMs) have shown remarkable proficiency in human-level reasoning and generation capabilities, which encourages extensive research on their application in mathematical problem solving. However, current work has been largely focused on text-based mathematical problems, with limited investigation in problems involving geometric information. Addressing this gap, we aim to enable LLMs to solve geometric problems by understanding image input. We first analyze the limitations of current Multimodal Large Language Models (MLLMs) in this area: they struggle to accurately comprehending basic geometric elements and their relationships. To overcome these challenges, we take advantage of the unique characteristics of geometric problems (such as unique geometric logical form, and geometric scalability) and the capacity of the textual LLMs to build an enriched multimodal geometry dataset based on existing data. The augmented dataset, Geo170K, contains more than 170K geometric image-caption and question-answer pairs. Utilizing our constructed Geo170K dataset, we develop G-LLaVA, which demonstrates exceptional performance in solving geometric problems, significantly outperforming GPT-4-V on the MathVista benchmark with only 7B parameters.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 56,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-18",
    "authors": [
      {
        "authorId": "144407296",
        "name": "Jiahui Gao"
      },
      {
        "authorId": "2066420772",
        "name": "Renjie Pi"
      },
      {
        "authorId": "50561049",
        "name": "Jipeng Zhang"
      },
      {
        "authorId": "65846898",
        "name": "Jiacheng Ye"
      },
      {
        "authorId": "2249763710",
        "name": "Wanjun Zhong"
      },
      {
        "authorId": "46395829",
        "name": "Yufei Wang"
      },
      {
        "authorId": "2150203873",
        "name": "Lanqing Hong"
      },
      {
        "authorId": "47180442",
        "name": "Jianhua Han"
      },
      {
        "authorId": "2257092965",
        "name": "Hang Xu"
      },
      {
        "authorId": "2257858413",
        "name": "Zhenguo Li"
      },
      {
        "authorId": "2260528279",
        "name": "Lingpeng Kong"
      }
    ],
    "source": "semantic_scholar",
    "score": 130.64576901751826
  },
  {
    "paperId": "ac258100ebe178287ae4ae3dc7ac78f8c27e017d",
    "url": "https://www.semanticscholar.org/paper/ac258100ebe178287ae4ae3dc7ac78f8c27e017d",
    "title": "Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game",
    "abstract": "Agents built with large language models (LLMs) have shown great potential across a wide range of domains. However, in complex decision-making tasks, pure LLM-based agents tend to exhibit intrinsic bias in their choice of actions, which is inherited from the model's training data and results in suboptimal performance. To develop strategic language agents, i.e., agents that generate flexible language actions and possess strong decision-making abilities, we propose a novel framework that powers LLM-based agents with reinforcement learning (RL). We consider Werewolf, a popular social deduction game, as a challenging testbed that emphasizes versatile communication and strategic gameplay. To mitigate the intrinsic bias in language actions, our agents use an LLM to perform deductive reasoning and generate a diverse set of action candidates. Then an RL policy trained to optimize the decision-making ability chooses an action from the candidates to play in the game. Extensive experiments show that our agents overcome the intrinsic bias and outperform existing LLM-based agents in the Werewolf game. We also conduct human-agent experiments and find that our agents achieve human-level performance and demonstrate strong strategic play.",
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "citationCount": 58,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-29",
    "authors": [
      {
        "authorId": "2170477930",
        "name": "Zelai Xu"
      },
      {
        "authorId": "2117922684",
        "name": "Chao Yu"
      },
      {
        "authorId": "2256992848",
        "name": "Fei Fang"
      },
      {
        "authorId": "2153607473",
        "name": "Yu Wang"
      },
      {
        "authorId": "2108052575",
        "name": "Yi Wu"
      }
    ],
    "source": "semantic_scholar",
    "score": 131.1630616585858
  },
  {
    "paperId": "27f0ce04403158b61328716ae4aaab5840c0d123",
    "url": "https://www.semanticscholar.org/paper/27f0ce04403158b61328716ae4aaab5840c0d123",
    "title": "Batch Prompting: Efficient Inference with Large Language Model APIs",
    "abstract": "Performing inference on large volumes of samples with large language models (LLMs) can be computationally and financially costly in industry and real-world use. We propose batch prompting, a simple yet effective prompting approach that enables the LLM to run inference in batches, instead of one sample at a time. Our method reduces both token and time costs while retaining downstream performance. We theoretically demonstrate that under a few-shot in-context learning setting, the inference costs decrease almost inverse linearly with the number of samples in each batch. We extensively validate the effectiveness of batch prompting on ten datasets across commonsense QA, arithmetic reasoning, and NLI/NLU: batch prompting significantly~(up to 5x with six samples in batch) reduces the LLM (Codex) inference token and time costs while achieving better or comparable performance. For state-of-the-art Chat-based LLMs, e.g., GPT-3.5 and GPT-4, we show the benefits of batch prompting also hold. Further analysis shows that the number of samples in each batch and the complexity of tasks affect its performance. Moreover, batch prompting can be applied across different reasoning methods using LLMs. Our code can be found at the site https://github.com/xlang-ai/batch-prompting.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 57,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2301.08721",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-01-19",
    "authors": [
      {
        "authorId": null,
        "name": "Zhoujun Cheng"
      },
      {
        "authorId": "11348687",
        "name": "Jungo Kasai"
      },
      {
        "authorId": "2117900202",
        "name": "Tao Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 130.90664515819628
  },
  {
    "paperId": "952997b4d41a87a00425ee2d731880cbb13606a5",
    "url": "https://www.semanticscholar.org/paper/952997b4d41a87a00425ee2d731880cbb13606a5",
    "title": "Large Language Models as Data Augmenters for Cold-Start Item Recommendation",
    "abstract": "The reasoning and generalization capabilities of LLMs can help us better understand user preferences and item characteristics, offering exciting prospects to enhance recommendation systems. Though effective while user-item interactions are abundant, conventional recommendation systems struggle to recommend cold-start items without historical interactions. To address this, we propose utilizing LLMs as data augmenters to bridge the knowledge gap on cold-start items during training. We employ LLMs to infer user preferences for cold-start items based on textual description of user historical behaviors and new item descriptions. The augmented training signals are then incorporated into learning the downstream recommendation models through an auxiliary pairwise loss. Through experiments on public Amazon datasets, we demonstrate that LLMs can effectively augment the training signals for cold-start items, leading to significant improvements in cold-start item recommendation for various recommendation models.",
    "venue": "The Web Conference",
    "year": 2024,
    "citationCount": 17,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2024-02-18",
    "authors": [
      {
        "authorId": "46584367",
        "name": "Jianling Wang"
      },
      {
        "authorId": "2887627",
        "name": "Haokai Lu"
      },
      {
        "authorId": "1697232",
        "name": "James Caverlee"
      },
      {
        "authorId": "2254270179",
        "name": "E. Chi"
      },
      {
        "authorId": "2240764638",
        "name": "Minmin Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.35557636844247
  },
  {
    "paperId": "74b4b993babe99bc5f5c589c27fef0f1baba606b",
    "url": "https://www.semanticscholar.org/paper/74b4b993babe99bc5f5c589c27fef0f1baba606b",
    "title": "Making Large Language Models Better Reasoners with Alignment",
    "abstract": "Reasoning is a cognitive process of using evidence to reach a sound conclusion. The reasoning capability is essential for large language models (LLMs) to serve as the brain of the artificial general intelligence agent. Recent studies reveal that fine-tuning LLMs on data with the chain of thought (COT) reasoning process can significantly enhance their reasoning capabilities. However, we find that the fine-tuned LLMs suffer from an \\textit{Assessment Misalignment} problem, i.e., they frequently assign higher scores to subpar COTs, leading to potential limitations in their reasoning abilities. To address this problem, we introduce an \\textit{Alignment Fine-Tuning (AFT)} paradigm, which involves three steps: 1) fine-tuning LLMs with COT training data; 2) generating multiple COT responses for each question, and categorizing them into positive and negative ones based on whether they achieve the correct answer; 3) calibrating the scores of positive and negative responses given by LLMs with a novel constraint alignment loss. Specifically, the constraint alignment loss has two objectives: a) Alignment, which guarantees that positive scores surpass negative scores to encourage answers with high-quality COTs; b) Constraint, which keeps the negative scores confined to a reasonable range to prevent the model degradation. Beyond just the binary positive and negative feedback, the constraint alignment loss can be seamlessly adapted to the ranking situations when ranking feedback is accessible. Furthermore, we also delve deeply into recent ranking-based alignment methods, such as DPO, RRHF, and PRO, and discover that the constraint, which has been overlooked by these approaches, is also crucial for their performance. Extensive experiments on four reasoning benchmarks with both binary and ranking feedback demonstrate the effectiveness of AFT.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 57,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.02144",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-05",
    "authors": [
      {
        "authorId": "144202874",
        "name": "Peiyi Wang"
      },
      {
        "authorId": "46255707",
        "name": "Lei Li"
      },
      {
        "authorId": "2146034504",
        "name": "Liang Chen"
      },
      {
        "authorId": "66947198",
        "name": "Feifan Song"
      },
      {
        "authorId": "3186130",
        "name": "Binghuai Lin"
      },
      {
        "authorId": "2238148953",
        "name": "Yunbo Cao"
      },
      {
        "authorId": "1701889",
        "name": "Tianyu Liu"
      },
      {
        "authorId": "3335836",
        "name": "Zhifang Sui"
      }
    ],
    "source": "semantic_scholar",
    "score": 130.90664515819628
  },
  {
    "paperId": "8663871588271b4f20645fefcc7d25d9bd1547cf",
    "url": "https://www.semanticscholar.org/paper/8663871588271b4f20645fefcc7d25d9bd1547cf",
    "title": "A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges",
    "abstract": "Recent advances in large language models (LLMs) have unlocked novel opportunities for machine learning applications in the financial domain. These models have demonstrated remarkable capabilities in understanding context, processing vast amounts of data, and generating human-preferred contents. In this survey, we explore the application of LLMs on various financial tasks, focusing on their potential to transform traditional practices and drive innovation. We provide a discussion of the progress and advantages of LLMs in financial contexts, analyzing their advanced technologies as well as prospective capabilities in contextual understanding, transfer learning flexibility, complex emotion detection, etc. We then highlight this survey for categorizing the existing literature into key application areas, including linguistic tasks, sentiment analysis, financial time series, financial reasoning, agent-based modeling, and other applications. For each application area, we delve into specific methodologies, such as textual analysis, knowledge-based analysis, forecasting, data augmentation, planning, decision support, and simulations. Furthermore, a comprehensive collection of datasets, model assets, and useful codes associated with mainstream applications are presented as resources for the researchers and practitioners. Finally, we outline the challenges and opportunities for future research, particularly emphasizing a number of distinctive aspects in this field. We hope our work can help facilitate the adoption and further development of LLMs in the financial sector.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 17,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Economics"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-06-15",
    "authors": [
      {
        "authorId": "2301235606",
        "name": "Yuqi Nie"
      },
      {
        "authorId": "2307607703",
        "name": "Yaxuan Kong"
      },
      {
        "authorId": "2279250404",
        "name": "Xiaowen Dong"
      },
      {
        "authorId": "2238766751",
        "name": "John M. Mulvey"
      },
      {
        "authorId": "2265928094",
        "name": "H. Poor"
      },
      {
        "authorId": "2307076087",
        "name": "Qingsong Wen"
      },
      {
        "authorId": "2279077978",
        "name": "Stefan Zohren"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.35557636844247
  },
  {
    "paperId": "7ef10b9b5bbce61573f647774004f95221efe665",
    "url": "https://www.semanticscholar.org/paper/7ef10b9b5bbce61573f647774004f95221efe665",
    "title": "AgentsCoDriver: Large Language Model Empowered Collaborative Driving with Lifelong Learning",
    "abstract": "Connected and autonomous driving is developing rapidly in recent years. However, current autonomous driving systems, which are primarily based on data-driven approaches, exhibit deficiencies in interpretability, generalization, and continuing learning capabilities. In addition, the single-vehicle autonomous driving systems lack of the ability of collaboration and negotiation with other vehicles, which is crucial for the safety and efficiency of autonomous driving systems. In order to address these issues, we leverage large language models (LLMs) to develop a novel framework, AgentsCoDriver, to enable multiple vehicles to conduct collaborative driving. AgentsCoDriver consists of five modules: observation module, reasoning engine, cognitive memory module, reinforcement reflection module, and communication module. It can accumulate knowledge, lessons, and experiences over time by continuously interacting with the environment, thereby making itself capable of lifelong learning. In addition, by leveraging the communication module, different agents can exchange information and realize negotiation and collaboration in complex traffic environments. Extensive experiments are conducted and show the superiority of AgentsCoDriver.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 15,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-09",
    "authors": [
      {
        "authorId": "2249844970",
        "name": "Senkang Hu"
      },
      {
        "authorId": "2256394815",
        "name": "Zhengru Fang"
      },
      {
        "authorId": "2265482139",
        "name": "Zihan Fang"
      },
      {
        "authorId": "145472873",
        "name": "Yiqin Deng"
      },
      {
        "authorId": "31211490",
        "name": "Xianhao Chen"
      },
      {
        "authorId": "2256784186",
        "name": "Yuguang Fang"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.58883083359672
  },
  {
    "paperId": "bff499d51b002fd0b1aa05ba151a4a515e5bf36f",
    "url": "https://www.semanticscholar.org/paper/bff499d51b002fd0b1aa05ba151a4a515e5bf36f",
    "title": "Emotional intelligence of Large Language Models",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable abilities across numerous disciplines, primarily assessed through tasks in language generation, knowledge utilization, and complex reasoning. However, their alignment with human emotions and values, which is critical for real-world applications, has not been systematically evaluated. Here, we assessed LLMs' Emotional Intelligence (EI), encompassing emotion recognition, interpretation, and understanding, which is necessary for effective communication and social interactions. Specifically, we first developed a novel psychometric assessment focusing on Emotion Understanding (EU), a core component of EI. This test is an objective, performance-driven, and text-based evaluation, which requires evaluating complex emotions in realistic scenarios, providing a consistent assessment for both human and LLM capabilities. With a reference frame constructed from over 500 adults, we tested a variety of mainstream LLMs. Most achieved above-average Emotional Quotient (EQ) scores, with GPT-4 exceeding 89% of human participants with an EQ of 117. Interestingly, a multivariate pattern analysis revealed that some LLMs apparently did not rely on the human-like mechanism to achieve human-level performance, as their representational patterns were qualitatively distinct from humans. In addition, we discussed the impact of factors such as model size, training method, and architecture on LLMs' EQ. In summary, our study presents one of the first psychometric evaluations of the human-like characteristics of LLMs, which may shed light on the future development of LLMs aiming for both high intellectual and emotional intelligence. Project website: https://emotional-intelligence.github.io/",
    "venue": "Journal of Pacific Rim Psychology",
    "year": 2023,
    "citationCount": 58,
    "openAccessPdf": {
      "url": "https://journals.sagepub.com/doi/pdf/10.1177/18344909231213958",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-01-01",
    "authors": [
      {
        "authorId": "2277578436",
        "name": "Xuena Wang"
      },
      {
        "authorId": "2325423751",
        "name": "Xueting Li"
      },
      {
        "authorId": "2069532941",
        "name": "Zi Yin"
      },
      {
        "authorId": "46220633",
        "name": "Yue Wu"
      },
      {
        "authorId": "2223759460",
        "name": "Liu Jia Department of PsychologyTsinghua Laboratory of Brain"
      },
      {
        "authorId": "2223775536",
        "name": "Intelligence"
      },
      {
        "authorId": "102392556",
        "name": "Tsinghua University"
      },
      {
        "authorId": "116398212",
        "name": "Departmentof Psychology"
      },
      {
        "authorId": "2223749060",
        "name": "Renmin University"
      }
    ],
    "source": "semantic_scholar",
    "score": 131.1630616585858
  },
  {
    "paperId": "84d99893ee24fc825e359598d44d602c45c4865e",
    "url": "https://www.semanticscholar.org/paper/84d99893ee24fc825e359598d44d602c45c4865e",
    "title": "LLM4Drive: A Survey of Large Language Models for Autonomous Driving",
    "abstract": "Autonomous driving technology, a catalyst for revolutionizing transportation and urban mobility, has the tend to transition from rule-based systems to data-driven strategies. Traditional module-based systems are constrained by cumulative errors among cascaded modules and inflexible pre-set rules. In contrast, end-to-end autonomous driving systems have the potential to avoid error accumulation due to their fully data-driven training process, although they often lack transparency due to their\"black box\"nature, complicating the validation and traceability of decisions. Recently, large language models (LLMs) have demonstrated abilities including understanding context, logical reasoning, and generating answers. A natural thought is to utilize these abilities to empower autonomous driving. By combining LLM with foundation vision models, it could open the door to open-world understanding, reasoning, and few-shot learning, which current autonomous driving systems are lacking. In this paper, we systematically review a research line about \\textit{Large Language Models for Autonomous Driving (LLM4AD)}. This study evaluates the current state of technological advancements, distinctly outlining the principal challenges and prospective directions for the field. For the convenience of researchers in academia and industry, we provide real-time updates on the latest advances in the field as well as relevant open-source resources via the designated link: https://github.com/Thinklab-SJTU/Awesome-LLM4AD.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 57,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-11-02",
    "authors": [
      {
        "authorId": "2264665546",
        "name": "Zhenjie Yang"
      },
      {
        "authorId": "1958998899",
        "name": "Xiaosong Jia"
      },
      {
        "authorId": "2263118181",
        "name": "Hongyang Li"
      },
      {
        "authorId": "2262560236",
        "name": "Junchi Yan"
      }
    ],
    "source": "semantic_scholar",
    "score": 130.90664515819628
  },
  {
    "paperId": "1eac02f299c171d17190fb257fc1f4c20f0d08ac",
    "url": "https://www.semanticscholar.org/paper/1eac02f299c171d17190fb257fc1f4c20f0d08ac",
    "title": "Large Language Models Are Neurosymbolic Reasoners",
    "abstract": "A wide range of real-world applications is characterized by their symbolic nature, necessitating a strong capability for symbolic reasoning. This paper investigates the potential application of Large Language Models (LLMs) as symbolic reasoners. We focus on text-based games, significant benchmarks for agents with natural language capabilities, particularly in symbolic tasks like math, map reading, sorting, and applying common sense in text-based worlds. To facilitate these agents, we propose an LLM agent designed to tackle symbolic challenges and achieve in-game objectives. We begin by initializing the LLM agent and informing it of its role. The agent then receives observations and a set of valid actions from the text-based games, along with a specific symbolic module. With these inputs, the LLM agent chooses an action and interacts with the game environments. Our experimental results demonstrate that our method significantly enhances the capability of LLMs as automated agents for symbolic reasoning, and our LLM agent is effective in text-based games involving symbolic tasks, achieving an average performance of 88% across all tasks.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2024,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-01-17",
    "authors": [
      {
        "authorId": "2277211659",
        "name": "Meng Fang"
      },
      {
        "authorId": "2277236582",
        "name": "Shilong Deng"
      },
      {
        "authorId": "2213717624",
        "name": "Yudi Zhang"
      },
      {
        "authorId": "2110009929",
        "name": "Zijing Shi"
      },
      {
        "authorId": "2277188371",
        "name": "Ling Chen"
      },
      {
        "authorId": "1691997",
        "name": "Mykola Pechenizkiy"
      },
      {
        "authorId": "2257572528",
        "name": "Jun Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.49820016084324
  },
  {
    "paperId": "13c85adfa950651ffcd91ef3018fa30801b74472",
    "url": "https://www.semanticscholar.org/paper/13c85adfa950651ffcd91ef3018fa30801b74472",
    "title": "Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration",
    "abstract": "Conversational systems based on Large Language Models (LLMs), such as ChatGPT, show exceptional proficiency in context understanding and response generation. However, despite their impressive capabilities, they still possess limitations, such as providing randomly-guessed answers to ambiguous queries or failing to refuse users' requests, both of which are considered aspects of a conversational agent's proactivity. This raises the question of whether LLM-based conversational systems are equipped to handle proactive dialogue problems. In this work, we conduct a comprehensive analysis of LLM-based conversational systems, specifically focusing on three aspects of proactive dialogue systems: clarification, target-guided, and non-collaborative dialogues. To trigger the proactivity of LLMs, we propose the Proactive Chain-of-Thought prompting scheme, which augments LLMs with the goal planning capability over descriptive reasoning chains. Empirical findings are discussed to promote future studies on LLM-based proactive dialogue systems.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 54,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.13626",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-23",
    "authors": [
      {
        "authorId": "145843537",
        "name": "Yang Deng"
      },
      {
        "authorId": "39165620",
        "name": "Wenqiang Lei"
      },
      {
        "authorId": "22642319",
        "name": "Hongru Wang"
      },
      {
        "authorId": "143779329",
        "name": "Tat-seng Chua"
      }
    ],
    "source": "semantic_scholar",
    "score": 130.10999777848707
  },
  {
    "paperId": "4993258852711c4e3d0011325ac3db680eae84f4",
    "url": "https://www.semanticscholar.org/paper/4993258852711c4e3d0011325ac3db680eae84f4",
    "title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models",
    "abstract": "Most of the existing Large Language Model (LLM) benchmarks on scientific problem reasoning focus on problems grounded in high-school subjects and are confined to elementary algebraic operations. To systematically examine the reasoning capabilities required for solving complex scientific problems, we introduce an expansive benchmark suite SciBench for LLMs. SciBench contains a carefully curated dataset featuring a range of collegiate-level scientific problems from mathematics, chemistry, and physics domains. Based on the dataset, we conduct an in-depth benchmarking study of representative open-source and proprietary LLMs with various prompting strategies. The results reveal that the current LLMs fall short of delivering satisfactory performance, with the best overall score of merely 43.22%. Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms the others and some strategies that demonstrate improvements in certain problem-solving skills could result in declines in other skills. We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.",
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "citationCount": 54,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.10635",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-07-20",
    "authors": [
      {
        "authorId": "2224092326",
        "name": "Xiaoxuan Wang"
      },
      {
        "authorId": "3407296",
        "name": "Ziniu Hu"
      },
      {
        "authorId": "2887562",
        "name": "Pan Lu"
      },
      {
        "authorId": "2653121",
        "name": "Yanqiao Zhu"
      },
      {
        "authorId": "47540245",
        "name": "Jieyu Zhang"
      },
      {
        "authorId": "2224018745",
        "name": "Satyen Subramaniam"
      },
      {
        "authorId": "2224017080",
        "name": "Arjun R. Loomba"
      },
      {
        "authorId": "2145408511",
        "name": "Shichang Zhang"
      },
      {
        "authorId": "2109461904",
        "name": "Yizhou Sun"
      },
      {
        "authorId": "2158624285",
        "name": "Wei Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 130.10999777848707
  },
  {
    "paperId": "735ea38114bd2406a8bbc7f060cba1fc7a254d89",
    "url": "https://www.semanticscholar.org/paper/735ea38114bd2406a8bbc7f060cba1fc7a254d89",
    "title": "Semantic anomaly detection with large language models",
    "abstract": null,
    "venue": "Autonomous Robots",
    "year": 2023,
    "citationCount": 51,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-18",
    "authors": [
      {
        "authorId": "71578349",
        "name": "Amine Elhafsi"
      },
      {
        "authorId": "2104277267",
        "name": "Rohan Sinha"
      },
      {
        "authorId": "1742185866",
        "name": "Christopher Agia"
      },
      {
        "authorId": "1868195",
        "name": "E. Schmerling"
      },
      {
        "authorId": "1754767",
        "name": "I. Nesnas"
      },
      {
        "authorId": "1696085",
        "name": "M. Pavone"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.26865577872141
  },
  {
    "paperId": "490d8006851b1562cfd9ec1f057471f2868289d1",
    "url": "https://www.semanticscholar.org/paper/490d8006851b1562cfd9ec1f057471f2868289d1",
    "title": "Rethinking with Retrieval: Faithful Large Language Model Inference",
    "abstract": "Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.",
    "venue": "arXiv.org",
    "year": 2022,
    "citationCount": 139,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2301.00303",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-12-31",
    "authors": [
      {
        "authorId": "7146703",
        "name": "Hangfeng He"
      },
      {
        "authorId": "2111112132",
        "name": "Hongming Zhang"
      },
      {
        "authorId": "144590225",
        "name": "D. Roth"
      }
    ],
    "source": "semantic_scholar",
    "score": 144.12463633913956
  },
  {
    "paperId": "0ef6788f630ed121e52265b048fcdf97a930e923",
    "url": "https://www.semanticscholar.org/paper/0ef6788f630ed121e52265b048fcdf97a930e923",
    "title": "Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions",
    "abstract": "LLMs have demonstrated impressive performance in answering medical questions, such as achieving passing scores on medical licensing examinations. However, medical board exam or general clinical questions do not capture the complexity of realistic clinical cases. Moreover, the lack of reference explanations means we cannot easily evaluate the reasoning of model decisions, a crucial component of supporting doctors in making complex medical decisions. To address these challenges, we construct two new datasets: JAMA Clinical Challenge and Medbullets. JAMA Clinical Challenge consists of questions based on challenging clinical cases, while Medbullets comprises simulated clinical questions. Both datasets are structured as multiple-choice question-answering tasks, accompanied by expert-written explanations. We evaluate seven LLMs on the two datasets using various prompts. Experiments demonstrate that our datasets are harder than previous benchmarks. Human and automatic evaluations of model-generated explanations provide insights into the promise and deficiency of LLMs for explainable medical QA.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 14,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-28",
    "authors": [
      {
        "authorId": "7315244",
        "name": "Hanjie Chen"
      },
      {
        "authorId": "2287924500",
        "name": "Zhouxiang Fang"
      },
      {
        "authorId": "2287921199",
        "name": "Yash Singla"
      },
      {
        "authorId": "1478928280",
        "name": "Mark Dredze"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "7eece37709dceba5086f48dc43ac1a69d0427486",
    "url": "https://www.semanticscholar.org/paper/7eece37709dceba5086f48dc43ac1a69d0427486",
    "title": "Joint Knowledge Graph and Large Language Model for Fault Diagnosis and Its Application in Aviation Assembly",
    "abstract": "In complex assembly industry settings, fault localization involves rapidly and accurately identifying the source of a fault and obtaining a troubleshooting solution based on fault symptoms. This study proposes a knowledge-enhanced joint model that incorporates aviation assembly knowledge graph (KG) embedding into large language models (LLMs). This model utilizes graph-structured Big Data within KGs to conduct prefix-tuning of the LLMs. The KGs for prefix-tuning enable an online reconfiguration of the LLMs, which avoids a massive computational load. Through the subgraph embedding learning process, the specialized knowledge of the joint model within the aviation assembly domain, especially in fault localization, is strengthened. In the context of aviation assembly functional testing, the joint model can generate knowledge subgraphs, fuse knowledge through retrieval augmentation, and ultimately provide knowledge-based reasoning responses. In practical industrial scenario experiments, the joint enhancement model demonstrates an accuracy of 98.5% for fault diagnosis and troubleshooting schemes.",
    "venue": "IEEE Transactions on Industrial Informatics",
    "year": 2024,
    "citationCount": 14,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-01",
    "authors": [
      {
        "authorId": "2217848572",
        "name": "Peifeng Liu"
      },
      {
        "authorId": "2072789781",
        "name": "Lu Qian"
      },
      {
        "authorId": "9873270",
        "name": "Xingwei Zhao"
      },
      {
        "authorId": "2242957770",
        "name": "Bo Tao"
      }
    ],
    "source": "semantic_scholar",
    "score": 116.62075301653314
  },
  {
    "paperId": "96ea9806a0db5d3aef2fbaa00a29f14b71831306",
    "url": "https://www.semanticscholar.org/paper/96ea9806a0db5d3aef2fbaa00a29f14b71831306",
    "title": "A Survey on Evaluation of Multimodal Large Language Models",
    "abstract": "Multimodal Large Language Models (MLLMs) mimic human perception and reasoning system by integrating powerful Large Language Models (LLMs) with various modality encoders (e.g., vision, audio), positioning LLMs as the\"brain\"and various modality encoders as sensory organs. This framework endows MLLMs with human-like capabilities, and suggests a potential pathway towards achieving artificial general intelligence (AGI). With the emergence of all-round MLLMs like GPT-4V and Gemini, a multitude of evaluation methods have been developed to assess their capabilities across different dimensions. This paper presents a systematic and comprehensive review of MLLM evaluation methods, covering the following key aspects: (1) the background of MLLMs and their evaluation; (2)\"what to evaluate\"that reviews and categorizes existing MLLM evaluation tasks based on the capabilities assessed, including general multimodal recognition, perception, reasoning and trustworthiness, and domain-specific applications such as socioeconomic, natural sciences and engineering, medical usage, AI agent, remote sensing, video and audio processing, 3D point cloud analysis, and others; (3)\"where to evaluate\"that summarizes MLLM evaluation benchmarks into general and specific benchmarks; (4)\"how to evaluate\"that reviews and illustrates MLLM evaluation steps and metrics; Our overarching goal is to provide valuable insights for researchers in the field of MLLM evaluation, thereby facilitating the development of more capable and reliable MLLMs. We emphasize that evaluation should be regarded as a critical discipline, essential for advancing the field of MLLMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 14,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-08-28",
    "authors": [
      {
        "authorId": "2115941903",
        "name": "Jiaxing Huang"
      },
      {
        "authorId": "2276743977",
        "name": "Jingyi Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "737b7de8abf6768a71a96e893c41b1bda758485c",
    "url": "https://www.semanticscholar.org/paper/737b7de8abf6768a71a96e893c41b1bda758485c",
    "title": "Interactive computer-aided diagnosis on medical image using large language models",
    "abstract": null,
    "venue": "Communications Engineer",
    "year": 2024,
    "citationCount": 14,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-17",
    "authors": [
      {
        "authorId": "2151487856",
        "name": "Sheng Wang"
      },
      {
        "authorId": "15594254",
        "name": "Zihao Zhao"
      },
      {
        "authorId": "2251555215",
        "name": "Ouyang Xi"
      },
      {
        "authorId": "2273644125",
        "name": "Tianming Liu"
      },
      {
        "authorId": "2260732131",
        "name": "Qian Wang"
      },
      {
        "authorId": "2239088988",
        "name": "Dinggang Shen"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.62075301653314
  },
  {
    "paperId": "992c554b1bf343eef3509579930b2552f1b6f1db",
    "url": "https://www.semanticscholar.org/paper/992c554b1bf343eef3509579930b2552f1b6f1db",
    "title": "Calibrating Large Language Models with Sample Consistency",
    "abstract": "Accurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application. However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale. In this work, we explore the potential of deriving confidence from the distribution of multiple randomly sampled model generations, via three measures of consistency. We perform an extensive evaluation across various open and closed-source models on nine reasoning datasets. Results show that consistency-based calibration methods outperform existing post-hoc approaches. Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult. Moreover, confidence scores obtained from consistency have the potential to enhance model performance. Finally, we offer practical guidance on choosing suitable consistency metrics for calibration, tailored to the characteristics of various LMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 14,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-21",
    "authors": [
      {
        "authorId": "1904906987",
        "name": "Qing Lyu"
      },
      {
        "authorId": "2266467511",
        "name": "Kumar Shridhar"
      },
      {
        "authorId": "8805254",
        "name": "Chaitanya Malaviya"
      },
      {
        "authorId": "2258934061",
        "name": "Li Zhang"
      },
      {
        "authorId": "51131518",
        "name": "Yanai Elazar"
      },
      {
        "authorId": "2261389843",
        "name": "Niket Tandon"
      },
      {
        "authorId": "2817917",
        "name": "Marianna Apidianaki"
      },
      {
        "authorId": "2790926",
        "name": "Mrinmaya Sachan"
      },
      {
        "authorId": "1763608",
        "name": "Chris Callison-Burch"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "7a57a52e1a273799bed7c882bc12177ca89609ab",
    "url": "https://www.semanticscholar.org/paper/7a57a52e1a273799bed7c882bc12177ca89609ab",
    "title": "Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems",
    "abstract": "Large language models (LLMs) and foundation models have been recently touted as a game-changer for 6 G systems. However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language models that were designed for natural language processing (NLP) applications. To address this challenge and create wireless-centric foundation models, this paper presents a comprehensive vision on how to design universal foundation models that are tailored towards the unique needs of next-generation wireless systems, thereby paving the way towards the deployment of artificial intelligence (AI)-native networks. Diverging from NLP-based foundation models, the proposed framework promotes the design of large multi-modal models (LMMs) fostered by three key capabilities: 1) processing of multi-modal sensing data, 2) grounding of physical symbol representations in real-world wireless systems using causal reasoning and retrieval-augmented generation (RAG), and 3) enabling instructibility from the wireless environment feedback to facilitate dynamic network adaptation thanks to logical and mathematical reasoning facilitated by neuro-symbolic AI. In essence, these properties enable the proposed LMM framework to build universal capabilities that cater to various cross-layer networking tasks and alignment of intents across different domains. Preliminary results from experimental evaluation demonstrate the efficacy of grounding using RAG in LMMs, and showcase the alignment of LMMs with wireless system designs. Furthermore, the enhanced rationale exhibited in the responses to mathematical questions by LMMs, compared to vanilla LLMs, demonstrates the logical and mathematical reasoning capabilities inherent in LMMs. Building on those results, we present a sequel of open questions and challenges for LMMs. We then conclude with a set of recommendations that ignite the path towards LMM-empowered AI-native systems.",
    "venue": "IEEE Network",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2402.01748",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-30",
    "authors": [
      {
        "authorId": "134770854",
        "name": "Shengzhe Xu"
      },
      {
        "authorId": "2275603455",
        "name": "Christo Kurisummoottil Thomas"
      },
      {
        "authorId": "2119144208",
        "name": "Omar Hashash"
      },
      {
        "authorId": "50027530",
        "name": "N. Muralidhar"
      },
      {
        "authorId": "2269149200",
        "name": "Walid Saad"
      },
      {
        "authorId": "2263149583",
        "name": "Naren Ramakrishnan"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.47424036192305
  },
  {
    "paperId": "3fbfbee501068681ca3c087f1a1b1d45a1308a07",
    "url": "https://www.semanticscholar.org/paper/3fbfbee501068681ca3c087f1a1b1d45a1308a07",
    "title": "From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities",
    "abstract": "Multi-modal Large Language Models (MLLMs) have shown impressive abilities in generating reasonable responses with respect to multi-modal contents. However, there is still a wide gap between the performance of recent MLLM-based applications and the expectation of the broad public, even though the most powerful OpenAI's GPT-4 and Google's Gemini have been deployed. This paper strives to enhance understanding of the gap through the lens of a qualitative study on the generalizability, trustworthiness, and causal reasoning capabilities of recent proprietary and open-source MLLMs across four modalities: ie, text, code, image, and video, ultimately aiming to improve the transparency of MLLMs. We believe these properties are several representative factors that define the reliability of MLLMs, in supporting various downstream applications. To be specific, we evaluate the closed-source GPT-4 and Gemini and 6 open-source LLMs and MLLMs. Overall we evaluate 230 manually designed cases, where the qualitative results are then summarized into 12 scores (ie, 4 modalities times 3 properties). In total, we uncover 14 empirical findings that are useful to understand the capabilities and limitations of both proprietary and open-source MLLMs, towards more reliable downstream multi-modal applications.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-26",
    "authors": [
      {
        "authorId": "2257179663",
        "name": "Chaochao Lu"
      },
      {
        "authorId": "2288537472",
        "name": "Chen Qian"
      },
      {
        "authorId": "2281647653",
        "name": "Guodong Zheng"
      },
      {
        "authorId": "2265583827",
        "name": "Hongxing Fan"
      },
      {
        "authorId": "2280292237",
        "name": "Hongzhi Gao"
      },
      {
        "authorId": "2288673056",
        "name": "Jie Zhang"
      },
      {
        "authorId": "2280138285",
        "name": "Jing Shao"
      },
      {
        "authorId": "2281710701",
        "name": "Jingyi Deng"
      },
      {
        "authorId": "2281643448",
        "name": "Jinlan Fu"
      },
      {
        "authorId": "2266421899",
        "name": "Kexin Huang"
      },
      {
        "authorId": "2115468491",
        "name": "Kunchang Li"
      },
      {
        "authorId": "2280261471",
        "name": "Lijun Li"
      },
      {
        "authorId": "2141353278",
        "name": "Limin Wang"
      },
      {
        "authorId": "2053947248",
        "name": "Lu Sheng"
      },
      {
        "authorId": "2278519688",
        "name": "Meiqiu Chen"
      },
      {
        "authorId": "2247626759",
        "name": "Ming Zhang"
      },
      {
        "authorId": "2281642501",
        "name": "Qibing Ren"
      },
      {
        "authorId": "2262744724",
        "name": "SI-YIN Chen"
      },
      {
        "authorId": "2067331064",
        "name": "Tao Gui"
      },
      {
        "authorId": "2254269925",
        "name": "Wanli Ouyang"
      },
      {
        "authorId": "47903936",
        "name": "Yali Wang"
      },
      {
        "authorId": "2266238818",
        "name": "Yan Teng"
      },
      {
        "authorId": "2266422261",
        "name": "Yaru Wang"
      },
      {
        "authorId": "2263510675",
        "name": "Yi Wang"
      },
      {
        "authorId": "2118918324",
        "name": "Yinan He"
      },
      {
        "authorId": "2266364817",
        "name": "Yingchun Wang"
      },
      {
        "authorId": "2266363141",
        "name": "Yixu Wang"
      },
      {
        "authorId": "2280256177",
        "name": "Yongting Zhang"
      },
      {
        "authorId": "2265493981",
        "name": "Yu Qiao"
      },
      {
        "authorId": "2281737654",
        "name": "Yujiong Shen"
      },
      {
        "authorId": "2281640548",
        "name": "Yurong Mou"
      },
      {
        "authorId": "2130055310",
        "name": "Yuxi Chen"
      },
      {
        "authorId": "2218417690",
        "name": "Zaibin Zhang"
      },
      {
        "authorId": "2144146362",
        "name": "Zhelun Shi"
      },
      {
        "authorId": "13050405",
        "name": "Zhen-fei Yin"
      },
      {
        "authorId": "2265545407",
        "name": "Zhipin Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "c58d72c073ce24cf4716b1c26fbf5ae919e2cc78",
    "url": "https://www.semanticscholar.org/paper/c58d72c073ce24cf4716b1c26fbf5ae919e2cc78",
    "title": "Large language models for preventing medication direction errors in online pharmacies",
    "abstract": null,
    "venue": "Nature Network Boston",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-04-25",
    "authors": [
      {
        "authorId": "2298445820",
        "name": "Cristobal Pais"
      },
      {
        "authorId": "2298449289",
        "name": "Jianfeng Liu"
      },
      {
        "authorId": "2298447495",
        "name": "Robert Voigt"
      },
      {
        "authorId": "2298604464",
        "name": "Vin Gupta"
      },
      {
        "authorId": "2298445818",
        "name": "Elizabeth Wade"
      },
      {
        "authorId": "2298446156",
        "name": "Mohsen Bayati"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.58585994422887
  },
  {
    "paperId": "6eb905faa9ff2e1f191780695f774250b86d6f2b",
    "url": "https://www.semanticscholar.org/paper/6eb905faa9ff2e1f191780695f774250b86d6f2b",
    "title": "Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning",
    "abstract": "As the applicability of Large Language Models (LLMs) extends beyond traditional text processing tasks, there is a burgeoning interest in their potential to excel in planning and reasoning assignments, realms traditionally reserved for System 2 cognitive competencies. Despite their perceived versatility, the research community is still unraveling effective strategies to harness these models in such complex domains. The recent discourse introduced by the paper on LLM Modulo marks a significant stride, proposing a conceptual framework that enhances the integration of LLMs into diverse planning and reasoning activities. This workshop paper delves into the practical application of this framework within the domain of travel planning, presenting a specific instance of its implementation. We are using the Travel Planning benchmark by the OSU NLP group, a benchmark for evaluating the performance of LLMs in producing valid itineraries based on user queries presented in natural language. While popular methods of enhancing the reasoning abilities of LLMs such as Chain of Thought, ReAct, and Reflexion achieve a meager 0%, 0.6%, and 0% with GPT3.5-Turbo respectively, our operationalization of the LLM-Modulo framework for TravelPlanning domain provides a remarkable improvement, enhancing baseline performances by 4.6x for GPT4-Turbo and even more for older models like GPT3.5-Turbo from 0% to 5%. Furthermore, we highlight the other useful roles of LLMs in the planning pipeline, as suggested in LLM-Modulo, which can be reliably operationalized such as extraction of useful critics and reformulator for critics.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-31",
    "authors": [
      {
        "authorId": "2224613953",
        "name": "Atharva Gundawar"
      },
      {
        "authorId": "151500192",
        "name": "Mudit Verma"
      },
      {
        "authorId": "144190085",
        "name": "L. Guan"
      },
      {
        "authorId": "144982263",
        "name": "Karthik Valmeekam"
      },
      {
        "authorId": "46211062",
        "name": "Siddhant Bhambri"
      },
      {
        "authorId": "2047340230",
        "name": "Subbarao Kambhampati"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "275a3955a83867dd36a3683788e0e053e00f8a89",
    "url": "https://www.semanticscholar.org/paper/275a3955a83867dd36a3683788e0e053e00f8a89",
    "title": "Collaborative Enhancement of Consistency and Accuracy in US Diagnosis of Thyroid Nodules Using Large Language Models.",
    "abstract": "Background Large language models (LLMs) hold substantial promise for medical imaging interpretation. However, there is a lack of studies on their feasibility in handling reasoning questions associated with medical diagnosis. Purpose To investigate the viability of leveraging three publicly available LLMs to enhance consistency and diagnostic accuracy in medical imaging based on standardized reporting, with pathology as the reference standard. Materials and Methods US images of thyroid nodules with pathologic results were retrospectively collected from a tertiary referral hospital between July 2022 and December 2022 and used to evaluate malignancy diagnoses generated by three LLMs-OpenAI's ChatGPT 3.5, ChatGPT 4.0, and Google's Bard. Inter- and intra-LLM agreement of diagnosis were evaluated. Then, diagnostic performance, including accuracy, sensitivity, specificity, and area under the receiver operating characteristic curve (AUC), was evaluated and compared for the LLMs and three interactive approaches: human reader combined with LLMs, image-to-text model combined with LLMs, and an end-to-end convolutional neural network model. Results A total of 1161 US images of thyroid nodules (498 benign, 663 malignant) from 725 patients (mean age, 42.2 years ± 14.1 [SD]; 516 women) were evaluated. ChatGPT 4.0 and Bard displayed substantial to almost perfect intra-LLM agreement (κ range, 0.65-0.86 [95% CI: 0.64, 0.86]), while ChatGPT 3.5 showed fair to substantial agreement (κ range, 0.36-0.68 [95% CI: 0.36, 0.68]). ChatGPT 4.0 had an accuracy of 78%-86% (95% CI: 76%, 88%) and sensitivity of 86%-95% (95% CI: 83%, 96%), compared with 74%-86% (95% CI: 71%, 88%) and 74%-91% (95% CI: 71%, 93%), respectively, for Bard. Moreover, with ChatGPT 4.0, the image-to-text-LLM strategy exhibited an AUC (0.83 [95% CI: 0.80, 0.85]) and accuracy (84% [95% CI: 82%, 86%]) comparable to those of the human-LLM interaction strategy with two senior readers and one junior reader and exceeding those of the human-LLM interaction strategy with one junior reader. Conclusion LLMs, particularly integrated with image-to-text approaches, show potential in enhancing diagnostic medical imaging. ChatGPT 4.0 was optimal for consistency and diagnostic accuracy when compared with Bard and ChatGPT 3.5. © RSNA, 2024 Supplemental material is available for this article.",
    "venue": "Radiology",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-01",
    "authors": [
      {
        "authorId": "1678966811",
        "name": "Shaohong Wu"
      },
      {
        "authorId": "2272038393",
        "name": "Wen-Juan Tong"
      },
      {
        "authorId": "2127991969",
        "name": "Ming-De Li"
      },
      {
        "authorId": "28890237",
        "name": "Hang-tong Hu"
      },
      {
        "authorId": "2187182790",
        "name": "Xiao-zhou Lu"
      },
      {
        "authorId": "2185218332",
        "name": "Ze-Rong Huang"
      },
      {
        "authorId": "2290915880",
        "name": "Xin-Xin Lin"
      },
      {
        "authorId": "2290897544",
        "name": "Rui-Fang Lu"
      },
      {
        "authorId": "2261915754",
        "name": "Ming-De Lu"
      },
      {
        "authorId": "6457299",
        "name": "Li-da Chen"
      },
      {
        "authorId": "2290866279",
        "name": "Wei Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "3a588665fb59801fcbdce825ed7a4cf59984567f",
    "url": "https://www.semanticscholar.org/paper/3a588665fb59801fcbdce825ed7a4cf59984567f",
    "title": "Internal Consistency and Self-Feedback in Large Language Models: A Survey",
    "abstract": "Large language models (LLMs) often exhibit deficient reasoning or generate hallucinations. To address these, studies prefixed with\"Self-\"such as Self-Consistency, Self-Improve, and Self-Refine have been initiated. They share a commonality: involving LLMs evaluating and updating themselves. Nonetheless, these efforts lack a unified perspective on summarization, as existing surveys predominantly focus on categorization. In this paper, we use a unified perspective of internal consistency, offering explanations for reasoning deficiencies and hallucinations. Internal consistency refers to the consistency in expressions among LLMs' latent, decoding, or response layers based on sampling methodologies. Then, we introduce an effective theoretical framework capable of mining internal consistency, named Self-Feedback. This framework consists of two modules: Self-Evaluation and Self-Update. The former captures internal consistency signals, while the latter leverages the signals to enhance either the model's response or the model itself. This framework has been employed in numerous studies. We systematically classify these studies by tasks and lines of work; summarize relevant evaluation methods and benchmarks; and delve into the concern,\"Does Self-Feedback Really Work?\"We also propose several critical viewpoints, including the\"Hourglass Evolution of Internal Consistency\",\"Consistency Is (Almost) Correctness\"hypothesis, and\"The Paradox of Latent and Explicit Reasoning\". The relevant resources are open-sourced at https://github.com/IAAR-Shanghai/ICSFSurvey.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-07-19",
    "authors": [
      {
        "authorId": "2268495952",
        "name": "Xun Liang"
      },
      {
        "authorId": "2268434524",
        "name": "Shichao Song"
      },
      {
        "authorId": "2302370745",
        "name": "Zifan Zheng"
      },
      {
        "authorId": "2284861141",
        "name": "Hanyu Wang"
      },
      {
        "authorId": "2278590555",
        "name": "Qingchen Yu"
      },
      {
        "authorId": "2268429288",
        "name": "Xunkai Li"
      },
      {
        "authorId": "2312235766",
        "name": "Rong-Hua Li"
      },
      {
        "authorId": "2268399953",
        "name": "Feiyu Xiong"
      },
      {
        "authorId": "2268429641",
        "name": "Zhiyu Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "2dc0c0a00544cd306abec905a98e4701e2401ba2",
    "url": "https://www.semanticscholar.org/paper/2dc0c0a00544cd306abec905a98e4701e2401ba2",
    "title": "OpenTab: Advancing Large Language Models as Open-domain Table Reasoners",
    "abstract": "Large Language Models (LLMs) trained on large volumes of data excel at various natural language tasks, but they cannot handle tasks requiring knowledge that has not been trained on previously. One solution is to use a retriever that fetches relevant information to expand LLM's knowledge scope. However, existing textual-oriented retrieval-based LLMs are not ideal on structured table data due to diversified data modalities and large table sizes. In this work, we propose OpenTab, an open-domain table reasoning framework powered by LLMs. Overall, OpenTab leverages table retriever to fetch relevant tables and then generates SQL programs to parse the retrieved tables efficiently. Utilizing the intermediate data derived from the SQL executions, it conducts grounded inference to produce accurate response. Extensive experimental evaluation shows that OpenTab significantly outperforms baselines in both open- and closed-domain settings, achieving up to 21.5% higher accuracy. We further run ablation studies to validate the efficacy of our proposed designs of the system.",
    "venue": "International Conference on Learning Representations",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-22",
    "authors": [
      {
        "authorId": "2284981676",
        "name": "Kezhi Kong"
      },
      {
        "authorId": "2258747730",
        "name": "Jiani Zhang"
      },
      {
        "authorId": "2223163421",
        "name": "Zhengyuan Shen"
      },
      {
        "authorId": "2057595515",
        "name": "Balasubramaniam Srinivasan"
      },
      {
        "authorId": "2223137915",
        "name": "Chuan Lei"
      },
      {
        "authorId": "2263543517",
        "name": "Christos Faloutsos"
      },
      {
        "authorId": "145344187",
        "name": "H. Rangwala"
      },
      {
        "authorId": "50877490",
        "name": "G. Karypis"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "e3ca1f399ed5ee21d0b73422f8837fba0985665f",
    "url": "https://www.semanticscholar.org/paper/e3ca1f399ed5ee21d0b73422f8837fba0985665f",
    "title": "OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models",
    "abstract": "Object navigation (ObjectNav) requires an agent to navigate through unseen environments to find queried objects. Many previous methods attempted to solve this task by relying on supervised or reinforcement learning, where they are trained on limited household datasets with close-set objects. However, two key challenges are unsolved: understanding free-form natural language instructions that demand open-set objects, and generalizing to new environments in a zero-shot manner. Aiming to solve the two challenges, in this paper, we propose OpenFMNav, an Open-set Foundation Model based framework for zero-shot object Navigation. We first unleash the reasoning abilities of large language models (LLMs) to extract proposed objects from natural language instructions that meet the user's demand. We then leverage the generalizability of large vision language models (VLMs) to actively discover and detect candidate objects from the scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting common sense reasoning on VSSM, our method can perform effective language-guided exploration and exploitation of the scene and finally reach the goal. By leveraging the reasoning and generalizing abilities of foundation models, our method can understand free-form human instructions and perform effective open-set zero-shot navigation in diverse environments. Extensive experiments on the HM3D ObjectNav benchmark show that our method surpasses all the strong baselines on all metrics, proving our method's effectiveness. Furthermore, we perform real robot demonstrations to validate our method's open-set-ness and generalizability to real-world environments.",
    "venue": "NAACL-HLT",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-16",
    "authors": [
      {
        "authorId": "2256991055",
        "name": "Yuxuan Kuang"
      },
      {
        "authorId": "2284652482",
        "name": "Hai Lin"
      },
      {
        "authorId": "2152155718",
        "name": "Meng Jiang"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "d552ca135ea18a688b385984f8aab983d2098dc7",
    "url": "https://www.semanticscholar.org/paper/d552ca135ea18a688b385984f8aab983d2098dc7",
    "title": "Can Small Language Models be Good Reasoners for Sequential Recommendation?",
    "abstract": "Large language models (LLMs) open up new horizons for sequential recommendations, owing to their remarkable language comprehension and generation capabilities. However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real sequential recommender systems. In this paper, we propose a novel Step-by-step knowLedge dIstillation fraMework for recommendation (SLIM), paving a promising path for sequential recommenders to enjoy the exceptional reasoning capabilities of LLMs in a \"slim\" (i.e. resource-efficient) manner. We introduce CoT prompting based on user behavior sequences for the larger teacher model. The rationales generated by the teacher model are then utilized as labels to distill the downstream smaller student model (e.g., LLaMA2-7B). In this way, the student model acquires the step-by-step reasoning capabilities in recommendation tasks. We encode the generated rationales from the student model into a dense vector, which empowers recommendation in both ID-based and ID-agnostic scenarios. Extensive experiments demonstrate the effectiveness of SLIM over state-of-the-art baselines, and further analysis showcasing its ability to generate meaningful recommendation reasoning at affordable costs.",
    "venue": "The Web Conference",
    "year": 2024,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2024-03-07",
    "authors": [
      {
        "authorId": "2286603430",
        "name": "Yuling Wang"
      },
      {
        "authorId": "2290178133",
        "name": "Changxin Tian"
      },
      {
        "authorId": "2279160677",
        "name": "Binbin Hu"
      },
      {
        "authorId": "2281950230",
        "name": "Yanhua Yu"
      },
      {
        "authorId": "2284032340",
        "name": "Ziqi Liu"
      },
      {
        "authorId": "2266809812",
        "name": "Zhiqiang Zhang"
      },
      {
        "authorId": "2272742832",
        "name": "Jun Zhou"
      },
      {
        "authorId": "2286323497",
        "name": "Liang Pang"
      },
      {
        "authorId": "2299437974",
        "name": "Xiao Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "605108c38df32011b3f65a75e24aab3a14b3a72c",
    "url": "https://www.semanticscholar.org/paper/605108c38df32011b3f65a75e24aab3a14b3a72c",
    "title": "LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration",
    "abstract": "Large language models (LLMs) have demonstrated impressive performance in understanding language and executing complex reasoning tasks. However, LLMs with long context windows have been notorious for their expensive training costs and high inference latency. Even the most advanced models such as GPT-4 and Claude2 often make mistakes when processing inputs of over $100k$ tokens, a phenomenon also known as \\textit{lost in the middle}. In this paper, we propose \\textsc{LongAgent}, a method based on multi-agent collaboration, which scales LLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiority in long-text processing compared to GPT-4. In \\textsc{LongAgent}, a leader is responsible for understanding user intent and directing team members to acquire information from documents. Due to members' hallucinations, it is non-trivial for a leader to obtain accurate information from the responses of dozens to hundreds of members. To address this, we develop an \\textit{inter-member communication} mechanism to resolve response conflicts caused by hallucinations through information sharing. Our experimental results indicate that \\textsc{LongAgent} offers a promising alternative for long-text processing. The agent team instantiated with LLaMA-7B achieves significant improvements in tasks such as 128k-long text retrieval, multi-hop question answering, compared to GPT-4.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-18",
    "authors": [
      {
        "authorId": "2257315251",
        "name": "Jun Zhao"
      },
      {
        "authorId": "15190018",
        "name": "Can Zu"
      },
      {
        "authorId": "2290748445",
        "name": "Haotian Xu"
      },
      {
        "authorId": "2143773562",
        "name": "Yi Lu"
      },
      {
        "authorId": "2241408708",
        "name": "Wei He"
      },
      {
        "authorId": "2240473116",
        "name": "Yiwen Ding"
      },
      {
        "authorId": "2067331064",
        "name": "Tao Gui"
      },
      {
        "authorId": "2257376355",
        "name": "Qi Zhang"
      },
      {
        "authorId": "2257129989",
        "name": "Xuanjing Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "69dd4df178437efad68a675cf8742ed09e7d53c8",
    "url": "https://www.semanticscholar.org/paper/69dd4df178437efad68a675cf8742ed09e7d53c8",
    "title": "Empowering LLM to use Smartphone for Intelligent Task Automation",
    "abstract": "Mobile task automation is an attractive technique that aims to enable voice-based hands-free user interaction with smart-phones. However, existing approaches suffer from poor scalability due to the limited language understanding ability and the non-trivial manual efforts required from developers or end-users. The recent advance of large language models (LLMs) in language understanding and reasoning inspires us to re-think the problem from a model-centric perspective, where task preparation, comprehension, and execution are handled by a unified language model. In this work, we introduce Auto-Droid, a mobile task automation system that can handle arbitrary tasks on any Android application without manual efforts. The key insight is to combine the commonsense knowledge of LLMs and domain-specific knowledge of apps through automated dynamic analysis. The main components include a functionality-aware UI representation method that bridges the UI with the LLM, exploration-based memory injection techniques that augment the app-specific domain knowledge of LLM, and a multi-granularity query optimization module that reduces the cost of model inference. We integrate AutoDroid with off-the-shelf LLMs including online GPT-4/GPT-3.5 and on-device Vicuna, and evaluate its performance on a new benchmark for memory-augmented Android task automation with 158 common tasks. The results demonstrated that Auto-Droid is able to precisely generate actions with an accuracy of 90.9%, and complete tasks with a success rate of 71.3%, outperforming the GPT-4-powered baselines by 36.4% and 39.7%. The demo, benchmark suites, and source code of AutoDroid will be released at https://autodroid-sys.github.io/.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 46,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.15272",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2188861830",
        "name": "Hao Wen"
      },
      {
        "authorId": "2144416765",
        "name": "Yuanchun Li"
      },
      {
        "authorId": "2235520970",
        "name": "Guohong Liu"
      },
      {
        "authorId": "2294974757",
        "name": "Shanhui Zhao"
      },
      {
        "authorId": "2256865742",
        "name": "Tao Yu"
      },
      {
        "authorId": "2261394820",
        "name": "Toby Jia-Jun Li"
      },
      {
        "authorId": "2170664295",
        "name": "Shiqi Jiang"
      },
      {
        "authorId": "2235192744",
        "name": "Yunhao Liu"
      },
      {
        "authorId": "2108201285",
        "name": "Yaqin Zhang"
      },
      {
        "authorId": "2261731955",
        "name": "Yunxin Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 127.75221402565089
  },
  {
    "paperId": "94701d9c6cfc1aecc174ff62ccda939f790c1710",
    "url": "https://www.semanticscholar.org/paper/94701d9c6cfc1aecc174ff62ccda939f790c1710",
    "title": "ReGAL: Refactoring Programs to Discover Generalizable Abstractions",
    "abstract": "While large language models (LLMs) are increasingly being used for program synthesis, they lack the global view needed to develop useful abstractions; they generally predict programs one at a time, often repeating the same functionality. Generating redundant code from scratch is both inefficient and error-prone. To address this, we propose Refactoring for Generalizable Abstraction Learning (ReGAL), a gradient-free method for learning a library of reusable functions via code refactorization, i.e., restructuring code without changing its execution output. ReGAL learns from a small set of existing programs, iteratively verifying and refining its abstractions via execution. We find that the shared function libraries discovered by ReGAL make programs easier to predict across diverse domains. On five datasets -- LOGO graphics generation, Date reasoning, TextCraft (a Minecraft-based text-game) MATH, and TabMWP -- both open-source and proprietary LLMs improve in accuracy when predicting programs with ReGAL functions. For CodeLlama-13B, ReGAL results in absolute accuracy increases of 11.5% on LOGO, 26.1% on date understanding, and 8.1% on TextCraft, outperforming GPT-3.5 in two of three domains. Our analysis reveals ReGAL's abstractions encapsulate frequently-used subroutines as well as environment dynamics.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-01-29",
    "authors": [
      {
        "authorId": "2281825070",
        "name": "Elias Stengel-Eskin"
      },
      {
        "authorId": "1677896557",
        "name": "Archiki Prasad"
      },
      {
        "authorId": "2281826842",
        "name": "Mohit Bansal"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "411408008cb3f5347b3fcb8b4631b3c6b0c2ff2b",
    "url": "https://www.semanticscholar.org/paper/411408008cb3f5347b3fcb8b4631b3c6b0c2ff2b",
    "title": "TnT-LLM: Text Mining at Scale with Large Language Models",
    "abstract": "Transforming unstructured text into structured and meaningful forms, organized by useful category labels, is a fundamental step in text mining for downstream analysis and application. However, most existing methods for producing label taxonomies and building text-based label classifiers still rely heavily on domain expertise and manual curation, making the process expensive and time-consuming. This is particularly challenging when the label space is under-specified and large-scale data annotations are unavailable. In this paper, we address these challenges with Large Language Models (LLMs), whose prompt-based interface facilitates the induction and use of large-scale pseudo labels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate the process of end-to-end label generation and assignment with minimal human effort for any given use-case. In the first phase, we introduce a zero-shot, multi-stage reasoning approach which enables LLMs to produce and refine a label taxonomy iteratively. In the second phase, LLMs are used as data labelers that yield training samples so that lightweight supervised classifiers can be reliably built, deployed, and served at scale. We apply TnT-LLM to the analysis of user intent and conversational domain for Bing Copilot (formerly Bing Chat), an open-domain chat-based search engine. Extensive experiments using both human and automatic evaluation metrics demonstrate that TnT-LLM generates more accurate and relevant label taxonomies when compared against state-of-the-art baselines, and achieves a favorable balance between accuracy and efficiency for classification at scale. We also share our practical experiences and insights on the challenges and opportunities of using LLMs for large-scale text mining in real-world applications.",
    "venue": "Knowledge Discovery and Data Mining",
    "year": 2024,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-18",
    "authors": [
      {
        "authorId": "2266754711",
        "name": "Mengting Wan"
      },
      {
        "authorId": "2243180014",
        "name": "Tara Safavi"
      },
      {
        "authorId": "3001990",
        "name": "S. Jauhar"
      },
      {
        "authorId": "2292658604",
        "name": "Yujin Kim"
      },
      {
        "authorId": "2292200904",
        "name": "Scott Counts"
      },
      {
        "authorId": "2243181687",
        "name": "Jennifer Neville"
      },
      {
        "authorId": "2274318270",
        "name": "Siddharth Suri"
      },
      {
        "authorId": "2266488842",
        "name": "Chirag Shah"
      },
      {
        "authorId": "2264224760",
        "name": "Ryen W. White"
      },
      {
        "authorId": "2266801772",
        "name": "Longqi Yang"
      },
      {
        "authorId": "2243181473",
        "name": "Reid Andersen"
      },
      {
        "authorId": "2243178870",
        "name": "Georg Buscher"
      },
      {
        "authorId": "2292164509",
        "name": "Dhruv Joshi"
      },
      {
        "authorId": "46551415",
        "name": "N.Kasturi Rangan"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "db8c398681aba69c605f3528dcbb9f853a382f14",
    "url": "https://www.semanticscholar.org/paper/db8c398681aba69c605f3528dcbb9f853a382f14",
    "title": "CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments",
    "abstract": "The introduction of genome engineering technology has transformed biomedical research, making it possible to make precise changes to genetic information. However, creating an efficient gene-editing system requires a deep understanding of CRISPR technology, and the complex experimental systems under investigation. While Large Language Models (LLMs) have shown promise in various tasks, they often lack specific knowledge and struggle to accurately solve biological design problems. In this work, we introduce CRISPR-GPT, an LLM agent augmented with domain knowledge and external tools to automate and enhance the design process of CRISPR-based gene-editing experiments. CRISPR-GPT leverages the reasoning ability of LLMs to facilitate the process of selecting CRISPR systems, designing guide RNAs, recommending cellular delivery methods, drafting protocols, and designing validation experiments to confirm editing outcomes. We showcase the potential of CRISPR-GPT for assisting non-expert researchers with gene-editing experiments from scratch and validate the agent’s effectiveness in a real-world use case. Furthermore, we explore the ethical and regulatory considerations associated with automated gene-editing design, highlighting the need for responsible and transparent use of these tools. Our work aims to bridge the gap between biological researchers across various fields with CRISPR genome engineering technology and demonstrate the potential of LLM agents in facilitating complex biological discovery tasks.",
    "venue": "bioRxiv",
    "year": 2024,
    "citationCount": 11,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2404.18021",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Medicine",
      "Biology",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-27",
    "authors": [
      {
        "authorId": "2299118690",
        "name": "Yuanhao Qu"
      },
      {
        "authorId": "2242535459",
        "name": "Kaixuan Huang"
      },
      {
        "authorId": "1828816642",
        "name": "H. Cousins"
      },
      {
        "authorId": "2298884172",
        "name": "William A. Johnson"
      },
      {
        "authorId": "2224505530",
        "name": "Di Yin"
      },
      {
        "authorId": "2301196678",
        "name": "Mihir Shah"
      },
      {
        "authorId": "2298937433",
        "name": "Denny Zhou"
      },
      {
        "authorId": "2055661716",
        "name": "R. Altman"
      },
      {
        "authorId": "2145361446",
        "name": "Mengdi Wang"
      },
      {
        "authorId": "2274945270",
        "name": "Le Cong"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "5d879530c443dd06d3686f31d32cfe34c7ade9bc",
    "url": "https://www.semanticscholar.org/paper/5d879530c443dd06d3686f31d32cfe34c7ade9bc",
    "title": "Towards Interpretable Mental Health Analysis with Large Language Models",
    "abstract": "The latest large language models (LLMs) such as ChatGPT, exhibit strong capabilities in automated mental health analysis. However, existing relevant studies bear several limitations, including inadequate evaluations, lack of prompting strategies, and ignorance of exploring LLMs for explainability. To bridge these gaps, we comprehensively evaluate the mental health analysis and emotional reasoning ability of LLMs on 11 datasets across 5 tasks. We explore the effects of different prompting strategies with unsupervised and distantly supervised emotional information. Based on these prompts, we explore LLMs for interpretable mental health analysis by instructing them to generate explanations for each of their decisions. We convey strict human evaluations to assess the quality of the generated explanations, leading to a novel dataset with 163 human-assessed explanations. We benchmark existing automatic evaluation metrics on this dataset to guide future related works. According to the results, ChatGPT shows strong in-context learning ability but still has a significant gap with advanced task-specific methods. Careful prompt engineering with emotional cues and expert-written few-shot examples can also effectively improve performance on mental health analysis. In addition, ChatGPT generates explanations that approach human performance, showing its great potential in explainable mental health analysis.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 47,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.emnlp-main.370.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-04-06",
    "authors": [
      {
        "authorId": "2003396186",
        "name": "Kailai Yang"
      },
      {
        "authorId": "51394448",
        "name": "Shaoxiong Ji"
      },
      {
        "authorId": "9416328",
        "name": "Tianlin Zhang"
      },
      {
        "authorId": "2257034586",
        "name": "Qianqian Xie"
      },
      {
        "authorId": "13627871",
        "name": "Zi-Zhou Kuang"
      },
      {
        "authorId": "2240623492",
        "name": "Sophia Ananiadou"
      }
    ],
    "source": "semantic_scholar",
    "score": 128.06801516361838
  },
  {
    "paperId": "10158879cdb64ce7d3f7bb5572c4617ea808602e",
    "url": "https://www.semanticscholar.org/paper/10158879cdb64ce7d3f7bb5572c4617ea808602e",
    "title": "Receive, Reason, and React: Drive as You Say, With Large Language Models in Autonomous Vehicles",
    "abstract": "The fusion of human-centric design and artificial intelligence capabilities has opened up new possibilities for next-generation autonomous vehicles that go beyond traditional transportation. These vehicles can dynamically interact with passengers and adapt to their preferences. This article proposes a novel framework that leverages large language models (LLMs) to enhance the decision-making process in autonomous vehicles. By utilizing LLMs’ contextual understanding abilities with specialized tools, we aim to integrate the language and reasoning capabilities of LLMs into autonomous vehicles. Our research includes experiments in HighwayEnv, a collection of environments for autonomous driving and tactical decision-making tasks, to explore LLMs’ interpretation, interaction, and reasoning in various scenarios. We also examine some well-defined real-time personalized driving tasks, demonstrating how LLMs can influence driving behaviors based on drivers’ verbal commands. Our empirical results highlight the substantial advantages of utilizing chain-of-thought prompting, leading to improved driving decisions and showing the potential for LLMs to enhance personalized driving experiences through ongoing verbal feedback. The proposed framework aims to transform autonomous vehicle operations, offering personalized support, transparent decision making, and continuous learning to enhance safety and effectiveness. We achieve user-centric, transparent, and adaptive autonomous driving ecosystems supported by the integration of LLMs into autonomous vehicles. Experiment videos are available at https://youtube.com/playlist?list=PLgcRcf9w8BmLJi_fqTGq-7KCZsbpEIE4a&si=dhH9lgaeSmB5K94t.",
    "venue": "IEEE Intelligent Transportation Systems Magazine",
    "year": 2023,
    "citationCount": 48,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2310.08034",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-12",
    "authors": [
      {
        "authorId": "2242949023",
        "name": "Can Cui"
      },
      {
        "authorId": "2109267955",
        "name": "Yunsheng Ma"
      },
      {
        "authorId": "2258143637",
        "name": "Xu Cao"
      },
      {
        "authorId": "122298411",
        "name": "Wenqian Ye"
      },
      {
        "authorId": "2243360882",
        "name": "Ziran Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 134.3773044716594
  },
  {
    "paperId": "ca261cb681b082e90ca6c7a9d325b4265ed1dc28",
    "url": "https://www.semanticscholar.org/paper/ca261cb681b082e90ca6c7a9d325b4265ed1dc28",
    "title": "MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models",
    "abstract": "Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, we propose a novel prompting pipeline, named \\method, that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency. Our method enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge. Moreover, our method elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge. We evaluate our method on diverse question \\&answering tasks, especially in medical domains, and show significant improvements over baselines. We also introduce a new hallucination evaluation benchmark and analyze the effects of different components of our method. Our results demonstrate the effectiveness and robustness of our method in merging knowledge from LLMs and KGs for combined inference. To reproduce our results and extend the framework further, we make our codebase available at https://github.com/wyl-willing/MindMap.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 44,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.09729",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-08-17",
    "authors": [
      {
        "authorId": "2051265784",
        "name": "Yilin Wen"
      },
      {
        "authorId": "2108733162",
        "name": "Zifeng Wang"
      },
      {
        "authorId": "49991208",
        "name": "Jimeng Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 127.0999373465548
  },
  {
    "paperId": "5f58863dd6474d6f127be995b5871e7c60f2792f",
    "url": "https://www.semanticscholar.org/paper/5f58863dd6474d6f127be995b5871e7c60f2792f",
    "title": "Video Understanding with Large Language Models: A Survey",
    "abstract": "With the burgeoning growth of online video platforms and the escalating volume of video content, the demand for proficient video understanding tools has intensified markedly. Given the remarkable capabilities of large language models (LLMs) in language and multimodal tasks, this survey provides a detailed overview of recent advancements in video understanding that harness the power of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended multi-granularity (general, temporal, and spatiotemporal) reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding. We examine the unique characteristics and capabilities of Vid-LLMs, categorizing the approaches into three main types: Video Analyzer x LLM, Video Embedder x LLM, and (Analyzer + Embedder) x LLM. Furthermore, we identify five sub-types based on the functions of LLMs in Vid-LLMs: LLM as Summarizer, LLM as Manager, LLM as Text Decoder, LLM as Regressor, and LLM as Hidden Layer. Furthermore, this survey presents a comprehensive study of the tasks, datasets, benchmarks, and evaluation methodologies for Vid-LLMs. Additionally, it explores the expansive applications of Vid-LLMs across various domains, highlighting their remarkable scalability and versatility in real-world video understanding challenges. Finally, it summarizes the limitations of existing Vid-LLMs and outlines directions for future research. For more information, readers are recommended to visit the repository at https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 49,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-12-29",
    "authors": [
      {
        "authorId": "2119309562",
        "name": "Yunlong Tang"
      },
      {
        "authorId": "2066776633",
        "name": "Jing Bi"
      },
      {
        "authorId": "2186266050",
        "name": "Siting Xu"
      },
      {
        "authorId": "2242154602",
        "name": "Luchuan Song"
      },
      {
        "authorId": "2153545235",
        "name": "Susan Liang"
      },
      {
        "authorId": "2269119984",
        "name": "Teng Wang"
      },
      {
        "authorId": "2266412651",
        "name": "Daoan Zhang"
      },
      {
        "authorId": "2277235738",
        "name": "Jie An"
      },
      {
        "authorId": "2277246502",
        "name": "Jingyang Lin"
      },
      {
        "authorId": "2277251955",
        "name": "Rongyi Zhu"
      },
      {
        "authorId": "82946757",
        "name": "A. Vosoughi"
      },
      {
        "authorId": "2161012966",
        "name": "Chao Huang"
      },
      {
        "authorId": "2155280541",
        "name": "Zeliang Zhang"
      },
      {
        "authorId": "2146483847",
        "name": "Feng Zheng"
      },
      {
        "authorId": "2277204434",
        "name": "Jianguo Zhang"
      },
      {
        "authorId": "2277217764",
        "name": "Ping Luo"
      },
      {
        "authorId": "2257204717",
        "name": "Jiebo Luo"
      },
      {
        "authorId": "2242467374",
        "name": "Chenliang Xu"
      }
    ],
    "source": "semantic_scholar",
    "score": 128.6803450814222
  },
  {
    "paperId": "681253389d2cc27103753749f4c7556699d55471",
    "url": "https://www.semanticscholar.org/paper/681253389d2cc27103753749f4c7556699d55471",
    "title": "Temporal Data Meets LLM - Explainable Financial Time Series Forecasting",
    "abstract": "This paper presents a novel study on harnessing Large Language Models' (LLMs) outstanding knowledge and reasoning abilities for explainable financial time series forecasting. The application of machine learning models to financial time series comes with several challenges, including the difficulty in cross-sequence reasoning and inference, the hurdle of incorporating multi-modal signals from historical news, financial knowledge graphs, etc., and the issue of interpreting and explaining the model results. In this paper, we focus on NASDAQ-100 stocks, making use of publicly accessible historical stock price data, company metadata, and historical economic/financial news. We conduct experiments to illustrate the potential of LLMs in offering a unified solution to the aforementioned challenges. Our experiments include trying zero-shot/few-shot inference with GPT-4 and instruction-based fine-tuning with a public LLM model Open LLaMA. We demonstrate our approach outperforms a few baselines, including the widely applied classic ARMA-GARCH model and a gradient-boosting tree model. Through the performance comparison results and a few examples, we find LLMs can make a well-thought decision by reasoning over information from both textual news and price time series and extracting insights, leveraging cross-sequence information, and utilizing the inherent knowledge embedded within the LLM. Additionally, we show that a publicly available LLM such as Open-LLaMA, after fine-tuning, can comprehend the instruction to generate explainable forecasts and achieve reasonable performance, albeit relatively inferior in comparison to GPT-4.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 44,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.11025",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Economics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-19",
    "authors": [
      {
        "authorId": "2118210915",
        "name": "Xinli Yu"
      },
      {
        "authorId": "2141144864",
        "name": "Zheng Chen"
      },
      {
        "authorId": "48907594",
        "name": "Yuan Ling"
      },
      {
        "authorId": "2190030596",
        "name": "Shujing Dong"
      },
      {
        "authorId": "47781311",
        "name": "Zongying Liu"
      },
      {
        "authorId": "2220668748",
        "name": "Yanbin Lu"
      }
    ],
    "source": "semantic_scholar",
    "score": 127.0999373465548
  },
  {
    "paperId": "cb23a59fdf3ade707600f076df4ff27a03941fba",
    "url": "https://www.semanticscholar.org/paper/cb23a59fdf3ade707600f076df4ff27a03941fba",
    "title": "AutoDroid: LLM-powered Task Automation in Android",
    "abstract": "Mobile task automation is an attractive technique that aims to enable voice-based hands-free user interaction with smartphones. However, existing approaches suffer from poor scalability due to the limited language understanding ability and the non-trivial manual efforts required from developers or endusers. The recent advance of large language models (LLMs) in language understanding and reasoning inspires us to rethink the problem from a model-centric perspective, where task preparation, comprehension, and execution are handled by a unified language model. In this work, we introduce AutoDroid, a mobile task automation system capable of handling arbitrary tasks on any Android application without manual efforts. The key insight is to combine the commonsense knowledge of LLMs and domain-specific knowledge of apps through automated dynamic analysis. The main components include a functionality-aware UI representation method that bridges the UI with the LLM, exploration-based memory injection techniques that augment the app-specific domain knowledge of LLM, and a multi-granularity query optimization module that reduces the cost of model inference. We integrate AutoDroid with off-the-shelf LLMs including online GPT-4/GPT-3.5 and on-device Vicuna, and evaluate its performance on a new benchmark for memory-augmented Android task automation with 158 common tasks. The results demonstrated that AutoDroid is able to precisely generate actions with an accuracy of 90.9%, and complete tasks with a success rate of 71.3%, outperforming the GPT-4-powered baselines by 36.4% and 39.7%.",
    "venue": "ACM/IEEE International Conference on Mobile Computing and Networking",
    "year": 2023,
    "citationCount": 39,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3636534.3649379",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-08-29",
    "authors": [
      {
        "authorId": "2188861830",
        "name": "Hao Wen"
      },
      {
        "authorId": "2144416765",
        "name": "Yuanchun Li"
      },
      {
        "authorId": "2235520970",
        "name": "Guohong Liu"
      },
      {
        "authorId": "92736469",
        "name": "Shanhui Zhao"
      },
      {
        "authorId": "2256865742",
        "name": "Tao Yu"
      },
      {
        "authorId": "34997918",
        "name": "Toby Jia-Jun Li"
      },
      {
        "authorId": "2170664295",
        "name": "Shiqi Jiang"
      },
      {
        "authorId": "2235192744",
        "name": "Yunhao Liu"
      },
      {
        "authorId": "2108201285",
        "name": "Yaqin Zhang"
      },
      {
        "authorId": "2161440197",
        "name": "Yunxin Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 131.33319181170904
  },
  {
    "paperId": "7810eb9c7bf9d3f4409de3954f8e8001cb93ea94",
    "url": "https://www.semanticscholar.org/paper/7810eb9c7bf9d3f4409de3954f8e8001cb93ea94",
    "title": "InsightLens: Discovering and Exploring Insights from Conversational Contexts in Large-Language-Model-Powered Data Analysis",
    "abstract": "— The proliferation of large language models (LLMs) has revolutionized the capabilities of natural language interfaces (NLIs) for data analysis. LLMs can perform multi-step and complex reasoning to generate data insights based on users’ analytic intents. However, these insights often entangle with an abundance of contexts in analytic conversations such as code, visualizations, and natural language explanations. This hinders efficient identification, verification, and interpretation of insights within the current chat-based interfaces of LLMs. In this paper, we first conduct a formative study with eight experienced data analysts to understand their general workflow and pain points during LLM-powered data analysis. Then, we propose an LLM-based multi-agent framework to automatically extract, associate, and organize insights along with the analysis process. Based on this, we introduce InsightLens , an interactive system that visualizes the intricate conversational contexts from multiple aspects to facilitate insight discovery and exploration. A user study with twelve data analysts demonstrates the effectiveness of InsightLens , showing that it significantly reduces users’ manual and cognitive effort without disrupting their conversational data analysis workflow, leading to a more efficient analysis experience",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2186416127",
        "name": "Luoxuan Weng"
      },
      {
        "authorId": "2294681864",
        "name": "Xingbo Wang"
      },
      {
        "authorId": "2294609178",
        "name": "Junyu Lu"
      },
      {
        "authorId": "2134046581",
        "name": "Yingchaojie Feng"
      },
      {
        "authorId": "2186737911",
        "name": "Yihan Liu"
      },
      {
        "authorId": "2284188257",
        "name": "Wei Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "b5bb10b374b52a830e96e3c427d8486def7d882c",
    "url": "https://www.semanticscholar.org/paper/b5bb10b374b52a830e96e3c427d8486def7d882c",
    "title": "Let Me Do It For You: Towards LLM Empowered Recommendation via Tool Learning",
    "abstract": "Conventional recommender systems (RSs) face challenges in precisely capturing users' fine-grained preferences. Large language models (LLMs) have shown capabilities in commonsense reasoning and leveraging external tools that may help address these challenges. However, existing LLM-based RSs suffer from hallucinations, misalignment between the semantic space of items and the behavior space of users, or overly simplistic control strategies (e.g., whether to rank or directly present existing results). To bridge these gap, we introduce ToolRec, a framework for LLM-empowered recommendations via tool learning that uses LLMs as surrogate users, thereby guiding the recommendation process and invoking external tools to generate a recommendation list that aligns closely with users' nuanced preferences. We formulate the recommendation process as a process aimed at exploring user interests in attribute granularity. The process factors in the nuances of the context and user preferences. The LLM then invokes external tools based on a user's attribute instructions and probes different segments of the item pool. We consider two types of attribute-oriented tools: rank tools and retrieval tools. Through the integration of LLMs, ToolRec enables conventional recommender systems to become external tools with a natural language interface. Extensive experiments verify the effectiveness of ToolRec, particularly in scenarios that are rich in semantic content.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2405.15114",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-24",
    "authors": [
      {
        "authorId": "2109814746",
        "name": "Yuyue Zhao"
      },
      {
        "authorId": "1491035012",
        "name": "Jiancan Wu"
      },
      {
        "authorId": "2259678005",
        "name": "Xiang Wang"
      },
      {
        "authorId": "2303792476",
        "name": "Wei Tang"
      },
      {
        "authorId": "2303281488",
        "name": "Dingxian Wang"
      },
      {
        "authorId": "2265490493",
        "name": "M. D. Rijke"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.53877639491068
  },
  {
    "paperId": "4aa8b64c51e987d703045023cd72b3ba5e115f99",
    "url": "https://www.semanticscholar.org/paper/4aa8b64c51e987d703045023cd72b3ba5e115f99",
    "title": "Assessment of Multimodal Large Language Models in Alignment with Human Values",
    "abstract": "Large Language Models (LLMs) aim to serve as versatile assistants aligned with human values, as defined by the principles of being helpful, honest, and harmless (hhh). However, in terms of Multimodal Large Language Models (MLLMs), despite their commendable performance in perception and reasoning tasks, their alignment with human values remains largely unexplored, given the complexity of defining hhh dimensions in the visual world and the difficulty in collecting relevant data that accurately mirrors real-world situations. To address this gap, we introduce Ch3Ef, a Compreh3ensive Evaluation dataset and strategy for assessing alignment with human expectations. Ch3Ef dataset contains 1002 human-annotated data samples, covering 12 domains and 46 tasks based on the hhh principle. We also present a unified evaluation strategy supporting assessment across various scenarios and different perspectives. Based on the evaluation results, we summarize over 10 key findings that deepen the understanding of MLLM capabilities, limitations, and the dynamic relationships between evaluation levels, guiding future advancements in the field.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-26",
    "authors": [
      {
        "authorId": "2144146362",
        "name": "Zhelun Shi"
      },
      {
        "authorId": "2265545407",
        "name": "Zhipin Wang"
      },
      {
        "authorId": "2265583827",
        "name": "Hongxing Fan"
      },
      {
        "authorId": "2218417690",
        "name": "Zaibin Zhang"
      },
      {
        "authorId": "2280261471",
        "name": "Lijun Li"
      },
      {
        "authorId": "2280256177",
        "name": "Yongting Zhang"
      },
      {
        "authorId": "13050405",
        "name": "Zhen-fei Yin"
      },
      {
        "authorId": "2290983876",
        "name": "Lu Sheng"
      },
      {
        "authorId": "2265493981",
        "name": "Yu Qiao"
      },
      {
        "authorId": "2254280929",
        "name": "Jing Shao"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "1391ace6e01248da666dc731b969bb27e6b90785",
    "url": "https://www.semanticscholar.org/paper/1391ace6e01248da666dc731b969bb27e6b90785",
    "title": "AI and Generative AI for Research Discovery and Summarization",
    "abstract": "AI and generative AI tools, including chatbots like ChatGPT that rely on large language models (LLMs), have burst onto the scene this year, creating incredible opportunities to increase work productivity and improve our lives. Statisticians and data scientists have begun experiencing the benefits from the availability of these tools in numerous ways, such as the generation of programming code from text prompts to analyze data or fit statistical models. One area that these tools can make a substantial impact is in research discovery and summarization. Standalone tools and plugins to chatbots are being developed that allow researchers to more quickly find relevant literature than pre-2023 search tools. Furthermore, generative AI tools have improved to the point where they can summarize and extract the key points from research articles in succinct language. Finally, chatbots based on highly parameterized LLMs can be used to simulate abductive reasoning, which provides researchers the ability to make connections among related technical topics, which can also be used for research discovery. We review the developments in AI and generative AI for research discovery and summarization, and propose directions where these types of tools are likely to head in the future that may be of interest to statistician and data scientists.",
    "venue": "Special Issue 5: Grappling With the Generative AI Revolution",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-01-08",
    "authors": [
      {
        "authorId": "2279541505",
        "name": "Mark Glickman"
      },
      {
        "authorId": "2279677112",
        "name": "Yi Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "8a439444d888202a711b8b8a195934cdb138342e",
    "url": "https://www.semanticscholar.org/paper/8a439444d888202a711b8b8a195934cdb138342e",
    "title": "Towards Efficient and Effective Unlearning of Large Language Models for Recommendation",
    "abstract": "The significant advancements in large language models (LLMs) give rise to a promising research direction, i.e., leveraging LLMs as recommenders (LLMRec). The efficacy of LLMRec arises from the open-world knowledge and reasoning capabilities inherent in LLMs. LLMRec acquires the recommendation capabilities through instruction tuning based on user interaction data. However, in order to protect user privacy and optimize utility, it is also crucial for LLMRec to intentionally forget specific user data, which is generally referred to as recommendation unlearning. In the era of LLMs, recommendation unlearning poses new challenges for LLMRec in terms of \\textit{inefficiency} and \\textit{ineffectiveness}. Existing unlearning methods require updating billions of parameters in LLMRec, which is costly and time-consuming. Besides, they always impact the model utility during the unlearning process. To this end, we propose \\textbf{E2URec}, the first \\underline{E}fficient and \\underline{E}ffective \\underline{U}nlearning method for LLM\\underline{Rec}. Our proposed E2URec enhances the unlearning efficiency by updating only a few additional LoRA parameters, and improves the unlearning effectiveness by employing a teacher-student framework, where we maintain multiple teacher networks to guide the unlearning process. Extensive experiments show that E2URec outperforms state-of-the-art baselines on two real-world datasets. Specifically, E2URec can efficiently forget specific data without affecting recommendation performance. The source code is at \\url{https://github.com/justarter/E2URec}.",
    "venue": "Frontiers Comput. Sci.",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-06",
    "authors": [
      {
        "authorId": "2283141638",
        "name": "Hangyu Wang"
      },
      {
        "authorId": "2144908858",
        "name": "Jianghao Lin"
      },
      {
        "authorId": "2258709565",
        "name": "Bo Chen"
      },
      {
        "authorId": "2290248265",
        "name": "Yang Yang"
      },
      {
        "authorId": "2257180930",
        "name": "Ruiming Tang"
      },
      {
        "authorId": "2240768092",
        "name": "Weinan Zhang"
      },
      {
        "authorId": "2237958078",
        "name": "Yong Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "cf89542d61a7b29da70d7866d651ac5b630f9a35",
    "url": "https://www.semanticscholar.org/paper/cf89542d61a7b29da70d7866d651ac5b630f9a35",
    "title": "Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet",
    "abstract": "With the explosive influence caused by the success of large language models (LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work showing that foundation models can be used to solve a large variety of tasks. However, there is very limited work that shares insights on multi-agent planning. Multi-agent planning is different from other domains by combining the difficulty of multi-agent coordination and planning, and making it hard to leverage external tools to facilitate the reasoning needed. In this paper, we focus on the problem of multi-agent path finding (MAPF), which is also known as multi-robot route planning, and study the performance of solving MAPF with LLMs. We first show the motivating success on an empty room map without obstacles, then the failure to plan on the harder room map and maze map of the standard MAPF benchmark. We present our position on why directly solving MAPF with LLMs has not been successful yet, and we use various experiments to support our hypothesis. Based on our results, we discussed how researchers with different backgrounds could help with this problem from different perspectives.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-08",
    "authors": [
      {
        "authorId": "2278582297",
        "name": "Weizhe Chen"
      },
      {
        "authorId": "2256845651",
        "name": "Sven Koenig"
      },
      {
        "authorId": "1796375",
        "name": "B. Dilkina"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "7f96bb27a8fca35b1f7d02ee319a64be04114809",
    "url": "https://www.semanticscholar.org/paper/7f96bb27a8fca35b1f7d02ee319a64be04114809",
    "title": "LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments",
    "abstract": "Traffic congestion in metropolitan areas presents a formidable challenge with far-reaching economic, environmental, and societal ramifications. Therefore, effective congestion management is imperative, with traffic signal control (TSC) systems being pivotal in this endeavor. Conventional TSC systems, designed upon rule-based algorithms or reinforcement learning (RL), frequently exhibit deficiencies in managing the complexities and variabilities of urban traffic flows, constrained by their limited capacity for adaptation to unfamiliar scenarios. In response to these limitations, this work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties. Specifically, a hybrid framework that augments LLMs with a suite of perception and decision-making tools is proposed, facilitating the interrogation of both the static and dynamic traffic information. This design places the LLM at the center of the decision-making process, combining external traffic data with established TSC methods. Moreover, a simulation platform is developed to corroborate the efficacy of the proposed framework. The findings from our simulations attest to the system's adeptness in adjusting to a multiplicity of traffic environments without the need for additional training. Notably, in cases of Sensor Outage (SO), our approach surpasses conventional RL-based systems by reducing the average waiting time by $20.4\\%$. This research signifies a notable advance in TSC strategies and paves the way for the integration of LLMs into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management. The related code is available at https://github.com/Traffic-Alpha/LLM-Assisted-Light.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-13",
    "authors": [
      {
        "authorId": "2265927745",
        "name": "Maonan Wang"
      },
      {
        "authorId": "2219695355",
        "name": "Aoyu Pang"
      },
      {
        "authorId": "7592365",
        "name": "Yuheng Kan"
      },
      {
        "authorId": "144305489",
        "name": "Man-On Pun"
      },
      {
        "authorId": "2292117616",
        "name": "Chung Shue Chen"
      },
      {
        "authorId": "2291077490",
        "name": "Bo Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "e3772a4f880c6cc4ea6087b9f6ec4d3a6f33c4ec",
    "url": "https://www.semanticscholar.org/paper/e3772a4f880c6cc4ea6087b9f6ec4d3a6f33c4ec",
    "title": "TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models",
    "abstract": "Classical planning formulations like the Planning Domain Definition Language (PDDL) admit action sequences guaranteed to achieve a goal state given an initial state if any are possible. However, reasoning problems defined in PDDL do not capture temporal aspects of action taking, for example that two agents in the domain can execute an action simultaneously if postconditions of each do not interfere with preconditions of the other. A human expert can decompose a goal into largely independent constituent parts and assign each agent to one of these subgoals to take advantage of simultaneous actions for faster execution of plan steps, each using only single agent planning. By contrast, large language models (LLMs) used for directly inferring plan steps do not guarantee execution success, but do leverage commonsense reasoning to assemble action sequences. We combine the strengths of classical planning and LLMs by approximating human intuitions for two-agent planning goal decomposition. We demonstrate that LLM-based goal decomposition leads to faster planning times than solving multi-agent PDDL problems directly while simultaneously achieving fewer plan execution steps than a single agent plan alone and preserving execution success. Additionally, we find that LLM-based approximations of subgoals can achieve similar multi-agent execution steps than those specified by human experts. Website and resources at https://glamor-usc.github.io/twostep",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-25",
    "authors": [
      {
        "authorId": "144399662",
        "name": "Ishika Singh"
      },
      {
        "authorId": "2251500479",
        "name": "David Traum"
      },
      {
        "authorId": "2266752407",
        "name": "Jesse Thomason"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "cf95279b1da9de1aad9e7c651f5048f69af295ed",
    "url": "https://www.semanticscholar.org/paper/cf95279b1da9de1aad9e7c651f5048f69af295ed",
    "title": "AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents",
    "abstract": "AI agents aim to solve complex tasks by combining text-based reasoning with external tool calls. Unfortunately, AI agents are vulnerable to prompt injection attacks where data returned by external tools hijacks the agent to execute malicious tasks. To measure the adversarial robustness of AI agents, we introduce AgentDojo, an evaluation framework for agents that execute tools over untrusted data. To capture the evolving nature of attacks and defenses, AgentDojo is not a static test suite, but rather an extensible environment for designing and evaluating new agent tasks, defenses, and adaptive attacks. We populate the environment with 97 realistic tasks (e.g., managing an email client, navigating an e-banking website, or making travel bookings), 629 security test cases, and various attack and defense paradigms from the literature. We find that AgentDojo poses a challenge for both attacks and defenses: state-of-the-art LLMs fail at many tasks (even in the absence of attacks), and existing prompt injection attacks break some security properties but not all. We hope that AgentDojo can foster research on new design principles for AI agents that solve common tasks in a reliable and robust manner.. We release the code for AgentDojo at https://github.com/ethz-spylab/agentdojo.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-19",
    "authors": [
      {
        "authorId": "2175939276",
        "name": "Edoardo Debenedetti"
      },
      {
        "authorId": "2299061721",
        "name": "Jie Zhang"
      },
      {
        "authorId": "2138580250",
        "name": "Mislav Balunovi'c"
      },
      {
        "authorId": "2150869869",
        "name": "Luca Beurer-Kellner"
      },
      {
        "authorId": "2307472727",
        "name": "Marc Fischer"
      },
      {
        "authorId": "2267733649",
        "name": "F. Tramèr"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "1fd84f4414420485cc982158f068ca929a16db18",
    "url": "https://www.semanticscholar.org/paper/1fd84f4414420485cc982158f068ca929a16db18",
    "title": "Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs Perfect Reasoners",
    "abstract": "Chain of Thought prompting strategy has enhanced the performance of Large Language Models (LLMs) across various NLP tasks. However, it still has shortcomings when dealing with complex reasoning tasks, following Wei et al. (2022), including understanding errors, calculation errors and process errors (e.g. missing-step and hallucinations). Subse-quently, Our in-depth analysis of various error types has found that deeply understanding the whole problem is critical in addressing complicated reasoning tasks. In this paper, we proposed a novel prompt strategy called Deeply Understanding the Problems (DUP) prompting, inspired by how humans solve complex reasoning problems, designed to enhance the comprehensive understanding of problems by LLMs. It consists of three stages: 1) extract the core question; 2) find out problem-solving information based on the core question; 3) generate and extract answers by LLMs. We evaluate the performance of DUP prompting on ten diverse reasoning datasets. Experimental results suggest that DUP prompting significantly outperforms Zero-Shot CoT (Kojima et al., 2022) across all datasets. Notably, DUP achieves state-of-the-art on SVAMP (90.4% to 94.2%) and GSM8K (94.6% to 97.1%).",
    "venue": "",
    "year": null,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2114810150",
        "name": "Qihuang Zhong"
      },
      {
        "authorId": "2297998382",
        "name": "Kang Wang"
      },
      {
        "authorId": "2291967415",
        "name": "Ziyang Xu"
      },
      {
        "authorId": "46701032",
        "name": "Juhua Liu"
      },
      {
        "authorId": "46573238",
        "name": "Liang Ding"
      },
      {
        "authorId": "2289611686",
        "name": "Bo Du"
      },
      {
        "authorId": "2255502438",
        "name": "D. Tao"
      },
      {
        "authorId": "2257278245",
        "name": "Ouyang Long"
      },
      {
        "authorId": "2298945959",
        "name": "Jeffrey Wu"
      },
      {
        "authorId": "2298950344",
        "name": "Xu Jiang"
      },
      {
        "authorId": "2275252021",
        "name": "Diogo Almeida"
      },
      {
        "authorId": "2275245962",
        "name": "Carroll L. Wainwright"
      },
      {
        "authorId": "2051714782",
        "name": "Pamela Mishkin"
      },
      {
        "authorId": "2262080679",
        "name": "Chong Zhang"
      },
      {
        "authorId": "144517868",
        "name": "Sandhini Agarwal"
      },
      {
        "authorId": "2117680841",
        "name": "Katarina Slama"
      },
      {
        "authorId": "2253525058",
        "name": "Alex Ray"
      },
      {
        "authorId": "2297873691",
        "name": "John Schulman"
      },
      {
        "authorId": "2285925921",
        "name": "Jacob Hilton"
      },
      {
        "authorId": "2151735262",
        "name": "Fraser Kelton"
      },
      {
        "authorId": "2298421583",
        "name": "Luke Miller"
      },
      {
        "authorId": "2151735251",
        "name": "Maddie Simens"
      },
      {
        "authorId": "2220750220",
        "name": "Amanda Askell"
      },
      {
        "authorId": "2930640",
        "name": "P. Welinder"
      },
      {
        "authorId": "145791315",
        "name": "P. Christiano"
      },
      {
        "authorId": "2990741",
        "name": "J. Leike"
      },
      {
        "authorId": "2298887813",
        "name": "Ryan Lowe. 2022"
      },
      {
        "authorId": "51918868",
        "name": "Victor Sanh"
      },
      {
        "authorId": "2291172852",
        "name": "Albert Webson"
      },
      {
        "authorId": "2269733851",
        "name": "Colin Raffel"
      },
      {
        "authorId": "2260131842",
        "name": "Stephen H. Bach"
      },
      {
        "authorId": "35566806",
        "name": "Lintang Sutawika"
      },
      {
        "authorId": "25098419",
        "name": "Zaid Alyafeai"
      },
      {
        "authorId": "2260103542",
        "name": "Antoine Chaffin"
      },
      {
        "authorId": "114762823",
        "name": "Arnaud Stiegler"
      },
      {
        "authorId": "2048082186",
        "name": "Arun Raja"
      },
      {
        "authorId": "1879591269",
        "name": "Manan Dey"
      },
      {
        "authorId": "2054179756",
        "name": "Saiful Bari"
      },
      {
        "authorId": "2257127518",
        "name": "Canwen Xu"
      },
      {
        "authorId": "70296695",
        "name": "Urmish Thakker"
      },
      {
        "authorId": "1409842673",
        "name": "Shanya Sharma"
      },
      {
        "authorId": "50812522",
        "name": "Eliza Szczechla"
      },
      {
        "authorId": "2260285228",
        "name": "Taewoon Kim"
      },
      {
        "authorId": "1509809381",
        "name": "Gunjan Chhablani"
      },
      {
        "authorId": "2291171140",
        "name": "Ni-hal Nayak"
      },
      {
        "authorId": "2286156570",
        "name": "Debajyoti Datta"
      },
      {
        "authorId": "2291171662",
        "name": "Mike Jonathan Chang"
      },
      {
        "authorId": "2266778091",
        "name": "Tianyuan Jiang"
      },
      {
        "authorId": "2260364582",
        "name": "Han Wang"
      },
      {
        "authorId": "2258957810",
        "name": "Matteo Manica"
      },
      {
        "authorId": "2260301730",
        "name": "Sheng Shen"
      },
      {
        "authorId": "2282475073",
        "name": "Zheng-Xin Yong"
      },
      {
        "authorId": "144834468",
        "name": "Harshit Pandey"
      },
      {
        "authorId": "48983885",
        "name": "Rachel Bawden"
      },
      {
        "authorId": "2291311504",
        "name": "Thomas Wang"
      },
      {
        "authorId": "10729963",
        "name": "Trishala Neeraj"
      },
      {
        "authorId": "120419790",
        "name": "Jos Rozen"
      },
      {
        "authorId": "2051500420",
        "name": "Abheesht Sharma"
      },
      {
        "authorId": "2286145116",
        "name": "A. Santilli"
      },
      {
        "authorId": "2286145107",
        "name": "Thibault Févry"
      },
      {
        "authorId": "31592365",
        "name": "Jason Alan Fries"
      },
      {
        "authorId": "2131107966",
        "name": "Ryan Teehan"
      },
      {
        "authorId": "1379806208",
        "name": "Teven Le Scao"
      },
      {
        "authorId": null,
        "name": "Stella Biderman"
      },
      {
        "authorId": null,
        "name": "Leo Gao"
      },
      {
        "authorId": "2257007291",
        "name": "Thomas Wolf"
      },
      {
        "authorId": "2291171084",
        "name": "A. M. R. 2022"
      },
      {
        "authorId": "2291170811",
        "name": "Multi-task"
      },
      {
        "authorId": "12371246",
        "name": "Alon Talmor"
      },
      {
        "authorId": "47426264",
        "name": "Jonathan Herzig"
      },
      {
        "authorId": "35219984",
        "name": "Nicholas Lourie"
      },
      {
        "authorId": "2298888060",
        "name": "Jonathan Berant. 2019"
      },
      {
        "authorId": "2298887963",
        "name": "Commonsenseqa"
      },
      {
        "authorId": "9501591",
        "name": "R. Thoppilan"
      },
      {
        "authorId": "1490889580",
        "name": "Daniel De Freitas"
      },
      {
        "authorId": "2115876918",
        "name": "Jamie Hall"
      },
      {
        "authorId": "1846258",
        "name": "Noam M. Shazeer"
      },
      {
        "authorId": "1490888815",
        "name": "Apoorv Kulshreshtha"
      },
      {
        "authorId": "2257129838",
        "name": "Heng-Tze Cheng"
      },
      {
        "authorId": "2150572756",
        "name": "Alicia Jin"
      },
      {
        "authorId": "2150572221",
        "name": "Taylor Bos"
      },
      {
        "authorId": "2150777298",
        "name": "Leslie Baker"
      },
      {
        "authorId": "2298979636",
        "name": "Yu Du"
      },
      {
        "authorId": "2290402489",
        "name": "Hugo Touvron"
      },
      {
        "authorId": "2294886663",
        "name": "Louis Martin"
      },
      {
        "authorId": "2282542714",
        "name": "Kevin Stone"
      },
      {
        "authorId": "2282542430",
        "name": "Peter Al-bert"
      },
      {
        "authorId": "2634674",
        "name": "Amjad Almahairi"
      },
      {
        "authorId": "2223764353",
        "name": "Yasmine Babaei"
      },
      {
        "authorId": "2223756247",
        "name": "Nikolay Bashlykov"
      },
      {
        "authorId": "47505161",
        "name": "Soumya Batra"
      },
      {
        "authorId": "51229603",
        "name": "Prajjwal Bhargava"
      },
      {
        "authorId": "2298888009",
        "name": "Shruti"
      },
      {
        "authorId": "2285687970",
        "name": "Lei Wang"
      },
      {
        "authorId": "2143418409",
        "name": "Wanyu Xu"
      },
      {
        "authorId": "2150277971",
        "name": "Yihuai Lan"
      },
      {
        "authorId": "2253994107",
        "name": "Zhiqiang Hu"
      },
      {
        "authorId": "2285688334",
        "name": "Yunshi Lan"
      },
      {
        "authorId": "2224755498",
        "name": "Roy Ka-Wei"
      },
      {
        "authorId": "2264352643",
        "name": "Xuezhi Wang"
      },
      {
        "authorId": "2253952872",
        "name": "Jason Wei"
      },
      {
        "authorId": "2265994815",
        "name": "Dale Schuurmans"
      },
      {
        "authorId": "2261982078",
        "name": "V. Quoc"
      },
      {
        "authorId": "2287047415",
        "name": "H. LeEd"
      },
      {
        "authorId": "2286993223",
        "name": "Sharan Chi"
      },
      {
        "authorId": "2286818568",
        "name": "Aakanksha Narang"
      },
      {
        "authorId": "2298887914",
        "name": "Chowdhery Denny"
      },
      {
        "authorId": "2285325700",
        "name": "Zhou"
      },
      {
        "authorId": "40377863",
        "name": "Maarten Bosma"
      },
      {
        "authorId": "2253469026",
        "name": "E. Chi"
      },
      {
        "authorId": "2256995069",
        "name": "Quoc V. Le"
      },
      {
        "authorId": "2261982605",
        "name": "Denny Zhou. 2022"
      },
      {
        "authorId": "2279658967",
        "name": "Xiaohan Xu"
      },
      {
        "authorId": "2287928517",
        "name": "Chongyang Tao"
      },
      {
        "authorId": "2289691565",
        "name": "Tao Shen"
      },
      {
        "authorId": "2256984659",
        "name": "Shunyu Yao"
      },
      {
        "authorId": "2292744698",
        "name": "Dian Yu"
      },
      {
        "authorId": "2144551262",
        "name": "Jeffrey Zhao"
      },
      {
        "authorId": "1697494",
        "name": "Izhak Shafran"
      },
      {
        "authorId": "2239948923",
        "name": "Thomas L. Griffiths"
      },
      {
        "authorId": "2125444580",
        "name": "Yuan Cao"
      },
      {
        "authorId": "2286995125",
        "name": "Karthik Narasimhan. 2023"
      },
      {
        "authorId": "2266365818",
        "name": "Longhui Yu"
      },
      {
        "authorId": null,
        "name": "Weisen Jiang"
      },
      {
        "authorId": "2285182555",
        "name": "Han Shi"
      },
      {
        "authorId": "2193887687",
        "name": "Jincheng Yu"
      },
      {
        "authorId": "2239065052",
        "name": "Zhengying Liu"
      },
      {
        "authorId": "2299278501",
        "name": "Yu Zhang"
      },
      {
        "authorId": "2243335442",
        "name": "James T. Kwok"
      },
      {
        "authorId": "2249755860",
        "name": "Zhenguo Li"
      },
      {
        "authorId": "2289135167",
        "name": "Adrian Weller"
      },
      {
        "authorId": "2243412679",
        "name": "Weiyang Liu"
      },
      {
        "authorId": "2298888107",
        "name": "Meta-math"
      },
      {
        "authorId": "2112340945",
        "name": "Zheng Yuan"
      },
      {
        "authorId": "2114128654",
        "name": "Hongyi Yuan"
      },
      {
        "authorId": "2257039734",
        "name": "Chengpeng Li"
      },
      {
        "authorId": "2298889130",
        "name": "Guanting Dong"
      },
      {
        "authorId": "2153781985",
        "name": "Ke Lu"
      },
      {
        "authorId": "2257356604",
        "name": "Chuanqi Tan"
      },
      {
        "authorId": "2298952052",
        "name": "Chang Zhou"
      },
      {
        "authorId": "2298888103",
        "name": "Jingren Zhou. 2023"
      },
      {
        "authorId": "3094520",
        "name": "Nathanael Schärli"
      },
      {
        "authorId": "2153400663",
        "name": "Le Hou"
      }
    ],
    "source": "semantic_scholar",
    "score": 81.19162312519754
  },
  {
    "paperId": "d408272835f6bde7343a46fd9fe858c65226d742",
    "url": "https://www.semanticscholar.org/paper/d408272835f6bde7343a46fd9fe858c65226d742",
    "title": "HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments",
    "abstract": "Recent advances in high-fidelity virtual environments serve as one of the major driving forces for building intelligent embodied agents to perceive, reason and interact with the physical world. Typically, these environments remain unchanged unless agents interact with them. However, in real-world scenarios, agents might also face dynamically changing environments characterized by unexpected events and need to rapidly take action accordingly. To remedy this gap, we propose a new simulated embodied benchmark, called HAZARD, specifically designed to assess the decision-making abilities of embodied agents in dynamic situations. HAZARD consists of three unexpected disaster scenarios, including fire, flood, and wind, and specifically supports the utilization of large language models (LLMs) to assist common sense reasoning and decision-making. This benchmark enables us to evaluate autonomous agents' decision-making capabilities across various pipelines, including reinforcement learning (RL), rule-based, and search-based methods in dynamically changing environments. As a first step toward addressing this challenge using large language models, we further develop an LLM-based agent and perform an in-depth analysis of its promise and challenge of solving these challenging tasks. HAZARD is available at https://vis-www.cs.umass.edu/hazard/.",
    "venue": "International Conference on Learning Representations",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-23",
    "authors": [
      {
        "authorId": "2107604346",
        "name": "Qinhong Zhou"
      },
      {
        "authorId": "2261356954",
        "name": "Sunli Chen"
      },
      {
        "authorId": "2280381623",
        "name": "Yisong Wang"
      },
      {
        "authorId": "2280627018",
        "name": "Haozhe Xu"
      },
      {
        "authorId": "2214830561",
        "name": "Weihua Du"
      },
      {
        "authorId": "2118083343",
        "name": "Hongxin Zhang"
      },
      {
        "authorId": "2258799458",
        "name": "Yilun Du"
      },
      {
        "authorId": "2243002911",
        "name": "Josh Tenenbaum"
      },
      {
        "authorId": "2280331955",
        "name": "Chuang Gan"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "9e5e96f78f4e5f53fa0dc8f090d189ceae5bac7b",
    "url": "https://www.semanticscholar.org/paper/9e5e96f78f4e5f53fa0dc8f090d189ceae5bac7b",
    "title": "Hello Again! LLM-powered Personalized Agent for Long-term Dialogue",
    "abstract": "Open-domain dialogue systems have seen remarkable advancements with the development of large language models (LLMs). Nonetheless, most existing dialogue systems predominantly focus on brief single-session interactions, neglecting the real-world demands for long-term companionship and personalized interactions with chatbots. Crucial to addressing this real-world need are event summary and persona management, which enable reasoning for appropriate long-term dialogue responses. Recent progress in the human-like cognitive and reasoning capabilities of LLMs suggests that LLM-based agents could significantly enhance automated perception, decision-making, and problem-solving. In response to this potential, we introduce a model-agnostic framework, the Long-term Dialogue Agent (LD-Agent), which incorporates three independently tunable modules dedicated to event perception, persona extraction, and response generation. For the event memory module, long and short-term memory banks are employed to separately focus on historical and ongoing sessions, while a topic-based retrieval mechanism is introduced to enhance the accuracy of memory retrieval. Furthermore, the persona module conducts dynamic persona modeling for both users and agents. The integration of retrieved memories and extracted personas is subsequently fed into the generator to induce appropriate responses. The effectiveness, generality, and cross-domain capabilities of LD-Agent are empirically demonstrated across various illustrative benchmarks, models, and tasks. The code is released at https://github.com/leolee99/LD-Agent.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-09",
    "authors": [
      {
        "authorId": "2249757805",
        "name": "Hao Li"
      },
      {
        "authorId": "2306354816",
        "name": "Chenghao Yang"
      },
      {
        "authorId": "2153659066",
        "name": "An Zhang"
      },
      {
        "authorId": "145843537",
        "name": "Yang Deng"
      },
      {
        "authorId": "2257436645",
        "name": "Xiang Wang"
      },
      {
        "authorId": "2257036129",
        "name": "Tat-Seng Chua"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "61b3260f774bdaf2f7c1894286a49f2eb4dfbb10",
    "url": "https://www.semanticscholar.org/paper/61b3260f774bdaf2f7c1894286a49f2eb4dfbb10",
    "title": "AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through Process Feedback",
    "abstract": "The notable success of large language models (LLMs) has sparked an upsurge in building language agents to complete various complex tasks. We present AMOR, an agent framework based on open-source LLMs, which reasons with external knowledge bases and adapts to specific domains through human supervision to the reasoning process. AMOR builds reasoning logic over a finite state machine (FSM) that solves problems through autonomous executions and transitions over disentangled modules. This allows humans to provide direct feedback to the individual modules, and thus naturally forms process supervision. Based on this reasoning and feedback framework, we develop AMOR through two-stage fine-tuning: warm-up and adaptation. The former fine-tunes the LLM with examples automatically constructed from various public datasets, enabling AMOR to generalize across different knowledge environments, while the latter tailors AMOR to specific domains using process feedback. Extensive experiments across multiple domains demonstrate the advantage of AMOR to strong baselines, thanks to its FSM-based reasoning and process feedback mechanism. The code and data are publicly available at \\url{https://github.com/JianGuanTHU/AMOR}.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-02",
    "authors": [
      {
        "authorId": "2323534462",
        "name": "Jian Guan"
      },
      {
        "authorId": "2282529632",
        "name": "Wei Wu"
      },
      {
        "authorId": "2282474990",
        "name": "Zujie Wen"
      },
      {
        "authorId": "2282476357",
        "name": "Peng Xu"
      },
      {
        "authorId": "2257375900",
        "name": "Hongning Wang"
      },
      {
        "authorId": "2261448024",
        "name": "Minlie Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "a472e5b8dc76edb9306aebc427240f0aaa7847ec",
    "url": "https://www.semanticscholar.org/paper/a472e5b8dc76edb9306aebc427240f0aaa7847ec",
    "title": "Prompted Contextual Vectors for Spear-Phishing Detection",
    "abstract": "Spear-phishing attacks present a significant security challenge, with large language models (LLMs) escalating the threat by generating convincing emails and facilitating target reconnaissance. To address this, we propose a detection approach based on a novel document vectorization method that utilizes an ensemble of LLMs to create representation vectors. By prompting LLMs to reason and respond to human-crafted questions, we quantify the presence of common persuasion principles in the email's content, producing prompted contextual document vectors for a downstream supervised machine learning model. We evaluate our method using a unique dataset generated by a proprietary system that automates target reconnaissance and spear-phishing email creation. Our method achieves a 91\\% F1 score in identifying LLM-generated spear-phishing emails, with the training set comprising only traditional phishing and benign emails. Key contributions include a novel document vectorization method utilizing LLM reasoning, a publicly available dataset of high-quality spear-phishing emails, and the demonstrated effectiveness of our method in detecting such emails. This methodology can be utilized for various document classification tasks, particularly in adversarial problem domains.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-13",
    "authors": [
      {
        "authorId": "2283932669",
        "name": "Daniel Nahmias"
      },
      {
        "authorId": "2278485958",
        "name": "Gal Engelberg"
      },
      {
        "authorId": "2278485187",
        "name": "Dan Klein"
      },
      {
        "authorId": "1720589",
        "name": "A. Shabtai"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "8bb5b517012530244497beb4d1b7257d3c76661c",
    "url": "https://www.semanticscholar.org/paper/8bb5b517012530244497beb4d1b7257d3c76661c",
    "title": "LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on Path Planning",
    "abstract": "Path planning is a fundamental scientific problem in robotics and autonomous navigation, requiring the derivation of efficient routes from starting to destination points while avoiding obstacles. Traditional algorithms like A* and its variants are capable of ensuring path validity but suffer from significant computational and memory inefficiencies as the state space grows. Conversely, large language models (LLMs) excel in broader environmental analysis through contextual understanding, providing global insights into environments. However, they fall short in detailed spatial and temporal reasoning, often leading to invalid or inefficient routes. In this work, we propose LLM-A*, an new LLM based route planning method that synergistically combines the precise pathfinding capabilities of A* with the global reasoning capability of LLMs. This hybrid approach aims to enhance pathfinding efficiency in terms of time and space complexity while maintaining the integrity of path validity, especially in large-scale scenarios. By integrating the strengths of both methodologies, LLM-A* addresses the computational and memory limitations of conventional algorithms without compromising on the validity required for effective pathfinding.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-20",
    "authors": [
      {
        "authorId": "2309478884",
        "name": "Silin Meng"
      },
      {
        "authorId": "2280103482",
        "name": "Yiwei Wang"
      },
      {
        "authorId": "2269763744",
        "name": "Cheng-Fu Yang"
      },
      {
        "authorId": "2256996328",
        "name": "Nanyun Peng"
      },
      {
        "authorId": "2260996794",
        "name": "Kai-Wei Chang"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "5d47f05c1d2b58c72a874a24902fa44a0bb417b7",
    "url": "https://www.semanticscholar.org/paper/5d47f05c1d2b58c72a874a24902fa44a0bb417b7",
    "title": "LegalReasoner: A Multi-Stage Framework for Legal Judgment Prediction via Large Language Models and Knowledge Integration",
    "abstract": "Legal judgment prediction (LJP) presents a formidable challenge in artificial intelligence, demanding intricate comprehension of legal texts, nuanced interpretation of statutes, and complex reasoning over multifaceted case elements. While recent advancements in natural language processing have shown promise, existing approaches often struggle to capture the sophisticated interplay between facts, legal principles, and precedents that characterize legal decision-making. This paper introduces LegalReasoner, a novel multi-stage framework that leverages large language models (LLMs) and integrates domain-specific knowledge for enhanced legal judgment prediction. Our approach encompasses four key stages: 1) legal knowledge infusion, where we pre-train an LLM on a vast corpus of legal literature using contrastive learning techniques; 2) case-law retrieval, employing a graph neural network to identify relevant precedents and statutes; 3) multi-hop reasoning, utilizing a transformer-based architecture with a hierarchical attention mechanism to navigate complex legal arguments; and 4) judgment synthesis, where we employ a generative adversarial network to produce coherent and legally sound judgments. We evaluate LegalReasoner on two diverse datasets: the European Court of Human Rights (ECHR) cases and the Chinese AI and Law Challenge (CAIL2018). Our framework demonstrates substantial improvements over state-of-the-art baselines, achieving an average accuracy increase of 7.8% across all datasets. Furthermore, we conduct extensive ablation studies and interpretability analyses to elucidate the contributions of each component and provide insights into the model’s decision-making process. Our work not only advances the field of automated legal reasoning but also offers a transparent and explainable system that could serve as a valuable tool for legal professionals. By bridging the gap between AI and legal expertise, LegalReasoner paves the way for more efficient, consistent, and fair legal decision-making processes.",
    "venue": "IEEE Access",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2330430953",
        "name": "Xuran Wang"
      },
      {
        "authorId": "2331323724",
        "name": "Xinguang Zhang"
      },
      {
        "authorId": "2330484668",
        "name": "Vanessa Hoo"
      },
      {
        "authorId": "2330440271",
        "name": "Zhouhang Shao"
      },
      {
        "authorId": "2330978070",
        "name": "Xuguang Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.95836866004329
  },
  {
    "paperId": "6fede93e6b6388a0f4d766a3adf22367e751de93",
    "url": "https://www.semanticscholar.org/paper/6fede93e6b6388a0f4d766a3adf22367e751de93",
    "title": "Verification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving",
    "abstract": "Natural language explanations represent a proxy for evaluating explanation-based and multi-step Natural Language Inference (NLI) models. However, assessing the validity of explanations for NLI is challenging as it typically involves the crowd-sourcing of apposite datasets, a process that is time-consuming and prone to logical errors. To address existing limitations, this paper investigates the verification and refinement of natural language explanations through the integration of Large Language Models (LLMs) and Theorem Provers (TPs). Specifically, we present a neuro-symbolic framework, named Explanation-Refiner, that integrates TPs with LLMs to generate and formalise explanatory sentences and suggest potential inference strategies for NLI. In turn, the TP is employed to provide formal guarantees on the logical validity of the explanations and to generate feedback for subsequent improvements. We demonstrate how Explanation-Refiner can be jointly used to evaluate explanatory reasoning, autoformalisation, and error correction mechanisms of state-of-the-art LLMs as well as to automatically enhance the quality of explanations of variable complexity in different domains.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-02",
    "authors": [
      {
        "authorId": "2282137151",
        "name": "Xin Quan"
      },
      {
        "authorId": "34102057",
        "name": "Marco Valentino"
      },
      {
        "authorId": "2282137030",
        "name": "Louise A. Dennis"
      },
      {
        "authorId": "2242981659",
        "name": "André Freitas"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "b908824639d18f11883abcab21efeb22e315ab9c",
    "url": "https://www.semanticscholar.org/paper/b908824639d18f11883abcab21efeb22e315ab9c",
    "title": "Multimodal Procedural Planning via Dual Text-Image Prompting",
    "abstract": "Embodied agents have achieved prominent performance in following human instructions to complete tasks. However, the potential of providing instructions informed by texts and images to assist humans in completing tasks remains underexplored. To uncover this capability, we present the multimodal procedural planning (MPP) task, in which models are given a high-level goal and generate plans of paired text-image steps, providing more complementary and informative guidance than unimodal plans. The key challenges of MPP are to ensure the informativeness, temporal coherence,and accuracy of plans across modalities. To tackle this, we propose Text-Image Prompting (TIP), a dual-modality prompting method that jointly leverages zero-shot reasoning ability in large language models (LLMs) and compelling text-to-image generation ability from diffusion-based models. TIP improves the interaction in the dual modalities using Text-to-Image Bridge and Image-to-Text Bridge, allowing LLMs to guide the textual-grounded image plan generation and leveraging the descriptions of image plans to ground the textual plan reversely. To address the lack of relevant datasets, we collect WIKIPLAN and RECIPEPLAN as a testbed for MPP. Our results show compelling human preferences and automatic scores against unimodal and multimodal baselines on WIKIPLAN and RECIPEPLAN in terms of informativeness, temporal coherence, and plan accuracy. Our code and data: https://github.com/YujieLu10/MPP.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 37,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.01795",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-02",
    "authors": [
      {
        "authorId": "47006228",
        "name": "Yujie Lu"
      },
      {
        "authorId": "2887562",
        "name": "Pan Lu"
      },
      {
        "authorId": "2142370346",
        "name": "Zhiyu Chen"
      },
      {
        "authorId": "51439692",
        "name": "Wanrong Zhu"
      },
      {
        "authorId": "47120131",
        "name": "X. Wang"
      },
      {
        "authorId": "1682479",
        "name": "William Yang Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.56379239589579
  },
  {
    "paperId": "e17c58d7a48b6b811df023484161a3b9c03e0d6b",
    "url": "https://www.semanticscholar.org/paper/e17c58d7a48b6b811df023484161a3b9c03e0d6b",
    "title": "Theory of Mind for Multi-Agent Collaboration via Large Language Models",
    "abstract": "While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 38,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.emnlp-main.13.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-16",
    "authors": [
      {
        "authorId": "2109032235",
        "name": "Huao Li"
      },
      {
        "authorId": "2258959405",
        "name": "Yu Quan Chong"
      },
      {
        "authorId": "8083127",
        "name": "Simon Stepputtis"
      },
      {
        "authorId": "39860257",
        "name": "Joseph Campbell"
      },
      {
        "authorId": "143626678",
        "name": "Dana Hughes"
      },
      {
        "authorId": "2117280035",
        "name": "Michael Lewis"
      },
      {
        "authorId": "2239104696",
        "name": "Katia P. Sycara"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.95342469194469
  },
  {
    "paperId": "fdb03aa9c310fa61df0be724705fb6f4ab20d37e",
    "url": "https://www.semanticscholar.org/paper/fdb03aa9c310fa61df0be724705fb6f4ab20d37e",
    "title": "Large Language Models as Zero-Shot Human Models for Human-Robot Interaction",
    "abstract": "Human models play a crucial role in human-robot interaction (HRI), enabling robots to consider the impact of their actions on people and plan their behavior accordingly. However, crafting good human models is challenging; capturing context-dependent human behavior requires significant prior knowledge and/or large amounts of interaction data, both of which are difficult to obtain. In this work, we explore the potential of large language models (LLMs) — which have consumed vast amounts of human-generated text data — to act as zero-shot human models for HRI. Our experiments on three social datasets yield promising results; the LLMs are able to achieve performance comparable to purpose-built models. That said, we also discuss current limitations, such as sensitivity to prompts and spatial/numerical reasoning mishaps. Based on our findings, we demonstrate how LLM-based human models can be integrated into a social robot's planning process and applied in HRI scenarios focused on the important element of trust. Specifically, we present one case study on a simulated trust-based table-clearing task and replicate past results that relied on custom models. Next, we conduct a new robot utensil-passing experiment ($n=65$) where preliminary results show that planning with an LLM-based human model can achieve gains over a basic myopic plan. In summary, our results show that LLMs offer a promising (but incomplete) approach to human modeling for HRI.",
    "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
    "year": 2023,
    "citationCount": 37,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2303.03548",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-03-06",
    "authors": [
      {
        "authorId": null,
        "name": "Bowen Zhang"
      },
      {
        "authorId": "5489509",
        "name": "Harold Soh"
      }
    ],
    "source": "semantic_scholar",
    "score": 130.5637923958958
  },
  {
    "paperId": "006aa1580fae5968417538c7acb4662c7b58088f",
    "url": "https://www.semanticscholar.org/paper/006aa1580fae5968417538c7acb4662c7b58088f",
    "title": "LLM-Rec: Personalized Recommendation via Prompting Large Language Models",
    "abstract": "Text-based recommendation holds a wide range of practical applications due to its versatility, as textual descriptions can represent nearly any type of item. However, directly employing the original item descriptions may not yield optimal recommendation performance due to the lack of comprehensive information to align with user preferences. Recent advances in large language models (LLMs) have showcased their remarkable ability to harness commonsense knowledge and reasoning. In this study, we introduce a novel approach, coined LLM-Rec, which incorporates four distinct prompting strategies of text enrichment for improving personalized text-based recommendations. Our empirical experiments reveal that using LLM-augmented text significantly enhances recommendation quality. Even basic MLP (Multi-Layer Perceptron) models achieve comparable or even better results than complex content-based methods. Notably, the success of LLM-Rec lies in its prompting strategies, which effectively tap into the language model's comprehension of both general and specific item characteristics. This highlights the importance of employing diverse prompts and input augmentation techniques to boost the recommendation effectiveness of LLMs.",
    "venue": "NAACL-HLT",
    "year": 2023,
    "citationCount": 36,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.15780",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-24",
    "authors": [
      {
        "authorId": "1486450921",
        "name": "Hanjia Lyu"
      },
      {
        "authorId": "2249954878",
        "name": "Song Jiang"
      },
      {
        "authorId": "1750905107",
        "name": "Hanqing Zeng"
      },
      {
        "authorId": "35846319",
        "name": "Yinglong Xia"
      },
      {
        "authorId": "2116783457",
        "name": "Jiebo Luo"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.16376868966336
  },
  {
    "paperId": "3a79545719fb193a6b4042ef7d1d87cfd267be06",
    "url": "https://www.semanticscholar.org/paper/3a79545719fb193a6b4042ef7d1d87cfd267be06",
    "title": "Controllable Text-to-Image Generation with GPT-4",
    "abstract": "Current text-to-image generation models often struggle to follow textual instructions, especially the ones requiring spatial reasoning. On the other hand, Large Language Models (LLMs), such as GPT-4, have shown remarkable precision in generating code snippets for sketching out text inputs graphically, e.g., via TikZ. In this work, we introduce Control-GPT to guide the diffusion-based text-to-image pipelines with programmatic sketches generated by GPT-4, enhancing their abilities for instruction following. Control-GPT works by querying GPT-4 to write TikZ code, and the generated sketches are used as references alongside the text instructions for diffusion models (e.g., ControlNet) to generate photo-realistic images. One major challenge to training our pipeline is the lack of a dataset containing aligned text, images, and sketches. We address the issue by converting instance masks in existing datasets into polygons to mimic the sketches used at test time. As a result, Control-GPT greatly boosts the controllability of image generation. It establishes a new state-of-art on the spatial arrangement and object positioning generation and enhances users' control of object positions, sizes, etc., nearly doubling the accuracy of prior models. Our work, as a first attempt, shows the potential for employing LLMs to enhance the performance in computer vision tasks.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 35,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.18583",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-29",
    "authors": [
      {
        "authorId": "2218634341",
        "name": "Tianjun Zhang"
      },
      {
        "authorId": "2153910714",
        "name": "Yi Zhang"
      },
      {
        "authorId": "143729959",
        "name": "Vibhav Vineet"
      },
      {
        "authorId": "39678486",
        "name": "Neel Joshi"
      },
      {
        "authorId": "2153689937",
        "name": "Xin Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 123.75278407684165
  },
  {
    "paperId": "94c82c25d8943fea9461d804257ee16c9d672548",
    "url": "https://www.semanticscholar.org/paper/94c82c25d8943fea9461d804257ee16c9d672548",
    "title": "OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data",
    "abstract": "Mathematical reasoning continues to be a critical challenge in large language model (LLM) development with significant interest. However, most of the cutting-edge progress in mathematical reasoning with LLMs has become \\emph{closed-source} due to lack of access to training data. This lack of data access limits researchers from understanding the impact of different choices for synthesizing and utilizing the data. With the goal of creating a high-quality finetuning (SFT) dataset for math reasoning, we conduct careful ablation experiments on data synthesis using the recently released \\texttt{Llama3.1} family of models. Our experiments show that: (a) solution format matters, with excessively verbose solutions proving detrimental to SFT performance, (b) data generated by a strong teacher outperforms equally-sized data generated by a weak student model, (c) SFT is robust to low-quality solutions, allowing for imprecise data filtering, and (d) question diversity is crucial for achieving data scaling gains. Based on these insights, we create the OpenMathInstruct-2 dataset, which consists of 14M question-solution pairs ($\\approx$ 600K unique questions), making it nearly eight times larger than the previous largest open-source math reasoning dataset. Finetuning the \\texttt{Llama-3.1-8B-Base} using OpenMathInstruct-2 outperforms \\texttt{Llama3.1-8B-Instruct} on MATH by an absolute 15.9\\% (51.9\\% $\\rightarrow$ 67.8\\%). Finally, to accelerate the open-source efforts, we release the code, the finetuned models, and the OpenMathInstruct-2 dataset under a commercially permissive license.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-02",
    "authors": [
      {
        "authorId": "2634203",
        "name": "Shubham Toshniwal"
      },
      {
        "authorId": "2323910998",
        "name": "Wei Du"
      },
      {
        "authorId": "2284217750",
        "name": "Ivan Moshkov"
      },
      {
        "authorId": "9107622",
        "name": "B. Kisačanin"
      },
      {
        "authorId": "2323782209",
        "name": "Alexan Ayrapetyan"
      },
      {
        "authorId": "25683112",
        "name": "Igor Gitman"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "e4eaaaf1e435061bb2eed5f6dc187e453c3bb086",
    "url": "https://www.semanticscholar.org/paper/e4eaaaf1e435061bb2eed5f6dc187e453c3bb086",
    "title": "Large Language Models Can Automatically Engineer Features for Few-Shot Tabular Learning",
    "abstract": "Large Language Models (LLMs), with their remarkable ability to tackle challenging and unseen reasoning problems, hold immense potential for tabular learning, that is vital for many real-world applications. In this paper, we propose a novel in-context learning framework, FeatLLM, which employs LLMs as feature engineers to produce an input data set that is optimally suited for tabular predictions. The generated features are used to infer class likelihood with a simple downstream machine learning model, such as linear regression and yields high performance few-shot learning. The proposed FeatLLM framework only uses this simple predictive model with the discovered features at inference time. Compared to existing LLM-based approaches, FeatLLM eliminates the need to send queries to the LLM for each sample at inference time. Moreover, it merely requires API-level access to LLMs, and overcomes prompt size limitations. As demonstrated across numerous tabular datasets from a wide range of domains, FeatLLM generates high-quality rules, significantly (10% on average) outperforming alternatives such as TabLLM and STUNT.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-15",
    "authors": [
      {
        "authorId": "2296740936",
        "name": "Sungwon Han"
      },
      {
        "authorId": "2256335437",
        "name": "Jinsung Yoon"
      },
      {
        "authorId": "2676352",
        "name": "Sercan Ö. Arik"
      },
      {
        "authorId": "2264567300",
        "name": "Tomas Pfister"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "40b7a181cbd906ef0358141d4fa4f284e74cde59",
    "url": "https://www.semanticscholar.org/paper/40b7a181cbd906ef0358141d4fa4f284e74cde59",
    "title": "Benchmarking Large Language Models in Complex Question Answering Attribution using Knowledge Graphs",
    "abstract": "The attribution of question answering is to provide citations for supporting generated statements, and has attracted wide research attention. The current methods for automatically evaluating the attribution, which are often based on Large Language Models (LLMs), are still inadequate, particularly in recognizing subtle differences between attributions, and complex relationships between citations and statements. To compare these attribution evaluation methods and develop new ones, we introduce a set of fine-grained categories (i.e., supportive, insufficient, contradictory and irrelevant) for measuring the attribution, and develop a Complex Attributed Question Answering (CAQA) benchmark by leveraging knowledge graphs (KGs) for automatically generating attributions of different categories to question-answer pairs. Our analysis reveals that existing evaluators perform poorly under fine-grained attribution settings and exhibit weaknesses in complex citation-statement reasoning. Our CAQA benchmark, validated with human annotations, emerges as a promising tool for selecting and developing LLM attribution evaluators.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-26",
    "authors": [
      {
        "authorId": "2154445064",
        "name": "Nan Hu"
      },
      {
        "authorId": "2269695376",
        "name": "Jiaoyan Chen"
      },
      {
        "authorId": "2281709529",
        "name": "Yike Wu"
      },
      {
        "authorId": "2261812515",
        "name": "Guilin Qi"
      },
      {
        "authorId": "2065970582",
        "name": "Sheng Bi"
      },
      {
        "authorId": "1514917592",
        "name": "Tongtong Wu"
      },
      {
        "authorId": "2281582268",
        "name": "Jeff Z. Pan"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "4906654ecb2d42cfee13f46bebe2ec02bb6442b1",
    "url": "https://www.semanticscholar.org/paper/4906654ecb2d42cfee13f46bebe2ec02bb6442b1",
    "title": "A Survey of Useful LLM Evaluation",
    "abstract": "LLMs have gotten attention across various research domains due to their exceptional performance on a wide range of complex tasks. Therefore, refined methods to evaluate the capabilities of LLMs are needed to determine the tasks and responsibility they should undertake. Our study mainly discussed how LLMs, as useful tools, should be effectively assessed. We proposed the two-stage framework: from ``core ability'' to ``agent'', clearly explaining how LLMs can be applied based on their specific capabilities, along with the evaluation methods in each stage. Core ability refers to the capabilities that LLMs need in order to generate high-quality natural language texts. After confirming LLMs possess core ability, they can solve real-world and complex tasks as agent. In the\"core ability\"stage, we discussed the reasoning ability, societal impact, and domain knowledge of LLMs. In the ``agent'' stage, we demonstrated embodied action, planning, and tool learning of LLMs agent applications. Finally, we examined the challenges currently confronting the evaluation methods for LLMs, as well as the directions for future development.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-06-03",
    "authors": [
      {
        "authorId": "2304518297",
        "name": "Ji-Lun Peng"
      },
      {
        "authorId": "2294318054",
        "name": "Sijia Cheng"
      },
      {
        "authorId": "2304478854",
        "name": "Egil Diau"
      },
      {
        "authorId": "2304479371",
        "name": "Yung-Yu Shih"
      },
      {
        "authorId": "2294305437",
        "name": "Po-Heng Chen"
      },
      {
        "authorId": "1557269413",
        "name": "Yen-Ting Lin"
      },
      {
        "authorId": "2269063224",
        "name": "Yun-Nung Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "00c6a500726a85090e1b2e7e8f4c5226ff56d86d",
    "url": "https://www.semanticscholar.org/paper/00c6a500726a85090e1b2e7e8f4c5226ff56d86d",
    "title": "LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation",
    "abstract": "The rise of multimodal misinformation on social platforms poses significant challenges for individuals and societies. Its increased credibility and broader impact compared to textual misinformation make detection complex, requiring robust reasoning across diverse media types and profound knowledge for accurate verification. The emergence of Large Vision Language Model (LVLM) offers a potential solution to this problem. Leveraging their proficiency in processing visual and textual information, LVLM demonstrates promising capabilities in recognizing complex information and exhibiting strong reasoning skills. In this paper, we first investigate the potential of LVLM on multimodal misinformation detection. We find that even though LVLM has a superior performance compared to LLMs, its profound reasoning may present limited power with a lack of evidence. Based on these observations, we propose LEMMA: LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation. LEMMA leverages LVLM intuition and reasoning capabilities while augmenting them with external knowledge to enhance the accuracy of misinformation detection. Our method improves the accuracy over the top baseline LVLM by 7% and 13% on Twitter and Fakeddit datasets respectively.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-19",
    "authors": [
      {
        "authorId": "2159165546",
        "name": "Keyang Xuan"
      },
      {
        "authorId": "2284699143",
        "name": "Li Yi"
      },
      {
        "authorId": "2284752655",
        "name": "Fan Yang"
      },
      {
        "authorId": "2284825622",
        "name": "Ruochen Wu"
      },
      {
        "authorId": "51135899",
        "name": "Y. Fung"
      },
      {
        "authorId": "2277409745",
        "name": "Heng Ji"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "71e3c22ec3b1522dfeef118d168ecd144121489b",
    "url": "https://www.semanticscholar.org/paper/71e3c22ec3b1522dfeef118d168ecd144121489b",
    "title": "Integrate the Essence and Eliminate the Dross: Fine-Grained Self-Consistency for Free-Form Language Generation",
    "abstract": "Self-consistency (SC), leveraging multiple samples from LLMs, shows significant gains on various reasoning tasks but struggles with free-form generation due to the difficulty of aggregating answers. Its variants, UCS and USC, rely on sample selection or voting mechanisms to improve output quality. These methods, however, face limitations due to their inability to fully utilize the nuanced consensus knowledge present within multiple candidate samples, often resulting in suboptimal outputs. We propose Fine-Grained Self-Consistency (FSC) to addresses these limitations by extracting and integrating segment-level commonalities from candidate samples, enhancing the performance of LLMs both in open-ended and reasoning tasks. Based on this, we present two additional strategies: candidate filtering, which enhances overall quality by identifying highly similar candidate sets, and merging, which reduces input token requirements by combining similar samples. The effectiveness of FSC is demonstrated through extensive experiments on various tasks, including summarization, code generation, and mathematical reasoning, using GPT-3.5-turbo and GPT-4. The results indicate significant improvements over baseline methods, showcasing the potential of FSC to optimize output quality by effectively synthesizing fine-grained consensus knowledge from multiple samples.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-02",
    "authors": [
      {
        "authorId": "2254837328",
        "name": "Xinglin Wang"
      },
      {
        "authorId": "2275295939",
        "name": "Yiwei Li"
      },
      {
        "authorId": "1519472504",
        "name": "Shaoxiong Feng"
      },
      {
        "authorId": "2253609260",
        "name": "Peiwen Yuan"
      },
      {
        "authorId": "2275135250",
        "name": "Boyuan Pan"
      },
      {
        "authorId": "2275449154",
        "name": "Heda Wang"
      },
      {
        "authorId": "2309659583",
        "name": "Yao Hu"
      },
      {
        "authorId": "2253855196",
        "name": "Kan Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "86cbb54b25892c004fab29ab73c4e82cc5d4eb28",
    "url": "https://www.semanticscholar.org/paper/86cbb54b25892c004fab29ab73c4e82cc5d4eb28",
    "title": "Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction",
    "abstract": "The task of condensing large chunks of textual information into concise and structured tables has gained attention recently due to the emergence of Large Language Models (LLMs) and their potential benefit for downstream tasks, such as text summarization and text mining. Previous approaches often generate tables that directly replicate information from the text, limiting their applicability in broader contexts, as text-to-table generation in real-life scenarios necessitates information extraction, reasoning, and integration. However, there is a lack of both datasets and methodologies towards this task. In this paper, we introduce LiveSum, a new benchmark dataset created for generating summary tables of competitions based on real-time commentary texts. We evaluate the performances of state-of-the-art LLMs on this task in both fine-tuning and zero-shot settings, and additionally propose a novel pipeline called T^3(Text-Tuple-Table) to improve their performances. Extensive experimental results demonstrate that LLMs still struggle with this task even after fine-tuning, while our approach can offer substantial performance gains without explicit training. Further analyses demonstrate that our method exhibits strong generalization abilities, surpassing previous approaches on several other text-to-table datasets. Our codeand data can be found at https://github.com/HKUST-KnowComp/LiveSum.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-22",
    "authors": [
      {
        "authorId": "2260296509",
        "name": "Zheye Deng"
      },
      {
        "authorId": "2216598559",
        "name": "Chunkit Chan"
      },
      {
        "authorId": "1587728690",
        "name": "Weiqi Wang"
      },
      {
        "authorId": "2297817701",
        "name": "Yuxi Sun"
      },
      {
        "authorId": "2262397115",
        "name": "Wei Fan"
      },
      {
        "authorId": "2209990450",
        "name": "Tianshi ZHENG"
      },
      {
        "authorId": "2297769543",
        "name": "Yauwai Yim"
      },
      {
        "authorId": "2241325169",
        "name": "Yangqiu Song"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "d24013cc145edc491504c9a39bf9cc3af275a99e",
    "url": "https://www.semanticscholar.org/paper/d24013cc145edc491504c9a39bf9cc3af275a99e",
    "title": "Retrieval-enhanced Knowledge Editing in Language Models for Multi-Hop Question Answering",
    "abstract": "Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging when dealing with multi-hop questions, since they require LLMs to update and integrate multiple knowledge pieces relevant to the questions. To tackle the problem, we propose the Retrieval-Augmented model Editing (RAE) framework for multi-hop question answering. RAE first retrieves edited facts and then refines the language model through in-context learning. Specifically, our retrieval approach, based on mutual information maximization, leverages the reasoning abilities of LLMs to identify chain facts that traditional similarity-based searches might miss. In addition, our framework includes a pruning strategy to eliminate redundant information from the retrieved facts, which enhances the editing accuracy and mitigates the hallucination problem. Our framework is supported by theoretical justification for its fact retrieval efficacy. Finally, comprehensive evaluation across various LLMs validates RAE's ability in providing accurate answers with updated knowledge. Our code is available at: https://github.com/sycny/RAE.",
    "venue": "International Conference on Information and Knowledge Management",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2403.19631",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-28",
    "authors": [
      {
        "authorId": "2249001715",
        "name": "Yucheng Shi"
      },
      {
        "authorId": "2293725776",
        "name": "Qiaoyu Tan"
      },
      {
        "authorId": "2145346360",
        "name": "Xuansheng Wu"
      },
      {
        "authorId": "2181946372",
        "name": "Shaochen Zhong"
      },
      {
        "authorId": "2261280451",
        "name": "Kaixiong Zhou"
      },
      {
        "authorId": "2256183798",
        "name": "Ninghao Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "e80252dabd94148ad959c42e0121795939a78198",
    "url": "https://www.semanticscholar.org/paper/e80252dabd94148ad959c42e0121795939a78198",
    "title": "Remote Sensing ChatGPT: Solving Remote Sensing Tasks with ChatGPT and Visual Models",
    "abstract": "Recently, there has been a surge in interest in Large Language Models (LLMs), with ChatGPT standing out for its exceptional capabilities in language comprehension, reasoning, and interactive communication. These models have garnered attention from a diverse array of users and researchers across various disciplines. While LLMs have demonstrated remarkable proficiency in mimicking human task execution through natural language, their application in remote sensing interpretation remains largely uncharted. Furthermore, the current lack of automation in remote sensing task planning limits the accessibility of these sophisticated interpretation techniques, especially for non-specialists in the field. To bridge this gap, we introduce Remote Sensing ChatGPT, an innovative LLM-driven agent that integrates ChatGPT with a suite of AI-powered remote sensing models to tackle complex interpretation challenges. This system is designed to interpret user requests, delineate task planning based on the functionalities required, execute each subtask sequentially, and compile the final output by synthesizing the results from each stage. Given that LLMs, trained predominantly on natural language, do not inherently comprehend visual elements present in remote sensing imagery, we have devised a method to incorporate visual cues, effectively embedding the visual context of remote sensing images into the ChatGPT framework. With Remote Sensing ChatGPT, users can effortlessly submit a remote sensing image alongside their query and promptly receive detailed interpretation outcomes along with comprehensive linguistic feedback. Experiments and case studies demonstrate that our method is adept at handling a diverse range of remote sensing tasks and has the potential to be expanded to encompass an even wider array of applications with the integration of more advanced models, such as remote sensing foundation model. The code and demo of Remote Sensing ChatGPT is publicly available at https://github.com/HaonanGuo/Remote-Sensing-ChatGPT.",
    "venue": "IEEE International Geoscience and Remote Sensing Symposium",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-17",
    "authors": [
      {
        "authorId": "145381798",
        "name": "Haonan Guo"
      },
      {
        "authorId": "2106353717",
        "name": "Xin Su"
      },
      {
        "authorId": "143622134",
        "name": "Chen Wu"
      },
      {
        "authorId": "2249761976",
        "name": "Bo Du"
      },
      {
        "authorId": "1720539",
        "name": "L. Zhang"
      },
      {
        "authorId": "2279205829",
        "name": "Deren Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.19162312519754
  },
  {
    "paperId": "4bfb6070f6a80de6a85b9edab1fb332fd0a56e69",
    "url": "https://www.semanticscholar.org/paper/4bfb6070f6a80de6a85b9edab1fb332fd0a56e69",
    "title": "Learning From Correctness Without Prompting Makes LLM Efficient Reasoner",
    "abstract": "Large language models (LLMs) have demonstrated outstanding performance across various tasks, yet they still exhibit limitations such as hallucination, unfaithful reasoning, and toxic content. One potential approach to mitigate these issues is learning from human or external feedback (e.g. tools). In this paper, we introduce an intrinsic self-correct reasoning framework for LLMs that eliminates the need for human feedback, external tools, and handcraft prompts. The proposed framework, based on a multi-step reasoning paradigm \\textbf{Le}arning from \\textbf{Co}rrectness (\\textsc{LeCo}), improves reasoning performance without needing to learn from errors. This paradigm prioritizes learning from correct reasoning steps, and a unique method to measure confidence for each reasoning step based on generation logits. Experimental results across various multi-step reasoning tasks demonstrate the effectiveness of the framework in improving reasoning performance with reduced token consumption.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-28",
    "authors": [
      {
        "authorId": "2257363981",
        "name": "Yuxuan Yao"
      },
      {
        "authorId": "2284619810",
        "name": "Han Wu"
      },
      {
        "authorId": "2681038",
        "name": "Zhijiang Guo"
      },
      {
        "authorId": "2294146906",
        "name": "Biyan Zhou"
      },
      {
        "authorId": "2281982336",
        "name": "Jiahui Gao"
      },
      {
        "authorId": "2182235729",
        "name": "Sichun Luo"
      },
      {
        "authorId": "2270631179",
        "name": "Hanxu Hou"
      },
      {
        "authorId": "2266426334",
        "name": "Xiaojin Fu"
      },
      {
        "authorId": "2257556686",
        "name": "Linqi Song"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "60d26e0d96671391d17815c50750f68a64fe241d",
    "url": "https://www.semanticscholar.org/paper/60d26e0d96671391d17815c50750f68a64fe241d",
    "title": "Can We Verify Step by Step for Incorrect Answer Detection?",
    "abstract": "Chain-of-Thought (CoT) prompting has marked a significant advancement in enhancing the reasoning capabilities of large language models (LLMs). Previous studies have developed various extensions of CoT, which focus primarily on enhancing end-task performance. In addition, there has been research on assessing the quality of reasoning chains in CoT. This raises an intriguing question: Is it possible to predict the accuracy of LLM outputs by scrutinizing the reasoning chains they generate? To answer this research question, we introduce a benchmark, R2PE, designed specifically to explore the relationship between reasoning chains and performance in various reasoning tasks spanning five different domains. This benchmark aims to measure the falsehood of the final output of LLMs based on the reasoning steps. To make full use of information in multiple reasoning chains, we propose the process discernibility score (PDS) framework that beats the answer-checking baseline by a large margin. Concretely, this resulted in an average of $5.1\\%$ increase in the F1 score and $2.97\\%$ improvement in AUC-PR across all 45 subsets within R2PE. We further demonstrate our PDS's efficacy in advancing open-domain QA accuracy.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-16",
    "authors": [
      {
        "authorId": "2284822089",
        "name": "Xin Xu"
      },
      {
        "authorId": "2305683433",
        "name": "Shizhe Diao"
      },
      {
        "authorId": "2284640247",
        "name": "Can Yang"
      },
      {
        "authorId": "2284893872",
        "name": "Yang Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "0a39a9d3d884501ae103ec47d48608b6b642203a",
    "url": "https://www.semanticscholar.org/paper/0a39a9d3d884501ae103ec47d48608b6b642203a",
    "title": "Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective",
    "abstract": "Recent advancements in Large Language Models (LLMs) have facilitated the development of Multimodal LLMs (MLLMs). Despite their impressive capabilities, MLLMs often suffer from over-reliance on unimodal biases (e.g., language bias and vision bias), leading to incorrect answers or hallucinations in complex multimodal tasks. To investigate this issue, we propose a causal framework to interpret the biases in Visual Question Answering (VQA) problems. Within this framework, we conduct an in-depth causal analysis to assess the causal effect of these biases on MLLM predictions. Based on the analysis, we introduce 1) a novel MORE dataset with 12,000 challenging VQA instances requiring multi-hop reasoning and overcoming unimodal biases. 2) a causality-enhanced agent framework CAVE that guides models to comprehensively integrate information from different modalities and mitigate biases. Our experiments show that MLLMs perform poorly on MORE, indicating strong unimodal biases and limited semantic understanding. However, when integrated with our CAVE, promising improvements in reasoning and bias mitigation can be seen. These findings provide important insights for the development of more robust MLLMs and contribute to the broader goal of advancing multimodal AI systems capable of deeper understanding and reasoning. Our project page is at https://github.com/OpenCausaLab/MORE.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-27",
    "authors": [
      {
        "authorId": "2108612706",
        "name": "Meiqi Chen"
      },
      {
        "authorId": "2258676539",
        "name": "Yixin Cao"
      },
      {
        "authorId": "2293661296",
        "name": "Yan Zhang"
      },
      {
        "authorId": "2293680941",
        "name": "Chaochao Lu"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "f833cb351af8cf540808fc523a24a3eb405a8150",
    "url": "https://www.semanticscholar.org/paper/f833cb351af8cf540808fc523a24a3eb405a8150",
    "title": "AdaMoLE: Fine-Tuning Large Language Models with Adaptive Mixture of Low-Rank Adaptation Experts",
    "abstract": "We introduce AdaMoLE, a novel method for fine-tuning large language models (LLMs) through an Adaptive Mixture of Low-Rank Adaptation (LoRA) Experts. Moving beyond conventional methods that employ a static top-k strategy for activating experts, AdaMoLE dynamically adjusts the activation threshold using a dedicated threshold network, adaptively responding to the varying complexities of different tasks. By replacing a single LoRA in a layer with multiple LoRA experts and integrating a gating function with the threshold mechanism, AdaMoLE effectively selects and activates the most appropriate experts based on the input context. Our extensive evaluations across a variety of commonsense reasoning and natural language processing tasks show that AdaMoLE exceeds baseline performance. This enhancement highlights the advantages of AdaMoLE's adaptive selection of LoRA experts, improving model effectiveness without a corresponding increase in the expert count. The experimental validation not only confirms AdaMoLE as a robust approach for enhancing LLMs but also suggests valuable directions for future research in adaptive expert selection mechanisms, potentially broadening the scope for optimizing model performance across diverse language processing tasks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-01",
    "authors": [
      {
        "authorId": "2299293948",
        "name": "Zefang Liu"
      },
      {
        "authorId": "2299176690",
        "name": "Jiahua Luo"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "f091acf9dc2cc486c253dcead3a74ba916c64eb7",
    "url": "https://www.semanticscholar.org/paper/f091acf9dc2cc486c253dcead3a74ba916c64eb7",
    "title": "IM-RAG: Multi-Round Retrieval-Augmented Generation Through Learning Inner Monologues",
    "abstract": "Although the Retrieval-Augmented Generation (RAG) paradigms can use external knowledge to enhance and ground the outputs of Large Language Models (LLMs) to mitigate generative hallucinations and static knowledge base problems, they still suffer from limited flexibility in adopting Information Retrieval (IR) systems with varying capabilities, constrained interpretability during the multi-round retrieval process, and a lack of end-to-end optimization. To address these challenges, we propose a novel LLM-centric approach, IM-RAG, that integrates IR systems with LLMs to support multi-round RAG through learning Inner Monologues (IM, i.e., the human inner voice that narrates one's thoughts). During the IM process, the LLM serves as the core reasoning model (i.e., Reasoner) to either propose queries to collect more information via the Retriever or to provide a final answer based on the conversational context. We also introduce a Refiner that improves the outputs from the Retriever, effectively bridging the gap between the Reasoner and IR modules with varying capabilities and fostering multi-round communications. The entire IM process is optimized via Reinforcement Learning (RL) where a Progress Tracker is incorporated to provide mid-step rewards, and the answer prediction is further separately optimized via Supervised Fine-Tuning (SFT). We conduct extensive experiments with the HotPotQA dataset, a popular benchmark for retrieval-based, multi-step question-answering. The results show that our approach achieves state-of-the-art (SOTA) performance while providing high flexibility in integrating IR modules as well as strong interpretability exhibited in the learned inner monologues.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-15",
    "authors": [
      {
        "authorId": "2326258875",
        "name": "Diji Yang"
      },
      {
        "authorId": "2238635148",
        "name": "Jinmeng Rao"
      },
      {
        "authorId": "2238911526",
        "name": "Kezhen Chen"
      },
      {
        "authorId": "2284184369",
        "name": "Xiaoyuan Guo"
      },
      {
        "authorId": "2145040404",
        "name": "Yawen Zhang"
      },
      {
        "authorId": "2239161755",
        "name": "Jie Yang"
      },
      {
        "authorId": "2311353391",
        "name": "Yi Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.1886522358297
  },
  {
    "paperId": "d08218f29da505a11abfd1f245b3cb8e121480a4",
    "url": "https://www.semanticscholar.org/paper/d08218f29da505a11abfd1f245b3cb8e121480a4",
    "title": "CoD, Towards an Interpretable Medical Agent using Chain of Diagnosis",
    "abstract": "The field of medical diagnosis has undergone a significant transformation with the advent of large language models (LLMs), yet the challenges of interpretability within these models remain largely unaddressed. This study introduces Chain-of-Diagnosis (CoD) to enhance the interpretability of LLM-based medical diagnostics. CoD transforms the diagnostic process into a diagnostic chain that mirrors a physician's thought process, providing a transparent reasoning pathway. Additionally, CoD outputs the disease confidence distribution to ensure transparency in decision-making. This interpretability makes model diagnostics controllable and aids in identifying critical symptoms for inquiry through the entropy reduction of confidences. With CoD, we developed DiagnosisGPT, capable of diagnosing 9604 diseases. Experimental results demonstrate that DiagnosisGPT outperforms other LLMs on diagnostic benchmarks. Moreover, DiagnosisGPT provides interpretability while ensuring controllability in diagnostic rigor.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-18",
    "authors": [
      {
        "authorId": "2108170007",
        "name": "Junying Chen"
      },
      {
        "authorId": "2311890257",
        "name": "Chi Gui"
      },
      {
        "authorId": "2266840612",
        "name": "Anningzhe Gao"
      },
      {
        "authorId": "2304467368",
        "name": "Ke Ji"
      },
      {
        "authorId": "2267007135",
        "name": "Xidong Wang"
      },
      {
        "authorId": "2101317304",
        "name": "Xiang Wan"
      },
      {
        "authorId": "2267007505",
        "name": "Benyou Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "4582779668ab801f29db457790cd291767510035",
    "url": "https://www.semanticscholar.org/paper/4582779668ab801f29db457790cd291767510035",
    "title": "Logic Query of Thoughts: Guiding Large Language Models to Answer Complex Logic Queries with Knowledge Graphs",
    "abstract": "Despite the superb performance in many tasks, large language models (LLMs) bear the risk of generating hallucination or even wrong answers when confronted with tasks that demand the accuracy of knowledge. The issue becomes even more noticeable when addressing logic queries that require multiple logic reasoning steps. On the other hand, knowledge graph (KG) based question answering methods are capable of accurately identifying the correct answers with the help of knowledge graph, yet its accuracy could quickly deteriorate when the knowledge graph itself is sparse and incomplete. It remains a critical challenge on how to integrate knowledge graph reasoning with LLMs in a mutually beneficial way so as to mitigate both the hallucination problem of LLMs as well as the incompleteness issue of knowledge graphs. In this paper, we propose 'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs with knowledge graph based logic query reasoning. LGOT seamlessly combines knowledge graph reasoning and LLMs, effectively breaking down complex logic queries into easy to answer subquestions. Through the utilization of both knowledge graph reasoning and LLMs, it successfully derives answers for each subquestion. By aggregating these results and selecting the highest quality candidate answers for each step, LGOT achieves accurate results to complex questions. Our experimental findings demonstrate substantial performance enhancements, with up to 20% improvement over ChatGPT.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-17",
    "authors": [
      {
        "authorId": "1505803281",
        "name": "Lihui Liu"
      },
      {
        "authorId": "1390878075",
        "name": "Zihao Wang"
      },
      {
        "authorId": "2187298875",
        "name": "Ruizhong Qiu"
      },
      {
        "authorId": "51280934",
        "name": "Yikun Ban"
      },
      {
        "authorId": "2254262311",
        "name": "Hanghang Tong"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "9d851cd329dbcb5b4fc6a56c277303cfbd9c646e",
    "url": "https://www.semanticscholar.org/paper/9d851cd329dbcb5b4fc6a56c277303cfbd9c646e",
    "title": "Small Language Models: Survey, Measurements, and Insights",
    "abstract": "Small language models (SLMs), despite their widespread adoption in modern smart devices, have received significantly less academic attention compared to their large language model (LLM) counterparts, which are predominantly deployed in data centers and cloud environments. While researchers continue to improve the capabilities of LLMs in the pursuit of artificial general intelligence, SLM research aims to make machine intelligence more accessible, affordable, and efficient for everyday tasks. Focusing on transformer-based, decoder-only language models with 100M-5B parameters, we survey 59 state-of-the-art open-source SLMs, analyzing their technical innovations across three axes: architectures, training datasets, and training algorithms. In addition, we evaluate their capabilities in various domains, including commonsense reasoning, in-context learning, mathematics, and coding. To gain further insight into their on-device runtime costs, we benchmark their inference latency and memory footprints. Through in-depth analysis of our benchmarking data, we offer valuable insights to advance research in this field.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-09-24",
    "authors": [
      {
        "authorId": "2279828070",
        "name": "Zhenyan Lu"
      },
      {
        "authorId": "2307049398",
        "name": "Xiang Li"
      },
      {
        "authorId": "1751449",
        "name": "Dongqi Cai"
      },
      {
        "authorId": "2170360770",
        "name": "Rongjie Yi"
      },
      {
        "authorId": "2287944355",
        "name": "Fangming Liu"
      },
      {
        "authorId": "2318084177",
        "name": "Xiwen Zhang"
      },
      {
        "authorId": "2298756346",
        "name": "N. D. Lane"
      },
      {
        "authorId": "2239403838",
        "name": "Mengwei Xu"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "39b2b3d543b01ec192cba9c7fce7c177bfc33869",
    "url": "https://www.semanticscholar.org/paper/39b2b3d543b01ec192cba9c7fce7c177bfc33869",
    "title": "Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks",
    "abstract": "Medical knowledge is context-dependent and requires consistent reasoning across various natural language expressions of semantically equivalent phrases. This is particularly crucial for drug names, where patients often use brand names like Advil or Tylenol instead of their generic equivalents. To study this, we create a new robustness dataset, RABBITS, to evaluate performance differences on medical benchmarks after swapping brand and generic drug names using physician expert annotations. We assess both open-source and API-based LLMs on MedQA and MedMCQA, revealing a consistent performance drop ranging from 1-10\\%. Furthermore, we identify a potential source of this fragility as the contamination of test data in widely used pre-training datasets. All code is accessible at https://github.com/BittermanLab/RABBITS, and a HuggingFace leaderboard is available at https://huggingface.co/spaces/AIM-Harvard/rabbits-leaderboard.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-17",
    "authors": [
      {
        "authorId": "2111654456",
        "name": "J. Gallifant"
      },
      {
        "authorId": "119258238",
        "name": "Shan Chen"
      },
      {
        "authorId": "2300370785",
        "name": "Pedro Moreira"
      },
      {
        "authorId": "2300370619",
        "name": "Nikolaj Munch"
      },
      {
        "authorId": "2300813512",
        "name": "Mingye Gao"
      },
      {
        "authorId": "2307076471",
        "name": "Jackson Pond"
      },
      {
        "authorId": "2258952340",
        "name": "L. Celi"
      },
      {
        "authorId": "2300370066",
        "name": "Hugo Aerts"
      },
      {
        "authorId": "32452740",
        "name": "Thomas Hartvigsen"
      },
      {
        "authorId": "2260340486",
        "name": "D. Bitterman"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "d3650dfbc521f2fec534b3d92f3b33f4faaf29f1",
    "url": "https://www.semanticscholar.org/paper/d3650dfbc521f2fec534b3d92f3b33f4faaf29f1",
    "title": "Circuit Transformer: End-to-end Circuit Design by Predicting the Next Gate",
    "abstract": "Language, a prominent human ability to express through sequential symbols, has been computationally mastered by recent advances of large language models (LLMs). By predicting the next word recurrently with huge neural models, LLMs have shown unprecedented capabilities in understanding and reasoning. Circuit, as the\"language\"of electronic design, specifies the functionality of an electronic device by cascade connections of logic gates. Then, can circuits also be mastered by a a sufficiently large\"circuit model\", which can conquer electronic design tasks by simply predicting the next logic gate? In this work, we take the first step to explore such possibilities. Two primary barriers impede the straightforward application of LLMs to circuits: their complex, non-sequential structure, and the intolerance of hallucination due to strict constraints (e.g., equivalence). For the first barrier, we encode a circuit as a memory-less, depth-first traversal trajectory, which allows Transformer-based neural models to better leverage its structural information, and predict the next gate on the trajectory as a circuit model. For the second barrier, we introduce an equivalence-preserving decoding process, which ensures that every token in the generated trajectory adheres to the specified equivalence constraints. Moreover, the circuit model can also be regarded as a stochastic policy to tackle optimization-oriented circuit design tasks. Experimentally, we trained a Transformer-based model of 88M parameters, named\"Circuit Transformer\", which demonstrates impressive performance in end-to-end logic synthesis. With Monte-Carlo tree search, Circuit Transformer significantly improves over resyn2 while retaining strict equivalence, showcasing the potential of generative AI in conquering electronic design challenges.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-14",
    "authors": [
      {
        "authorId": "2108414275",
        "name": "Xihan Li"
      },
      {
        "authorId": "2155447824",
        "name": "Xing Li"
      },
      {
        "authorId": "2238389220",
        "name": "Lei Chen"
      },
      {
        "authorId": "2292936988",
        "name": "Xing Zhang"
      },
      {
        "authorId": "2241572771",
        "name": "Mingxuan Yuan"
      },
      {
        "authorId": "2293033006",
        "name": "Jun Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "7e5340568db83df201d183de65cf9ff68c48eeb8",
    "url": "https://www.semanticscholar.org/paper/7e5340568db83df201d183de65cf9ff68c48eeb8",
    "title": "RT: a Retrieving and Chain-of-Thought framework for few-shot medical named entity recognition",
    "abstract": "Abstract Objectives This article aims to enhance the performance of larger language models (LLMs) on the few-shot biomedical named entity recognition (NER) task by developing a simple and effective method called Retrieving and Chain-of-Thought (RT) framework and to evaluate the improvement after applying RT framework. Materials and Methods Given the remarkable advancements in retrieval-based language model and Chain-of-Thought across various natural language processing tasks, we propose a pioneering RT framework designed to amalgamate both approaches. The RT approach encompasses dedicated modules for information retrieval and Chain-of-Thought processes. In the retrieval module, RT discerns pertinent examples from demonstrations during instructional tuning for each input sentence. Subsequently, the Chain-of-Thought module employs a systematic reasoning process to identify entities. We conducted a comprehensive comparative analysis of our RT framework against 16 other models for few-shot NER tasks on BC5CDR and NCBI corpora. Additionally, we explored the impacts of negative samples, output formats, and missing data on performance. Results Our proposed RT framework outperforms other LMs for few-shot NER tasks with micro-F1 scores of 93.50 and 91.76 on BC5CDR and NCBI corpora, respectively. We found that using both positive and negative samples, Chain-of-Thought (vs Tree-of-Thought) performed better. Additionally, utilization of a partially annotated dataset has a marginal effect of the model performance. Discussion This is the first investigation to combine a retrieval-based LLM and Chain-of-Thought methodology to enhance the performance in biomedical few-shot NER. The retrieval-based LLM aids in retrieving the most relevant examples of the input sentence, offering crucial knowledge to predict the entity in the sentence. We also conducted a meticulous examination of our methodology, incorporating an ablation study. Conclusion The RT framework with LLM has demonstrated state-of-the-art performance on few-shot NER tasks.",
    "venue": "J. Am. Medical Informatics Assoc.",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "https://academic.oup.com/jamia/advance-article-pdf/doi/10.1093/jamia/ocae095/57416202/ocae095.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-06",
    "authors": [
      {
        "authorId": "2262448361",
        "name": "Mingchen Li"
      },
      {
        "authorId": "2184546868",
        "name": "Huixue Zhou"
      },
      {
        "authorId": "2277810561",
        "name": "Han Yang"
      },
      {
        "authorId": "2261751642",
        "name": "Rui Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "ca60cb73d770f7093100cdea12b19881e7610df1",
    "url": "https://www.semanticscholar.org/paper/ca60cb73d770f7093100cdea12b19881e7610df1",
    "title": "DOSA: A Dataset of Social Artifacts from Different Indian Geographical Subcultures",
    "abstract": "Generative models are increasingly being used in various applications, such as text generation, commonsense reasoning, and question-answering. To be effective globally, these models must be aware of and account for local socio-cultural contexts, making it necessary to have benchmarks to evaluate the models for their cultural familiarity. Since the training data for LLMs is web-based and the Web is limited in its representation of information, it does not capture knowledge present within communities that are not on the Web. Thus, these models exacerbate the inequities, semantic misalignment, and stereotypes from the Web. There has been a growing call for community-centered participatory research methods in NLP. In this work, we respond to this call by using participatory research methods to introduce DOSA, the first community-generated Dataset of 615 Social Artifacts, by engaging with 260 participants from 19 different Indian geographic subcultures. We use a gamified framework that relies on collective sensemaking to collect the names and descriptions of these artifacts such that the descriptions semantically align with the shared sensibilities of the individuals from those cultures. Next, we benchmark four popular LLMs and find that they show significant variation across regional sub-cultures in their ability to infer the artifacts.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-23",
    "authors": [
      {
        "authorId": "2261734475",
        "name": "Agrima Seth"
      },
      {
        "authorId": "2266387781",
        "name": "Sanchit Ahuja"
      },
      {
        "authorId": "3086996",
        "name": "Kalika Bali"
      },
      {
        "authorId": "2256989615",
        "name": "Sunayana Sitaram"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "96f8ad79c40166bde112817009bf2205fee1c6ae",
    "url": "https://www.semanticscholar.org/paper/96f8ad79c40166bde112817009bf2205fee1c6ae",
    "title": "USimAgent: Large Language Models for Simulating Search Users",
    "abstract": "Due to the advantages in the cost-efficiency and reproducibility, user simulation has become a promising solution to the user-centric evaluation of information retrieval systems. Nonetheless, accurately simulating user search behaviors has long been a challenge, because users' actions in search are highly complex and driven by intricate cognitive processes such as learning, reasoning, and planning. Recently, Large Language Models (LLMs) have demonstrated remarked potential in simulating human-level intelligence and have been used in building autonomous agents for various tasks. However, the potential of using LLMs in simulating search behaviors has not yet been fully explored. In this paper, we introduce a LLM-based user search behavior simulator, USimAgent. The proposed simulator can simulate users' querying, clicking, and stopping behaviors during search, and thus, is capable of generating complete search sessions for specific search tasks. Empirical investigation on a real user behavior dataset shows that the proposed simulator outperforms existing methods in query generation and is comparable to traditional methods in predicting user clicks and stopping behaviors. These results not only validate the effectiveness of using LLMs for user simulation but also shed light on the development of a more robust and generic user simulators. The code and data are accessible at https://github.com/Meow-E/USimAgent.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-14",
    "authors": [
      {
        "authorId": "2291137915",
        "name": "Erhan Zhang"
      },
      {
        "authorId": "2291202074",
        "name": "Xingzhu Wang"
      },
      {
        "authorId": "2275055956",
        "name": "Peiyuan Gong"
      },
      {
        "authorId": "2291410987",
        "name": "Yankai Lin"
      },
      {
        "authorId": "2275056887",
        "name": "Jiaxin Mao"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.1886522358297
  },
  {
    "paperId": "0a470fbfebb5515e8af8d9a09f603df4dfce40a0",
    "url": "https://www.semanticscholar.org/paper/0a470fbfebb5515e8af8d9a09f603df4dfce40a0",
    "title": "A Systematic Evaluation of Large Language Models for Natural",
    "abstract": "“Recent efforts have evaluated large language models (LLMs) in areas such as com-monsense reasoning, mathematical reasoning, and code generation. However, to thebest of our knowledge, no work has specifically investigated the performance of LLMsin natural language generation (NLG) tasks, a pivotal criterion for determining modelexcellence. Thus, this paper conducts a comprehensive evaluation of well-known andhigh-performing LLMs, namely ChatGPT, ChatGLM, T5-based models, LLaMA-basedmodels, and Pythia-based models, in the context of NLG tasks. We select English andChinese datasets encompassing Dialogue Generation and Text Summarization. More-over, we propose a common evaluation setting that incorporates input templates andpost-processing strategies. Our study reports both automatic results, accompanied by adetailed analysis.”",
    "venue": "China National Conference on Chinese Computational Linguistics",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-16",
    "authors": [
      {
        "authorId": "2212474955",
        "name": "Xuanfan Ni"
      },
      {
        "authorId": "2295936549",
        "name": "Piji Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "5b6efeaf59d14944b415337c8c3b16766e47fc1f",
    "url": "https://www.semanticscholar.org/paper/5b6efeaf59d14944b415337c8c3b16766e47fc1f",
    "title": "Optimizing Numerical Estimation and Operational Efficiency in the Legal Domain through Large Language Models",
    "abstract": "The legal landscape encompasses a wide array of lawsuit types, presenting lawyers with challenges in delivering timely and accurate information to clients, particularly concerning critical aspects like potential imprisonment duration or financial repercussions. Compounded by the scarcity of legal experts, there's an urgent need to enhance the efficiency of traditional legal workflows. Recent advances in deep learning, especially Large Language Models (LLMs), offer promising solutions to this challenge. Leveraging LLMs' mathematical reasoning capabilities, we propose a novel approach integrating LLM-based methodologies with specially designed prompts to address precision requirements in legal Artificial Intelligence (LegalAI) applications. The proposed work seeks to bridge the gap between traditional legal practices and modern technological advancements, paving the way for a more accessible, efficient, and equitable legal system. To validate this method, we introduce a curated dataset tailored to precision-oriented LegalAI tasks, serving as a benchmark for evaluating LLM-based approaches. Extensive experimentation confirms the efficacy of our methodology in generating accurate numerical estimates within the legal domain, emphasizing the role of LLMs in streamlining legal processes and meeting the evolving demands of LegalAI.",
    "venue": "International Conference on Information and Knowledge Management",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-26",
    "authors": [
      {
        "authorId": "2298943481",
        "name": "Jia-Hong Huang"
      },
      {
        "authorId": "2313643070",
        "name": "Chao-Chun Yang"
      },
      {
        "authorId": "2314339507",
        "name": "Yixian Shen"
      },
      {
        "authorId": "74138001",
        "name": "A. M. Pacces"
      },
      {
        "authorId": "1713134",
        "name": "E. Kanoulas"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "36f71673d9337b432babc51da77ef38b2070b5ed",
    "url": "https://www.semanticscholar.org/paper/36f71673d9337b432babc51da77ef38b2070b5ed",
    "title": "An LLM Compiler for Parallel Function Calling",
    "abstract": "The reasoning capabilities of the recent LLMs enable them to execute external function calls to overcome their inherent limitations, such as knowledge cutoffs, poor arithmetic skills, or lack of access to private data. This development has allowed LLMs to select and coordinate multiple functions based on the context to tackle more complex problems. However, current methods for function calling often require sequential reasoning and acting for each function which can result in high latency, cost, and sometimes inaccurate behavior. To address this, we introduce LLMCompiler, which executes functions in parallel to efficiently orchestrate multiple function calls. Drawing inspiration from the principles of classical compilers, LLMCompiler enables parallel function calling with three components: (i) a Function Calling Planner, formulating execution plans for function calling; (ii) a Task Fetching Unit, dispatching function calling tasks; and (iii) an Executor, executing these tasks in parallel. LLMCompiler automatically generates an optimized orchestration for the function calls and can be used with both open-source and closed-source models. We have benchmarked LLMCompiler on a range of tasks with different patterns of function calling. We observe consistent latency speedup of up to 3.7x, cost savings of up to 6.7x, and accuracy improvement of up to ~9% compared to ReAct. Our code is available at https://github.com/SqueezeAILab/LLMCompiler.",
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "citationCount": 33,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-07",
    "authors": [
      {
        "authorId": "2262511276",
        "name": "Sehoon Kim"
      },
      {
        "authorId": "2237424458",
        "name": "Suhong Moon"
      },
      {
        "authorId": "2270943053",
        "name": "Ryan Tabrizi"
      },
      {
        "authorId": "2167739385",
        "name": "Nicholas Lee"
      },
      {
        "authorId": "2271923589",
        "name": "Michael W. Mahoney"
      },
      {
        "authorId": "2242659602",
        "name": "Kurt Keutzer"
      },
      {
        "authorId": "10419477",
        "name": "A. Gholami"
      }
    ],
    "source": "semantic_scholar",
    "score": 122.89540786924243
  },
  {
    "paperId": "c74e9642ec71c6dfaadd3b8638c110d4048ff53e",
    "url": "https://www.semanticscholar.org/paper/c74e9642ec71c6dfaadd3b8638c110d4048ff53e",
    "title": "Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4",
    "abstract": "Unlike perfect information games, where all elements are known to every player, imperfect information games emulate the real-world complexities of decision-making under uncertain or incomplete information. GPT-4, the recent breakthrough in large language models (LLMs) trained on massive passive data, is notable for its knowledge retrieval and reasoning abilities. This paper delves into the applicability of GPT-4's learned knowledge for imperfect information games. To achieve this, we introduce \\textbf{Suspicion-Agent}, an innovative agent that leverages GPT-4's capabilities for performing in imperfect information games. With proper prompt engineering to achieve different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable adaptability across a range of imperfect information card games. Importantly, GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it can understand others and intentionally impact others' behavior. Leveraging this, we design a planning strategy that enables GPT-4 to competently play against different opponents, adapting its gameplay style as needed, while requiring only the game rules and descriptions of observations as input. In the experiments, we qualitatively showcase the capabilities of Suspicion-Agent across three different imperfect information games and then quantitatively evaluate it in Leduc Hold'em. The results show that Suspicion-Agent can potentially outperform traditional algorithms designed for imperfect information games, without any specialized training or examples. In order to encourage and foster deeper insights within the community, we make our game-related data publicly available.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 32,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.17277",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-29",
    "authors": [
      {
        "authorId": "15563286",
        "name": "Jiaxian Guo"
      },
      {
        "authorId": "2249871674",
        "name": "Bo Yang"
      },
      {
        "authorId": "143626110",
        "name": "Paul Yoo"
      },
      {
        "authorId": "51583409",
        "name": "Bill Yuchen Lin"
      },
      {
        "authorId": "1715282",
        "name": "Yusuke Iwasawa"
      },
      {
        "authorId": "2241471533",
        "name": "Yutaka Matsuo"
      }
    ],
    "source": "semantic_scholar",
    "score": 122.4476134219972
  },
  {
    "paperId": "eb34bd715e3a7c68477ec0824ebb171b948b80c2",
    "url": "https://www.semanticscholar.org/paper/eb34bd715e3a7c68477ec0824ebb171b948b80c2",
    "title": "FinMem: A Performance-Enhanced LLM Trading Agent with Layered Memory and Character Design",
    "abstract": "Recent advancements in Large Language Models (LLMs) have exhibited notable efficacy in question-answering (QA) tasks across diverse domains. Their prowess in integrating extensive web knowledge has fueled interest in developing LLM-based autonomous agents. While LLMs are efficient in decoding human instructions and deriving solutions by holistically processing historical inputs, transitioning to purpose-driven agents requires a supplementary rational architecture to process multi-source information, establish reasoning chains, and prioritize critical tasks. Addressing this, we introduce FinMem, a novel LLM-based agent framework devised for financial decision-making. It encompasses three core modules: Profiling, to customize the agent's characteristics; Memory, with layered message processing, to aid the agent in assimilating hierarchical financial data; and Decision-making, to convert insights gained from memories into investment decisions. Notably, FinMem's memory module aligns closely with the cognitive structure of human traders, offering robust interpretability and real-time tuning. Its adjustable cognitive span allows for the retention of critical information beyond human perceptual limits, thereby enhancing trading outcomes. This framework enables the agent to self-evolve its professional knowledge, react agilely to new investment cues, and continuously refine trading decisions in the volatile financial environment. We first compare FinMem with various algorithmic agents on a scalable real-world financial dataset, underscoring its leading trading performance in stocks. We then fine-tuned the agent's perceptual span and character setting to achieve a significantly enhanced trading performance. Collectively, FinMem presents a cutting-edge LLM agent framework for automated trading, boosting cumulative investment returns.",
    "venue": "AAAI Spring Symposia",
    "year": 2023,
    "citationCount": 33,
    "openAccessPdf": {
      "url": "https://ojs.aaai.org/index.php/AAAI-SS/article/download/31290/33450",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Economics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-23",
    "authors": [
      {
        "authorId": "2238569503",
        "name": "Yangyang Yu"
      },
      {
        "authorId": "2238311785",
        "name": "Haohang Li"
      },
      {
        "authorId": "2268374009",
        "name": "Zhi Chen"
      },
      {
        "authorId": "2268397502",
        "name": "Yuechen Jiang"
      },
      {
        "authorId": "2238654224",
        "name": "Yang Li"
      },
      {
        "authorId": "2268373308",
        "name": "Denghui Zhang"
      },
      {
        "authorId": "2268379593",
        "name": "Rong Liu"
      },
      {
        "authorId": "2176225",
        "name": "Jordan W. Suchow"
      },
      {
        "authorId": "2045951",
        "name": "K. Khashanah"
      }
    ],
    "source": "semantic_scholar",
    "score": 122.89540786924243
  },
  {
    "paperId": "5a9d4bcffa9989cac4139b2844358884ae023e8d",
    "url": "https://www.semanticscholar.org/paper/5a9d4bcffa9989cac4139b2844358884ae023e8d",
    "title": "SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments",
    "abstract": "Semantic reasoning and dynamic planning capabilities are crucial for an autonomous agent to perform complex navigation tasks in unknown environments. It requires a large amount of common-sense knowledge, that humans possess, to succeed in these tasks. We present SayNav, a new approach that leverages human knowledge from Large Language Models (LLMs) for efficient generalization to complex navigation tasks in unknown large-scale environments. SayNav uses a novel grounding mechanism, that incrementally builds a 3D scene graph of the explored environment as inputs to LLMs, for generating feasible and contextually appropriate high-level plans for navigation. The LLM-generated plan is then executed by a pre-trained low-level planner, that treats each planned step as a short-distance point-goal navigation sub-task. SayNav dynamically generates step-by-step instructions during navigation and continuously refines future steps based on newly perceived information. We evaluate SayNav on multi-object navigation (MultiON) task, that requires the agent to utilize a massive amount of human knowledge to efficiently search multiple different objects in an unknown environment. We also introduce a benchmark dataset for MultiON task employing ProcTHOR framework that provides large photo-realistic indoor environments with variety of objects. SayNav achieves state-of-the-art results and even outperforms an oracle based baseline with strong ground-truth assumptions by more than 8% in terms of success rate, highlighting its ability to generate dynamic plans for successfully locating objects in large-scale new environments. The code, benchmark dataset and demonstration videos are accessible at https://www.sri.com/ics/computer-vision/saynav.",
    "venue": "International Conference on Automated Planning and Scheduling",
    "year": 2023,
    "citationCount": 33,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.04077",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-09-08",
    "authors": [
      {
        "authorId": "2744092",
        "name": "Abhinav Rajvanshi"
      },
      {
        "authorId": "39707211",
        "name": "Karan Sikka"
      },
      {
        "authorId": "2238948285",
        "name": "Xiao Lin"
      },
      {
        "authorId": "2238891847",
        "name": "Bhoram Lee"
      },
      {
        "authorId": "2238578299",
        "name": "Han-Pang Chiu"
      },
      {
        "authorId": "2238587696",
        "name": "Alvaro Velasquez"
      }
    ],
    "source": "semantic_scholar",
    "score": 122.89540786924243
  },
  {
    "paperId": "68ad0ed1e21fd9fd7b2bd58769d8bec88c996b01",
    "url": "https://www.semanticscholar.org/paper/68ad0ed1e21fd9fd7b2bd58769d8bec88c996b01",
    "title": "Exploring Chain-of-Thought Style Prompting for Text-to-SQL",
    "abstract": "In-context learning with large language models (LLMs) has recently caught increasing attention due to its superior few-shot performance on various tasks. However, its performance on text-to-SQL parsing still has much room for improvement. In this paper, we hypothesize that a crucial aspect of LLMs to improve for text-to-SQL parsing is their multi-step reasoning ability. Thus, we systematically study how to enhance LLMs' reasoning ability through chain of thought (CoT) style prompting, including the original chain-of-thought prompting (Wei et al., 2022b) and least-to-most prompting (Zhou et al., 2023). Our experiments demonstrate that iterative prompting as in Zhou et al. (2023) may be unnecessary for text-to-SQL parsing, and using detailed reasoning steps tends to have more error propagation issues. Based on these findings, we propose a new CoT-style prompting method for text-to-SQL parsing. It brings 5.2 and 6.5 point absolute gains on the Spider development set and the Spider Realistic set, respectively, compared to the standard prompting method without reasoning steps; 2.4 and 1.5 point absolute gains, compared to the least-to-most prompting method.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 33,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.14215",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-23",
    "authors": [
      {
        "authorId": "77145310",
        "name": "Chang-You Tai"
      },
      {
        "authorId": "11832104",
        "name": "Ziru Chen"
      },
      {
        "authorId": "2167773622",
        "name": "Tianshu Zhang"
      },
      {
        "authorId": "145924070",
        "name": "Xiang Deng"
      },
      {
        "authorId": "1515546612",
        "name": "Huan Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 122.89540786924243
  },
  {
    "paperId": "3f421c7defc20c21e97abad5fa92887a06ec5df8",
    "url": "https://www.semanticscholar.org/paper/3f421c7defc20c21e97abad5fa92887a06ec5df8",
    "title": "Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU",
    "abstract": "Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable datasets. In this work, we introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university entrance exams in Indonesia. By employing professional teachers, we obtain 14,981 questions across 64 tasks and education levels, with 46% of the questions focusing on assessing proficiency in the Indonesian language and knowledge of nine local languages and cultures in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon perform at even lower levels.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 29,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.04928",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-07",
    "authors": [
      {
        "authorId": "2789148",
        "name": "Fajri Koto"
      },
      {
        "authorId": "2256987672",
        "name": "Nurul Aisyah"
      },
      {
        "authorId": "49404498",
        "name": "Haonan Li"
      },
      {
        "authorId": "2256987316",
        "name": "Timothy Baldwin"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.01796072493232
  },
  {
    "paperId": "5c577988ccebfea96de86678d04fd94fad367d2e",
    "url": "https://www.semanticscholar.org/paper/5c577988ccebfea96de86678d04fd94fad367d2e",
    "title": "Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models",
    "abstract": "We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://huggingface.co/inception-mbzuai/jais-13b-chat",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 30,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.16149",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-08-30",
    "authors": [
      {
        "authorId": "1880394",
        "name": "Neha Sengupta"
      },
      {
        "authorId": "3422905",
        "name": "Sunil Kumar Sahu"
      },
      {
        "authorId": "2087720002",
        "name": "Bokang Jia"
      },
      {
        "authorId": "2235818050",
        "name": "Satheesh Katipomu"
      },
      {
        "authorId": "49404498",
        "name": "Haonan Li"
      },
      {
        "authorId": "2789148",
        "name": "Fajri Koto"
      },
      {
        "authorId": "49843300",
        "name": "Osama Mohammed Afzal"
      },
      {
        "authorId": "2203791403",
        "name": "Samta Kamboj"
      },
      {
        "authorId": "22171629",
        "name": "O. Pandit"
      },
      {
        "authorId": "2235794681",
        "name": "Rahul Pal"
      },
      {
        "authorId": "2076256459",
        "name": "Lalit Pradhan"
      },
      {
        "authorId": "2266755049",
        "name": "Zain Muhammad Mujahid"
      },
      {
        "authorId": "1380273855",
        "name": "Massa Baali"
      },
      {
        "authorId": "2110982198",
        "name": "Xudong Han"
      },
      {
        "authorId": "8129718",
        "name": "Alham Fikri Aji"
      },
      {
        "authorId": "100468503",
        "name": "Zhengzhong Liu"
      },
      {
        "authorId": "2235826325",
        "name": "Andy Hock"
      },
      {
        "authorId": "77917645",
        "name": "Andrew Feldman"
      },
      {
        "authorId": "2235945609",
        "name": "Jonathan Lee"
      },
      {
        "authorId": "2064974174",
        "name": "A. Jackson"
      },
      {
        "authorId": "1683562",
        "name": "Preslav Nakov"
      },
      {
        "authorId": "145465286",
        "name": "Timothy Baldwin"
      },
      {
        "authorId": "2064963077",
        "name": "Eric P. Xing"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.50980806727719
  },
  {
    "paperId": "ffcbc17638beb1e1aff3eba1dd48735ed72a02d4",
    "url": "https://www.semanticscholar.org/paper/ffcbc17638beb1e1aff3eba1dd48735ed72a02d4",
    "title": "LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation",
    "abstract": "Following the impressive development of LLMs, vision-language alignment in LLMs is actively being researched to enable multimodal reasoning and visual IO. This direction of research is particularly relevant to medical imaging because medical image analysis and generation consist of reasoning based on a combination of visual features and prior knowledge. Many recent works have focused on training adapter networks that serve as an information bridge between image processing networks and LLMs; but presumably, in order to achieve maximum reasoning potential of LLMs on visual information as well, visual and language features should be allowed to interact more freely. This is especially important in the medical domain because understanding and generating medical images such as chest X-rays (CXR) require not only accurate visual and language-based reasoning but also a more intimate mapping between the two modalities. Thus, taking inspiration from previous work on the transformer and VQ-GAN combination for bidirectional image and text generation, we build upon this approach and develop a method for instruction-tuning an LLM pre-trained only on text to gain vision-language capabilities for medical images. Specifically, we leverage a pretrained LLM's existing question-answering and instruction-following abilities to teach it to understand visual inputs by instructing it to answer questions about image inputs and, symmetrically, output both text and image responses appropriate to a given query by tuning the LLM with diverse tasks that encompass image-based text-generation and text-based image-generation. We show that our model, LLM-CXR, trained in this approach shows better image-text alignment in both CXR understanding and generation tasks while being smaller in size compared to previously developed models that perform a narrower range of tasks. The code is at https://github.com/hyn2028/llm-cxr.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 27,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-19",
    "authors": [
      {
        "authorId": "2192028220",
        "name": "Suhyeon Lee"
      },
      {
        "authorId": "2218438671",
        "name": "Won Jun Kim"
      },
      {
        "authorId": "2259644910",
        "name": "Jinho Chang"
      },
      {
        "authorId": "30547794",
        "name": "Jong-Chul Ye"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.98306765262805
  },
  {
    "paperId": "6a8ef5e410da32958d0cba56e8d4c3f6302f02a4",
    "url": "https://www.semanticscholar.org/paper/6a8ef5e410da32958d0cba56e8d4c3f6302f02a4",
    "title": "Leveraging Large Language Models for Pre-trained Recommender Systems",
    "abstract": "Recent advancements in recommendation systems have shifted towards more comprehensive and personalized recommendations by utilizing large language models (LLM). However, effectively integrating LLM's commonsense knowledge and reasoning abilities into recommendation systems remains a challenging problem. In this paper, we propose RecSysLLM, a novel pre-trained recommendation model based on LLMs. RecSysLLM retains LLM reasoning and knowledge while integrating recommendation domain knowledge through unique designs of data, training, and inference. This allows RecSysLLM to leverage LLMs' capabilities for recommendation tasks in an efficient, unified framework. We demonstrate the effectiveness of RecSysLLM on benchmarks and real-world scenarios. RecSysLLM provides a promising approach to developing unified recommendation systems by fully exploiting the power of pre-trained language models.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 33,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.10837",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-08-21",
    "authors": [
      {
        "authorId": "1491708389",
        "name": "Zhixuan Chu"
      },
      {
        "authorId": "2065513241",
        "name": "Hongyan Hao"
      },
      {
        "authorId": "2262613411",
        "name": "Ouyang Xin"
      },
      {
        "authorId": "2143244250",
        "name": "Simeng Wang"
      },
      {
        "authorId": "2205710030",
        "name": "Yan Wang"
      },
      {
        "authorId": "2180240771",
        "name": "Yue Shen"
      },
      {
        "authorId": "50771151",
        "name": "Jinjie Gu"
      },
      {
        "authorId": "87484466",
        "name": "Qing Cui"
      },
      {
        "authorId": "2151580",
        "name": "Longfei Li"
      },
      {
        "authorId": "2149919635",
        "name": "Siqiao Xue"
      },
      {
        "authorId": "2124955096",
        "name": "James Y. Zhang"
      },
      {
        "authorId": "2153698676",
        "name": "Shenghe Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 122.89540786924243
  },
  {
    "paperId": "00a48c76e123ab77f301bf4dfd88b9b376b234c6",
    "url": "https://www.semanticscholar.org/paper/00a48c76e123ab77f301bf4dfd88b9b376b234c6",
    "title": "Chat with the Environment: Interactive Multimodal Perception Using Large Language Models",
    "abstract": "Programming robot behavior in a complex world faces challenges on multiple levels, from dextrous low-level skills to high-level planning and reasoning. Recent pre-trained Large Language Models (LLMs) have shown remarkable reasoning ability in few-shot robotic planning. However, it remains challenging to ground LLMs in multimodal sensory input and continuous action output, while enabling a robot to interact with its environment and acquire novel information as its policies unfold. We develop a robot interaction scenario with a partially observable state, which necessitates a robot to decide on a range of epistemic actions in order to sample sensory information among multiple modalities, before being able to execute the task correctly. An interactive perception framework is therefore proposed with an LLM as its backbone, whose ability is exploited to instruct epistemic actions and to reason over the resulting multimodal sensations (vision, sound, haptics, proprioception), as well as to plan an entire task execution based on the interactively acquired information. Our study demonstrates that LLMs can provide high-level planning and reasoning skills and control interactive robot behavior in a multimodal environment, while multimodal modules with the context of the environmental state help ground the LLMs and extend their processing ability. The project website can be found at https://matcha-model.github.io/.",
    "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
    "year": 2023,
    "citationCount": 33,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2303.08268",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-03-15",
    "authors": [
      {
        "authorId": "2145744284",
        "name": "Xufeng Zhao"
      },
      {
        "authorId": "2004701476",
        "name": "Mengdi Li"
      },
      {
        "authorId": "1798067",
        "name": "C. Weber"
      },
      {
        "authorId": "32606259",
        "name": "Muhammad Burhan Hafez"
      },
      {
        "authorId": "1736513",
        "name": "Stefan Wermter"
      }
    ],
    "source": "semantic_scholar",
    "score": 128.89540786924243
  },
  {
    "paperId": "b217b6bc340af9a10bebbf8acc36ea30871769bd",
    "url": "https://www.semanticscholar.org/paper/b217b6bc340af9a10bebbf8acc36ea30871769bd",
    "title": "In-Context Learning with Iterative Demonstration Selection",
    "abstract": "Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Based on how the test sample is answered, we propose Iterative Demonstration Selection (IDS) to leverage the merits of both dimensions. Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is then used to choose demonstrations that are prepended to the test sample for inference. The generated answer is followed by its corresponding reasoning path for extracting a new set of demonstrations in the next iteration. After several iterations, IDS adopts majority voting to obtain the final result. Through extensive experiments on tasks including reasoning, question answering, and topic classification, we demonstrate that IDS can consistently outperform existing ICL demonstration selection methods.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 32,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-15",
    "authors": [
      {
        "authorId": "2084609980",
        "name": "Chengwei Qin"
      },
      {
        "authorId": "2258754564",
        "name": "Aston Zhang"
      },
      {
        "authorId": "1627060158",
        "name": "Anirudh Dagar"
      },
      {
        "authorId": "2258719488",
        "name": "Wenming Ye"
      }
    ],
    "source": "semantic_scholar",
    "score": 122.4476134219972
  },
  {
    "paperId": "b10482ab3dd1d340c3c926d92c3e617c24ee3949",
    "url": "https://www.semanticscholar.org/paper/b10482ab3dd1d340c3c926d92c3e617c24ee3949",
    "title": "Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content. This issue is common in both non-retrieval-based generation and retrieval-augmented generation approaches, and existing post-hoc rectification methods may not address the accumulated hallucination errors that may be caused by the\"snowballing\"issue, especially in reasoning tasks. To tackle these challenges, we introduce a novel approach called Real-time Verification and Rectification (Ever). Instead of waiting until the end of the generation process to rectify hallucinations, Ever employs a real-time, step-wise generation and hallucination rectification strategy. The primary objective is to detect and rectify hallucinations as they occur during the text generation process. When compared to both retrieval-based and non-retrieval-based baselines, Ever demonstrates a significant improvement in generating trustworthy and factually accurate text across a diverse range of tasks, including short-form QA, biography generation, and multi-hop reasoning.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 27,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-15",
    "authors": [
      {
        "authorId": "2266813792",
        "name": "Haoqiang Kang"
      },
      {
        "authorId": "2266755863",
        "name": "Juntong Ni"
      },
      {
        "authorId": "2266811937",
        "name": "Huaxiu Yao"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.98306765262805
  },
  {
    "paperId": "17e8f71dfdfa03f5df6e5ed7b40ec3b396f17a79",
    "url": "https://www.semanticscholar.org/paper/17e8f71dfdfa03f5df6e5ed7b40ec3b396f17a79",
    "title": "Strategic Behavior of Large Language Models: Game Structure vs. Contextual Framing",
    "abstract": "This paper investigates the strategic decision-making capabilities of three Large Language Models (LLMs): GPT-3.5, GPT-4, and LLaMa-2, within the framework of game theory. Utilizing four canonical two-player games -- Prisoner's Dilemma, Stag Hunt, Snowdrift, and Prisoner's Delight -- we explore how these models navigate social dilemmas, situations where players can either cooperate for a collective benefit or defect for individual gain. Crucially, we extend our analysis to examine the role of contextual framing, such as diplomatic relations or casual friendships, in shaping the models' decisions. Our findings reveal a complex landscape: while GPT-3.5 is highly sensitive to contextual framing, it shows limited ability to engage in abstract strategic reasoning. Both GPT-4 and LLaMa-2 adjust their strategies based on game structure and context, but LLaMa-2 exhibits a more nuanced understanding of the games' underlying mechanics. These results highlight the current limitations and varied proficiencies of LLMs in strategic decision-making, cautioning against their unqualified use in tasks requiring complex strategic reasoning.",
    "venue": "Social Science Research Network",
    "year": 2023,
    "citationCount": 28,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.05898",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Economics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-12",
    "authors": [
      {
        "authorId": "2214364704",
        "name": "Nunzio Lorè"
      },
      {
        "authorId": "2214389680",
        "name": "Babak Heydari"
      }
    ],
    "source": "semantic_scholar",
    "score": 130.5094374497971
  },
  {
    "paperId": "3b508a48a4b48d2a16dd790a2a04ffcf51c0b4a6",
    "url": "https://www.semanticscholar.org/paper/3b508a48a4b48d2a16dd790a2a04ffcf51c0b4a6",
    "title": "SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models",
    "abstract": "Diffusion models, which have emerged to become popular text-to-image generation models, can produce high-quality and content-rich images guided by textual prompts. However, there are limitations to semantic understanding and commonsense reasoning in existing models when the input prompts are concise narrative, resulting in low-quality image generation. To improve the capacities for narrative prompts, we propose a simple-yet-effective parameter-efficient fine-tuning approach called the Semantic Understanding and Reasoning adapter (SUR-adapter) for pre-trained diffusion models. To reach this goal, we first collect and annotate a new dataset SURD which consists of more than 57,000 semantically corrected multi-modal samples. Each sample contains a simple narrative prompt, a complex keyword-based prompt, and a high-quality image. Then, we align the semantic representation of narrative prompts to the complex prompts and transfer knowledge of large language models (LLMs) to our SUR-adapter via knowledge distillation so that it can acquire the powerful semantic understanding and reasoning capabilities to build a high-quality textual semantic representation for text-to-image generation. We conduct experiments by integrating multiple LLMs and popular pre-trained diffusion models to show the effectiveness of our approach in enabling diffusion models to understand and reason concise natural language without image quality degradation. Our approach can make text-to-image diffusion models easier to use with better user experience, which demonstrates our approach has the potential for further advancing the development of user-friendly text-to-image generation models by bridging the semantic gap between simple narrative prompts and complex keyword-based prompts. The code is released at https://github.com/Qrange-group/SUR-adapter.",
    "venue": "ACM Multimedia",
    "year": 2023,
    "citationCount": 28,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2305.05189",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2023-05-09",
    "authors": [
      {
        "authorId": "2053866242",
        "name": "Shan Zhong"
      },
      {
        "authorId": "2109670338",
        "name": "Zhongzhan Huang"
      },
      {
        "authorId": "1726975",
        "name": "Wushao Wen"
      },
      {
        "authorId": "9102638",
        "name": "Jinghui Qin"
      },
      {
        "authorId": "2181395240",
        "name": "Liang Lin"
      }
    ],
    "source": "semantic_scholar",
    "score": 126.5094374497971
  },
  {
    "paperId": "837a3c0417fb677d4f22c346b345a450ec417f2c",
    "url": "https://www.semanticscholar.org/paper/837a3c0417fb677d4f22c346b345a450ec417f2c",
    "title": "FELM: Benchmarking Factuality Evaluation of Large Language Models",
    "abstract": "Assessing factuality of text generated by large language models (LLMs) is an emerging yet crucial research area, aimed at alerting users to potential errors and guiding the development of more reliable LLMs. Nonetheless, the evaluators assessing factuality necessitate suitable evaluation themselves to gauge progress and foster advancements. This direction remains under-explored, resulting in substantial impediments to the progress of factuality evaluators. To mitigate this issue, we introduce a benchmark for Factuality Evaluation of large Language Models, referred to as felm. In this benchmark, we collect responses generated from LLMs and annotate factuality labels in a fine-grained manner. Contrary to previous studies that primarily concentrate on the factuality of world knowledge (e.g.~information from Wikipedia), felm focuses on factuality across diverse domains, spanning from world knowledge to math and reasoning. Our annotation is based on text segments, which can help pinpoint specific factual errors. The factuality annotations are further supplemented by predefined error types and reference links that either support or contradict the statement. In our experiments, we investigate the performance of several LLM-based factuality evaluators on felm, including both vanilla LLMs and those augmented with retrieval mechanisms and chain-of-thought processes. Our findings reveal that while retrieval aids factuality evaluation, current LLMs are far from satisfactory to faithfully detect factual errors.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 25,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.00741",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-01",
    "authors": [
      {
        "authorId": "2108956946",
        "name": "Shiqi Chen"
      },
      {
        "authorId": "2249827256",
        "name": "Yiran Zhao"
      },
      {
        "authorId": "6062056",
        "name": "Jinghan Zhang"
      },
      {
        "authorId": "2047713083",
        "name": "Ethan Chern"
      },
      {
        "authorId": "2255016668",
        "name": "Siyang Gao"
      },
      {
        "authorId": "2250461151",
        "name": "Pengfei Liu"
      },
      {
        "authorId": "2264312133",
        "name": "Junxian He"
      }
    ],
    "source": "semantic_scholar",
    "score": 125.87144807032223
  },
  {
    "paperId": "a7307613447a828fc0fa2d233311b6f62fbadfb4",
    "url": "https://www.semanticscholar.org/paper/a7307613447a828fc0fa2d233311b6f62fbadfb4",
    "title": "SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue Agents",
    "abstract": "Task-oriented dialogue (TOD) models have made significant progress in recent years. However, previous studies primarily focus on datasets written by annotators, which has resulted in a gap between academic research and real-world spoken conversation scenarios. While several small-scale spoken TOD datasets are proposed to address robustness issues such as ASR errors, they ignore the unique challenges in spoken conversation. To tackle the limitations, we introduce SpokenWOZ, a large-scale speech-text dataset for spoken TOD, containing 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from human-to-human spoken conversations. SpokenWOZ further incorporates common spoken characteristics such as word-by-word processing and reasoning in spoken language. Based on these characteristics, we present cross-turn slot and reasoning slot detection as new challenges. We conduct experiments on various baselines, including text-modal models, newly proposed dual-modal models, and LLMs, e.g., ChatGPT. The results show that the current models still have substantial room for improvement in spoken conversation, where the most advanced dialogue state tracker only achieves 25.65% in joint goal accuracy and the SOTA end-to-end model only correctly completes the user request in 52.1% of dialogues. The dataset, code, and leaderboard are available: https://spokenwoz.github.io/.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 23,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-22",
    "authors": [
      {
        "authorId": "2053739525",
        "name": "Shuzheng Si"
      },
      {
        "authorId": "2111665725",
        "name": "Wen-Cheng Ma"
      },
      {
        "authorId": "2191085441",
        "name": "Haoyu Gao"
      },
      {
        "authorId": "2142637404",
        "name": "Yuchuan Wu"
      },
      {
        "authorId": "31255666",
        "name": "Ting-En Lin"
      },
      {
        "authorId": "30087809",
        "name": "Yinpei Dai"
      },
      {
        "authorId": "2118386175",
        "name": "Hangyu Li"
      },
      {
        "authorId": "2055863231",
        "name": "Rui Yan"
      },
      {
        "authorId": "143857288",
        "name": "Fei Huang"
      },
      {
        "authorId": "1527090216",
        "name": "Yongbin Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.67080745521918
  },
  {
    "paperId": "19f57c712d0a5dd47fb499aef9f76374dd5acea9",
    "url": "https://www.semanticscholar.org/paper/19f57c712d0a5dd47fb499aef9f76374dd5acea9",
    "title": "A Large Language Model Enhanced Conversational Recommender System",
    "abstract": "Conversational recommender systems (CRSs) aim to recommend high-quality items to users through a dialogue interface. It usually contains multiple sub-tasks, such as user preference elicitation, recommendation, explanation, and item information search. To develop effective CRSs, there are some challenges: 1) how to properly manage sub-tasks; 2) how to effectively solve different sub-tasks; and 3) how to correctly generate responses that interact with users. Recently, Large Language Models (LLMs) have exhibited an unprecedented ability to reason and generate, presenting a new opportunity to develop more powerful CRSs. In this work, we propose a new LLM-based CRS, referred to as LLMCRS, to address the above challenges. For sub-task management, we leverage the reasoning ability of LLM to effectively manage sub-task. For sub-task solving, we collaborate LLM with expert models of different sub-tasks to achieve the enhanced performance. For response generation, we utilize the generation ability of LLM as a language interface to better interact with users. Specifically, LLMCRS divides the workflow into four stages: sub-task detection, model matching, sub-task execution, and response generation. LLMCRS also designs schema-based instruction, demonstration-based instruction, dynamic sub-task and model matching, and summary-based generation to instruct LLM to generate desired results in the workflow. Finally, to adapt LLM to conversational recommendations, we also propose to fine-tune LLM with reinforcement learning from CRSs performance feedback, referred to as RLPF. Experimental results on benchmark datasets show that LLMCRS with RLPF outperforms the existing methods.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 23,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-08-11",
    "authors": [
      {
        "authorId": "2047904135",
        "name": "Yue Feng"
      },
      {
        "authorId": "50152132",
        "name": "Shuchang Liu"
      },
      {
        "authorId": "2093481204",
        "name": "Zhenghai Xue"
      },
      {
        "authorId": "144994208",
        "name": "Qingpeng Cai"
      },
      {
        "authorId": "2191842",
        "name": "Lantao Hu"
      },
      {
        "authorId": "2061280682",
        "name": "Peng Jiang"
      },
      {
        "authorId": "20029557",
        "name": "Kun Gai"
      },
      {
        "authorId": "143770118",
        "name": "Fei Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 117.67080745521918
  },
  {
    "paperId": "bb4516ad6eb7adda97d81f09d4bb92b3ad056c42",
    "url": "https://www.semanticscholar.org/paper/bb4516ad6eb7adda97d81f09d4bb92b3ad056c42",
    "title": "Large Language Models are Temporal and Causal Reasoners for Video Question Answering",
    "abstract": "Large Language Models (LLMs) have shown remarkable performances on a wide range of natural language understanding and generation tasks. We observe that the LLMs provide effective priors in exploiting $\\textit{linguistic shortcuts}$ for temporal and causal reasoning in Video Question Answering (VideoQA). However, such priors often cause suboptimal results on VideoQA by leading the model to over-rely on questions, $\\textit{i.e.}$, $\\textit{linguistic bias}$, while ignoring visual content. This is also known as `ungrounded guesses' or `hallucinations'. To address this problem while leveraging LLMs' prior on VideoQA, we propose a novel framework, Flipped-VQA, encouraging the model to predict all the combinations of $\\langle$V, Q, A$\\rangle$ triplet by flipping the source pair and the target label to understand their complex relationships, $\\textit{i.e.}$, predict A, Q, and V given a VQ, VA, and QA pairs, respectively. In this paper, we develop LLaMA-VQA by applying Flipped-VQA to LLaMA, and it outperforms both LLMs-based and non-LLMs-based models on five challenging VideoQA benchmarks. Furthermore, our Flipped-VQA is a general framework that is applicable to various LLMs (OPT and GPT-J) and consistently improves their performances. We empirically demonstrate that Flipped-VQA not only enhances the exploitation of linguistic shortcuts but also mitigates the linguistic bias, which causes incorrect answers over-relying on the question. Code is available at https://github.com/mlvlab/Flipped-VQA.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 23,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-24",
    "authors": [
      {
        "authorId": "2136072008",
        "name": "Dohwan Ko"
      },
      {
        "authorId": "2232661859",
        "name": "Ji Soo Lee"
      },
      {
        "authorId": "2261392244",
        "name": "Wooyoung Kang"
      },
      {
        "authorId": "2254299439",
        "name": "Byungseok Roh"
      },
      {
        "authorId": "2261450804",
        "name": "Hyunwoo J. Kim"
      }
    ],
    "source": "semantic_scholar",
    "score": 117.67080745521918
  },
  {
    "paperId": "bb4516ad6eb7adda97d81f09d4bb92b3ad056c42",
    "url": "https://www.semanticscholar.org/paper/bb4516ad6eb7adda97d81f09d4bb92b3ad056c42",
    "title": "Large Language Models are Temporal and Causal Reasoners for Video Question Answering",
    "abstract": "Large Language Models (LLMs) have shown remarkable performances on a wide range of natural language understanding and generation tasks. We observe that the LLMs provide effective priors in exploiting $\\textit{linguistic shortcuts}$ for temporal and causal reasoning in Video Question Answering (VideoQA). However, such priors often cause suboptimal results on VideoQA by leading the model to over-rely on questions, $\\textit{i.e.}$, $\\textit{linguistic bias}$, while ignoring visual content. This is also known as `ungrounded guesses' or `hallucinations'. To address this problem while leveraging LLMs' prior on VideoQA, we propose a novel framework, Flipped-VQA, encouraging the model to predict all the combinations of $\\langle$V, Q, A$\\rangle$ triplet by flipping the source pair and the target label to understand their complex relationships, $\\textit{i.e.}$, predict A, Q, and V given a VQ, VA, and QA pairs, respectively. In this paper, we develop LLaMA-VQA by applying Flipped-VQA to LLaMA, and it outperforms both LLMs-based and non-LLMs-based models on five challenging VideoQA benchmarks. Furthermore, our Flipped-VQA is a general framework that is applicable to various LLMs (OPT and GPT-J) and consistently improves their performances. We empirically demonstrate that Flipped-VQA not only enhances the exploitation of linguistic shortcuts but also mitigates the linguistic bias, which causes incorrect answers over-relying on the question. Code is available at https://github.com/mlvlab/Flipped-VQA.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 23,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-24",
    "authors": [
      {
        "authorId": "2136072008",
        "name": "Dohwan Ko"
      },
      {
        "authorId": "2232661859",
        "name": "Ji Soo Lee"
      },
      {
        "authorId": "2261392244",
        "name": "Wooyoung Kang"
      },
      {
        "authorId": "2254299439",
        "name": "Byungseok Roh"
      },
      {
        "authorId": "2261450804",
        "name": "Hyunwoo J. Kim"
      }
    ],
    "source": "semantic_scholar",
    "score": 117.67080745521918
  },
  {
    "paperId": "ee2e0077ec46704f2cb930958c9bf3739a904227",
    "url": "https://www.semanticscholar.org/paper/ee2e0077ec46704f2cb930958c9bf3739a904227",
    "title": "Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot Agents",
    "abstract": "Recent advancements in large language models (LLMs) have enabled a new research domain, LLM agents, for solving robotics and planning tasks by leveraging the world knowledge and general reasoning abilities of LLMs obtained during pretraining. However, while considerable effort has been made to teach the robot the \"dos\", the \"don’ts\" received relatively less attention. We argue that, for any practical usage, it is as crucial to teach the robot the \"don’ts\": conveying explicit instructions about prohibited actions, assessing the robot’s comprehension of these restrictions, and, most importantly, ensuring compliance. Moreover, verifiable safe operation is essential for deployments that satisfy worldwide standards such as ISO 61508, which defines standards for safely deploying robots in industrial factory environments worldwide. Aiming at deploying the LLM agents in a collaborative environment, we propose a queryable safety constraint module based on linear temporal logic (LTL) that simultaneously enables natural language (NL) to temporal constraints encoding, safety violation reasoning and explaining, and unsafe action pruning. To demonstrate the effectiveness of our system, we conducted experiments in VirtualHome environment and on a real robot. The experimental results show that our system strictly adheres to the safety constraints and scales well with complex safety constraints, highlighting its potential for practical utility.",
    "venue": "IEEE International Conference on Robotics and Automation",
    "year": 2023,
    "citationCount": 23,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-09-18",
    "authors": [
      {
        "authorId": "2243392790",
        "name": "Ziyi Yang"
      },
      {
        "authorId": "2242964116",
        "name": "S. S. Raman"
      },
      {
        "authorId": "31017418",
        "name": "Ankit Shah"
      },
      {
        "authorId": "2913681",
        "name": "Stefanie Tellex"
      }
    ],
    "source": "semantic_scholar",
    "score": 123.67080745521918
  },
  {
    "paperId": "582c2f270a6c0ce89679eebaa78797711fa20293",
    "url": "https://www.semanticscholar.org/paper/582c2f270a6c0ce89679eebaa78797711fa20293",
    "title": "Making Large Language Models Perform Better in Knowledge Graph Completion",
    "abstract": "Large language model (LLM) based knowledge graph completion (KGC) aims to predict the missing triples in the KGs with LLMs. However, research about LLM-based KGC fails to sufficiently harness LLMs' inference proficiencies, overlooking critical structural information integral to KGs. In this paper, we explore methods to incorporate structural information into the LLMs, with the overarching goal of facilitating structure-aware reasoning. We first discuss on the existing LLM paradigms like in-context learning and instruction tuning, proposing basic structural information injection approaches. Then we propose a Knowledge Prefix Adapter (KoPA) to fulfill this stated goal. The KoPA uses a structural pre-training phase to comprehend the intricate entities and relations within KGs, representing them as structural embeddings. Then KoPA communicates such cross-modal structural information understanding to the LLMs through a knowledge prefix adapter which projects the structural embeddings into the textual space and obtains virtual knowledge tokens positioned as a prefix of the input prompt. We conduct comprehensive experiments and provide incisive analysis concerning how the introduction of cross-modal structural information would be better for LLM's factual knowledge reasoning ability. Our code and data are available at https://github.com/zjukg/KoPA .",
    "venue": "ACM Multimedia",
    "year": 2023,
    "citationCount": 23,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.06671",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-10",
    "authors": [
      {
        "authorId": "2118158068",
        "name": "Yichi Zhang"
      },
      {
        "authorId": "2283992213",
        "name": "Zhuo Chen"
      },
      {
        "authorId": "2256109363",
        "name": "Wen Zhang"
      },
      {
        "authorId": "14499025",
        "name": "Hua-zeng Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 123.67080745521918
  },
  {
    "paperId": "eafc97d979e790a84329f0a49d1b627bd5979499",
    "url": "https://www.semanticscholar.org/paper/eafc97d979e790a84329f0a49d1b627bd5979499",
    "title": "Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation",
    "abstract": "The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation. The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks. Inspired by Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for reasoning, we formulate KBQG task as a reasoning problem, where the generation of a complete question is splitted into a series of sub-question generation. Our proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form. Then, we write a prompt to explicit the reasoning chain of generating complicated questions based on the selected demonstrations. To further ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the logical forms by their complexity. We conduct extensive experiments over three public KBQG datasets. The results demonstrate that our prompting method consistently outperforms other prompting baselines on the evaluated datasets. Remarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of the PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4, METEOR, and ROUGE-L, respectively.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 25,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.08395",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-12",
    "authors": [
      {
        "authorId": "2236010036",
        "name": "Yuanyuan Liang"
      },
      {
        "authorId": "2257376175",
        "name": "Jianing Wang"
      },
      {
        "authorId": "2257905552",
        "name": "Hanlun Zhu"
      },
      {
        "authorId": "145131956",
        "name": "Lei Wang"
      },
      {
        "authorId": "2260654319",
        "name": "Weining Qian"
      },
      {
        "authorId": "3458560",
        "name": "Yunshi Lan"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.87144807032223
  },
  {
    "paperId": "47c447d9f350eef8d51bc13460ccd241a4c13c79",
    "url": "https://www.semanticscholar.org/paper/47c447d9f350eef8d51bc13460ccd241a4c13c79",
    "title": "Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition",
    "abstract": "The past years have witnessed a proliferation of large language models (LLMs). Yet, automated and unbiased evaluation of LLMs is challenging due to the inaccuracy of standard metrics in reflecting human preferences and the inefficiency in sampling informative and diverse test examples. While human evaluation remains the gold standard, it is expensive and time-consuming, especially when dealing with a large number of testing samples. To address this problem, we propose a sample-efficient human evaluation method based on MAximum Discrepancy (MAD) competition. MAD automatically selects a small set of informative and diverse instructions, each adapted to two LLMs, whose responses are subject to three-alternative forced choice by human subjects. The pairwise comparison results are then aggregated into a global ranking using the Elo rating system. We select eight representative LLMs and compare them in terms of four skills: knowledge understanding, mathematical reasoning, writing, and coding. Experimental results show that the proposed method achieves a reliable and sensible ranking of LLMs' capabilities, identifies their relative strengths and weaknesses, and offers valuable insights for further LLM advancement.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-10",
    "authors": [
      {
        "authorId": "2296598967",
        "name": "Kehua Feng"
      },
      {
        "authorId": "2254282405",
        "name": "Keyan Ding"
      },
      {
        "authorId": "2296750540",
        "name": "Kede Ma"
      },
      {
        "authorId": "2296657037",
        "name": "Zhihua Wang"
      },
      {
        "authorId": "2254277130",
        "name": "Qiang Zhang"
      },
      {
        "authorId": "2254343747",
        "name": "Huajun Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "dba4e0c1690650a73a4a4d1a3651a719df9986f5",
    "url": "https://www.semanticscholar.org/paper/dba4e0c1690650a73a4a4d1a3651a719df9986f5",
    "title": "Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models",
    "abstract": "Recent advancements in large language models (LLMs) have led to significant breakthroughs in mathematical reasoning capabilities. However, existing benchmarks like GSM8K or MATH are now being solved with high accuracy (e.g., OpenAI o1 achieves 94.8\\% on MATH dataset), indicating their inadequacy for truly challenging these models. To bridge this gap, we propose a comprehensive and challenging benchmark specifically designed to assess LLMs' mathematical reasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks, our dataset focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels, enabling a holistic assessment of model performance in Olympiad-mathematical reasoning. Furthermore, we conducted an in-depth analysis based on this benchmark. Our experimental results show that even the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle with highly challenging Olympiad-level problems, with 60.54\\% and 52.55\\% accuracy, highlighting significant challenges in Olympiad-level mathematical reasoning.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-10-10",
    "authors": [
      {
        "authorId": "2186406832",
        "name": "Bofei Gao"
      },
      {
        "authorId": "2325153259",
        "name": "Feifan Song"
      },
      {
        "authorId": "2257389788",
        "name": "Zhe Yang"
      },
      {
        "authorId": "2117632647",
        "name": "Zefan Cai"
      },
      {
        "authorId": "2319604149",
        "name": "Yibo Miao"
      },
      {
        "authorId": "2047143813",
        "name": "Qingxiu Dong"
      },
      {
        "authorId": "49192881",
        "name": "Lei Li"
      },
      {
        "authorId": "2325163484",
        "name": "Chenghao Ma"
      },
      {
        "authorId": "2286955403",
        "name": "Liang Chen"
      },
      {
        "authorId": "1748844142",
        "name": "Runxin Xu"
      },
      {
        "authorId": "2284834012",
        "name": "Zhengyang Tang"
      },
      {
        "authorId": "2284827140",
        "name": "Benyou Wang"
      },
      {
        "authorId": "2134434187",
        "name": "Daoguang Zan"
      },
      {
        "authorId": "2325151882",
        "name": "Shanghaoran Quan"
      },
      {
        "authorId": "2319593518",
        "name": "Ge Zhang"
      },
      {
        "authorId": "39058310",
        "name": "Lei Sha"
      },
      {
        "authorId": "29343468",
        "name": "Yichang Zhang"
      },
      {
        "authorId": "2312110505",
        "name": "Xuancheng Ren"
      },
      {
        "authorId": "2254792825",
        "name": "Tianyu Liu"
      },
      {
        "authorId": "2261083637",
        "name": "Baobao Chang"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "2b5175bc9494e301f898f22c11b98d9604c3705d",
    "url": "https://www.semanticscholar.org/paper/2b5175bc9494e301f898f22c11b98d9604c3705d",
    "title": "Predicting Learning Performance with Large Language Models: A Study in Adult Literacy",
    "abstract": "Intelligent Tutoring Systems (ITSs) have significantly enhanced adult literacy training, a key factor for societal participation, employment opportunities, and lifelong learning. Our study investigates the application of advanced AI models, including Large Language Models (LLMs) like GPT-4, for predicting learning performance in adult literacy programs in ITSs. This research is motivated by the potential of LLMs to predict learning performance based on its inherent reasoning and computational capabilities. By using reading comprehension datasets from the ITS, AutoTutor, we evaluate the predictive capabilities of GPT-4 versus traditional machine learning methods in predicting learning performance through five-fold cross-validation techniques. Our findings show that the GPT-4 presents the competitive predictive abilities with traditional machine learning methods such as Bayesian Knowledge Tracing, Performance Factor Analysis, Sparse Factor Analysis Lite (SPARFA-Lite), tensor factorization and eXtreme Gradient Boosting (XGBoost). While XGBoost (trained on local machine) outperforms GPT-4 in predictive accuracy, GPT-4-selected XGBoost and its subsequent tuning on the GPT-4 platform demonstrates superior performance compared to local machine execution. Moreover, our investigation into hyper-parameter tuning by GPT-4 versus grid-search suggests comparable performance, albeit with less stability in the automated approach, using XGBoost as the case study. Our study contributes to the field by highlighting the potential of integrating LLMs with traditional machine learning models to enhance predictive accuracy and personalize adult literacy education, setting a foundation for future research in applying LLMs within ITSs.",
    "venue": "Interacción",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-04",
    "authors": [
      {
        "authorId": "2282926239",
        "name": "Liang Zhang"
      },
      {
        "authorId": "2248806319",
        "name": "Jionghao Lin"
      },
      {
        "authorId": "2273374932",
        "name": "Conrad Borchers"
      },
      {
        "authorId": "2293171703",
        "name": "John Sabatini"
      },
      {
        "authorId": "2286978550",
        "name": "John Hollander"
      },
      {
        "authorId": "2282542173",
        "name": "Meng Cao"
      },
      {
        "authorId": "2282545553",
        "name": "Xiangen Hu"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "367e43d1561fce27c919e2d370e42399a40846bd",
    "url": "https://www.semanticscholar.org/paper/367e43d1561fce27c919e2d370e42399a40846bd",
    "title": "Ask-before-Plan: Proactive Language Agents for Real-World Planning",
    "abstract": "The evolution of large language models (LLMs) has enhanced the planning capabilities of language agents in diverse real-world scenarios. Despite these advancements, the potential of LLM-powered agents to comprehend ambiguous user instructions for reasoning and decision-making is still under exploration. In this work, we introduce a new task, Proactive Agent Planning, which requires language agents to predict clarification needs based on user-agent conversation and agent-environment interaction, invoke external tools to collect valid information, and generate a plan to fulfill the user's demands. To study this practical problem, we establish a new benchmark dataset, Ask-before-Plan. To tackle the deficiency of LLMs in proactive planning, we propose a novel multi-agent framework, Clarification-Execution-Planning (\\texttt{CEP}), which consists of three agents specialized in clarification, execution, and planning. We introduce the trajectory tuning scheme for the clarification agent and static execution agent, as well as the memory recollection mechanism for the dynamic execution agent. Extensive evaluations and comprehensive analyses conducted on the Ask-before-Plan dataset validate the effectiveness of our proposed framework.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-18",
    "authors": [
      {
        "authorId": "2286379501",
        "name": "Xuan Zhang"
      },
      {
        "authorId": "145843537",
        "name": "Yang Deng"
      },
      {
        "authorId": "2158173064",
        "name": "Zifeng Ren"
      },
      {
        "authorId": "2241348826",
        "name": "See-Kiong Ng"
      },
      {
        "authorId": "2257036129",
        "name": "Tat-Seng Chua"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "6867e65205548c31e0832d41e5d3c511ea0d4db5",
    "url": "https://www.semanticscholar.org/paper/6867e65205548c31e0832d41e5d3c511ea0d4db5",
    "title": "Planning and Editing What You Retrieve for Enhanced Tool Learning",
    "abstract": "Recent advancements in integrating external tools with Large Language Models (LLMs) have opened new frontiers, with applications in mathematical reasoning, code generators, and smart assistants. However, existing methods, relying on simple one-time retrieval strategies, fall short on effectively and accurately shortlisting relevant tools. This paper introduces a novel PLUTO (Planning, Learning, and Understanding for TOols) approach, encompassing `Plan-and-Retrieve (P&R)` and `Edit-and-Ground (E&G)` paradigms. The P&R paradigm consists of a neural retrieval module for shortlisting relevant tools and an LLM-based query planner that decomposes complex queries into actionable tasks, enhancing the effectiveness of tool utilization. The E&G paradigm utilizes LLMs to enrich tool descriptions based on user scenarios, bridging the gap between user queries and tool functionalities. Experiment results demonstrate that these paradigms significantly improve the recall and NDCG in tool retrieval tasks, significantly surpassing current state-of-the-art models.",
    "venue": "NAACL-HLT",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-30",
    "authors": [
      {
        "authorId": "2110510944",
        "name": "Tenghao Huang"
      },
      {
        "authorId": "2294361005",
        "name": "Dongwon Jung"
      },
      {
        "authorId": "1998918",
        "name": "Muhao Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "491eef6231d7dd13546a38eab1738a0ffe6f3125",
    "url": "https://www.semanticscholar.org/paper/491eef6231d7dd13546a38eab1738a0ffe6f3125",
    "title": "On the Use of Large Language Models to Generate Capability Ontologies",
    "abstract": "Capability ontologies are increasingly used to model functionalities of systems or machines. The creation of such onto-logical models with all properties and constraints of capabilities is very complex and can only be done by ontology experts. However, Large Language Models (LLMs) have shown that they can generate machine-interpretable models from natural language text input and thus support engineers / ontology experts. Therefore, this paper investigates how LLMs can be used to create capability ontologies. We present a study with a series of experiments in which capabilities with varying complexities are generated using different prompting techniques and with different LLMs. Errors in the generated ontologies are recorded and compared. To analyze the quality of the generated ontologies, a semi-automated approach based on RDF syntax checking, OWL reasoning, and SHACL constraints is used. The results of this study are very promising because even for complex capabilities, the generated ontologies are almost free of errors.",
    "venue": "IEEE International Conference on Emerging Technologies and Factory Automation",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2404.17524",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-26",
    "authors": [
      {
        "authorId": "1995912690",
        "name": "Luis Miguel Vieira da Silva"
      },
      {
        "authorId": "24355966",
        "name": "Aljosha Köcher"
      },
      {
        "authorId": "80825133",
        "name": "Felix Gehlhoff"
      },
      {
        "authorId": "2274103647",
        "name": "Alexander Fay"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.87639203842082
  },
  {
    "paperId": "086168c2ad02930279fe481a52641a565b55c4a0",
    "url": "https://www.semanticscholar.org/paper/086168c2ad02930279fe481a52641a565b55c4a0",
    "title": "Democratizing Fine-grained Visual Recognition with Large Language Models",
    "abstract": "Identifying subordinate-level categories from images is a longstanding task in computer vision and is referred to as fine-grained visual recognition (FGVR). It has tremendous significance in real-world applications since an average layperson does not excel at differentiating species of birds or mushrooms due to subtle differences among the species. A major bottleneck in developing FGVR systems is caused by the need of high-quality paired expert annotations. To circumvent the need of expert knowledge we propose Fine-grained Semantic Category Reasoning (FineR) that internally leverages the world knowledge of large language models (LLMs) as a proxy in order to reason about fine-grained category names. In detail, to bridge the modality gap between images and LLM, we extract part-level visual attributes from images as text and feed that information to a LLM. Based on the visual attributes and its internal world knowledge the LLM reasons about the subordinate-level category names. Our training-free FineR outperforms several state-of-the-art FGVR and language and vision assistant models and shows promise in working in the wild and in new domains where gathering expert annotation is arduous.",
    "venue": "International Conference on Learning Representations",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-24",
    "authors": [
      {
        "authorId": "2281304418",
        "name": "Mingxuan Liu"
      },
      {
        "authorId": "2260291513",
        "name": "Subhankar Roy"
      },
      {
        "authorId": "2281338083",
        "name": "Wenjing Li"
      },
      {
        "authorId": "2273559424",
        "name": "Zhun Zhong"
      },
      {
        "authorId": "1429806753",
        "name": "N. Sebe"
      },
      {
        "authorId": "40811261",
        "name": "E. Ricci"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "6d892169ba8ad60cbc1d127d8b3c74cbe9ba7237",
    "url": "https://www.semanticscholar.org/paper/6d892169ba8ad60cbc1d127d8b3c74cbe9ba7237",
    "title": "Query Rewriting via Large Language Models",
    "abstract": "Query rewriting is one of the most effective techniques for coping with poorly written queries before passing them down to the query optimizer. Manual rewriting is not scalable, as it is error-prone and requires deep expertise. Similarly, traditional query rewriting algorithms can only handle a small subset of queries: rule-based techniques do not generalize to new query patterns and synthesis-based techniques cannot handle complex queries. Fortunately, the rise of Large Language Models (LLMs), equipped with broad general knowledge and advanced reasoning capabilities, has created hopes for solving some of these previously open problems. In this paper, we present GenRewrite, the first holistic system that leverages LLMs for query rewriting. We introduce the notion of Natural Language Rewrite Rules (NLR2s), and use them as hints to the LLM but also a means for transferring knowledge from rewriting one query to another, and thus becoming smarter and more effective over time. We present a novel counterexample-guided technique that iteratively corrects the syntactic and semantic errors in the rewritten query, significantly reducing the LLM costs and the manual effort required for verification. GenRewrite speeds up 22 out of 99 TPC queries (the most complex public benchmark) by more than 2x, which is 2.5x--3.2x higher coverage than state-of-the-art traditional query rewriting and 2.1x higher than the out-of-the-box LLM baseline.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-14",
    "authors": [
      {
        "authorId": "2291206776",
        "name": "Jie Liu"
      },
      {
        "authorId": "2198667",
        "name": "Barzan Mozafari"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "ece2e04102b54c0796c65c84cf216f427ac7866f",
    "url": "https://www.semanticscholar.org/paper/ece2e04102b54c0796c65c84cf216f427ac7866f",
    "title": "Grounding Language Plans in Demonstrations Through Counterfactual Perturbations",
    "abstract": "Grounding the common-sense reasoning of Large Language Models (LLMs) in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural network to predict successful trajectories from failures and as a by-product learns classifiers that ground low-level states and images in mode families without dense labeling. The learned grounding classifiers can further be used to translate language plans into reactive policies in the physical domain in an interpretable manner. We show our approach improves the interpretability and reactivity of imitation learning through 2D navigation and simulated and real robot manipulation tasks. Website: https://yanweiw.github.io/glide",
    "venue": "International Conference on Learning Representations",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-25",
    "authors": [
      {
        "authorId": "2169573685",
        "name": "Yanwei Wang"
      },
      {
        "authorId": "2333316915",
        "name": "Tsun-Hsuan Wang"
      },
      {
        "authorId": "2323437497",
        "name": "Jiayuan Mao"
      },
      {
        "authorId": "2293396155",
        "name": "Michael Hagenow"
      },
      {
        "authorId": "2256425930",
        "name": "Julie Shah"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "9bca4b8d0594122b353cf7d50e6bbdc1681b9265",
    "url": "https://www.semanticscholar.org/paper/9bca4b8d0594122b353cf7d50e6bbdc1681b9265",
    "title": "Re-Search for The Truth: Multi-round Retrieval-augmented Large Language Models are Strong Fake News Detectors",
    "abstract": "The proliferation of fake news has had far-reaching implications on politics, the economy, and society at large. While Fake news detection methods have been employed to mitigate this issue, they primarily depend on two essential elements: the quality and relevance of the evidence, and the effectiveness of the verdict prediction mechanism. Traditional methods, which often source information from static repositories like Wikipedia, are limited by outdated or incomplete data, particularly for emerging or rare claims. Large Language Models (LLMs), known for their remarkable reasoning and generative capabilities, introduce a new frontier for fake news detection. However, like traditional methods, LLM-based solutions also grapple with the limitations of stale and long-tail knowledge. Additionally, retrieval-enhanced LLMs frequently struggle with issues such as low-quality evidence retrieval and context length constraints. To address these challenges, we introduce a novel, retrieval-augmented LLMs framework--the first of its kind to automatically and strategically extract key evidence from web sources for claim verification. Employing a multi-round retrieval strategy, our framework ensures the acquisition of sufficient, relevant evidence, thereby enhancing performance. Comprehensive experiments across three real-world datasets validate the framework's superiority over existing methods. Importantly, our model not only delivers accurate verdicts but also offers human-readable explanations to improve result interpretability.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-14",
    "authors": [
      {
        "authorId": "2290661618",
        "name": "Guanghua Li"
      },
      {
        "authorId": "2290901469",
        "name": "Wensheng Lu"
      },
      {
        "authorId": "2290577268",
        "name": "Wei Zhang"
      },
      {
        "authorId": "2267335798",
        "name": "Defu Lian"
      },
      {
        "authorId": "2260653723",
        "name": "Kezhong Lu"
      },
      {
        "authorId": "2291912633",
        "name": "Rui Mao"
      },
      {
        "authorId": "2291965743",
        "name": "Kai Shu"
      },
      {
        "authorId": "2067746794",
        "name": "Hao Liao"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "3c027c68d02afe472bd575dced933fb807cc1ac2",
    "url": "https://www.semanticscholar.org/paper/3c027c68d02afe472bd575dced933fb807cc1ac2",
    "title": "GeoVQA: A Comprehensive Multimodal Geometry Dataset for Secondary Education",
    "abstract": "Large language models (LLMs) have exhibited impressive capabilities in human-like reasoning and generation, prompting widespread exploration of their utility in mathematical problem-solving. However, existing research predominantly focuses on text-based mathematical challenges, neglecting those involving geometric concepts. To bridge this gap, our research aims to empower LLMs to tackle geometric problems by integrating image understanding. We investigate the limitations of current Multimodal Large Language Models (MLLMs) in this context, particularly their struggle to accurately interpret complex geometric elements and their mathematical reasoning. To support this endeavor, we introduce GeoVQA, a specialized multimodal math dataset tailored for secondary school education. Furthermore, we conduct experiments to Instruction-tune LLaVA [14] and G-LLaVA [11] models, evaluating their performance against benchmarks such as PGPS9K [21] & Geometry3k [15]. Additionally, we compare the effectiveness of Gemini pro [19] and GPT4 [1] in handling geometric tasks using various prompting techniques, employing metrics like accuracy and human evaluation for meticulous verification. GPT-4 has the best performance on zero-shot inference on our dataset. LLaVA-v1.5-13B model finetuned on our dataset outperforms GPT-4 on PGPS9K benchmark. GitHub: GeoVQA.",
    "venue": "Conference on Multimedia Information Processing and Retrieval",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-08-07",
    "authors": [
      {
        "authorId": "2223123570",
        "name": "Avinash Anand"
      },
      {
        "authorId": "2261399651",
        "name": "Raj Jaiswal"
      },
      {
        "authorId": "2326128202",
        "name": "Abhishek Dharmadhikari"
      },
      {
        "authorId": "2326126008",
        "name": "Atharva Marathe"
      },
      {
        "authorId": "2326123719",
        "name": "Harsh Popat"
      },
      {
        "authorId": "2326125925",
        "name": "Harshil Mital"
      },
      {
        "authorId": "2326962025",
        "name": "Ashwin R Nair"
      },
      {
        "authorId": "2271980805",
        "name": "Kritarth Prasad"
      },
      {
        "authorId": "2326251791",
        "name": "Sidharth Kumar"
      },
      {
        "authorId": "2271625822",
        "name": "Astha Verma"
      },
      {
        "authorId": "1753278",
        "name": "R. Shah"
      },
      {
        "authorId": "2297672250",
        "name": "Roger Zimmermann"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "d01a0dd3ca1433574d496f9e6acdf36a79b9036b",
    "url": "https://www.semanticscholar.org/paper/d01a0dd3ca1433574d496f9e6acdf36a79b9036b",
    "title": "Text2Reaction : Enabling Reactive Task Planning Using Large Language Models",
    "abstract": "To complete tasks in dynamic environments, robots need to timely update their plans to react to environment changes. Traditional stripe-like or learning-based planners struggle to achieve this due to their high reliance on meticulously predefined planning rules or labeled data. Fortunately, recent works find that Large Language Models (LLMs) can be effectively prompted to solve planning problems. Thus, we investigate the strategies for LLMs to master reactive planning problems without complex definitions and extra training. We propose Text2Reaction, an LLM-based framework enabling robots to continuously reason and update plans according to the latest environment changes. Inspired from human's step-by-step re-planning process, we present the Re-planning Prompt, which informs LLMs the basic principles of re-planning and fosters the gradual development of a current plan to a new one in a three-hop reasoning manner–cause analysis, consequence inference, and plan adjustment. In addition, Text2Reaction is designed to first generate an initial plan based on the task description before execution, allowing for subsequent iterative updates of this plan. We demonstrate the superior performance of Text2Reaction over prior works in reacting to various environment changes and completing varied tasks. In addition, we validate the reliability of our re-planning prompt through ablation experiments and its capability when deployed in real-world robots, enabling continuous reasoning in the face of diverse changes until the user instructions are successfully completed.",
    "venue": "IEEE Robotics and Automation Letters",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-01",
    "authors": [
      {
        "authorId": "2211564256",
        "name": "Zejun Yang"
      },
      {
        "authorId": "2199444338",
        "name": "Li Ning"
      },
      {
        "authorId": "2155118311",
        "name": "Haitao Wang"
      },
      {
        "authorId": "2106542628",
        "name": "Tianyu Jiang"
      },
      {
        "authorId": "2116577968",
        "name": "Shaolin Zhang"
      },
      {
        "authorId": "1853836031",
        "name": "Shaowei Cui"
      },
      {
        "authorId": "2289790413",
        "name": "Hao Jiang"
      },
      {
        "authorId": "2289260883",
        "name": "Chunpeng Li"
      },
      {
        "authorId": "2249130986",
        "name": "Shuo Wang"
      },
      {
        "authorId": "2289489575",
        "name": "Zhaoqi Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.87639203842082
  },
  {
    "paperId": "7781abb2c62d17379a932898874bc90bf0d604e4",
    "url": "https://www.semanticscholar.org/paper/7781abb2c62d17379a932898874bc90bf0d604e4",
    "title": "Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty",
    "abstract": "Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Ad-ditionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty–aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches. Furthermore, we assess the effectiveness of introspective planning in conjunction with conformal prediction, revealing that this combination yields tighter confidence bounds, thereby maintaining statistical success guarantees with fewer superfluous user clarification queries.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2087743517",
        "name": "Kaiqu Liang"
      },
      {
        "authorId": "2283800150",
        "name": "Zixu Zhang"
      },
      {
        "authorId": "2265578799",
        "name": "J. F. Fisac"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "fb5e1b25480e759e2f310aed6ff431de1ec23135",
    "url": "https://www.semanticscholar.org/paper/fb5e1b25480e759e2f310aed6ff431de1ec23135",
    "title": "Economics Arena for Large Language Models",
    "abstract": "Large language models (LLMs) have been extensively used as the backbones for general-purpose agents, and some economics literature suggest that LLMs are capable of playing various types of economics games. Following these works, to overcome the limitation of evaluating LLMs using static benchmarks, we propose to explore competitive games as an evaluation for LLMs to incorporate multi-players and dynamicise the environment. By varying the game history revealed to LLMs-based players, we find that most of LLMs are rational in that they play strategies that can increase their payoffs, but not as rational as indicated by Nash Equilibria (NEs). Moreover, when game history are available, certain types of LLMs, such as GPT-4, can converge faster to the NE strategies, which suggests higher rationality level in comparison to other models. In the meantime, certain types of LLMs can win more often when game history are available, and we argue that the winning rate reflects the reasoning ability with respect to the strategies of other players. Throughout all our experiments, we observe that the ability to strictly follow the game rules described by natural languages also vary among the LLMs we tested. In this work, we provide an economics arena for the LLMs research community as a dynamic simulation to test the above-mentioned abilities of LLMs, i.e. rationality, strategic reasoning ability, and instruction-following capability.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-03",
    "authors": [
      {
        "authorId": "3431365",
        "name": "Shangmin Guo"
      },
      {
        "authorId": "2277598160",
        "name": "Haoran Bu"
      },
      {
        "authorId": "2277689256",
        "name": "Haochuan Wang"
      },
      {
        "authorId": "2115242507",
        "name": "Yi Ren"
      },
      {
        "authorId": "2261388493",
        "name": "Dianbo Sui"
      },
      {
        "authorId": "2277598324",
        "name": "Yuming Shang"
      },
      {
        "authorId": "2277688347",
        "name": "Siting Lu"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "e09a2e4fa4fe89e6553b844ed841643769c1118c",
    "url": "https://www.semanticscholar.org/paper/e09a2e4fa4fe89e6553b844ed841643769c1118c",
    "title": "Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take TravelPlanner as an Example",
    "abstract": "Large language models (LLMs) have brought autonomous agents closer to artificial general intelligence (AGI) due to their promising generalization and emergent capabilities. There is, however, a lack of studies on how LLM-based agents behave, why they could potentially fail, and how to improve them, particularly in demanding real-world planning tasks. In this paper, as an effort to fill the gap, we present our study using a realistic benchmark, TravelPlanner, where an agent must meet multiple constraints to generate accurate plans. We leverage this benchmark to address four key research questions: (1) are LLM agents robust enough to lengthy and noisy contexts when it comes to reasoning and planning? (2) can few-shot prompting adversely impact the performance of LLM agents in scenarios with long context? (3) can we rely on refinement to improve plans, and (4) can fine-tuning LLMs with both positive and negative feedback lead to further improvement? Our comprehensive experiments indicate that, firstly, LLMs often fail to attend to crucial parts of a long context, despite their ability to handle extensive reference information and few-shot examples; secondly, they still struggle with analyzing the long plans and cannot provide accurate feedback for refinement; thirdly, we propose Feedback-Aware Fine-Tuning (FAFT), which leverages both positive and negative feedback, resulting in substantial gains over Supervised Fine-Tuning (SFT). Our findings offer in-depth insights to the community on various aspects related to real-world planning applications.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-12",
    "authors": [
      {
        "authorId": "2315946457",
        "name": "Yanan Chen"
      },
      {
        "authorId": "2699756",
        "name": "Ali Pesaranghader"
      },
      {
        "authorId": "38936015",
        "name": "Tanmana Sadhu"
      },
      {
        "authorId": "2315935036",
        "name": "Dong Hoon Yi"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "fed4d28bd44764c04120820244e2c60f4e34a8d4",
    "url": "https://www.semanticscholar.org/paper/fed4d28bd44764c04120820244e2c60f4e34a8d4",
    "title": "Counting-Stars: A Multi-evidence, Position-aware, and Scalable Benchmark for Evaluating Long-Context Large Language Models",
    "abstract": "Despite recent efforts to develop large language models with robust long-context capabilities, the lack of long-context benchmarks means that relatively little is known about their performance. To alleviate this gap, in this paper, we propose \\textbf{Counting-Stars}, a multi-evidence, position-aware, and scalable benchmark designed to evaluate the multi-evidence retrieval capabilities of long-context LLMs. \\textbf{Counting-Stars} comprises two counting-based multiple pieces of evidence retrieval sub-tasks: searching and reasoning. Using Counting-Stars, we conduct experiments to evaluate several long-context LLMs, including GPT-4 Turbo, Gemini 1.5 Pro, Claude3 Opus, GLM-4, and Moonshot-v1. Extensive experimental results demonstrate that Gemini 1.5 Pro achieves the best overall results, while GPT-4 Turbo exhibits the most stable performance across various tasks. Furthermore, our analysis of these LLMs, which have been extended to handle long-context scenarios, indicates that significant room for improvement remains as the length of the input context and the complexity of the tasks increase.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-18",
    "authors": [
      {
        "authorId": "2292180381",
        "name": "Mingyang Song"
      },
      {
        "authorId": "2292367270",
        "name": "Mao Zheng"
      },
      {
        "authorId": "2292215178",
        "name": "Xuan Luo"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "d9293bafb3b7ff2d14f69f35b681d7ce8d192e46",
    "url": "https://www.semanticscholar.org/paper/d9293bafb3b7ff2d14f69f35b681d7ce8d192e46",
    "title": "Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models",
    "abstract": "This study explores the realm of knowledge base question answering (KBQA). KBQA is considered a challenging task, particularly in parsing intricate questions into executable logical forms. Traditional semantic parsing (SP)-based methods require extensive data annotations, which result in significant costs. Recently, the advent of few-shot in-context learning, powered by large language models (LLMs), has showcased promising capabilities. However, fully leveraging LLMs to parse questions into logical forms in low-resource scenarios poses a substantial challenge. To tackle these hurdles, we introduce Interactive-KBQA, a framework designed to generate logical forms through direct interaction with knowledge bases (KBs). Within this framework, we have developed three generic APIs for KB interaction. For each category of complex question, we devised exemplars to guide LLMs through the reasoning processes. Our method achieves competitive results on the WebQuestionsSP, ComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal number of examples (shots). Importantly, our approach supports manual intervention, allowing for the iterative refinement of LLM outputs. By annotating a dataset with step-wise reasoning processes, we showcase our model's adaptability and highlight its potential for contributing significant enhancements to the field.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-23",
    "authors": [
      {
        "authorId": "2267039181",
        "name": "Guanming Xiong"
      },
      {
        "authorId": "2286303797",
        "name": "Junwei Bao"
      },
      {
        "authorId": "2267103606",
        "name": "Wen Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "e0985ed4b7745afc3d54c02d55e8f332b788bbcf",
    "url": "https://www.semanticscholar.org/paper/e0985ed4b7745afc3d54c02d55e8f332b788bbcf",
    "title": "Large Language Model with Graph Convolution for Recommendation",
    "abstract": "In recent years, efforts have been made to use text information for better user profiling and item characterization in recommendations. However, text information can sometimes be of low quality, hindering its effectiveness for real-world applications. With knowledge and reasoning capabilities capsuled in Large Language Models (LLMs), utilizing LLMs emerges as a promising way for description improvement. However, existing ways of prompting LLMs with raw texts ignore structured knowledge of user-item interactions, which may lead to hallucination problems like inconsistent description generation. To this end, we propose a Graph-aware Convolutional LLM method to elicit LLMs to capture high-order relations in the user-item graph. To adapt text-based LLMs with structured graphs, We use the LLM as an aggregator in graph processing, allowing it to understand graph-based information step by step. Specifically, the LLM is required for description enhancement by exploring multi-hop neighbors layer by layer, thereby propagating information progressively in the graph. To enable LLMs to capture large-scale graph information, we break down the description task into smaller parts, which drastically reduces the context length of the token input with each step. Extensive experiments on three real-world datasets show that our method consistently outperforms state-of-the-art methods.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-14",
    "authors": [
      {
        "authorId": "51123128",
        "name": "Yingpeng Du"
      },
      {
        "authorId": "2326425",
        "name": "Ziteng Wang"
      },
      {
        "authorId": "2284074007",
        "name": "Zhu Sun"
      },
      {
        "authorId": "2284062835",
        "name": "Haoyan Chua"
      },
      {
        "authorId": "2109503105",
        "name": "Hongzhi Liu"
      },
      {
        "authorId": "2260608759",
        "name": "Zhonghai Wu"
      },
      {
        "authorId": "2284661349",
        "name": "Yining Ma"
      },
      {
        "authorId": "2257228461",
        "name": "Jie Zhang"
      },
      {
        "authorId": "2240529933",
        "name": "Youchen Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "53c0197c0961082581bef89deb7b39ba32f95416",
    "url": "https://www.semanticscholar.org/paper/53c0197c0961082581bef89deb7b39ba32f95416",
    "title": "Understanding Long Videos with Multimodal Language Models",
    "abstract": "Large Language Models (LLMs) have allowed recent LLM-based approaches to achieve excellent performance on long-video understanding benchmarks. We investigate how extensive world knowledge and strong reasoning skills of underlying LLMs influence this strong performance. Surprisingly, we discover that LLM-based approaches can yield surprisingly good accuracy on long-video tasks with limited video information, sometimes even with no video specific information. Building on this, we exploring injecting video-specific information into an LLM-based framework. We utilize off-the-shelf vision tools to extract three object-centric information modalities from videos and then leverage natural language as a medium for fusing this information. Our resulting Multimodal Video Understanding (MVU) framework demonstrates state-of-the-art performance across multiple video understanding benchmarks. Strong performance also on robotics domain tasks establish its strong generality. Our code will be released publicly.",
    "venue": "",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-03-25",
    "authors": [
      {
        "authorId": "48430646",
        "name": "Kanchana Ranasinghe"
      },
      {
        "authorId": "2256119676",
        "name": "Xiang Li"
      },
      {
        "authorId": "51504770",
        "name": "Kumara Kahatapitiya"
      },
      {
        "authorId": "1766489",
        "name": "M. Ryoo"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "03ae1a664b52503594e4753f59615443d70695b6",
    "url": "https://www.semanticscholar.org/paper/03ae1a664b52503594e4753f59615443d70695b6",
    "title": "RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners",
    "abstract": "Large Language Models (LLMs) have achieved impressive performance across various reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT are prone to logical errors during their reasoning processes. Existing solutions, such as deploying task-specific verifiers or voting over multiple reasoning paths, either require extensive human annotations or fail in scenarios with inconsistent responses. To address these challenges, we introduce RankPrompt, a new prompting method that enables LLMs to self-rank their responses without additional resources. RankPrompt breaks down the ranking problem into a series of comparisons among diverse responses, leveraging the inherent capabilities of LLMs to generate chains of comparison as contextual exemplars. Our experiments across 11 arithmetic and commonsense reasoning tasks show that RankPrompt significantly enhances the reasoning performance of ChatGPT and GPT-4, with improvements of up to 13%. Moreover, RankPrompt excels in LLM-based automatic evaluations for open-ended tasks, aligning with human judgments 74% of the time in the AlpacaEval dataset. It also exhibits robustness to variations in response order and consistency. Collectively, our results validate RankPrompt as an effective method for eliciting high-quality feedback from language models.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-19",
    "authors": [
      {
        "authorId": "114172611",
        "name": "Chi Hu"
      },
      {
        "authorId": "2288399464",
        "name": "Yuan Ge"
      },
      {
        "authorId": "2292213161",
        "name": "Xiangnan Ma"
      },
      {
        "authorId": "2153244464",
        "name": "Hang Cao"
      },
      {
        "authorId": "2293022527",
        "name": "Qiang Li"
      },
      {
        "authorId": "2292667847",
        "name": "Yonghua Yang"
      },
      {
        "authorId": "2287922214",
        "name": "Tong Xiao"
      },
      {
        "authorId": "2240940961",
        "name": "Jingbo Zhu"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "59eed4c468846a4d45105a4603dabf72e2bef830",
    "url": "https://www.semanticscholar.org/paper/59eed4c468846a4d45105a4603dabf72e2bef830",
    "title": "Generative Reward Models",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has greatly improved the performance of modern Large Language Models (LLMs). The RLHF process is resource-intensive and technically challenging, generally requiring a large collection of human preference labels over model-generated outputs. Reinforcement Learning from AI Feedback (RLAIF) addresses this data collection challenge by leveraging synthetic preferences generated by an LLM. However, recent work has shown that synthetic preferences labels may not align well with human preference judgments. To address this, we propose a hybrid approach that unifies RLHF and RLAIF methodologies. We introduce GenRM, an iterative algorithm that trains an LLM on self-generated reasoning traces, leading to synthetic preference labels matching human preference judgments. Empirically, we show that zero-shot LLM-based judgments under-perform compared to Bradley-Terry reward models on in-distribution tasks (between 9-36%). In contrast, GenRM achieves in-distribution accuracy comparable to Bradley-Terry models, while significantly outperforming them on out-of-distribution tasks (between 10-45%). Moreover, GenRM surpasses the performance of using LLMs as judges on both in-distribution (by 9-31%) and out-of-distribution tasks (by 2- 6%). Our results show that combining the strengths of RLHF and RLAIF offers a promising approach for improving the quality of synthetic preference labels.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-02",
    "authors": [
      {
        "authorId": "2287928420",
        "name": "Dakota Mahan"
      },
      {
        "authorId": "2273535247",
        "name": "Duy Phung"
      },
      {
        "authorId": "2301519261",
        "name": "Rafael Rafailov"
      },
      {
        "authorId": "2326300653",
        "name": "Chase Blagden"
      },
      {
        "authorId": "2283848553",
        "name": "nathan lile"
      },
      {
        "authorId": "28933528",
        "name": "Louis Castricato"
      },
      {
        "authorId": "2220625823",
        "name": "Jan-Philipp Franken"
      },
      {
        "authorId": "2284774407",
        "name": "Chelsea Finn"
      },
      {
        "authorId": "2044198106",
        "name": "Alon Albalak"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "3831b44b72e122c2b5d758c8170876ec5ad6e4b1",
    "url": "https://www.semanticscholar.org/paper/3831b44b72e122c2b5d758c8170876ec5ad6e4b1",
    "title": "Large Language Models are Zero-Shot Next Location Predictors",
    "abstract": "Predicting the locations an individual will visit in the future is crucial for solving many societal issues like disease diffusion and reduction of pollution. However, next-location predictors require a significant amount of individual-level information that may be scarce or unavailable in some scenarios (e.g., cold-start). Large Language Models (LLMs) have shown good generalization and reasoning capabilities and are rich in geographical knowledge, allowing us to believe that these models can act as zero-shot next-location predictors. We tested more than 15 LLMs on three real-world mobility datasets and we found that LLMs can obtain accuracies up to 36.2%, a significant relative improvement of almost 640% when compared to other models specifically designed for human mobility. We also test for data contamination and explored the possibility of using LLMs as text-based explainers for next-location prediction, showing that, regardless of the model size, LLMs can explain their decision.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-31",
    "authors": [
      {
        "authorId": "2304324148",
        "name": "Ciro Beneduce"
      },
      {
        "authorId": "2267779682",
        "name": "Bruno Lepri"
      },
      {
        "authorId": "48249758",
        "name": "Massimiliano Luca"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "2f97fe9ce214394151536118b6e1f0bdd3b190ee",
    "url": "https://www.semanticscholar.org/paper/2f97fe9ce214394151536118b6e1f0bdd3b190ee",
    "title": "ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting",
    "abstract": "Chain-of-Thought (CoT) prompting can enhance the reasoning capabilities of large language models (LLMs), establishing itself as a primary approach to solving complex reasoning tasks. Existing CoT synthesis approaches usually focus on simpler reasoning tasks and thus result in low-quality and inconsistent CoT prompts. In response to this challenge, we present an empirical investigation of CoT prompting and introduce CoTGenius, a novel framework designed for the automatic generation of superior CoT prompts. CoTGenius is developed based on three major evolution strategies, i.e., complicate, diversify, and specify—alongside two filtering mechanisms: evolutionary success judgement and correctness verification. We further employ CoTGenius to create an extensive CoT dataset, and subsequently fine-tune the Llama 2-Chat 7B and 13B models on this dataset. We call the resulting model ChainLM. To deal with the cumulative error issue in reasoning steps, we propose a step-level debating method, wherein multiple debaters discuss each reasoning step to arrive at the correct answer. Extensive experiments demonstrate that our ChainLM models exhibit enhanced proficiency in addressing a spectrum of complex reasoning problems compared to existing models. In addition, we conduct an in-depth analysis of the impact of data categories within CoTGenius on the model performance. We release our dataset and code at https://github.com/RUCAIBox/ChainLM.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-21",
    "authors": [
      {
        "authorId": "2149479237",
        "name": "Xiaoxue Cheng"
      },
      {
        "authorId": "2018027",
        "name": "Junyi Li"
      },
      {
        "authorId": "2257376413",
        "name": "Wayne Xin Zhao"
      },
      {
        "authorId": "2274218622",
        "name": "Ji-Rong Wen"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "ae9cc35ac5ec41c5f0fef597d8e8958045e095bf",
    "url": "https://www.semanticscholar.org/paper/ae9cc35ac5ec41c5f0fef597d8e8958045e095bf",
    "title": "Enhancing Exploratory Testing by Large Language Model and Knowledge Graph",
    "abstract": "Exploratory testing leverages the tester's knowledge and creativity to design test cases for effectively uncovering system-level bugs from the end user's perspective. Researchers have worked on test scenario generation to support exploratory testing based on a system knowledge graph, enriched with scenario and oracle knowledge from bug reports. Nevertheless, the adoption of this approach is hindered by difficulties in handling bug reports of inconsistent quality and varied expression styles, along with the infeasibility of the generated test scenarios. To overcome these limitations, we utilize the superior natural language understanding (NLU) capabilities of Large Language Models (LLMs) to construct a System KG of User Tasks and Failures (SysKG-UTF). Leveraging the system and bug knowledge from the KG, along with the logical reasoning capabilities of LLMs, we generate test scenarios with high feasibility and coherence. Particularly, we design chain-of-thought (CoT) reasoning to extract human-like knowledge and logical reasoning from LLMs, simulating a developer's process of validating test scenario feasibility. Our evaluation shows that our approach significantly enhances the KG construction, particularly for bug reports with low quality. Furthermore, our approach generates test scenarios with high feasibility and coherence. The user study further proves the effectiveness of our generated test scenarios in supporting exploratory testing. Specifically, 8 participants find 36 bugs from 8 seed bugs in two hours using our test scenarios, a significant improvement over the 21 bugs found by the state-of-the-art baseline.",
    "venue": "International Conference on Software Engineering",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2024-04-12",
    "authors": [
      {
        "authorId": "2118000426",
        "name": "Yanqi Su"
      },
      {
        "authorId": "2197087303",
        "name": "Dianshu Liao"
      },
      {
        "authorId": "2247838339",
        "name": "Zhenchang Xing"
      },
      {
        "authorId": "2181288908",
        "name": "Qing Huang"
      },
      {
        "authorId": "1871535985",
        "name": "Mulong Xie"
      },
      {
        "authorId": "2117522274",
        "name": "Qinghua Lu"
      },
      {
        "authorId": "2266422366",
        "name": "Xiwei Xu"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "5cdba098f7b91106333008244fd8286d83af229b",
    "url": "https://www.semanticscholar.org/paper/5cdba098f7b91106333008244fd8286d83af229b",
    "title": "Multi-perspective Improvement of Knowledge Graph Completion with Large Language Models",
    "abstract": "Knowledge graph completion (KGC) is a widely used method to tackle incompleteness in knowledge graphs (KGs) by making predictions for missing links. Description-based KGC leverages pre-trained language models to learn entity and relation representations with their names or descriptions, which shows promising results. However, the performance of description-based KGC is still limited by the quality of text and the incomplete structure, as it lacks sufficient entity descriptions and relies solely on relation names, leading to sub-optimal results. To address this issue, we propose MPIKGC, a general framework to compensate for the deficiency of contextualized knowledge and improve KGC by querying large language models (LLMs) from various perspectives, which involves leveraging the reasoning, explanation, and summarization capabilities of LLMs to expand entity descriptions, understand relations, and extract structures, respectively. We conducted extensive evaluation of the effectiveness and improvement of our framework based on four description-based KGC models, for both link prediction and triplet classification tasks. All codes and generated data will be publicly available after review.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-03-04",
    "authors": [
      {
        "authorId": "2262514619",
        "name": "Derong Xu"
      },
      {
        "authorId": "2030976630",
        "name": "Ziheng Zhang"
      },
      {
        "authorId": "2258562926",
        "name": "Zhenxi Lin"
      },
      {
        "authorId": "2277462592",
        "name": "Xian Wu"
      },
      {
        "authorId": "2288043255",
        "name": "Zhihong Zhu"
      },
      {
        "authorId": "2277237058",
        "name": "Tong Xu"
      },
      {
        "authorId": "2281902096",
        "name": "Xiangyu Zhao"
      },
      {
        "authorId": "2237585282",
        "name": "Yefeng Zheng"
      },
      {
        "authorId": "2265580543",
        "name": "Enhong Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "3455e135151b8026d8e68b1363b212982697a65c",
    "url": "https://www.semanticscholar.org/paper/3455e135151b8026d8e68b1363b212982697a65c",
    "title": "Instructing Large Language Models to Identify and Ignore Irrelevant Conditions",
    "abstract": "Math word problem (MWP) solving requires generating a reasoning path based on a given problem description that often contains irrelevant conditions.Existing chain-of-thought (CoT) prompting methods elicited multi-step reasoning abilities of large language models (LLMs) to solve MWPs.However, they were seriously confused by the irrelevant conditions, resulting in low accuracy.In this paper, we propose a novel approach named I^3C that instructs LLMs to identify and ignore irrelevant conditions.It identifies a set of irrelevant condition candidates that have a weak semantic relevance with the question.Then it prompts LLMs to verify the irrelevant conditions.Lastly it instructs the LLMs with the verification on relevant and irrelevant conditions to avoid confusion and improve reasoning paths.Moreover, we propose to select (problem, reasoning paths) pairs as demonstrations to enhance I^3C with few-shot reasoning. We develop I^3C-Select that selects the most confusing problems based on the semantic relevance measurement.We conduct extensive experiments on eight MWP datasets.I^3C can be combined with any CoT prompting methods to improve the performance of solving MWPs.Notably, with GPT-3.5-Turbo and I^3C-Select, we achieve an accuracy of 96.0 and 94.1 on GSM-IC2-1K and GSM-ICM-1K, respectively, significantly outperforming the state-of-the-art few-shot prompting method Complex-CoT by +11.7 and +11.1.Our implementation is made publicly available at https://wzy6642.github.io/I3C.github.io/.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-19",
    "authors": [
      {
        "authorId": "2273843167",
        "name": "Zhenyu Wu"
      },
      {
        "authorId": "2273948026",
        "name": "Chao Shen"
      },
      {
        "authorId": "2273911868",
        "name": "Meng Jiang"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "eea208084595740441c01e2856711e6046ae188b",
    "url": "https://www.semanticscholar.org/paper/eea208084595740441c01e2856711e6046ae188b",
    "title": "Towards Faithful Chain-of-Thought: Large Language Models are Bridging Reasoners",
    "abstract": "Large language models (LLMs) suffer from serious unfaithful chain-of-thought (CoT) issues. Previous work attempts to measure and explain it but lacks in-depth analysis within CoTs and does not consider the interactions among all reasoning components jointly. In this paper, we first study the CoT faithfulness issue at the granularity of CoT steps, identify two reasoning paradigms: centralized reasoning and distributed reasoning, and find their relationship with faithfulness. Subsequently, we conduct a joint analysis of the causal relevance among the context, CoT, and answer during reasoning. The result proves that, when the LLM predicts answers, it can recall correct information missing in the CoT from the context, leading to unfaithfulness issues. Finally, we propose the inferential bridging method to mitigate this issue, in which we use the attribution method to recall information as hints for CoT generation and filter out noisy CoTs based on their semantic consistency and attribution scores. Extensive experiments demonstrate that our approach effectively alleviates the unfaithful CoT problem.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-29",
    "authors": [
      {
        "authorId": "2203948041",
        "name": "Jiachun Li"
      },
      {
        "authorId": "49776272",
        "name": "Pengfei Cao"
      },
      {
        "authorId": "1763402",
        "name": "Yubo Chen"
      },
      {
        "authorId": "77397868",
        "name": "Kang Liu"
      },
      {
        "authorId": "2269147239",
        "name": "Jun Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "d9c66832ce4c8e5fe7c07184441c8036e7d589f1",
    "url": "https://www.semanticscholar.org/paper/d9c66832ce4c8e5fe7c07184441c8036e7d589f1",
    "title": "A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?",
    "abstract": "Large language models (LLMs) have exhibited remarkable capabilities across various domains and tasks, pushing the boundaries of our knowledge in learning and cognition. The latest model, OpenAI's o1, stands out as the first LLM with an internalized chain-of-thought technique using reinforcement learning strategies. While it has demonstrated surprisingly strong capabilities on various general language tasks, its performance in specialized fields such as medicine remains unknown. To this end, this report provides a comprehensive exploration of o1 on different medical scenarios, examining 3 key aspects: understanding, reasoning, and multilinguality. Specifically, our evaluation encompasses 6 tasks using data from 37 medical datasets, including two newly constructed and more challenging question-answering (QA) tasks based on professional medical quizzes from the New England Journal of Medicine (NEJM) and The Lancet. These datasets offer greater clinical relevance compared to standard medical QA benchmarks such as MedQA, translating more effectively into real-world clinical utility. Our analysis of o1 suggests that the enhanced reasoning ability of LLMs may (significantly) benefit their capability to understand various medical instructions and reason through complex clinical scenarios. Notably, o1 surpasses the previous GPT-4 in accuracy by an average of 6.2% and 6.6% across 19 datasets and two newly created complex QA scenarios. But meanwhile, we identify several weaknesses in both the model capability and the existing evaluation protocols, including hallucination, inconsistent multilingual ability, and discrepant metrics for evaluation. We release our raw data and model outputs at https://ucsc-vlaa.github.io/o1_medicine/ for future research.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-23",
    "authors": [
      {
        "authorId": "2315648431",
        "name": "Yunfei Xie"
      },
      {
        "authorId": "2315276567",
        "name": "Juncheng Wu"
      },
      {
        "authorId": "2239200170",
        "name": "Haoqin Tu"
      },
      {
        "authorId": "2284185238",
        "name": "Siwei Yang"
      },
      {
        "authorId": "2256773374",
        "name": "Bingchen Zhao"
      },
      {
        "authorId": "152218610",
        "name": "Yongshuo Zong"
      },
      {
        "authorId": "2322455156",
        "name": "Qiao Jin"
      },
      {
        "authorId": "2239227247",
        "name": "Cihang Xie"
      },
      {
        "authorId": "2306069928",
        "name": "Yuyin Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "2a7720c878c20ed21608978e31507ca9d9936d1c",
    "url": "https://www.semanticscholar.org/paper/2a7720c878c20ed21608978e31507ca9d9936d1c",
    "title": "SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation",
    "abstract": "This paper introduces Self-aware Knowledge Retrieval (SeaKR), a novel adaptive RAG model that extracts self-aware uncertainty of LLMs from their internal states. SeaKR activates retrieval when the LLMs present high self-aware uncertainty for generation. To effectively integrate retrieved knowledge snippets, SeaKR re-ranks them based on LLM's self-aware uncertainty to preserve the snippet that reduces their uncertainty to the utmost. To facilitate solving complex tasks that require multiple retrievals, SeaKR utilizes their self-aware uncertainty to choose among different reasoning strategies. Our experiments on both complex and simple Question Answering datasets show that SeaKR outperforms existing adaptive RAG methods. We release our code at https://github.com/THU-KEG/SeaKR.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-27",
    "authors": [
      {
        "authorId": "2273946831",
        "name": "Zijun Yao"
      },
      {
        "authorId": "2149667177",
        "name": "Weijian Qi"
      },
      {
        "authorId": "2308643805",
        "name": "Liangming Pan"
      },
      {
        "authorId": "1712738522",
        "name": "S. Cao"
      },
      {
        "authorId": "2282510494",
        "name": "Linmei Hu"
      },
      {
        "authorId": "2308556832",
        "name": "Weichuan Liu"
      },
      {
        "authorId": "2284777109",
        "name": "Lei Hou"
      },
      {
        "authorId": "2133353675",
        "name": "Juanzi Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "050be0010b4625a9b7ed2c77bd1a48d3c618468e",
    "url": "https://www.semanticscholar.org/paper/050be0010b4625a9b7ed2c77bd1a48d3c618468e",
    "title": "Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees",
    "abstract": "Tool-augmented large language models (LLMs) leverage tools, often in the form of APIs, to enhance their reasoning capabilities on complex tasks, thus taking on the role of intelligent agents interacting with the real world. The recently introduced ToolLLaMA model by Qin et al. [2024] utilizes the depth-first search-based decision tree (DFSDT) method for reasoning with $16000+$ real-world APIs, which effectively improves the planning and inferencing performance of tool-augmented LLMs compared to traditional chain reasoning approaches. However, their approach only employs successful paths from decision trees (also called inference trees) for supervised fine-tuning (SFT) during training, which does not fully exploit the advantages of the tree of thought. In this study, we propose an inference trajectory optimization framework based on the preference data extracted from decision trees to address this limitation. We first introduce a novel method for constructing preference data from the tree of thought, capitalizing on the failed explorations previously overlooked in the trees. Specifically, we generate an effective step-wise preference dataset, named ToolPreference, for tool use based on the ToolBench dataset. In the subsequent training phase, we first fine-tune the LLM with tool-usage expert trajectories and then use these step-wise preference pairs for direct preference optimization (DPO) to update the policy of the LLM, resulting in our ToolPrefer-LLaMA (TP-LLaMA) model. Our experiments demonstrate that by obtaining insights from errors in inference trees, TP-LLaMA significantly outperforms the baselines across almost all test scenarios by a large margin and exhibits better generalization capabilities with unseen APIs. At the same time, TP-LLaMA has also demonstrated superior reasoning efficiency compared to the baselines, making it more suitable for complex tool-usage reasoning tasks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-11",
    "authors": [
      {
        "authorId": "2306069080",
        "name": "Sijia Chen"
      },
      {
        "authorId": "2214727131",
        "name": "Yibo Wang"
      },
      {
        "authorId": "2305723906",
        "name": "Yi-Feng Wu"
      },
      {
        "authorId": "2304395071",
        "name": "Qingguo Chen"
      },
      {
        "authorId": "2304361003",
        "name": "Zhao Xu"
      },
      {
        "authorId": "2305289815",
        "name": "Weihua Luo"
      },
      {
        "authorId": "2304530663",
        "name": "Kaifu Zhang"
      },
      {
        "authorId": "2300816565",
        "name": "Lijun Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "557c4691bee459825502988349beedb7d81e4d4c",
    "url": "https://www.semanticscholar.org/paper/557c4691bee459825502988349beedb7d81e4d4c",
    "title": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources",
    "abstract": "Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage. In this paper, we propose Source2Synth: a new method that can be used for teaching LLMs new skills without relying on costly human annotations. Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources. Source2Synth improves the dataset quality by discarding low-quality generations based on their answerability. We demonstrate the generality of this approach by applying it to two challenging domains: we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA). Our method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotPotQA compared to the fine-tuned baselines.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-12",
    "authors": [
      {
        "authorId": "2223643390",
        "name": "A. Lupidi"
      },
      {
        "authorId": "2320807392",
        "name": "Carlos Gemmell"
      },
      {
        "authorId": "2313189469",
        "name": "Nicola Cancedda"
      },
      {
        "authorId": "2173509991",
        "name": "Jane Dwivedi-Yu"
      },
      {
        "authorId": "2320804435",
        "name": "Jason Weston"
      },
      {
        "authorId": "2320805323",
        "name": "Jakob Foerster"
      },
      {
        "authorId": "48647153",
        "name": "Roberta Raileanu"
      },
      {
        "authorId": "2253400960",
        "name": "Maria Lomeli"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "6af200052b1aae40a7214b73cc7c59fb475772d5",
    "url": "https://www.semanticscholar.org/paper/6af200052b1aae40a7214b73cc7c59fb475772d5",
    "title": "Operationalizing Contextual Integrity in Privacy-Conscious Assistants",
    "abstract": "Advanced AI assistants combine frontier LLMs and tool access to autonomously perform complex tasks on behalf of users. While the helpfulness of such assistants can increase dramatically with access to user information including emails and documents, this raises privacy concerns about assistants sharing inappropriate information with third parties without user supervision. To steer information-sharing assistants to behave in accordance with privacy expectations, we propose to operationalize contextual integrity (CI), a framework that equates privacy with the appropriate flow of information in a given context. In particular, we design and evaluate a number of strategies to steer assistants' information-sharing actions to be CI compliant. Our evaluation is based on a novel form filling benchmark composed of human annotations of common webform applications, and it reveals that prompting frontier LLMs to perform CI-based reasoning yields strong results.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-05",
    "authors": [
      {
        "authorId": "2052021549",
        "name": "Sahra Ghalebikesabi"
      },
      {
        "authorId": "2285738434",
        "name": "Eugene Bagdasaryan"
      },
      {
        "authorId": "2300283748",
        "name": "Ren Yi"
      },
      {
        "authorId": "2283305710",
        "name": "Itay Yona"
      },
      {
        "authorId": "47473421",
        "name": "Ilia Shumailov"
      },
      {
        "authorId": "50117905",
        "name": "Aneesh Pappu"
      },
      {
        "authorId": "2315686403",
        "name": "Chongyang Shi"
      },
      {
        "authorId": "51932191",
        "name": "Laura Weidinger"
      },
      {
        "authorId": "49860489",
        "name": "Robert Stanforth"
      },
      {
        "authorId": "48092709",
        "name": "Leonard Berrada"
      },
      {
        "authorId": "143967473",
        "name": "Pushmeet Kohli"
      },
      {
        "authorId": "2268826600",
        "name": "Po-Sen Huang"
      },
      {
        "authorId": "2300283979",
        "name": "Borja Balle"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "931d1a4e3f28829225cc1bc83897f97bc38a156b",
    "url": "https://www.semanticscholar.org/paper/931d1a4e3f28829225cc1bc83897f97bc38a156b",
    "title": "Lifelong Robot Library Learning: Bootstrapping Composable and Generalizable Skills for Embodied Control with Language Models",
    "abstract": "Large Language Models (LLMs) have emerged as a new paradigm for embodied reasoning and control, most recently by generating robot policy code that utilizes a custom library of vision and control primitive skills. However, prior arts fix their skills library and steer the LLM with carefully handcrafted prompt engineering, limiting the agent to a stationary range of addressable tasks. In this work, we introduce LRLL, an LLM-based lifelong learning agent that continuously grows the robot skill library to tackle manipulation tasks of ever-growing complexity. LRLL achieves this with four novel contributions: 1) a soft memory module that allows dynamic storage and retrieval of past experiences to serve as context, 2) a self-guided exploration policy that proposes new tasks in simulation, 3) a skill abstractor that distills recent experiences into new library skills, and 4) a lifelong learning algorithm for enabling human users to bootstrap new skills with minimal online interaction. LRLL continuously transfers knowledge from the memory to the library, building composable, general and interpretable policies, while bypassing gradient-based optimization, thus relieving the learner from catastrophic forgetting. Empirical evaluation in a simulated tabletop environment shows that LRLL outperforms end-to-end and vanilla LLM approaches in the lifelong setup while learning skills that are transferable to the real world. Project material will become available at the webpage https://gtziafas.github.io/LRLL_project/.",
    "venue": "IEEE International Conference on Robotics and Automation",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-13",
    "authors": [
      {
        "authorId": "1753497636",
        "name": "Georgios Tziafas"
      },
      {
        "authorId": "69533920",
        "name": "H. Kasaei"
      }
    ],
    "source": "semantic_scholar",
    "score": 100.1415686865115
  },
  {
    "paperId": "12b22bfd71071e589d64ed17d1da059436d86083",
    "url": "https://www.semanticscholar.org/paper/12b22bfd71071e589d64ed17d1da059436d86083",
    "title": "LLM-Driven Robots Risk Enacting Discrimination, Violence, and Unlawful Actions",
    "abstract": "Members of the Human-Robot Interaction (HRI) and Artificial Intelligence (AI) communities have proposed Large Language Models (LLMs) as a promising resource for robotics tasks such as natural language interactions, doing household and workplace tasks, approximating `common sense reasoning', and modeling humans. However, recent research has raised concerns about the potential for LLMs to produce discriminatory outcomes and unsafe behaviors in real-world robot experiments and applications. To address these concerns, we conduct an HRI-based evaluation of discrimination and safety criteria on several highly-rated LLMs. Our evaluation reveals that LLMs currently lack robustness when encountering people across a diverse range of protected identity characteristics (e.g., race, gender, disability status, nationality, religion, and their intersections), producing biased outputs consistent with directly discriminatory outcomes -- e.g. `gypsy' and `mute' people are labeled untrustworthy, but not `european' or `able-bodied' people. Furthermore, we test models in settings with unconstrained natural language (open vocabulary) inputs, and find they fail to act safely, generating responses that accept dangerous, violent, or unlawful instructions -- such as incident-causing misstatements, taking people's mobility aids, and sexual predation. Our results underscore the urgent need for systematic, routine, and comprehensive risk assessments and assurances to improve outcomes and ensure LLMs only operate on robots when it is safe, effective, and just to do so. Data and code will be made available.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-13",
    "authors": [
      {
        "authorId": "2306258826",
        "name": "Rumaisa Azeem"
      },
      {
        "authorId": "47936089",
        "name": "Andrew Hundt"
      },
      {
        "authorId": "2306216665",
        "name": "Masoumeh Mansouri"
      },
      {
        "authorId": "2306261436",
        "name": "Martim Brandao"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "27a6dc0300eb6a002e8f53c2dacd3495c034f9ff",
    "url": "https://www.semanticscholar.org/paper/27a6dc0300eb6a002e8f53c2dacd3495c034f9ff",
    "title": "RoT: Enhancing Large Language Models with Reflection on Search Trees",
    "abstract": "Large language models (LLMs) have demonstrated impressive capability in reasoning and planning when integrated with tree-search-based prompting methods. However, since these methods ignore the previous search experiences, they often make the same mistakes in the search process. To address this issue, we introduce Reflection on search Trees (RoT), an LLM reflection framework designed to improve the performance of tree-search-based prompting methods. It uses a strong LLM to summarize guidelines from previous tree search experiences to enhance the ability of a weak LLM. The guidelines are instructions about solving this task through tree search which can prevent the weak LLMs from making similar mistakes in the past search process. In addition, we proposed a novel state selection method, which identifies the critical information from historical search processes to help RoT generate more specific and meaningful guidelines. In our extensive experiments, we find that RoT significantly improves the performance of LLMs in reasoning or planning tasks with various tree-search-based prompting methods (e.g., BFS and MCTS). Non-tree-search-based prompting methods such as Chain-of-Thought (CoT) can also benefit from RoT guidelines since RoT can provide task-specific knowledge collected from the search experience.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-08",
    "authors": [
      {
        "authorId": "2265753236",
        "name": "Wenyang Hui"
      },
      {
        "authorId": "2295654095",
        "name": "Yan Wang"
      },
      {
        "authorId": "40341553",
        "name": "Kewei Tu"
      },
      {
        "authorId": "2115486297",
        "name": "Chengyue Jiang"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "0021b60bde8d4f0999794344dfc2365f3357e095",
    "url": "https://www.semanticscholar.org/paper/0021b60bde8d4f0999794344dfc2365f3357e095",
    "title": "Asynchronous Large Language Model Enhanced Planner for Autonomous Driving",
    "abstract": "Despite real-time planners exhibiting remarkable performance in autonomous driving, the growing exploration of Large Language Models (LLMs) has opened avenues for enhancing the interpretability and controllability of motion planning. Nevertheless, LLM-based planners continue to encounter significant challenges, including elevated resource consumption and extended inference times, which pose substantial obstacles to practical deployment. In light of these challenges, we introduce AsyncDriver, a new asynchronous LLM-enhanced closed-loop framework designed to leverage scene-associated instruction features produced by LLM to guide real-time planners in making precise and controllable trajectory predictions. On one hand, our method highlights the prowess of LLMs in comprehending and reasoning with vectorized scene data and a series of routing instructions, demonstrating its effective assistance to real-time planners. On the other hand, the proposed framework decouples the inference processes of the LLM and real-time planners. By capitalizing on the asynchronous nature of their inference frequencies, our approach have successfully reduced the computational cost introduced by LLM, while maintaining comparable performance. Experiments show that our approach achieves superior closed-loop evaluation performance on nuPlan's challenging scenarios.",
    "venue": "European Conference on Computer Vision",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-20",
    "authors": [
      {
        "authorId": "2144036087",
        "name": "Yuan Chen"
      },
      {
        "authorId": "2264574237",
        "name": "Zihan Ding"
      },
      {
        "authorId": "2265646739",
        "name": "Ziqin Wang"
      },
      {
        "authorId": "2311773343",
        "name": "Yan Wang"
      },
      {
        "authorId": "2307504946",
        "name": "Lijun Zhang"
      },
      {
        "authorId": "2307567846",
        "name": "Si Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "efb4cdfe6869ad84b158cb6662801b88e8832be9",
    "url": "https://www.semanticscholar.org/paper/efb4cdfe6869ad84b158cb6662801b88e8832be9",
    "title": "Distillation Matters: Empowering Sequential Recommenders to Match the Performance of Large Language Model",
    "abstract": "Owing to their powerful semantic reasoning capabilities, Large Language Models (LLMs) have been effectively utilized as recommenders, achieving impressive performance. However, the high inference latency of LLMs significantly restricts their practical deployment. To address this issue, this work investigates knowledge distillation from cumbersome LLM-based recommendation models to lightweight conventional sequential models. It encounters three challenges: 1) the teacher's knowledge may not always be reliable; 2) the capacity gap between the teacher and student makes it difficult for the student to assimilate the teacher's knowledge; 3) divergence in semantic space poses a challenge to distill the knowledge from embeddings. To tackle these challenges, this work proposes a novel distillation strategy, DLLM2Rec, specifically tailored for knowledge distillation from LLM-based recommendation models to conventional sequential models. DLLM2Rec comprises: 1) Importance-aware ranking distillation, which filters reliable and student-friendly knowledge by weighting instances according to teacher confidence and student-teacher consistency; 2) Collaborative embedding distillation integrates knowledge from teacher embeddings with collaborative signals mined from the data. Extensive experiments demonstrate the effectiveness of the proposed DLLM2Rec, boosting three typical sequential models with an average improvement of 47.97%, even enabling them to surpass LLM-based recommenders in some cases.",
    "venue": "ACM Conference on Recommender Systems",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3640457.3688118",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-01",
    "authors": [
      {
        "authorId": "2299291942",
        "name": "Yu Cui"
      },
      {
        "authorId": "2299184777",
        "name": "Feng Liu"
      },
      {
        "authorId": "2299279348",
        "name": "Pengbo Wang"
      },
      {
        "authorId": "2284850843",
        "name": "Bohao Wang"
      },
      {
        "authorId": "2286620972",
        "name": "Heng Tang"
      },
      {
        "authorId": "2299340472",
        "name": "Yi Wan"
      },
      {
        "authorId": "2300016985",
        "name": "Jun Wang"
      },
      {
        "authorId": "2265720058",
        "name": "Jiawei Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 100.1415686865115
  },
  {
    "paperId": "4b311ca4cd2be91cc9a29be5c11192d857facc3a",
    "url": "https://www.semanticscholar.org/paper/4b311ca4cd2be91cc9a29be5c11192d857facc3a",
    "title": "SpreadsheetLLM: Encoding Spreadsheets for Large Language Models",
    "abstract": "Spreadsheets, with their extensive two-dimensional grids, various layouts, and diverse formatting options, present notable challenges for large language models (LLMs). In response, we introduce SpreadsheetLLM, pioneering an efficient encoding method designed to unleash and optimize LLMs' powerful understanding and reasoning capability on spreadsheets. Initially, we propose a vanilla serialization approach that incorporates cell addresses, values, and formats. However, this approach was limited by LLMs' token constraints, making it impractical for most applications. To tackle this challenge, we develop SheetCompressor, an innovative encoding framework that compresses spreadsheets effectively for LLMs. It comprises three modules: structural-anchor-based compression, inverse index translation, and data-format-aware aggregation. It significantly improves performance in spreadsheet table detection task, outperforming the vanilla approach by 25.6% in GPT4's in-context learning setting. Moreover, fine-tuned LLM with SheetCompressor has an average compression ratio of 25 times, but achieves a state-of-the-art 78.9% F1 score, surpassing the best existing models by 12.3%. Finally, we propose Chain of Spreadsheet for downstream tasks of spreadsheet understanding and validate in a new and demanding spreadsheet QA task. We methodically leverage the inherent layout and structure of spreadsheets, demonstrating that SpreadsheetLLM is highly effective across a variety of spreadsheet tasks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-12",
    "authors": [
      {
        "authorId": "2303462146",
        "name": "Yuzhang Tian"
      },
      {
        "authorId": "2303471929",
        "name": "Jianbo Zhao"
      },
      {
        "authorId": "2153721991",
        "name": "Haoyu Dong"
      },
      {
        "authorId": "2303412966",
        "name": "Junyu Xiong"
      },
      {
        "authorId": "2303406539",
        "name": "Shiyu Xia"
      },
      {
        "authorId": "144203509",
        "name": "Mengyu Zhou"
      },
      {
        "authorId": "2292270538",
        "name": "Yun Lin"
      },
      {
        "authorId": "2311112640",
        "name": "José Cambronero"
      },
      {
        "authorId": "2258672950",
        "name": "Yeye He"
      },
      {
        "authorId": "2285058007",
        "name": "Shi Han"
      },
      {
        "authorId": "2140415600",
        "name": "Dongmei Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "7712faa0aeba822f56ec570c84188620f50c1de6",
    "url": "https://www.semanticscholar.org/paper/7712faa0aeba822f56ec570c84188620f50c1de6",
    "title": "Tina: Think, Interaction, and Action Framework for Zero-Shot Vision Language Navigation",
    "abstract": "Zero-shot navigation is a critical challenge in Vision-Language Navigation (VLN) tasks, where the ability to adapt to unfamiliar instructions and to act in unknown environments is essential. Existing supervised learning-based models, trained using annotated data through reinforcement learning, exhibit limitations in generalization capabilities. Large Language Models (LLMs), with their extensive knowledge and emergent reasoning abilities, present a potential path-way for achieving zero-shot navigation. This paper presents a VLN agent based on LLMs, exploring approaches to the zero-shot navigation problem. To compensate for the shortcomings of LLMs in environmental perception, we propose the Thinking, Interacting, and Action (TINA) framework. TINA enables the agent to scrutinize perceptual information and autonomously query key clues within the environment through an introduced question-answering module, thereby aligning instructions with specific perceptual data. The navigation agent’s perceptual abilities are enhanced through the TINA framework, while the explicit thought and query processes also improve the navigational procedure’s explainability and transparency. We evaluate the performance of our method on the Room-to-Room dataset. The experiment results indicate that our approach improves the navigation performance of LLM-based agents. Our approach also outperformed some supervised learning-based methods, highlighting its efficacy in zero-shot navigation.",
    "venue": "IEEE International Conference on Multimedia and Expo",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2403.08833",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-13",
    "authors": [
      {
        "authorId": "2151495863",
        "name": "Dingbang Li"
      },
      {
        "authorId": "2277018699",
        "name": "Wenzhou Chen"
      },
      {
        "authorId": "2277235749",
        "name": "Xin Lin"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.79441541679836
  },
  {
    "paperId": "078bedf41f7ff2850bd52f39e8c0b3b239f709d5",
    "url": "https://www.semanticscholar.org/paper/078bedf41f7ff2850bd52f39e8c0b3b239f709d5",
    "title": "Re-Invoke: Tool Invocation Rewriting for Zero-Shot Tool Retrieval",
    "abstract": "Recent advances in large language models (LLMs) have enabled autonomous agents with complex reasoning and task-fulfillment capabilities using a wide range of tools. However, effectively identifying the most relevant tools for a given task becomes a key bottleneck as the toolset size grows, hindering reliable tool utilization. To address this, we introduce Re-Invoke, an unsupervised tool retrieval method designed to scale effectively to large toolsets without training. Specifically, we first generate a diverse set of synthetic queries that comprehensively cover different aspects of the query space associated with each tool document during the tool indexing phase. Second, we leverage LLM's query understanding capabilities to extract key tool-related context and underlying intents from user queries during the inference phase. Finally, we employ a novel multi-view similarity ranking strategy based on intents to pinpoint the most relevant tools for each query. Our evaluation demonstrates that Re-Invoke significantly outperforms state-of-the-art alternatives in both single-tool and multi-tool scenarios, all within a fully unsupervised setting. Notably, on the ToolE datasets, we achieve a 20% relative improvement in nDCG@5 for single-tool retrieval and a 39% improvement for multi-tool retrieval.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-08-03",
    "authors": [
      {
        "authorId": "2258603407",
        "name": "Yanfei Chen"
      },
      {
        "authorId": "2256335437",
        "name": "Jinsung Yoon"
      },
      {
        "authorId": "39670454",
        "name": "Devendra Singh Sachan"
      },
      {
        "authorId": "2275272527",
        "name": "Qingze Wang"
      },
      {
        "authorId": "2305595978",
        "name": "Vincent Cohen-Addad"
      },
      {
        "authorId": "70638610",
        "name": "M. Bateni"
      },
      {
        "authorId": "2278969944",
        "name": "Chen-Yu Lee"
      },
      {
        "authorId": "2305619900",
        "name": "Tomas Pfister"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "aaa3af38d80e7c9ac401d97aac644c5f54f2a45f",
    "url": "https://www.semanticscholar.org/paper/aaa3af38d80e7c9ac401d97aac644c5f54f2a45f",
    "title": "KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using Large Language Models",
    "abstract": "Harnessing the robust capabilities of Large Language Models (LLMs) for narrative generation, logical reasoning, and common-sense knowledge integration, this study delves into utilizing LLMs to enhance automated radiology report generation (R2Gen). Despite the wealth of knowledge within LLMs, efficiently triggering relevant knowledge within these large models for specific tasks like R2Gen poses a critical research challenge. This paper presents KARGEN, a Knowledge-enhanced Automated radiology Report GENeration framework based on LLMs. Utilizing a frozen LLM to generate reports, the framework integrates a knowledge graph to unlock chest disease-related knowledge within the LLM to enhance the clinical utility of generated reports. This is achieved by leveraging the knowledge graph to distill disease-related features in a designed way. Since a radiology report encompasses both normal and disease-related findings, the extracted graph-enhanced disease-related features are integrated with regional image features, attending to both aspects. We explore two fusion methods to automatically prioritize and select the most relevant features. The fused features are employed by LLM to generate reports that are more sensitive to diseases and of improved quality. Our approach demonstrates promising results on the MIMIC-CXR and IU-Xray datasets.",
    "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-09",
    "authors": [
      {
        "authorId": "2264476423",
        "name": "Yingshu Li"
      },
      {
        "authorId": "2268638120",
        "name": "Zhanyu Wang"
      },
      {
        "authorId": "2155979050",
        "name": "Yunyi Liu"
      },
      {
        "authorId": "2275610871",
        "name": "Lei Wang"
      },
      {
        "authorId": "2243894782",
        "name": "Lingqiao Liu"
      },
      {
        "authorId": "2264005560",
        "name": "Luping Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "6b4da2023c3a11aea6e3ccb9ab13e594833c47eb",
    "url": "https://www.semanticscholar.org/paper/6b4da2023c3a11aea6e3ccb9ab13e594833c47eb",
    "title": "Self-Refined Large Language Model as Automated Reward Function Designer for Deep Reinforcement Learning in Robotics",
    "abstract": "Although Deep Reinforcement Learning (DRL) has achieved notable success in numerous robotic applications, designing a high-performing reward function remains a challenging task that often requires substantial manual input. Recently, Large Language Models (LLMs) have been extensively adopted to address tasks demanding in-depth common-sense knowledge, such as reasoning and planning. Recognizing that reward function design is also inherently linked to such knowledge, LLM offers a promising potential in this context. Motivated by this, we propose in this work a novel LLM framework with a self-refinement mechanism for automated reward function design. The framework commences with the LLM formulating an initial reward function based on natural language inputs. Then, the performance of the reward function is assessed, and the results are presented back to the LLM for guiding its self-refinement process. We examine the performance of our proposed framework through a variety of continuous robotic control tasks across three diverse robotic systems. The results indicate that our LLM-designed reward functions are able to rival or even surpass manually designed reward functions, highlighting the efficacy and applicability of our approach.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 22,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.06687",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-13",
    "authors": [
      {
        "authorId": "3427937",
        "name": "Jiayang Song"
      },
      {
        "authorId": "2239501205",
        "name": "Zhehua Zhou"
      },
      {
        "authorId": "2239555012",
        "name": "Jiawei Liu"
      },
      {
        "authorId": "2239197945",
        "name": "Chunrong Fang"
      },
      {
        "authorId": "2234398874",
        "name": "Zhan Shu"
      },
      {
        "authorId": null,
        "name": "Lei Ma"
      }
    ],
    "source": "semantic_scholar",
    "score": 117.03241323893724
  },
  {
    "paperId": "c450bce3f08f2db4f2e7fabbe1260ff119ca43d3",
    "url": "https://www.semanticscholar.org/paper/c450bce3f08f2db4f2e7fabbe1260ff119ca43d3",
    "title": "CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation",
    "abstract": "The long-tail recommendation is a challenging task for traditional recommender systems, due to data sparsity and data imbalance issues. The recent development of large language models (LLMs) has shown their abilities in complex reasoning, which can help to deduce users' preferences based on very few previous interactions. However, since most LLM-based systems rely on items' semantic meaning as the sole evidence for reasoning, the collaborative information of user-item interactions is neglected, which can cause the LLM's reasoning to be misaligned with task-specific collaborative information of the dataset. To further align LLMs' reasoning to task-specific user-item interaction knowledge, we introduce collaborative retrieval-augmented LLMs, CoRAL, which directly incorporate collaborative evidence into the prompts. Based on the retrieved user-item interactions, the LLM can analyze shared and distinct preferences among users, and summarize the patterns indicating which types of users would be attracted by certain items. The retrieved collaborative evidence prompts the LLM to align its reasoning with the user-item interaction patterns in the dataset. However, since the capacity of the input prompt is limited, finding the minimally-sufficient collaborative information for recommendation tasks can be challenging. We propose to find the optimal interaction set through a sequential decision-making process and develop a retrieval policy learned through a reinforcement learning (RL) framework, CoRAL. Our experimental results show that CoRAL can significantly improve LLMs' reasoning abilities on specific recommendation tasks. Our analysis also reveals that CoRAL can more efficiently explore collaborative information through reinforcement learning.",
    "venue": "Knowledge Discovery and Data Mining",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-11",
    "authors": [
      {
        "authorId": "2146666303",
        "name": "Junda Wu"
      },
      {
        "authorId": "2290879533",
        "name": "Cheng-Chun Chang"
      },
      {
        "authorId": "1500399016",
        "name": "Tong Yu"
      },
      {
        "authorId": "51002202",
        "name": "Zhankui He"
      },
      {
        "authorId": "2290853464",
        "name": "Jianing Wang"
      },
      {
        "authorId": "151472453",
        "name": "Yupeng Hou"
      },
      {
        "authorId": "2258552056",
        "name": "Julian McAuley"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "443e5490e7f9fedf912a330fc7a0456d4247ab3d",
    "url": "https://www.semanticscholar.org/paper/443e5490e7f9fedf912a330fc7a0456d4247ab3d",
    "title": "MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction",
    "abstract": "Molecular property prediction (MPP) is a fundamental and crucial task in drug discovery. However, prior methods are limited by the requirement for a large number of labeled molecules and their restricted ability to generalize for unseen and new tasks, both of which are essential for real-world applications. To address these challenges, we present MolecularGPT for few-shot MPP. From a perspective on instruction tuning, we fine-tune large language models (LLMs) based on curated molecular instructions spanning over 1000 property prediction tasks. This enables building a versatile and specialized LLM that can be adapted to novel MPP tasks without any fine-tuning through zero- and few-shot in-context learning (ICL). MolecularGPT exhibits competitive in-context reasoning capabilities across 10 downstream evaluation datasets, setting new benchmarks for few-shot molecular prediction tasks. More importantly, with just two-shot examples, MolecularGPT can outperform standard supervised graph neural network methods on 4 out of 7 datasets. It also excels state-of-the-art LLM baselines by up to 15.7% increase on classification accuracy and decrease of 17.9 on regression metrics (e.g., RMSE) under zero-shot. This study demonstrates the potential of LLMs as effective few-shot molecular property predictors. The code is available at https://github.com/NYUSHCS/MolecularGPT.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Biology"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-18",
    "authors": [
      {
        "authorId": "2307546734",
        "name": "Yuyan Liu"
      },
      {
        "authorId": "2307734628",
        "name": "Sirui Ding"
      },
      {
        "authorId": "2307558814",
        "name": "Sheng Zhou"
      },
      {
        "authorId": "2307774589",
        "name": "Wenqi Fan"
      },
      {
        "authorId": "2307467252",
        "name": "Qiaoyu Tan"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "3b2c8438d67f0ed2bd381ed0808fe5d68a3750bd",
    "url": "https://www.semanticscholar.org/paper/3b2c8438d67f0ed2bd381ed0808fe5d68a3750bd",
    "title": "GraphEval2000: Benchmarking and Improving Large Language Models on Graph Datasets",
    "abstract": "Large language models (LLMs) have achieved remarkable success in natural language processing (NLP), demonstrating significant capabilities in processing and understanding text data. However, recent studies have identified limitations in LLMs' ability to reason about graph-structured data. To address this gap, we introduce GraphEval2000, the first comprehensive graph dataset, comprising 40 graph data structure problems along with 2000 test cases. Additionally, we introduce an evaluation framework based on GraphEval2000, designed to assess the graph reasoning abilities of LLMs through coding challenges. Our dataset categorizes test cases into four primary and four sub-categories, ensuring a comprehensive evaluation. We evaluate eight popular LLMs on GraphEval2000, revealing that LLMs exhibit a better understanding of directed graphs compared to undirected ones. While private LLMs consistently outperform open-source models, the performance gap is narrowing. Furthermore, to improve the usability of our evaluation framework, we propose Structured Symbolic Decomposition (SSD), an instruction-based method designed to enhance LLM performance on GraphEval2000. Results show that SSD improves the performance of GPT-3.5, GPT-4, and GPT-4o on complex graph problems, with an increase of 11.11\\%, 33.37\\%, and 33.37\\%, respectively.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-23",
    "authors": [
      {
        "authorId": "2308071041",
        "name": "Qiming Wu"
      },
      {
        "authorId": "2266978057",
        "name": "Zichen Chen"
      },
      {
        "authorId": "2308034303",
        "name": "Will Corcoran"
      },
      {
        "authorId": "3024298",
        "name": "Misha Sra"
      },
      {
        "authorId": "2267221564",
        "name": "Ambuj Singh"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "0d9f07bcef91a80f3206c63a84220ad2fb84b711",
    "url": "https://www.semanticscholar.org/paper/0d9f07bcef91a80f3206c63a84220ad2fb84b711",
    "title": "LLM4DSR: Leveraing Large Language Model for Denoising Sequential Recommendation",
    "abstract": "Sequential Recommenders generate recommendations based on users' historical interaction sequences. However, in practice, these collected sequences are often contaminated by noisy interactions, which significantly impairs recommendation performance. Accurately identifying such noisy interactions without additional information is particularly challenging due to the absence of explicit supervisory signals indicating noise. Large Language Models (LLMs), equipped with extensive open knowledge and semantic reasoning abilities, offer a promising avenue to bridge this information gap. However, employing LLMs for denoising in sequential recommendation presents notable challenges: 1) Direct application of pretrained LLMs may not be competent for the denoising task, frequently generating nonsensical responses; 2) Even after fine-tuning, the reliability of LLM outputs remains questionable, especially given the complexity of the denoising task and the inherent hallucinatory issue of LLMs. To tackle these challenges, we propose LLM4DSR, a tailored approach for denoising sequential recommendation using LLMs. We constructed a self-supervised fine-tuning task to activate LLMs' capabilities to identify noisy items and suggest replacements. Furthermore, we developed an uncertainty estimation module that ensures only high-confidence responses are utilized for sequence corrections. Remarkably, LLM4DSR is model-agnostic, allowing corrected sequences to be flexibly applied across various recommendation models. Extensive experiments validate the superiority of LLM4DSR over existing methods.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-15",
    "authors": [
      {
        "authorId": "2284850843",
        "name": "Bohao Wang"
      },
      {
        "authorId": "2299184777",
        "name": "Feng Liu"
      },
      {
        "authorId": "2265720058",
        "name": "Jiawei Chen"
      },
      {
        "authorId": "2316164827",
        "name": "Yudi Wu"
      },
      {
        "authorId": "2293301766",
        "name": "Xingyu Lou"
      },
      {
        "authorId": "2300016985",
        "name": "Jun Wang"
      },
      {
        "authorId": "1692947908",
        "name": "Yan Feng"
      },
      {
        "authorId": "2109525713",
        "name": "Chun Chen"
      },
      {
        "authorId": "2279089364",
        "name": "Can Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "6514abaf6bdd1c31ac5dc4ad40760ff8422c6a4e",
    "url": "https://www.semanticscholar.org/paper/6514abaf6bdd1c31ac5dc4ad40760ff8422c6a4e",
    "title": "A Study on Training and Developing Large Language Models for Behavior Tree Generation",
    "abstract": "This paper presents an innovative exploration of the application potential of large language models (LLM) in addressing the challenging task of automatically generating behavior trees (BTs) for complex tasks. The conventional manual BT generation method is inefficient and heavily reliant on domain expertise. On the other hand, existing automatic BT generation technologies encounter bottlenecks related to task complexity, model adaptability, and reliability. In order to overcome these challenges, we propose a novel methodology that leverages the robust representation and reasoning abilities of LLMs. The core contribution of this paper lies in the design of a BT generation framework based on LLM, which encompasses the entire process, from data synthesis and model training to application developing and data verification. Synthetic data is introduced to train the BT generation model (BTGen model), enhancing its understanding and adaptability to various complex tasks, thereby significantly improving its overall performance. In order to ensure the effectiveness and executability of the generated BTs, we emphasize the importance of data verification and introduce a multilevel verification strategy. Additionally, we explore a range of agent design and development schemes with LLM as the central element. We hope that the work in this paper may provide a reference for the researchers who are interested in BT generation based on LLMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-16",
    "authors": [
      {
        "authorId": "2268097533",
        "name": "Fu Li"
      },
      {
        "authorId": "2275752398",
        "name": "Xueying Wang"
      },
      {
        "authorId": "2310332703",
        "name": "Bin Li"
      },
      {
        "authorId": "2201666820",
        "name": "Yunlong Wu"
      },
      {
        "authorId": "2163305789",
        "name": "Yanzhen Wang"
      },
      {
        "authorId": "2279554946",
        "name": "Xiaodong Yi"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "5f8b4e2e8c337447bfbcf47044af4a1d5f75f41e",
    "url": "https://www.semanticscholar.org/paper/5f8b4e2e8c337447bfbcf47044af4a1d5f75f41e",
    "title": "HYSYNTH: Context-Free LLM Approximation for Guiding Program Synthesis",
    "abstract": "Many structured prediction and reasoning tasks can be framed as program synthesis problems, where the goal is to generate a program in a domain-specific language (DSL) that transforms input data into the desired output. Unfortunately, purely neural approaches, such as large language models (LLMs), often fail to produce fully correct programs in unfamiliar DSLs, while purely symbolic methods based on combinatorial search scale poorly to complex problems. Motivated by these limitations, we introduce a hybrid approach, where LLM completions for a given task are used to learn a task-specific, context-free surrogate model, which is then used to guide program synthesis. We evaluate this hybrid approach on three domains, and show that it outperforms both unguided search and direct sampling from LLMs, as well as existing program synthesizers.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-24",
    "authors": [
      {
        "authorId": "115508430",
        "name": "Shraddha Barke"
      },
      {
        "authorId": "2303438153",
        "name": "Emmanuel Anaya Gonzalez"
      },
      {
        "authorId": "73097539",
        "name": "Saketh Ram Kasibatla"
      },
      {
        "authorId": "2303399361",
        "name": "Taylor Berg-Kirkpatrick"
      },
      {
        "authorId": "2258443511",
        "name": "Nadia Polikarpova"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "2902b725f9d41fbcb5cf15228b0cd88fb92be043",
    "url": "https://www.semanticscholar.org/paper/2902b725f9d41fbcb5cf15228b0cd88fb92be043",
    "title": "Tool-Planner: Dynamic Solution Tree Planning for Large Language Model with Tool Clustering",
    "abstract": "Large language models (LLMs) have demonstrated exceptional reasoning capabilities, enabling them to solve various complex problems. Recently, this ability has been applied to the paradigm of tool learning. Tool learning involves providing examples of tool usage and their corresponding functions, allowing LLMs to formulate plans and demonstrate the process of invoking and executing each tool. LLMs can address tasks that they cannot complete independently, thereby enhancing their potential across different tasks. However, this approach faces two key challenges. First, redundant error correction leads to unstable planning and long execution time. Additionally, designing a correct plan among multiple tools is also a challenge in tool learning. To address these issues, we propose Tool-Planner, a task-processing framework based on toolkits. Tool-Planner groups tools based on the API functions with the same function into a toolkit and allows LLMs to implement planning across the various toolkits. When a tool error occurs, the language model can reselect and adjust tools based on the toolkit. Experiments show that our approach demonstrates a high pass and win rate across different datasets and optimizes the planning scheme for tool learning in models such as GPT-4 and Claude 3, showcasing the potential of our method. Our code is public at https://github.com/OceannTwT/Tool-Planner .",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": null,
        "name": "Yanming Liu"
      },
      {
        "authorId": "2291082069",
        "name": "Xinyue Peng"
      },
      {
        "authorId": "2304977267",
        "name": "Yuwei Zhang"
      },
      {
        "authorId": "2291056022",
        "name": "Jiannan Cao"
      },
      {
        "authorId": "2290975291",
        "name": "Xuhong Zhang"
      },
      {
        "authorId": "2305585479",
        "name": "Sheng Cheng"
      },
      {
        "authorId": "2305030536",
        "name": "Xun Wang"
      },
      {
        "authorId": "2291142301",
        "name": "Jianwei Yin"
      },
      {
        "authorId": "2290912812",
        "name": "Tianyu Du"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "2bd09677fc5d39d8ace60a704b80ecffdb2a0bfe",
    "url": "https://www.semanticscholar.org/paper/2bd09677fc5d39d8ace60a704b80ecffdb2a0bfe",
    "title": "Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models",
    "abstract": "Recent work has showcased the powerful capability of large language models (LLMs) in recalling knowledge and reasoning. However, the reliability of LLMs in combining these two capabilities into reasoning through multi-hop facts has not been widely explored. This paper systematically investigates the possibilities for LLMs to utilize shortcuts based on direct connections between the initial and terminal entities of multi-hop knowledge. We first explore the existence of factual shortcuts through Knowledge Neurons, revealing that: (i) the strength of factual shortcuts is highly correlated with the frequency of co-occurrence of initial and terminal entities in the pre-training corpora; (ii) few-shot prompting leverage more shortcuts in answering multi-hop questions compared to chain-of-thought prompting. Then, we analyze the risks posed by factual shortcuts from the perspective of multi-hop knowledge editing. Analysis shows that approximately 20% of the failures are attributed to shortcuts, and the initial and terminal entities in these failure instances usually have higher co-occurrences in the pre-training corpus. Finally, we propose erasing shortcut neurons to mitigate the associated risks and find that this approach significantly reduces failures in multiple-hop knowledge editing caused by shortcuts.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-19",
    "authors": [
      {
        "authorId": "2283307652",
        "name": "Tianjie Ju"
      },
      {
        "authorId": "2284824305",
        "name": "Yijin Chen"
      },
      {
        "authorId": "2284703550",
        "name": "Xinwei Yuan"
      },
      {
        "authorId": "2284695096",
        "name": "Zhuosheng Zhang"
      },
      {
        "authorId": "2239100461",
        "name": "Wei Du"
      },
      {
        "authorId": "2159708641",
        "name": "Yubin Zheng"
      },
      {
        "authorId": "2267384727",
        "name": "Gongshen Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "26e48be4bdb56554c3d71c83076512fd23fef575",
    "url": "https://www.semanticscholar.org/paper/26e48be4bdb56554c3d71c83076512fd23fef575",
    "title": "Do Large Language Models Solve ARC Visual Analogies Like People Do?",
    "abstract": "The Abstraction Reasoning Corpus (ARC) is a visual analogical reasoning test designed for humans and machines (Chollet, 2019). We compared human and large language model (LLM) performance on a new child-friendly set of ARC items. Results show that both children and adults outperform most LLMs on these tasks. Error analysis revealed a similar\"fallback\"solution strategy in LLMs and young children, where part of the analogy is simply copied. In addition, we found two other error types, one based on seemingly grasping key concepts (e.g., Inside-Outside) and the other based on simple combinations of analogy input matrices. On the whole,\"concept\"errors were more common in humans, and\"matrix\"errors were more common in LLMs. This study sheds new light on LLM reasoning ability and the extent to which we can use error analyses and comparisons with human development to understand how LLMs solve visual analogies.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-13",
    "authors": [
      {
        "authorId": "2291964701",
        "name": "Gustaw Opielka"
      },
      {
        "authorId": "116421788",
        "name": "Hannes Rosenbusch"
      },
      {
        "authorId": "2291962490",
        "name": "Veerle Vijverberg"
      },
      {
        "authorId": "2266468271",
        "name": "Claire E. Stevenson"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "35fb19eb94473f46ca8a0fa8e14de56f0aeb81e5",
    "url": "https://www.semanticscholar.org/paper/35fb19eb94473f46ca8a0fa8e14de56f0aeb81e5",
    "title": "Speak It Out: Solving Symbol-Related Problems with Symbol-to-Language Conversion for Language Models",
    "abstract": "Symbols (or more broadly, non-natural language textual representations) such as numerical sequences, molecular formulas, and table delimiters widely exist, playing important roles in various tasks such as abstract reasoning, chemical property prediction, and table question answering. Despite the impressive natural language comprehension capabilities of large language models (LLMs), their reasoning abilities for symbols remain inadequate, which could attributed to the difference between symbol representations and general natural languages. We propose symbol-to-language (S2L), a tuning-free method that enables large language models to solve symbol-related problems with information expressed in natural language. Specifically, S2L first converts the symbols involved to language-based representations, which can be implemented by prompting LLMs or leveraging external tools, then these language-based representations are integrated into the original problem via direct substitution or concatenation, serving as useful input information for LLMs. We evaluate the S2L method using both API-based (GPT-4, ChatGPT) and open-source (OpenChat) models over eight symbol-related tasks, ranging from symbol-only abstract reasoning to sentiment analysis in social media. Experimental results show that S2L consistently leads to superior performance. For example, by employing S2L for GPT-4, there can be average significant improvements of +21.9% and +9.5% for subtasks in 1D-ARC and Dyck language, respectively. Codes and data are available at https://github.com/THUNLP-MT/symbol2language.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-22",
    "authors": [
      {
        "authorId": "2279098701",
        "name": "Yile Wang"
      },
      {
        "authorId": "2110844331",
        "name": "Sijie Cheng"
      },
      {
        "authorId": "2280196093",
        "name": "Zixin Sun"
      },
      {
        "authorId": "144326610",
        "name": "Peng Li"
      },
      {
        "authorId": "2254850259",
        "name": "Yang Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "3ff049bd27cb41c10bdd7278ff423e74f1283615",
    "url": "https://www.semanticscholar.org/paper/3ff049bd27cb41c10bdd7278ff423e74f1283615",
    "title": "LLM-Craft: Robotic Crafting of Elasto-Plastic Objects with Large Language Models",
    "abstract": "When humans create sculptures, we are able to reason about how geometrically we need to alter the clay state to reach our target goal. We are not computing point-wise similarity metrics, or reasoning about low-level positioning of our tools, but instead determining the higher-level changes that need to be made. In this work, we propose LLM-Craft, a novel pipeline that leverages large language models (LLMs) to iteratively reason about and generate deformation-based crafting action sequences. We simplify and couple the state and action representations to further encourage shape-based reasoning. To the best of our knowledge, LLM-Craft is the first system successfully leveraging LLMs for complex deformable object interactions. Through our experiments, we demonstrate that with the LLM-Craft framework, LLMs are able to successfully reason about the deformation behavior of elasto-plastic objects. Furthermore, we find that LLM-Craft is able to successfully create a set of simple letter shapes. Finally, we explore extending the framework to reaching more ambiguous semantic goals, such as\"thinner\"or\"bumpy\". For videos please see our website: https://sites.google.com/andrew.cmu.edu/llmcraft.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-12",
    "authors": [
      {
        "authorId": "2185508465",
        "name": "Alison Bartsch"
      },
      {
        "authorId": "3614493",
        "name": "A. Farimani"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "886940b402e326a84687fd30925d2de66147566f",
    "url": "https://www.semanticscholar.org/paper/886940b402e326a84687fd30925d2de66147566f",
    "title": "On the Brittle Foundations of ReAct Prompting for Agentic Large Language Models",
    "abstract": "The reasoning abilities of Large Language Models (LLMs) remain a topic of debate. Some methods such as ReAct-based prompting, have gained popularity for claiming to enhance sequential decision-making abilities of agentic LLMs. However, it is unclear what is the source of improvement in LLM reasoning with ReAct based prompting. In this paper we examine these claims of ReAct based prompting in improving agentic LLMs for sequential decision-making. By introducing systematic variations to the input prompt we perform a sensitivity analysis along the claims of ReAct and find that the performance is minimally influenced by the\"interleaving reasoning trace with action execution\"or the content of the generated reasoning traces in ReAct, contrary to original claims and common usage. Instead, the performance of LLMs is driven by the similarity between input example tasks and queries, implicitly forcing the prompt designer to provide instance-specific examples which significantly increases the cognitive burden on the human. Our investigation shows that the perceived reasoning abilities of LLMs stem from the exemplar-query similarity and approximate retrieval rather than any inherent reasoning abilities.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-22",
    "authors": [
      {
        "authorId": "151500192",
        "name": "Mudit Verma"
      },
      {
        "authorId": "46211062",
        "name": "Siddhant Bhambri"
      },
      {
        "authorId": "2047340230",
        "name": "Subbarao Kambhampati"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "a4362c3656aa892bd39e7100c1f0b89536709eb6",
    "url": "https://www.semanticscholar.org/paper/a4362c3656aa892bd39e7100c1f0b89536709eb6",
    "title": "ThinkRepair: Self-Directed Automated Program Repair",
    "abstract": "Though many approaches have been proposed for Automated Program Repair (APR) and indeed achieved remarkable performance, they still have limitations in fixing bugs that require analyzing and reasoning about the logic of the buggy program. Recently, large language models (LLMs) instructed by prompt engineering have attracted much attention for their powerful ability to address many kinds of tasks including bug-fixing. However, the quality of the prompt will highly affect the ability of LLMs and manually constructing high-quality prompts is a costly endeavor. To address this limitation, we propose a self-directed LLM-based automated program repair, ThinkRepair, with two main phases: collection phase and fixing phase. The former phase automatically collects various chains of thoughts that constitute pre-fixed knowledge by instructing LLMs with the Chain-of-Thought (CoT) prompt. The latter phase targets fixing a bug by first selecting examples for few-shot learning and second automatically interacting with LLMs, optionally appending with feedback of testing information. Evaluations on two widely studied datasets (Defects4J and QuixBugs) by comparing ThinkRepair with 12 SOTA APRs indicate the priority of ThinkRepair in fixing bugs. Notably, ThinkRepair fixes 98 bugs and improves baselines by 27%-344.4% on Defects4J V1.2. On Defects4J V2.0, ThinkRepair fixes 12-65 more bugs than the SOTA APRs. Additionally, ThinkRepair also makes a considerable improvement on QuixBugs (31 for Java and 21 for Python at most).",
    "venue": "International Symposium on Software Testing and Analysis",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-30",
    "authors": [
      {
        "authorId": "2202556931",
        "name": "Xin Yin"
      },
      {
        "authorId": "2294574801",
        "name": "Chao Ni"
      },
      {
        "authorId": "2298901331",
        "name": "Shaohua Wang"
      },
      {
        "authorId": "2313866698",
        "name": "Zhenhao Li"
      },
      {
        "authorId": "2313886920",
        "name": "Limin Zeng"
      },
      {
        "authorId": "2265717851",
        "name": "Xiaohu Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "59395cf4f9346ef4ccb37499a3a7e52c2978fc61",
    "url": "https://www.semanticscholar.org/paper/59395cf4f9346ef4ccb37499a3a7e52c2978fc61",
    "title": "Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering",
    "abstract": "Knowledge Graph Question Answering (KGQA) methods seek to answer Natural Language questions using the relational information stored in Knowledge Graphs (KGs). With the recent advancements of Large Language Models (LLMs) and their remarkable reasoning abilities, there is a growing trend to leverage them for KGQA. However, existing methodologies have only focused on answering factual questions, e.g., *“In which city was Silvio Berlusconi’s first wife born?”*, leaving questions involving commonsense reasoning that real-world users may pose more often, e.g., *“Do I need separate visas to see the Venus of Willendorf and attend the Olympics this summer?”* unaddressed. In this work, we first observe that existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especially since their reasoning processes are not easily verifiable. In response, we propose Right for Right Reasons (R^3), a commonsense KGQA methodology that allows for a verifiable reasoning procedure by axiomatically surfacing intrinsic commonsense knowledge of LLMs and grounding every factual reasoning step on KG triples. Through experimental evaluations across three different tasks—question answering, claim verification, and preference matching—our findings showcase R^3 as a superior approach, outperforming existing methodologies and notably reducing instances of hallucination and reasoning errors.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-03",
    "authors": [
      {
        "authorId": "1646622849",
        "name": "Armin Toroghi"
      },
      {
        "authorId": "2290061685",
        "name": "Willis Guo"
      },
      {
        "authorId": "104706163",
        "name": "Mohammad Mahdi Torabi pour"
      },
      {
        "authorId": "2273670268",
        "name": "Scott Sanner"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "aaa631bf0b8c2ec1ed09835e9bfe1e13c2b7684d",
    "url": "https://www.semanticscholar.org/paper/aaa631bf0b8c2ec1ed09835e9bfe1e13c2b7684d",
    "title": "Fine-tuning Large Language Models to Improve Accuracy and Comprehensibility of Automated Code Review",
    "abstract": "\n As code review is a tedious and costly software quality practice, researchers have proposed several machine learning-based methods to automate the process. The primary focus has been on accuracy, that is, how accurately the algorithms are able to detect issues in the code under review. However, human intervention still remains inevitable since results produced by automated code review are not 100% correct. To assist human reviewers in making their final decisions on automatically generated review comments, the comprehensibility of the comments underpinned by accurate localization and relevant explanations for the detected issues with repair suggestions is paramount. However, this has largely been neglected in the existing research. Large language models (LLMs) have the potential to generate code review comments that are more readable and comprehensible by humans thanks to their remarkable processing and reasoning capabilities. However, even mainstream LLMs perform poorly in detecting the presence of code issues because they have not been specifically trained for this binary classification task required in code review. In this paper, we contribute\n Carllm\n (Comprehensibility of Automated Code Review using Large Language Models), a novel fine-tuned LLM that has the ability to improve not only the accuracy but, more importantly, the comprehensibility of automated code review, as compared to state-of-the-art pre-trained models and general LLMs.\n",
    "venue": "ACM Transactions on Software Engineering and Methodology",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-09-14",
    "authors": [
      {
        "authorId": "2204259437",
        "name": "Yongda Yu"
      },
      {
        "authorId": "2313200",
        "name": "Guoping Rong"
      },
      {
        "authorId": "2287715989",
        "name": "Haifeng Shen"
      },
      {
        "authorId": "2214106067",
        "name": "He Zhang"
      },
      {
        "authorId": "145819712",
        "name": "Dong Shao"
      },
      {
        "authorId": "2280433378",
        "name": "Min Wang"
      },
      {
        "authorId": "2218582488",
        "name": "Zhao Wei"
      },
      {
        "authorId": "2280614332",
        "name": "Yong Xu"
      },
      {
        "authorId": "2218690375",
        "name": "Juhong Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.79441541679836
  },
  {
    "paperId": "453ca397e8403ed759f8119c42dd2d1cde422a1a",
    "url": "https://www.semanticscholar.org/paper/453ca397e8403ed759f8119c42dd2d1cde422a1a",
    "title": "SportQA: A Benchmark for Sports Understanding in Large Language Models",
    "abstract": "A deep understanding of sports, a field rich in strategic and dynamic content, is crucial for advancing Natural Language Processing (NLP). This holds particular significance in the context of evaluating and advancing Large Language Models (LLMs), given the existing gap in specialized benchmarks. To bridge this gap, we introduce SportQA, a novel benchmark specifically designed for evaluating LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenario-based reasoning tasks. We conducted a thorough evaluation of prevalent LLMs, mainly utilizing few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting. Our results reveal that while LLMs exhibit competent performance in basic sports knowledge, they struggle with more complex, scenario-based sports reasoning, lagging behind human expertise. The introduction of SportQA marks a significant step forward in NLP, offering a tool for assessing and enhancing sports understanding in LLMs. The dataset is available at https://github.com/haotianxia/SportQA",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-24",
    "authors": [
      {
        "authorId": "2275650018",
        "name": "Haotian Xia"
      },
      {
        "authorId": "2287045351",
        "name": "Zhengbang Yang"
      },
      {
        "authorId": "2108035197",
        "name": "Yuqing Wang"
      },
      {
        "authorId": "2186302083",
        "name": "Rhys Tracy"
      },
      {
        "authorId": "2287025051",
        "name": "Yun Zhao"
      },
      {
        "authorId": "2289892155",
        "name": "Dongdong Huang"
      },
      {
        "authorId": "2287135433",
        "name": "Zezhi Chen"
      },
      {
        "authorId": "2288343817",
        "name": "Yan Zhu"
      },
      {
        "authorId": "2287028049",
        "name": "Yuan-fang Wang"
      },
      {
        "authorId": "2289600385",
        "name": "Weining Shen"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "086c6252d9b0e81a541dbad3dc5ee1fbd01330b3",
    "url": "https://www.semanticscholar.org/paper/086c6252d9b0e81a541dbad3dc5ee1fbd01330b3",
    "title": "Benchmarking Large Language Models for Persian: A Preliminary Study Focusing on ChatGPT",
    "abstract": "This paper explores the efficacy of large language models (LLMs) for Persian. While ChatGPT and consequent LLMs have shown remarkable performance in English, their efficiency for more low-resource languages remains an open question. We present the first comprehensive benchmarking study of LLMs across diverse Persian language tasks. Our primary focus is on GPT-3.5-turbo, but we also include GPT-4 and OpenChat-3.5 to provide a more holistic evaluation. Our assessment encompasses a diverse set of tasks categorized into classic, reasoning, and knowledge-based domains. To enable a thorough comparison, we evaluate LLMs against existing task-specific fine-tuned models. Given the limited availability of Persian datasets for reasoning tasks, we introduce two new benchmarks: one based on elementary school math questions and another derived from the entrance exams for 7th and 10th grades. Our findings reveal that while LLMs, especially GPT-4, excel in tasks requiring reasoning abilities and a broad understanding of general knowledge, they often lag behind smaller pretrained models fine-tuned specifically for particular tasks. Additionally, we observe improved performance when test sets are translated to English before inputting them into GPT-3.5. These results highlight the significant potential for enhancing LLM performance in the Persian language. This is particularly noteworthy due to the unique attributes of Persian, including its distinct alphabet and writing styles. We have made our codes, prompts, and data available here: https://github.com/Ipouyall/Benchmarking_ChatGPT_for_Persian.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-03",
    "authors": [
      {
        "authorId": "2294720126",
        "name": "Amirhossein Abaskohi"
      },
      {
        "authorId": "2294720176",
        "name": "Sara Baruni"
      },
      {
        "authorId": "2294720598",
        "name": "Mostafa Masoudi"
      },
      {
        "authorId": "2294719655",
        "name": "Nesa Abbasi"
      },
      {
        "authorId": "2294719463",
        "name": "Mohammad Hadi Babalou"
      },
      {
        "authorId": "117005890",
        "name": "Ali Edalat"
      },
      {
        "authorId": "2294720585",
        "name": "Sepehr Kamahi"
      },
      {
        "authorId": "2294720038",
        "name": "Samin Mahdizadeh Sani"
      },
      {
        "authorId": "2294719553",
        "name": "Nikoo Naghavian"
      },
      {
        "authorId": "2294720173",
        "name": "Danial Namazifard"
      },
      {
        "authorId": "2294720666",
        "name": "Pouya Sadeghi"
      },
      {
        "authorId": "3261470",
        "name": "Yadollah Yaghoobzadeh"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "85391ba692f1962f61c79f58d68f13229f8a8f51",
    "url": "https://www.semanticscholar.org/paper/85391ba692f1962f61c79f58d68f13229f8a8f51",
    "title": "ALCM: Autonomous LLM-Augmented Causal Discovery Framework",
    "abstract": "To perform effective causal inference in high-dimensional datasets, initiating the process with causal discovery is imperative, wherein a causal graph is generated based on observational data. However, obtaining a complete and accurate causal graph poses a formidable challenge, recognized as an NP-hard problem. Recently, the advent of Large Language Models (LLMs) has ushered in a new era, indicating their emergent capabilities and widespread applicability in facilitating causal reasoning across diverse domains, such as medicine, finance, and science. The expansive knowledge base of LLMs holds the potential to elevate the field of causal reasoning by offering interpretability, making inferences, generalizability, and uncovering novel causal structures. In this paper, we introduce a new framework, named Autonomous LLM-Augmented Causal Discovery Framework (ALCM), to synergize data-driven causal discovery algorithms and LLMs, automating the generation of a more resilient, accurate, and explicable causal graph. The ALCM consists of three integral components: causal structure learning, causal wrapper, and LLM-driven causal refiner. These components autonomously collaborate within a dynamic environment to address causal discovery questions and deliver plausible causal graphs. We evaluate the ALCM framework by implementing two demonstrations on seven well-known datasets. Experimental results demonstrate that ALCM outperforms existing LLM methods and conventional data-driven causal reasoning mechanisms. This study not only shows the effectiveness of the ALCM but also underscores new research directions in leveraging the causal reasoning capabilities of LLMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-02",
    "authors": [
      {
        "authorId": "3094737",
        "name": "Elahe Khatibi"
      },
      {
        "authorId": "2182148069",
        "name": "Mahyar Abbasian"
      },
      {
        "authorId": "2268181174",
        "name": "Zhongqi Yang"
      },
      {
        "authorId": "2241201441",
        "name": "Iman Azimi"
      },
      {
        "authorId": "2224892821",
        "name": "Amir M. Rahmani"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "38ad7b52fd9c5b7bf81c78fd54ee0de1113ce092",
    "url": "https://www.semanticscholar.org/paper/38ad7b52fd9c5b7bf81c78fd54ee0de1113ce092",
    "title": "MetaReflection: Learning Instructions for Language Agents using Past Reflections",
    "abstract": "The popularity of Large Language Models (LLMs) have unleashed a new age of Language Agents for solving a diverse range of tasks. While contemporary frontier LLMs are capable enough to power reasonably good Language agents, the closed-API model makes it hard to improve in cases they perform sub-optimally. To address this, recent works have explored techniques to improve their performance using self reflection and prompt optimization techniques. While techniques like self reflection work well in an online setup, contemporary prompt optimization techniques are designed to work on simpler tasks. To address this, we introduce METAREFLECTION, a novel offline reinforcement learning technique that enhances the performance of Language Agents by augmenting a semantic memory based on experiential learnings from past trials. We demonstrate the efficacy of METAREFLECTION by evaluating across multiple domains, including complex logical reasoning, biomedical semantic similarity, open world question answering, and vulnerability threat detection, in Infrastructure-as-Code, with different agent design. METAREFLECTION boosts Language agents’ performance by 4 % to 16.82 % over the raw GPT-4 baseline and performs on par with existing state-of-the-art prompt optimization techniques while requiring fewer LLM calls.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-13",
    "authors": [
      {
        "authorId": "2119999692",
        "name": "Priyanshu Gupta"
      },
      {
        "authorId": "2218802680",
        "name": "Shashank Kirtania"
      },
      {
        "authorId": "2258715404",
        "name": "Ananya Singha"
      },
      {
        "authorId": "2108314",
        "name": "Sumit Gulwani"
      },
      {
        "authorId": "93750354",
        "name": "Arjun Radhakrishna"
      },
      {
        "authorId": "2303450096",
        "name": "Sherry Shi"
      },
      {
        "authorId": "2243334200",
        "name": "Gustavo Soares"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "3e58cc351a18ec11d2cda60941b87461e27b45f9",
    "url": "https://www.semanticscholar.org/paper/3e58cc351a18ec11d2cda60941b87461e27b45f9",
    "title": "LangBirds: An Agent for Angry Birds using a Large Language Model",
    "abstract": "The game Angry Birds is a challenging problem for artificial intelligence in that it requires physical reasoning ability. Previous approaches require domain knowledge or playing data, or have limitations in generalization performance. Inspired by human approach to physics-based puzzle games, we devise a new Angry Birds agent that separates AI’s thought into two stages. To this end, our method, LangBirds, uses Large Language Models (LLMs) that are recently considered to show human-level performance. We compared LangBirds to several reinforcement learning agents as well as heuristic agents. The results on the Phy-Q benchmark, which is a testbed based on Angry Birds, showed that our approach outperforms baselines. Moreover, the proposed approach allows us to understand the decision-making process since it uses natural language. Qualitative assessments indicated that the rationale for the decisions was reasonable.",
    "venue": "2024 IEEE Conference on Games (CoG)",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-08-05",
    "authors": [
      {
        "authorId": "2221144294",
        "name": "Seungwon Oh"
      },
      {
        "authorId": "2223773578",
        "name": "Insik Chung"
      },
      {
        "authorId": "2307127399",
        "name": "Kyung-Joong Kim"
      }
    ],
    "source": "semantic_scholar",
    "score": 92.47918433002164
  },
  {
    "paperId": "bd06f9d931d980117edebf8163298b88ac36e8ea",
    "url": "https://www.semanticscholar.org/paper/bd06f9d931d980117edebf8163298b88ac36e8ea",
    "title": "Do Large Language Models Understand Logic or Just Mimick Context?",
    "abstract": "Over the past few years, the abilities of large language models (LLMs) have received extensive attention, which have performed exceptionally well in complicated scenarios such as logical reasoning and symbolic inference. A significant factor contributing to this progress is the benefit of in-context learning and few-shot prompting. However, the reasons behind the success of such models using contextual reasoning have not been fully explored. Do LLMs have understand logical rules to draw inferences, or do they ``guess'' the answers by learning a type of probabilistic mapping through context? This paper investigates the reasoning capabilities of LLMs on two logical reasoning datasets by using counterfactual methods to replace context text and modify logical concepts. Based on our analysis, it is found that LLMs do not truly understand logical rules; rather, in-context learning has simply enhanced the likelihood of these models arriving at the correct answers. If one alters certain words in the context text or changes the concepts of logical terms, the outputs of LLMs can be significantly disrupted, leading to counter-intuitive responses. This work provides critical insights into the limitations of LLMs, underscoring the need for more robust mechanisms to ensure reliable logical reasoning in LLMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-19",
    "authors": [
      {
        "authorId": "2243406637",
        "name": "Junbing Yan"
      },
      {
        "authorId": "121899912",
        "name": "Chengyu Wang"
      },
      {
        "authorId": "2272790856",
        "name": "Junyuan Huang"
      },
      {
        "authorId": "2266570242",
        "name": "Wei Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "a558b5d94a387ba3bb3996307d2dd3c08dc4f708",
    "url": "https://www.semanticscholar.org/paper/a558b5d94a387ba3bb3996307d2dd3c08dc4f708",
    "title": "Prompt Engineering ChatGPT for Codenames",
    "abstract": "The word association game Codenames challenges the AI community with its requirements for multimodal language understanding, theory of mind, and epistemic reasoning. Previous attempts to develop AI agents for the game have focused on word embedding techniques, which while good with other models using the same technique, can sometimes suffer from brittle performance when paired with other models. Recently, Large Language Models (LLMs) have demonstrated enhanced capabilities, excelling in complex cognitive tasks, including symbolic and common sense reasoning. In this paper, we compare a range of recent prompt engineering techniques for GPT-based Codenames agents. While there was no significant game score improvement over the baseline agent, we did observe qualitative changes in agents’ strategies suggesting that further refinement has potential for score improvement. We also propose a revised Codenames AI competition specifically focusing on the use of LLM agents.",
    "venue": "2024 IEEE Conference on Games (CoG)",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-08-05",
    "authors": [
      {
        "authorId": "2214759756",
        "name": "Matthew Sidji"
      },
      {
        "authorId": "2317108866",
        "name": "Matthew Stephenson"
      }
    ],
    "source": "semantic_scholar",
    "score": 92.47918433002164
  },
  {
    "paperId": "e1c1e7c6fa67bde417c627f62ba54030119a1f82",
    "url": "https://www.semanticscholar.org/paper/e1c1e7c6fa67bde417c627f62ba54030119a1f82",
    "title": "Converging Paradigms: The Synergy of Symbolic and Connectionist AI in LLM-Empowered Autonomous Agents",
    "abstract": "This article explores the convergence of connectionist and symbolic artificial intelligence (AI), from historical debates to contemporary advancements. Traditionally considered distinct paradigms, connectionist AI focuses on neural networks, while symbolic AI emphasizes symbolic representation and logic. Recent advancements in large language models (LLMs), exemplified by ChatGPT and GPT-4, highlight the potential of connectionist architectures in handling human language as a form of symbols. The study argues that LLM-empowered Autonomous Agents (LAAs) embody this paradigm convergence. By utilizing LLMs for text-based knowledge modeling and representation, LAAs integrate neuro-symbolic AI principles, showcasing enhanced reasoning and decision-making capabilities. Comparing LAAs with Knowledge Graphs within the neuro-symbolic AI theme highlights the unique strengths of LAAs in mimicking human-like reasoning processes, scaling effectively with large datasets, and leveraging in-context samples without explicit re-training. The research underscores promising avenues in neuro-vector-symbolic integration, instructional encoding, and implicit reasoning, aimed at further enhancing LAA capabilities. By exploring the progression of neuro-symbolic AI and proposing future research trajectories, this work advances the understanding and development of AI technologies.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-11",
    "authors": [
      {
        "authorId": "2293483401",
        "name": "Haoyi Xiong"
      },
      {
        "authorId": "2310753409",
        "name": "Zhiyuan Wang"
      },
      {
        "authorId": "2293765581",
        "name": "Xuhong Li"
      },
      {
        "authorId": "2143957850",
        "name": "Jiang Bian"
      },
      {
        "authorId": "2311626289",
        "name": "Zeke Xie"
      },
      {
        "authorId": "2310697115",
        "name": "Shahid Mumtaz"
      },
      {
        "authorId": "2268975215",
        "name": "Laura E. Barnes"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "43603ea2fd5c2c4f72209dc8443e8123f516223e",
    "url": "https://www.semanticscholar.org/paper/43603ea2fd5c2c4f72209dc8443e8123f516223e",
    "title": "LOGIC-LM++: Multi-Step Refinement for Symbolic Formulations",
    "abstract": "In this paper we examine the limitations of Large Language Models (LLMs) for complex reasoning tasks. While current approaches leverage formal languages as intermediate representation for these reasoning problems, they still struggle with generating intermediate for-mal specifications with great correctness and in refining these representations. To address these issues, this paper proposes Logic-LM++, an improvement on Logic-LM (Pan et al., 2023). It uses the ability of LLMs to do pairwise comparisons, allowing the evaluation of the refinements suggested by the LLM. The paper demonstrates that Logic-LM++ outperforms Logic-LM and LLM based techniques on natural language reasoning tasks on two datasets, FOLIO, ProofWriter and AR-LSAT. Logic-LM++ show an average improvement of 18.5% on standard prompting, 12.3% on chain of thought prompting and 5% on Logic-LM.",
    "venue": "NLRSE",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-22",
    "authors": [
      {
        "authorId": "2218802680",
        "name": "Shashank Kirtania"
      },
      {
        "authorId": "2309799561",
        "name": "Priyanshu Gupta"
      },
      {
        "authorId": "2315327214",
        "name": "Arjun Radhakirshna"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "7a4969b40e3ba26dcf426e5e68e3287e52eff8d0",
    "url": "https://www.semanticscholar.org/paper/7a4969b40e3ba26dcf426e5e68e3287e52eff8d0",
    "title": "Autonomous Behavior Planning For Humanoid Loco-manipulation Through Grounded Language Model",
    "abstract": "Enabling humanoid robots to perform autonomously loco-manipulation in unstructured environments is crucial and highly challenging for achieving embodied intelligence. This involves robots being able to plan their actions and behaviors in long-horizon tasks while using multi-modality to perceive deviations between task execution and high-level planning. Recently, large language models (LLMs) have demonstrated powerful planning and reasoning capabilities for comprehension and processing of semantic information through robot control tasks, as well as the usability of analytical judgment and decision-making for multi-modal inputs. To leverage the power of LLMs towards humanoid loco-manipulation, we propose a novel language-model based framework that enables robots to autonomously plan behaviors and low-level execution under given textual instructions, while observing and correcting failures that may occur during task execution. To systematically evaluate this framework in grounding LLMs, we created the robot ’action’ and ’sensing’ behavior library for task planning, and conducted mobile manipulation tasks and experiments in both simulated and real environments using the CENTAURO robot, and verified the effectiveness and application of this approach in robotic tasks with autonomous behavioral planning. Video: https://youtu.be/mmnaxthEX34",
    "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-08-15",
    "authors": [
      {
        "authorId": "2307932685",
        "name": "Jin Wang"
      },
      {
        "authorId": "20815291",
        "name": "Arturo Laurenzi"
      },
      {
        "authorId": "145887349",
        "name": "N. Tsagarakis"
      }
    ],
    "source": "semantic_scholar",
    "score": 92.47918433002164
  },
  {
    "paperId": "f587e7053d31b6962db5d3d587c073968f84893c",
    "url": "https://www.semanticscholar.org/paper/f587e7053d31b6962db5d3d587c073968f84893c",
    "title": "SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Models",
    "abstract": "Integrating the 3D world into large language models (3D-based LLMs) has been a promising research direction for 3D scene understanding. However, current 3D-based LLMs fall short in situated understanding due to two key limitations: 1) existing 3D datasets are constructed from a global perspective of the 3D scenes and lack situated context. 2) the architectures of existing 3D-based LLMs lack explicit alignment between the spatial representations of 3D scenes and natural language, limiting their performance in tasks requiring precise spatial reasoning. We address these issues by introducing a scalable situated 3D dataset, named Spartun3D, that incorporates various situated spatial reasoning tasks. Furthermore, we propose Spartun3D-LLM, built on an existing 3D-based LLM but integrated with a novel situated spatial alignment module, aiming to enhance the alignment between 3D visual representations and their corresponding textual descriptions. Experimental results demonstrate that both our proposed dataset and alignment module significantly enhance the situated spatial understanding of 3D-based LLMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-04",
    "authors": [
      {
        "authorId": "2280043021",
        "name": "Yue Zhang"
      },
      {
        "authorId": "2136442661",
        "name": "Zhiyang Xu"
      },
      {
        "authorId": "2218172912",
        "name": "Ying Shen"
      },
      {
        "authorId": "2190934",
        "name": "Parisa Kordjamshidi"
      },
      {
        "authorId": "2238885968",
        "name": "Lifu Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "d487db04e4146a4fb02a2c3318042c846b62f2d3",
    "url": "https://www.semanticscholar.org/paper/d487db04e4146a4fb02a2c3318042c846b62f2d3",
    "title": "PECC: Problem Extraction and Coding Challenges",
    "abstract": "Recent advancements in large language models (LLMs) have showcased their exceptional abilities across various tasks, such as code generation, problem-solving and reasoning. Existing benchmarks evaluate tasks in isolation, yet the extent to which LLMs can understand prose-style tasks, identify the underlying problems, and then generate appropriate code solutions is still unexplored. Addressing this gap, we introduce PECC, a novel benchmark derived from Advent Of Code (AoC) challenges and Project Euler, including 2396 problems. Unlike conventional benchmarks, PECC requires LLMs to interpret narrative-embedded problems, extract requirements, and generate executable code. A key feature of our dataset is the complexity added by natural language prompting in chat-based evaluations, mirroring real-world instruction ambiguities. Results show varying model performance between narrative and neutral problems, with specific challenges in the Euler math-based subset with GPT-3.5-Turbo passing 50% of the AoC challenges and only 8% on the Euler problems. By probing the limits of LLMs’ capabilities, our benchmark provides a framework to monitor and assess the subsequent progress of LLMs as a universal problem solver.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-29",
    "authors": [
      {
        "authorId": "2298902857",
        "name": "Patrick Haller"
      },
      {
        "authorId": "144983077",
        "name": "Jonas Golde"
      },
      {
        "authorId": "2273785959",
        "name": "Alan Akbik"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "097dc73d5d422b3c09286e72d16b2561ae5fb395",
    "url": "https://www.semanticscholar.org/paper/097dc73d5d422b3c09286e72d16b2561ae5fb395",
    "title": "Complementary Explanations for Effective In-Context Learning",
    "abstract": "Large language models (LLMs) have exhibited remarkable capabilities in learning from explanations in prompts, but there has been limited understanding of exactly how these explanations function or why they are effective. This work aims to better understand the mechanisms by which explanations are used for in-context learning. We first study the impact of two different factors on the performance of prompts with explanations: the computation trace (the way the solution is decomposed) and the natural language used to express the prompt. By perturbing explanations on three controlled tasks, we show that both factors contribute to the effectiveness of explanations. We further study how to form maximally effective sets of explanations for solving a given test query. We find that LLMs can benefit from the complementarity of the explanation set: diverse reasoning skills shown by different exemplars can lead to better performance. Therefore, we propose a maximal marginal relevance-based exemplar selection approach for constructing exemplar sets that are both relevant as well as complementary, which successfully improves the in-context learning performance across three real-world tasks on multiple LLMs.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "citationCount": 75,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2211.13892",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-11-25",
    "authors": [
      {
        "authorId": "50183897",
        "name": "Xi Ye"
      },
      {
        "authorId": "1900163",
        "name": "Srini Iyer"
      },
      {
        "authorId": "1709797",
        "name": "Asli Celikyilmaz"
      },
      {
        "authorId": "1389924486",
        "name": "Ves Stoyanov"
      },
      {
        "authorId": "1814094",
        "name": "Greg Durrett"
      },
      {
        "authorId": "10721120",
        "name": "Ramakanth Pasunuru"
      }
    ],
    "source": "semantic_scholar",
    "score": 134.96100010429495
  },
  {
    "paperId": "43f0b39bc787311b22e0a8659780743cb54a67be",
    "url": "https://www.semanticscholar.org/paper/43f0b39bc787311b22e0a8659780743cb54a67be",
    "title": "MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL",
    "abstract": "Recent LLM-based Text-to-SQL methods usually suffer from significant performance degradation on\"huge\"databases and complex user questions that require multi-step reasoning. Moreover, most existing methods neglect the crucial significance of LLMs utilizing external tools and model collaboration. To address these challenges, we introduce MAC-SQL, a novel LLM-based multi-agent collaborative framework. Our framework comprises a core decomposer agent for Text-to-SQL generation with few-shot chain-of-thought reasoning, accompanied by two auxiliary agents that utilize external tools or models to acquire smaller sub-databases and refine erroneous SQL queries. The decomposer agent collaborates with auxiliary agents, which are activated as needed and can be expanded to accommodate new features or tools for effective Text-to-SQL parsing. In our framework, We initially leverage GPT-4 as the strong backbone LLM for all agent tasks to determine the upper bound of our framework. We then fine-tune an open-sourced instruction-followed model, SQL-Llama, by leveraging Code Llama 7B, to accomplish all tasks as GPT-4 does. Experiments show that SQL-Llama achieves a comparable execution accuracy of 43.94, compared to the baseline accuracy of 46.35 for vanilla GPT-4. At the time of writing, MAC-SQL+GPT-4 achieves an execution accuracy of 59.59 when evaluated on the BIRD benchmark, establishing a new state-of-the-art (SOTA) on its holdout test set (https://github.com/wbbeyourself/MAC-SQL).",
    "venue": "International Conference on Computational Linguistics",
    "year": 2023,
    "citationCount": 21,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-18",
    "authors": [
      {
        "authorId": "2275240628",
        "name": "Bing Wang"
      },
      {
        "authorId": "2275114809",
        "name": "Changyu Ren"
      },
      {
        "authorId": "2276103971",
        "name": "Jian Yang"
      },
      {
        "authorId": "120436437",
        "name": "Xinnian Liang"
      },
      {
        "authorId": "2107018151",
        "name": "Jiaqi Bai"
      },
      {
        "authorId": "2275286357",
        "name": "Qian-Wen Zhang"
      },
      {
        "authorId": "2119231778",
        "name": "Zhao Yan"
      },
      {
        "authorId": "2258837278",
        "name": "Zhoujun Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 116.36563680037474
  },
  {
    "paperId": "0aa150619e07fa41492517368beaaf8ae56fe061",
    "url": "https://www.semanticscholar.org/paper/0aa150619e07fa41492517368beaaf8ae56fe061",
    "title": "Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities",
    "abstract": "Human interactions are deeply rooted in the interplay of thoughts, beliefs, and desires made possible by Theory of Mind (ToM): our cognitive ability to understand the mental states of ourselves and others. Although ToM may come naturally to us, emulating it presents a challenge to even the most advanced Large Language Models (LLMs). Recent improvements to LLMs' reasoning capabilities from simple yet effective prompting techniques such as Chain-of-Thought have seen limited applicability to ToM. In this paper, we turn to the prominent cognitive science theory\"Simulation Theory\"to bridge this gap. We introduce SimToM, a novel two-stage prompting framework inspired by Simulation Theory's notion of perspective-taking. To implement this idea on current ToM benchmarks, SimToM first filters context based on what the character in question knows before answering a question about their mental state. Our approach, which requires no additional training and minimal prompt-tuning, shows substantial improvement over existing methods, and our analysis reveals the importance of perspective-taking to Theory-of-Mind capabilities. Our findings suggest perspective-taking as a promising direction for future research into improving LLMs' ToM capabilities.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 20,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-16",
    "authors": [
      {
        "authorId": "2000786644",
        "name": "Alex Wilf"
      },
      {
        "authorId": "2267273567",
        "name": "Sihyun Shawn Lee"
      },
      {
        "authorId": "28130078",
        "name": "P. Liang"
      },
      {
        "authorId": "49933077",
        "name": "Louis-Philippe Morency"
      }
    ],
    "source": "semantic_scholar",
    "score": 115.66783656585135
  },
  {
    "paperId": "d53f4cdddd7494d5f6e64d81f627691f6d7dff95",
    "url": "https://www.semanticscholar.org/paper/d53f4cdddd7494d5f6e64d81f627691f6d7dff95",
    "title": "Think Outside the Code: Brainstorming Boosts Large Language Models in Code Generation",
    "abstract": "Code generation aims to automatically generate source code from high-level task specifications, which can significantly increase productivity of software engineering. Recently, approaches based on large language models (LLMs) have shown remarkable code generation abilities on simple tasks. However, generate code for more complex tasks, such as competition-level problems, remains challenging. In this paper, we introduce Brainstorm framework for code generation. It leverages a brainstorming step that generates and selects diverse thoughts on the problem to facilitate algorithmic reasoning, where the thoughts are possible blueprint of solving the problem. We demonstrate that Brainstorm significantly enhances the ability of LLMs to solve competition-level programming problems, resulting in a more than 50% increase in the pass@$k$ metrics for ChatGPT on the CodeContests benchmark, achieving state-of-the-art performance. Furthermore, our experiments conducted on LeetCode contests show that our framework boosts the ability of ChatGPT to a level comparable to that of human programmers.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 21,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.10679",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-18",
    "authors": [
      {
        "authorId": "2108481995",
        "name": "Xinyu Li"
      },
      {
        "authorId": "2217573580",
        "name": "Jiang-Tian Xue"
      },
      {
        "authorId": "2114115114",
        "name": "Zheng Xie"
      },
      {
        "authorId": "35834541",
        "name": "Ming Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 116.36563680037474
  },
  {
    "paperId": "0d22f06a1f5ad9f62b2f35c126b514f927586c85",
    "url": "https://www.semanticscholar.org/paper/0d22f06a1f5ad9f62b2f35c126b514f927586c85",
    "title": "Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency",
    "abstract": "Large language models (LLMs) have exhibited remarkable ability in code generation. However, generating the correct solution in a single attempt still remains a challenge. Prior works utilize verification properties in software engineering to verify and re-rank solutions in a majority voting manner. But the assumption behind them that generated verification properties have better qualities than solutions may not always hold. In this paper, we treat them equally as different perspectives of LLMs' reasoning processes. We propose the Multi-Perspective Self-Consistency (MPSC) framework incorporating both inter- and intra-consistency across outputs from multiple perspectives. Specifically, we prompt LLMs to generate diverse outputs from three perspectives, Solution, Specification and Test case, constructing a 3-partite graph. With two measure functions of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice of solutions is then determined based on analysis in the graph. MPSC significantly boosts performance of foundation models (ChatGPT in this paper) on various benchmarks, including HumanEval (+15.91%), MBPP (+6.43%) and CodeContests (+9.37%), even surpassing GPT-4.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 20,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.17272",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-09-29",
    "authors": [
      {
        "authorId": "2184278672",
        "name": "Baizhou Huang"
      },
      {
        "authorId": "2115338656",
        "name": "Shuai Lu"
      },
      {
        "authorId": "2249538838",
        "name": "Weizhu Chen"
      },
      {
        "authorId": "2257016300",
        "name": "Xiaojun Wan"
      },
      {
        "authorId": "46429989",
        "name": "Nan Duan"
      }
    ],
    "source": "semantic_scholar",
    "score": 115.66783656585135
  },
  {
    "paperId": "137094dc64e1bad43f68333dc1f82d56168a3de7",
    "url": "https://www.semanticscholar.org/paper/137094dc64e1bad43f68333dc1f82d56168a3de7",
    "title": "Are Large Language Models Geospatially Knowledgeable?",
    "abstract": "Despite the impressive performance of Large Language Models (LLM) for various natural language processing tasks, little is known about their comprehension of geographic data and related ability to facilitate informed geospatial decision-making. This paper investigates the extent of geospatial knowledge, awareness, and reasoning abilities encoded within such pretrained LLMs. With a focus on autoregressive language models, we devise experimental approaches related to (i) probing LLMs for geo-coordinates to assess geospatial knowledge, (ii) using geospatial and non-geospatial prepositions to gauge their geospatial awareness, and (iii) utilizing a multidimensional scaling (MDS) experiment to assess the models' geospatial reasoning capabilities and to determine locations of cities based on prompting. Our results confirm that it does not only take larger but also more sophisticated LLMs to synthesize geospatial knowledge from textual information. As such, this research contributes to understanding the potential and limitations of LLMs in dealing with geospatial information.",
    "venue": "SIGSPATIAL/GIS",
    "year": 2023,
    "citationCount": 21,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3589132.3625625",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2023-10-09",
    "authors": [
      {
        "authorId": "2261085965",
        "name": "Prabin Bhandari"
      },
      {
        "authorId": "2261741456",
        "name": "Antonios Anastasopoulos"
      },
      {
        "authorId": "2258036",
        "name": "D. Pfoser"
      }
    ],
    "source": "semantic_scholar",
    "score": 116.36563680037474
  },
  {
    "paperId": "4153b7fd52c2a65a0a8ef005bd8831b30b5f0b93",
    "url": "https://www.semanticscholar.org/paper/4153b7fd52c2a65a0a8ef005bd8831b30b5f0b93",
    "title": "On the limitations of large language models in clinical diagnosis",
    "abstract": "Background: The potential of large language models (LLM) such as GPT to support complex tasks such as differential diagnosis has been a subject of debate, with some ascribing near sentient abilities to the models and others claiming that LLMs merely perform \"autocomplete on steroids\". A recent study reported that the Generative Pretrained Transformer 4 (GPT-4) model performed well in complex differential diagnostic reasoning. The authors assessed the performance of GPT-4 in identifying the correct diagnosis in a series of case records from the New England Journal of Medicine. The authors constructed prompts based on the clinical presentation section of the case reports, and compared the results of GPT-4 to the actual diagnosis. GPT-4 returned the correct diagnosis as a part of its response in 64% of cases, with the correct diagnosis being at rank 1 in 39% of cases. However, such concise but comprehensive narratives of the clinical course are not typically available in electronic health records (EHRs). Further, if they were available, EHR records contain identifying information whose transmission is prohibited by Health Insurance Portability and Accountability Act (HIPAA) regulations. Methods: To assess the expected performance of GPT on comparable datasets that can be generated by text mining and by design cannot contain identifiable information, we parsed the texts of the case reports and extracted Human Phenotype Ontology (HPO) terms, from which prompts for GPT were constructed that contain largely the same clinical abnormalities but lack the surrounding narrative. Results: While the performance of GPT-4 on the original narrative-based text was good, with the final diagnosis being included in its differential in 29/75 cases (38.7%; rank 1 in 17.3% of cases; mean rank of 3.4), the performance of GPT-4 on the feature-based approach that includes the major clinical abnormalities without additional narrative texas substantially worse, with GPT-4 including the final diagnosis in its differential in 8/75 cases (10.7%; rank 1 in 4.0% of cases; mean rank of 3.9). Interpretation: We consider the feature-based queries to be a more appropriate test of the performance of GPT-4 in diagnostic tasks, since it is unlikely that the narrative approach can be used in actual clinical practice. Future research and algorithmic development is needed to determine the optimal approach to leveraging LLMs for clinical diagnosis.",
    "venue": "medRxiv",
    "year": 2023,
    "citationCount": 21,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-07-14",
    "authors": [
      {
        "authorId": "2065413516",
        "name": "J. Reese"
      },
      {
        "authorId": "34599387",
        "name": "D. Danis"
      },
      {
        "authorId": "2289913765",
        "name": "J. H. Caufield"
      },
      {
        "authorId": "1778643",
        "name": "T. Groza"
      },
      {
        "authorId": "2597140",
        "name": "E. Casiraghi"
      },
      {
        "authorId": "37659088",
        "name": "G. Valentini"
      },
      {
        "authorId": "52038267",
        "name": "C. Mungall"
      },
      {
        "authorId": "2150260091",
        "name": "Peter N. Robinson"
      }
    ],
    "source": "semantic_scholar",
    "score": 116.36563680037474
  },
  {
    "paperId": "a33c02803eed966eb135bc5179c42dd752d29c2b",
    "url": "https://www.semanticscholar.org/paper/a33c02803eed966eb135bc5179c42dd752d29c2b",
    "title": "Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model",
    "abstract": "Large language models (LLMs) have made a significant impact on the fields of general artificial intelligence. General purpose LLMs exhibit strong logic and reasoning skills and general world knowledge but can sometimes generate misleading results when prompted on specific subject areas. LLMs trained with domain-specific knowledge can reduce the generation of misleading information (i.e. hallucinations) and enhance the precision of LLMs in specialized contexts. Training new LLMs on specific corpora however can be resource intensive. Here we explored the use of a retrieval-augmented generation (RAG) model which we tested on literature specific to a biomedical research area. OpenAI’s GPT-3.5, GPT-4, Microsoft’s Prometheus, and a custom RAG model were used to answer 19 questions pertaining to diffuse large B-cell lymphoma (DLBCL) disease biology and treatment. Eight independent reviewers assessed LLM responses based on accuracy, relevance, and readability, rating responses on a 3-point scale for each category. These scores were then used to compare LLM performance. The performance of the LLMs varied across scoring categories. On accuracy and relevance, the RAG model outperformed other models with higher scores on average and the most top scores across questions. GPT-4 was more comparable to the RAG model on relevance versus accuracy. By the same measures, GPT-4 and GPT-3.5 had the highest scores for readability of answers when compared to the other LLMs. GPT-4 and 3.5 also had more answers with hallucinations than the other LLMs, due to non-existent references and inaccurate responses to clinical questions. Our findings suggest that an oncology research-focused RAG model may outperform general-purpose LLMs in accuracy and relevance when answering subject-related questions. This framework can be tailored to Q&A in other subject areas. Further research will help understand the impact of LLM architectures, RAG methodologies, and prompting techniques in answering questions across different subject areas.",
    "venue": "PLOS Digital Health",
    "year": 2023,
    "citationCount": 20,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-05-26",
    "authors": [
      {
        "authorId": "144506027",
        "name": "D. Soong"
      },
      {
        "authorId": "8797188",
        "name": "S. Sridhar"
      },
      {
        "authorId": "2170635354",
        "name": "Han Si"
      },
      {
        "authorId": "108082794",
        "name": "J. Wagner"
      },
      {
        "authorId": "2218489452",
        "name": "Ana Caroline Costa S'a"
      },
      {
        "authorId": "46756088",
        "name": "Christina Y. Yu"
      },
      {
        "authorId": "2407846",
        "name": "Kubra Karagoz"
      },
      {
        "authorId": "5182735",
        "name": "Meijian Guan"
      },
      {
        "authorId": "2085075720",
        "name": "Hisham K Hamadeh"
      },
      {
        "authorId": "2139794738",
        "name": "Brandon W. Higgs"
      }
    ],
    "source": "semantic_scholar",
    "score": 115.66783656585135
  },
  {
    "paperId": "a756b584f8f8b4307e52895ae2120bc339580ad8",
    "url": "https://www.semanticscholar.org/paper/a756b584f8f8b4307e52895ae2120bc339580ad8",
    "title": "See and Think: Embodied Agent in Virtual Environment",
    "abstract": "Large language models (LLMs) have achieved impressive pro-gress on several open-world tasks. Recently, using LLMs to build embodied agents has been a hotspot. This paper proposes STEVE, a comprehensive and visionary embodied agent in the Minecraft virtual environment. STEVE comprises three key components: vision perception, language instruction, and code action. Vision perception involves interpreting visual information in the environment, which is then integrated into the LLMs component with agent state and task instruction. Language instruction is responsible for iterative reasoning and decomposing complex tasks into manageable guidelines. Code action generates executable skill actions based on retrieval in skill database, enabling the agent to interact effectively within the Minecraft environment. We also collect STEVE-21K dataset, which includes 600+ vision-environment pairs, 20K knowledge question-answering pairs, and 200+ skill-code pairs. We conduct continuous block search, knowledge question and answering, and tech tree mastery to evaluate the performance. Extensive experiments show that STEVE achieves at most 1.5x faster unlocking key tech trees and 2.5x quicker in block search tasks.",
    "venue": "European Conference on Computer Vision",
    "year": 2023,
    "citationCount": 18,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-26",
    "authors": [
      {
        "authorId": "2143784044",
        "name": "Zhonghan Zhao"
      },
      {
        "authorId": "2174436552",
        "name": "Wenhao Chai"
      },
      {
        "authorId": "2268428385",
        "name": "Xuan Wang"
      },
      {
        "authorId": "2268409468",
        "name": "Li Boyi"
      },
      {
        "authorId": "2135345538",
        "name": "Shengyu Hao"
      },
      {
        "authorId": "2205657015",
        "name": "Shidong Cao"
      },
      {
        "authorId": "2210992524",
        "name": "Tianbo Ye"
      },
      {
        "authorId": "2264102147",
        "name": "Jenq-Neng Hwang"
      },
      {
        "authorId": "3385327",
        "name": "Gaoang Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.1665846874966
  },
  {
    "paperId": "b0a48d03fd9845461796ff51ec6b1238371bc18f",
    "url": "https://www.semanticscholar.org/paper/b0a48d03fd9845461796ff51ec6b1238371bc18f",
    "title": "LLM-MHR: A LLM-Augmented Multimodal Hashtag Recommendation Algorithm",
    "abstract": "The recommendation of suitable hashtags for mi-croposts encompassing multimodal content stands as a pivotal challenge for numerous Social Networking Service (SNS) applications such as Instagram, Weibo, etc. The accuracy of multimodal hashtag recommendation algorithms relies heavily on the comprehension of multimodal information, user historical information, and the reasoning ability based on such information. However, most previous works have not effectively utilized both historical and additional information simultaneously. Large Language Models (LLMs) learn a vast amount of implicit knowledge during the pre-training stage, which can serve as potential knowledge bases while also possessing strong reasoning abilities. Therefore, LLMs can provide additional information to help understand the micropost content and infer suitable hashtags with strong reasoning ability. However, introducing LLMs for multimodal hashtag recommendation faces three main challenges. Firstly, LLMs require an efficient modality alignment module to accept a multimodal input. Secondly, LLMs are highly sensitive to input order, while utilizing user historical information requires accepting multiple historical samples, necessitating the design of a robust historical information processing module to eliminate the influence of input order. Thirdly, fine-tuning LLMs entails substantial computational overheads, necessitating the reduction of additional trainable parameters. To address the first two challenges, this paper designs an efficient modality alignment module capable of processing multiple historical samples, simultaneously addressing the sensitivity of LLMs to input order changes. To tackle the third challenge, a hybrid prompt learning approach utilizing both soft and hard prompts is proposed to achieve parameter-efficient fine-tuning of LLMs. Finally, a LLM-augmented Multimodal Hashtag Recommendation algorithm (LLM-MHR) is implemented. Comprehensive experiments on the representative dataset MACON demonstrate that LLM-MHR has achieved SOTA performances with significant improvements.",
    "venue": "2024 IEEE International Conference on Web Services (ICWS)",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-07",
    "authors": [
      {
        "authorId": "2301445205",
        "name": "Zhijie Tan"
      },
      {
        "authorId": "2326242321",
        "name": "Yuzhi Li"
      },
      {
        "authorId": "2210838811",
        "name": "Xiang Yuan"
      },
      {
        "authorId": "2324108492",
        "name": "Shengwei Meng"
      },
      {
        "authorId": "2139261376",
        "name": "Weiping Li"
      },
      {
        "authorId": "8046305",
        "name": "Tong Mo"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.39720770839918
  },
  {
    "paperId": "4c0f029efd5371eed087d0794fb0df71238600cc",
    "url": "https://www.semanticscholar.org/paper/4c0f029efd5371eed087d0794fb0df71238600cc",
    "title": "KG-CoT: Chain-of-Thought Prompting of Large Language Models over Knowledge Graphs for Knowledge-Aware Question Answering",
    "abstract": "Large language models (LLMs) encounter challenges such as hallucination and factual errors in knowledge-intensive tasks. One the one hand, LLMs sometimes struggle to generate reliable answers based on the black-box parametric knowledge, due to the lack of responsible knowledge. Moreover, fragmented knowledge facts extracted by knowledge retrievers fail to provide explicit and coherent reasoning paths for improving LLM reasoning. To address these challenges, we propose KG-CoT, a novel knowledge-augmented paradigm that leverages a small-scale step-by-step graph reasoning model to reason over knowledge graphs (KGs) and utilizes a reasoning path generation method to generate chains of reasoning with high confidence for large-scale LLMs. Extensive experiments demonstrate that our KG-CoT significantly improves the performance of LLMs on knowledge-intensive question answering tasks, such as multi-hop, single-hop, and open-domain question answering benchmarks, without fine-tuning LLMs. KG-CoT outperforms the CoT prompting as well as prior retrieval-augmented and knowledge base question answering baselines. Moreover, KG-CoT can reduce the number of API calls and cost and generalize to various LLM backbones in a lightweight plug-and-play manner.",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-08-01",
    "authors": [
      {
        "authorId": "2293901145",
        "name": "Ruilin Zhao"
      },
      {
        "authorId": "2283265839",
        "name": "Feng Zhao"
      },
      {
        "authorId": "2313356601",
        "name": "Long Wang"
      },
      {
        "authorId": "2238149841",
        "name": "Xianzhi Wang"
      },
      {
        "authorId": "2283302879",
        "name": "Guandong Xu"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "559846635fdc3242898af592808d8e346f1ec00f",
    "url": "https://www.semanticscholar.org/paper/559846635fdc3242898af592808d8e346f1ec00f",
    "title": "How Do Humans Write Code? Large Models Do It the Same Way Too",
    "abstract": "Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought (CoT) as the most popular method in Large Language Models (LLMs) mathematical reasoning tasks by utilizing external tool calls to circumvent computational errors. However, our evaluation of the GPT-4 and Llama series reveals that using PoT introduces more reasoning errors, such as incorrect formulas or flawed logic, compared to CoT. To address this issue, we propose Human-Think Language (HTL), which leverages a suite of strategies that help integrate PoT and CoT, encompassing: (1) a new generation paradigm that uses full CoT reasoning to control code generation. (2) Focus Attention, that directs model attention to the CoT reasoning during PoT to generate more logical code. (3) reinforcement learning that utilizes the accuracy of both CoT and PoT responses as rewards to prevent repetitive reasoning steps in LLMs when solving difficult math problems. Our method achieves an average improvement of 6.5% on the Llama-Base model and 4.3% on the Mistral-Base model across 8 mathematical calculation datasets. It also shows significant effectiveness on five out-of-domain datasets by controlling the model’s information flow, exhibiting strong transferability. Additionally, HTL shows the most significant improvement in non-mathematical natural language inference task, contributing to a unified reasoning task framework.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-24",
    "authors": [
      {
        "authorId": "2286990981",
        "name": "Long Li"
      },
      {
        "authorId": "2329882207",
        "name": "Xuzheng He"
      },
      {
        "authorId": "2329873422",
        "name": "Haozhe Wang"
      },
      {
        "authorId": "2155372933",
        "name": "Linlin Wang"
      },
      {
        "authorId": "2275985730",
        "name": "Liang He"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "89435c392b611b1e804c12d0176661e91ea54ea1",
    "url": "https://www.semanticscholar.org/paper/89435c392b611b1e804c12d0176661e91ea54ea1",
    "title": "Embodied CoT Distillation From LLM To Off-the-shelf Agents",
    "abstract": "We address the challenge of utilizing large language models (LLMs) for complex embodied tasks, in the environment where decision-making systems operate timely on capacity-limited, off-the-shelf devices. We present DeDer, a framework for decomposing and distilling the embodied reasoning capabilities from LLMs to efficient, small language model (sLM)-based policies. In DeDer, the decision-making process of LLM-based strategies is restructured into a hierarchy with a reasoning-policy and planning-policy. The reasoning-policy is distilled from the data that is generated through the embodied in-context learning and self-verification of an LLM, so it can produce effective rationales. The planning-policy, guided by the rationales, can render optimized plans efficiently. In turn, DeDer allows for adopting sLMs for both policies, deployed on off-the-shelf devices. Furthermore, to enhance the quality of intermediate rationales, specific to embodied tasks, we devise the embodied knowledge graph, and to generate multiple rationales timely through a single inference, we also use the contrastively prompted attention model. Our experiments with the ALFRED benchmark demonstrate that DeDer surpasses leading language planning and distillation approaches, indicating the applicability and efficiency of sLM-based embodied policies derived through DeDer.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-12-16",
    "authors": [
      {
        "authorId": "2287939397",
        "name": "Wonje Choi"
      },
      {
        "authorId": "2174282424",
        "name": "Woo Kyung Kim"
      },
      {
        "authorId": "1381642501",
        "name": "Minjong Yoo"
      },
      {
        "authorId": "2283845059",
        "name": "Honguk Woo"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "3bf191c152c3eb693f2fe22eda99197516da9db3",
    "url": "https://www.semanticscholar.org/paper/3bf191c152c3eb693f2fe22eda99197516da9db3",
    "title": "T-Eval: Evaluating the Tool Utilization Capability Step by Step",
    "abstract": "Large language models (LLMs) have achieved remarkable performance on various NLP tasks and are augmented by tools for broader applications. Yet, how to evaluate and analyze the tool utilization capability of LLMs is still under-explored. In contrast to previous works that evaluate models holistically, we comprehensively decompose the tool utilization into multiple sub-processes, including instruction following, planning, reasoning, retrieval, understanding, and review. Based on that, we further introduce T-Eval to evaluate the tool-utilization capability step by step. T-Eval disentangles the tool utilization evaluation into several sub-domains along model capabilities, facilitating the inner understanding of both holistic and isolated competency of LLMs. We conduct extensive experiments on T-Eval and in-depth analysis of various LLMs. T-Eval not only exhibits consistency with the outcome-oriented evaluation but also provides a more fine-grained analysis of the capabilities of LLMs, providing a new perspective in LLM evaluation on tool-utilization ability. The benchmark will be available at https://github.com/open-compass/T-Eval.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 17,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "48354360",
        "name": "Zehui Chen"
      },
      {
        "authorId": "2214830561",
        "name": "Weihua Du"
      },
      {
        "authorId": "2266359401",
        "name": "Wenwei Zhang"
      },
      {
        "authorId": "2029335061",
        "name": "Kuikun Liu"
      },
      {
        "authorId": "2275801272",
        "name": "Jiangning Liu"
      },
      {
        "authorId": "2159678046",
        "name": "Miao Zheng"
      },
      {
        "authorId": "2275567484",
        "name": "Jingming Zhuo"
      },
      {
        "authorId": "2266356137",
        "name": "Songyang Zhang"
      },
      {
        "authorId": "2261095726",
        "name": "Dahua Lin"
      },
      {
        "authorId": "2275790072",
        "name": "Kai Chen"
      },
      {
        "authorId": "2275819548",
        "name": "Feng Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.35557636844247
  },
  {
    "paperId": "72273f7a050529fc71c7d45c0256d2b9754f56bb",
    "url": "https://www.semanticscholar.org/paper/72273f7a050529fc71c7d45c0256d2b9754f56bb",
    "title": "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration",
    "abstract": "Large Language Models (LLMs) have significantly advanced natural language processing, demonstrating exceptional reasoning, tool usage, and memory capabilities. As their applications expand into multi-agent environments, there arises a need for a comprehensive evaluation framework that captures LLMs’ reasoning, planning, collaboration, and other social abilities. This work introduces a novel competition-based benchmark framework specifically designed to assess LLMs within multi-agent settings, providing quantitative metrics to evaluate their judgment, reasoning, deception, self-awareness, cooperation, coordination, and rationality.We utilize two social deduction games alongside three game-theory scenarios to create diverse environments.Our frame is fortified with the probabilistic graphic modeling (PGM) method, enhancing the LLMs’ capabilities in navigating complex social and cognitive dimensions. We evaluate seven LLMs, quantitatively highlighting a significant capability gap of over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B. It also confirms that our PGM enhancement boosts the abilities of all selected models by an average of 37%. Our data and code can be found here https://github.com/cathyxl/MAgIC.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 17,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-14",
    "authors": [
      {
        "authorId": "2267005785",
        "name": "Lin Xu"
      },
      {
        "authorId": "2267163621",
        "name": "Zhiyuan Hu"
      },
      {
        "authorId": "18119920",
        "name": "Daquan Zhou"
      },
      {
        "authorId": "2266809202",
        "name": "Hongyu Ren"
      },
      {
        "authorId": "143879884",
        "name": "Zhen Dong"
      },
      {
        "authorId": "2242659602",
        "name": "Kurt Keutzer"
      },
      {
        "authorId": "2269762964",
        "name": "See-Kiong Ng"
      },
      {
        "authorId": "2256994948",
        "name": "Jiashi Feng"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.35557636844247
  },
  {
    "paperId": "2d4acd18db84560cec329f3749ae255c5f238c44",
    "url": "https://www.semanticscholar.org/paper/2d4acd18db84560cec329f3749ae255c5f238c44",
    "title": "Testing AI on language comprehension tasks reveals insensitivity to underlying meaning",
    "abstract": null,
    "venue": "Scientific Reports",
    "year": 2023,
    "citationCount": 17,
    "openAccessPdf": {
      "url": "https://www.nature.com/articles/s41598-024-79531-8.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-02-23",
    "authors": [
      {
        "authorId": "2119882111",
        "name": "Vittoria Dentella"
      },
      {
        "authorId": "2297848018",
        "name": "Fritz Guenther"
      },
      {
        "authorId": "5925649",
        "name": "Elliot Murphy"
      },
      {
        "authorId": "2084369310",
        "name": "G. Marcus"
      },
      {
        "authorId": "1867483",
        "name": "Evelina Leivada"
      }
    ],
    "source": "semantic_scholar",
    "score": 93.35557636844247
  },
  {
    "paperId": "71e996ff55b972946b9fe0f88394c19425f5a3ab",
    "url": "https://www.semanticscholar.org/paper/71e996ff55b972946b9fe0f88394c19425f5a3ab",
    "title": "Interleaving Pre-Trained Language Models and Large Language Models for Zero-Shot NL2SQL Generation",
    "abstract": "Zero-shot NL2SQL is crucial in achieving natural language to SQL that is adaptive to new environments (e.g., new databases, new linguistic phenomena or SQL structures) with zero annotated NL2SQL samples from such environments. Existing approaches either fine-tune pre-trained language models (PLMs) based on annotated data or use prompts to guide fixed large language models (LLMs) such as ChatGPT. PLMs can perform well in schema alignment but struggle to achieve complex reasoning, while LLMs is superior in complex reasoning tasks but cannot achieve precise schema alignment. In this paper, we propose a ZeroNL2SQL framework that combines the complementary advantages of PLMs and LLMs for supporting zero-shot NL2SQL. ZeroNL2SQL first uses PLMs to generate an SQL sketch via schema alignment, then uses LLMs to fill the missing information via complex reasoning. Moreover, in order to better align the generated SQL queries with values in the given database instances, we design a predicate calibration method to guide the LLM in completing the SQL sketches based on the database instances and select the optimal SQL query via an execution-based strategy. Comprehensive experiments show that ZeroNL2SQL can achieve the best zero-shot NL2SQL performance on real-world benchmarks. Specifically, ZeroNL2SQL outperforms the state-of-the-art PLM-based methods by 3.2% to 13% and exceeds LLM-based methods by 10% to 20% on execution accuracy.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 17,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.08891",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-15",
    "authors": [
      {
        "authorId": "2082344591",
        "name": "Zihui Gu"
      },
      {
        "authorId": "1704755170",
        "name": "Ju Fan"
      },
      {
        "authorId": "2213989628",
        "name": "Nan Tang"
      },
      {
        "authorId": "151226330",
        "name": "Songyue Zhang"
      },
      {
        "authorId": "2220137935",
        "name": "Yuxin Zhang"
      },
      {
        "authorId": "48354042",
        "name": "Zui Chen"
      },
      {
        "authorId": "50260667",
        "name": "Lei Cao"
      },
      {
        "authorId": "2108491555",
        "name": "Guoliang Li"
      },
      {
        "authorId": "2053630301",
        "name": "Sam Madden"
      },
      {
        "authorId": "2152944669",
        "name": "Xiaoyong Du"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.35557636844247
  },
  {
    "paperId": "862ed55c8a7d4ef1b23091995d42aae91d906a1b",
    "url": "https://www.semanticscholar.org/paper/862ed55c8a7d4ef1b23091995d42aae91d906a1b",
    "title": "Dynamic Retrieval Augmented Generation of Ontologies using Artificial Intelligence (DRAGON-AI)",
    "abstract": null,
    "venue": "Journal of Biomedical Semantics",
    "year": 2023,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-18",
    "authors": [
      {
        "authorId": "2266381638",
        "name": "Sabrina Toro"
      },
      {
        "authorId": "145011015",
        "name": "A. V. Anagnostopoulos"
      },
      {
        "authorId": "2275148170",
        "name": "Sue Bello"
      },
      {
        "authorId": "2275197596",
        "name": "Kai Blumberg"
      },
      {
        "authorId": "2154587410",
        "name": "Rhiannon Cameron"
      },
      {
        "authorId": "47658708",
        "name": "Leigh Carmody"
      },
      {
        "authorId": "2275197569",
        "name": "Alexander D. Diehl"
      },
      {
        "authorId": "34820117",
        "name": "Damion M. Dooley"
      },
      {
        "authorId": "2203581189",
        "name": "William Duncan"
      },
      {
        "authorId": "36059755",
        "name": "P. Fey"
      },
      {
        "authorId": "2275202161",
        "name": "Pascale Gaudet"
      },
      {
        "authorId": "2252771173",
        "name": "Nomi L. Harris"
      },
      {
        "authorId": "2111638",
        "name": "marcin p. joachimiak"
      },
      {
        "authorId": "2275195127",
        "name": "Leila Kiani"
      },
      {
        "authorId": "35522587",
        "name": "Tiago Lubiana"
      },
      {
        "authorId": "1399643910",
        "name": "M. Munoz-Torres"
      },
      {
        "authorId": "2241482835",
        "name": "Shawn T. O'Neil"
      },
      {
        "authorId": "1403853027",
        "name": "David Osumi-Sutherland"
      },
      {
        "authorId": "2275197400",
        "name": "Aleix Puig"
      },
      {
        "authorId": "2275203052",
        "name": "Justin P Reese"
      },
      {
        "authorId": "3297166",
        "name": "L. Reiser"
      },
      {
        "authorId": "2275201051",
        "name": "Sofia M. C. Robb"
      },
      {
        "authorId": "2275195895",
        "name": "Troy Ruemping"
      },
      {
        "authorId": "145515276",
        "name": "James Seager"
      },
      {
        "authorId": "2275197547",
        "name": "Eric Sid"
      },
      {
        "authorId": "2266379845",
        "name": "Ray Stefancsik"
      },
      {
        "authorId": "2276006194",
        "name": "Magalie Weber"
      },
      {
        "authorId": "2275201108",
        "name": "Valerie Wood"
      },
      {
        "authorId": "2237677943",
        "name": "M. Haendel"
      },
      {
        "authorId": "2239499194",
        "name": "Christopher J. Mungall"
      }
    ],
    "source": "semantic_scholar",
    "score": 92.49820016084324
  },
  {
    "paperId": "1c8840fc74ed2fa44b5995d6879ef10cf62101c4",
    "url": "https://www.semanticscholar.org/paper/1c8840fc74ed2fa44b5995d6879ef10cf62101c4",
    "title": "A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction",
    "abstract": "Large language models (LLMs) have demonstrated great potential for domain-specific applications, such as the law domain. However, recent disputes over GPT-4's law evaluation raise questions concerning their performance in real-world legal tasks. To systematically investigate their competency in the law, we design practical baseline solutions based on LLMs and test on the task of legal judgment prediction. In our solutions, LLMs can work alone to answer open questions or coordinate with an information retrieval (IR) system to learn from similar cases or solve simplified multi-choice questions. We show that similar cases and multi-choice options, namely label candidates, included in prompts can help LLMs recall domain knowledge that is critical for expertise legal reasoning. We additionally present an intriguing paradox wherein an IR system surpasses the performance of LLM+IR due to limited gains acquired by weaker LLMs from powerful IR systems. In such cases, the role of LLMs becomes redundant. Our evaluation pipeline can be easily extended into other tasks to facilitate evaluations in other domains. Code is available at https://github.com/srhthu/LM-CompEval-Legal",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 15,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-18",
    "authors": [
      {
        "authorId": "1667839719",
        "name": "Ruihao Shui"
      },
      {
        "authorId": "2258806194",
        "name": "Yixin Cao"
      },
      {
        "authorId": "2260296919",
        "name": "Xiang Wang"
      },
      {
        "authorId": "2257036129",
        "name": "Tat-Seng Chua"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.58883083359672
  },
  {
    "paperId": "3dd8a397abc6fcd7e945cea2d02a1ea3266e0bc9",
    "url": "https://www.semanticscholar.org/paper/3dd8a397abc6fcd7e945cea2d02a1ea3266e0bc9",
    "title": "What's Left? Concept Grounding with Logic-Enhanced Foundation Models",
    "abstract": "Recent works such as VisProg and ViperGPT have smartly composed foundation models for visual reasoning-using large language models (LLMs) to produce programs that can be executed by pre-trained vision-language models. However, they operate in limited domains, such as 2D images, not fully exploiting the generalization of language: abstract concepts like\"left\"can also be grounded in 3D, temporal, and action data, as in moving to your left. This limited generalization stems from these inference-only methods' inability to learn or adapt pre-trained models to a new domain. We propose the Logic-Enhanced Foundation Model (LEFT), a unified framework that learns to ground and reason with concepts across domains with a differentiable, domain-independent, first-order logic-based program executor. LEFT has an LLM interpreter that outputs a program represented in a general, logic-based reasoning language, which is shared across all domains and tasks. LEFT's executor then executes the program with trainable domain-specific grounding modules. We show that LEFT flexibly learns concepts in four domains: 2D images, 3D scenes, human motions, and robotic manipulation. It exhibits strong reasoning ability in a wide variety of tasks, including those that are complex and not seen during training, and can be easily applied to new domains.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-24",
    "authors": [
      {
        "authorId": "51169351",
        "name": "Joy Hsu"
      },
      {
        "authorId": "13589371",
        "name": "Jiayuan Mao"
      },
      {
        "authorId": "2238317928",
        "name": "J. B. Tenenbaum"
      },
      {
        "authorId": "3045089",
        "name": "Jiajun Wu"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.49820016084324
  },
  {
    "paperId": "7b0a186b0140ee91fb13991c9c7187f3dc3b0670",
    "url": "https://www.semanticscholar.org/paper/7b0a186b0140ee91fb13991c9c7187f3dc3b0670",
    "title": "Visual Programming for Zero-Shot Open-Vocabulary 3D Visual Grounding",
    "abstract": "3D Visual Grounding (3DVG) aims at localizing 3D object based on textual descriptions. Conventional supervised methods for 3DVG often necessitate extensive annotations and a predefined vocabulary, which can be restrictive. To address this issue, we propose a novel visual programming approach for zero-shot open-vocabulary 3DVG, leveraging the capabilities of large language models (LLMs). Our approach begins with a unique dialog-based method, engaging with LLMs to establish a foundational understanding of zero-shot 3DVG. Building on this, we design a visual program that consists of three types of modules, i.e., view-independent, view-dependent, and functional modules. These modules, specifically tailored for 3D scenarios, work collaboratively to perform complex reasoning and inference. Furthermore, we develop an innovative language-object correlation module to extend the scope of existing 3D object detectors into open-vocabulary scenarios. Extensive experiments demonstrate that our zero-shot approach can outperform some supervised baselines, marking a significant stride towards effective 3DVG. Code is available at https://curryyuan.github.io/Z5VG3D.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2023,
    "citationCount": 14,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2311.15383",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-26",
    "authors": [
      {
        "authorId": "2040839502",
        "name": "Zhihao Yuan"
      },
      {
        "authorId": "2267903909",
        "name": "Jinke Ren"
      },
      {
        "authorId": "2257010710",
        "name": "Chun-Mei Feng"
      },
      {
        "authorId": "2237588370",
        "name": "Hengshuang Zhao"
      },
      {
        "authorId": "2267443266",
        "name": "Shuguang Cui"
      },
      {
        "authorId": "2110120874",
        "name": "Zhen Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "e2f1f04f648a8863d11439aa4c80ee65d6caccda",
    "url": "https://www.semanticscholar.org/paper/e2f1f04f648a8863d11439aa4c80ee65d6caccda",
    "title": "ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models",
    "abstract": "Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior. To further unleash the power of LLMs to accomplish complex tasks, there is a growing trend to build agent framework that equips LLMs, such as ChatGPT, with tool-use abilities to connect with massive external APIs. In this work, we introduce ModelScope-Agent, a general and customizable agent framework for real-world applications, based on open-source LLMs as controllers. It provides a user-friendly system library, with customizable engine design to support model training on multiple open-source LLMs, while also enabling seamless integration with both model APIs and common APIs in a unified way. To equip the LLMs with tool-use abilities, a comprehensive framework has been proposed spanning over tool-use data collection, tool retrieval, tool registration, memory control, customized model training, and evaluation for practical real-world applications. Finally, we showcase ModelScopeGPT, a real-world intelligent assistant of ModelScope Community based on the ModelScope-Agent framework, which is able to connect open-source LLMs with more than 1000 public AI models and localized community knowledge in ModelScope. The ModelScope-Agent library\\footnote{https://github.com/modelscope/modelscope-agent} and online demo\\footnote{https://modelscope.cn/studios/damo/ModelScopeGPT/summary} are now publicly available.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 14,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.00986",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-09-02",
    "authors": [
      {
        "authorId": "143971529",
        "name": "Chenliang Li"
      },
      {
        "authorId": "123655156",
        "name": "Hehong Chen"
      },
      {
        "authorId": "2114009661",
        "name": "Mingshi Yan"
      },
      {
        "authorId": "2237809880",
        "name": "Weizhou Shen"
      },
      {
        "authorId": "153194420",
        "name": "Haiyang Xu"
      },
      {
        "authorId": "2238047773",
        "name": "Zhikai Wu"
      },
      {
        "authorId": "2237946970",
        "name": "Zhicheng Zhang"
      },
      {
        "authorId": "2237956023",
        "name": "Wenmeng Zhou"
      },
      {
        "authorId": "2237827934",
        "name": "Yingda Chen"
      },
      {
        "authorId": "2237996616",
        "name": "Chen Cheng"
      },
      {
        "authorId": "2238549612",
        "name": "Hongzhu Shi"
      },
      {
        "authorId": "2116921824",
        "name": "Ji Zhang"
      },
      {
        "authorId": "143857288",
        "name": "Fei Huang"
      },
      {
        "authorId": "2237981776",
        "name": "Jingren Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "5aaa39b360c39e8a43957e688d7adf549b1e95e5",
    "url": "https://www.semanticscholar.org/paper/5aaa39b360c39e8a43957e688d7adf549b1e95e5",
    "title": "Towards Interpretable Mental Health Analysis with ChatGPT",
    "abstract": "Automated mental health analysis shows great potential for enhancing the efﬁciency and accessibility of mental health care, with recent methods using pre-trained language models (PLMs) and incorporated emotional information. The latest large language models (LLMs), such as ChatGPT, exhibit dramatic capabilities on diverse natural language processing tasks. However, existing studies on Chat-GPT for mental health analysis bear limitations in inadequate evaluations, ignorance of emotional information, and lack of explain-ability. To bridge these gaps, we comprehensively evaluate the mental health analysis and emotional reasoning ability of ChatGPT on 11 datasets across 5 tasks, and analyze the effects of various emotion-based prompting strategies. Based on these prompts, we further explore LLMs for interpretable mental health analysis by instructing them to also generate explanations for each of their decisions. With an annotation protocol designed by domain experts, we convey human evaluations to assess the quality of explanations generated by Chat-GPT and GPT-3. The annotated corpus will be released for future research. Experimental results show that ChatGPT outperforms traditional neural network-based methods but still has a signiﬁcant gap with advanced task-speciﬁc methods. Prompt engineering with emotional cues can be effective in improving performance on mental health analysis but suffers from a lack of robustness and inaccurate reasoning. In addition, ChatGPT signiﬁcantly outperforms GPT-3 on all criteria in human evaluations of the explanations and approaches to human performance, showing its great potential in explainable mental health analysis.",
    "venue": "",
    "year": 2023,
    "citationCount": 14,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2003396186",
        "name": "Kailai Yang"
      },
      {
        "authorId": "51394448",
        "name": "Shaoxiong Ji"
      },
      {
        "authorId": "9416328",
        "name": "Tianlin Zhang"
      },
      {
        "authorId": "145229872",
        "name": "Qianqian Xie"
      },
      {
        "authorId": "13627871",
        "name": "Zi-Zhou Kuang"
      },
      {
        "authorId": "1881965",
        "name": "S. Ananiadou"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "f02e8f1c9b5ab12ddfb1977570f9f5445a99a973",
    "url": "https://www.semanticscholar.org/paper/f02e8f1c9b5ab12ddfb1977570f9f5445a99a973",
    "title": "Large Language Models are reasoners with Self-Verification",
    "abstract": "When a large language model (LLM) performs complex reasoning by chain of thought (CoT), it can be highly sensitive to individual mistakes. We have had to train verifiers to address this issue. As we all know, after human inferring a conclusion, they often check it by re-verifying it, which can avoid some mistakes. We propose a new method called self-verification that uses the conclusion of the CoT as a condition to build a new sample and asks the LLM to re-predict the original conditions which be masked. We calculate an explainable verification score based on the accuracy. This method can improve the accuracy of multiple arithmetics and logical reasoning datasets when using few-shot learning. we have demonstrated that LLMs can conduct explainable self-verification of their own conclusions and achieve competitive reasoning performance. Extensive experimentals have demonstrated that our method can help multiple large language models with self-verification can avoid interference from incorrect CoT. 1",
    "venue": "arXiv.org",
    "year": 2022,
    "citationCount": 58,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2212.09561",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2142839441",
        "name": "Yixuan Weng"
      },
      {
        "authorId": "2187504043",
        "name": "Minjun Zhu"
      },
      {
        "authorId": "2156072001",
        "name": "Bin Li"
      },
      {
        "authorId": "1954845",
        "name": "Shizhu He"
      },
      {
        "authorId": "2200096",
        "name": "Kang Liu"
      },
      {
        "authorId": "11447228",
        "name": "Jun Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 131.1630616585858
  },
  {
    "paperId": "63e6c297a3a9856fa357627eb41e7eecbde9244e",
    "url": "https://www.semanticscholar.org/paper/63e6c297a3a9856fa357627eb41e7eecbde9244e",
    "title": "Automated Interactive Domain-Specific Conversational Agents that Understand Human Dialogs",
    "abstract": "Achieving human-like communication with machines remains a classic, challenging topic in the field of Knowledge Representation and Reasoning and Natural Language Processing. These Large Language Models (LLMs) rely on pattern-matching rather than a true understanding of the semantic meaning of a sentence. As a result, they may generate incorrect responses. To generate an assuredly correct response, one has to\"understand\"the semantics of a sentence. To achieve this\"understanding\", logic-based (commonsense) reasoning methods such as Answer Set Programming (ASP) are arguably needed. In this paper, we describe the AutoConcierge system that leverages LLMs and ASP to develop a conversational agent that can truly\"understand\"human dialogs in restricted domains. AutoConcierge is focused on a specific domain-advising users about restaurants in their local area based on their preferences. AutoConcierge will interactively understand a user's utterances, identify the missing information in them, and request the user via a natural language sentence to provide it. Once AutoConcierge has determined that all the information has been received, it computes a restaurant recommendation based on the user-preferences it has acquired from the human user. AutoConcierge is based on our STAR framework developed earlier, which uses GPT-3 to convert human dialogs into predicates that capture the deep structure of the dialog's sentence. These predicates are then input into the goal-directed s(CASP) ASP system for performing commonsense reasoning. To the best of our knowledge, AutoConcierge is the first automated conversational agent that can realistically converse like a human and provide help to humans based on truly understanding human utterances.",
    "venue": "International Symposium on Practical Aspects of Declarative Languages",
    "year": 2023,
    "citationCount": 12,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2303.08941",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-03-15",
    "authors": [
      {
        "authorId": "2116806486",
        "name": "Yankai Zeng"
      },
      {
        "authorId": "2153463830",
        "name": "Abhiramon Rajasekharan"
      },
      {
        "authorId": "2151051389",
        "name": "Parth Padalkar"
      },
      {
        "authorId": "38428283",
        "name": "Kinjal Basu"
      },
      {
        "authorId": "144961951",
        "name": "Joaquín Arias"
      },
      {
        "authorId": "2146687315",
        "name": "Gopal Gupta"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "40c9280d87059c0cc28f2a08d46a7045fa3e9736",
    "url": "https://www.semanticscholar.org/paper/40c9280d87059c0cc28f2a08d46a7045fa3e9736",
    "title": "Divide and Prompt: Chain of Thought Prompting for Text-to-SQL",
    "abstract": "Chain-of-thought (CoT) prompting combined with large language models (LLMs) have achieved encouraging results on complex reasoning tasks. Text-to-SQL is a critical semantic parsing task that converts natural language questions into SQL statements, involving a complex reasoning process. However, there is little work about using CoT prompting to activate LLM's reasoning capabilities on Text-to-SQL tasks. In this work, we propose a new paradigm for prompting Text-to-SQL tasks, called Divide-and-Prompt, which first divides the task into subtasks, and then approach each subtask through CoT. We present 3 prompting-based methods to enhance the Text-to-SQL ability of LLMs. Experiments show that these prompts guide LLMs to generate Text-to-SQL with higher execution accuracy.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 12,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2304.11556",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-04-23",
    "authors": [
      {
        "authorId": "27469537",
        "name": "X. Liu"
      },
      {
        "authorId": "2093186622",
        "name": "Zhao Tan"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "69cc8b43f2e178102599c706efb856b4808e73eb",
    "url": "https://www.semanticscholar.org/paper/69cc8b43f2e178102599c706efb856b4808e73eb",
    "title": "Filling the Image Information Gap for VQA: Prompting Large Language Models to Proactively Ask Questions",
    "abstract": "Large Language Models (LLMs) demonstrate impressive reasoning ability and the maintenance of world knowledge not only in natural language tasks, but also in some vision-language tasks such as open-domain knowledge-based visual question answering (OK-VQA). As images are invisible to LLMs, researchers convert images to text to engage LLMs into the visual question reasoning procedure. This leads to discrepancies between images and their textual representations presented to LLMs, which consequently impedes final reasoning performance. To fill the information gap and better leverage the reasoning capability, we design a framework that enables LLMs to proactively ask relevant questions to unveil more details in the image, along with filters for refining the generated information. We validate our idea on OK-VQA and A-OKVQA. Our method continuously boosts the performance of baselines methods by an average gain of 2.15% on OK-VQA, and achieves consistent improvements across different LLMs.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-20",
    "authors": [
      {
        "authorId": "51063775",
        "name": "Ziyue Wang"
      },
      {
        "authorId": "2116885534",
        "name": "Chi Chen"
      },
      {
        "authorId": "144326610",
        "name": "Peng Li"
      },
      {
        "authorId": "2254850259",
        "name": "Yang Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "2619cb3957040ae9626158ca52675554b4ed2f43",
    "url": "https://www.semanticscholar.org/paper/2619cb3957040ae9626158ca52675554b4ed2f43",
    "title": "Improving Patient Pre-screening for Clinical Trials: Assisting Physicians with Large Language Models",
    "abstract": "Physicians considering clinical trials for their patients are met with the laborious process of checking many text based eligibility criteria. Large Language Models (LLMs) have shown to perform well for clinical information extraction and clinical reasoning, including medical tests, but not yet in real-world scenarios. This paper investigates the use of InstructGPT to assist physicians in determining eligibility for clinical trials based on a patient's summarised medical profile. Using a prompting strategy combining one-shot, selection-inference and chain-of-thought techniques, we investigate the performance of LLMs on 10 synthetically created patient profiles. Performance is evaluated at four levels: ability to identify screenable eligibility criteria from a trial given a medical profile; ability to classify for each individual criterion whether the patient qualifies; the overall classification whether a patient is eligible for a clinical trial and the percentage of criteria to be screened by physician. We evaluated against 146 clinical trials and a total of 4,135 eligibility criteria. The LLM was able to correctly identify the screenability of 72% (2,994/4,135) of the criteria. Additionally, 72% (341/471) of the screenable criteria were evaluated correctly. The resulting trial level classification as eligible or ineligible resulted in a recall of 0.5. By leveraging LLMs with a physician-in-the-loop, a recall of 1.0 and precision of 0.71 on clinical trial level can be achieved while reducing the amount of criteria to be checked by an estimated 90%. LLMs can be used to assist physicians with pre-screening of patients for clinical trials. By forcing instruction-tuned LLMs to produce chain-of-thought responses, the reasoning can be made transparent to and the decision process becomes amenable by physicians, thereby making such a system feasible for use in real-world scenarios.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 12,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2304.07396",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-04-14",
    "authors": [
      {
        "authorId": "144391588",
        "name": "D. Hamer"
      },
      {
        "authorId": "79315365",
        "name": "P. Schoor"
      },
      {
        "authorId": "82126441",
        "name": "T. Polak"
      },
      {
        "authorId": "2148713620",
        "name": "Daniel Kapitan"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "22cc37aac18f0d371355baf83bc043f1d141bd0b",
    "url": "https://www.semanticscholar.org/paper/22cc37aac18f0d371355baf83bc043f1d141bd0b",
    "title": "Large Language Models for Intent-Driven Session Recommendations",
    "abstract": "Intent-aware session recommendation (ISR) is pivotal in discerning user intents within sessions for precise predictions. Traditional approaches, however, face limitations due to their presumption of a uniform number of intents across all sessions. This assumption overlooks the dynamic nature of user sessions, where the number and type of intentions can significantly vary. In addition, these methods typically operate in latent spaces, thus hinder the model's transparency.Addressing these challenges, we introduce a novel ISR approach, utilizing the advanced reasoning capabilities of large language models (LLMs). First, this approach begins by generating an initial prompt that guides LLMs to predict the next item in a session, based on the varied intents manifested in user sessions. Then, to refine this process, we introduce an innovative prompt optimization mechanism that iteratively self-reflects and adjusts prompts. Furthermore, our prompt selection module, built upon the LLMs' broad adaptability, swiftly selects the most optimized prompts across diverse domains. This new paradigm empowers LLMs to discern diverse user intents at a semantic level, leading to more accurate and interpretable session recommendations. Our extensive experiments on three real-world datasets demonstrate the effectiveness of our method, marking a significant advancement in ISR systems.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2023,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-07",
    "authors": [
      {
        "authorId": "2274082438",
        "name": "Zhu Sun"
      },
      {
        "authorId": "2145593442",
        "name": "Hongyang Liu"
      },
      {
        "authorId": "40507824",
        "name": "Xinghua Qu"
      },
      {
        "authorId": "2273938691",
        "name": "Kaidong Feng"
      },
      {
        "authorId": "2288161909",
        "name": "Yan Wang"
      },
      {
        "authorId": "8748397",
        "name": "Y. Ong"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.47424036192305
  },
  {
    "paperId": "bc4e39364f591aec8cb850eb7f1ef6e92242ec41",
    "url": "https://www.semanticscholar.org/paper/bc4e39364f591aec8cb850eb7f1ef6e92242ec41",
    "title": "From RTL to SVA: LLM-assisted generation of Formal Verification Testbenches",
    "abstract": "—Formal property verification (FPV) has existed for decades and has been shown to be effective at finding intricate RTL bugs. However, formal properties, such as those written as SystemVerilog Assertions (SVA), are time-consuming and error-prone to write, even for experienced users. Prior work has attempted to lighten this burden by raising the abstraction level so that SVA is generated from high-level specifications. However, this does not eliminate the manual effort of reasoning and writing about the detailed hardware behavior. Motivated by the increased need for FPV in the era of heterogeneous hardware and the advances in large language models (LLMs), we set out to explore whether LLMs can capture RTL behavior and generate correct SVA properties. First, we design an FPV-based evaluation framework that measures the correctness and completeness of SVA. Then, we evaluate GPT4 iteratively to craft the set of syntax and semantic rules needed to prompt it toward creating better SVA. We extend the open-source AutoSVA framework by integrating our improved GPT4-based flow to generate safety properties, in addition to facilitating their existing flow for liveness properties. Lastly, our use cases evaluate (1) the FPV coverage of GPT4-generated SVA on complex open-source RTL and (2) using generated SVA to prompt GPT4 to create RTL from scratch. Through these experiments, we find that GPT4 can generate correct SVA even for flawed RTL—without mirroring design errors. Particularly, it generated SVA that exposed a bug in the RISC-V CVA6 core that eluded the prior work’s evaluation.",
    "venue": "",
    "year": 2023,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1647009803",
        "name": "Marcelo Orenes-Vera"
      },
      {
        "authorId": "1708269",
        "name": "M. Martonosi"
      },
      {
        "authorId": "1752172",
        "name": "D. Wentzlaff"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "e1b2481025763e087b375e57c948a0560e39eebf",
    "url": "https://www.semanticscholar.org/paper/e1b2481025763e087b375e57c948a0560e39eebf",
    "title": "LLM As DBA",
    "abstract": "Database administrators (DBAs) play a crucial role in managing, maintaining and optimizing a database system to ensure data availability, performance, and reliability. However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases). Recently large language models (LLMs) have shown great potential to understand valuable documents and accordingly generate reasonable answers. Thus, we propose D-Bot, a LLM-based database administrator that can continuously acquire database maintenance experience from textual sources, and provide reasonable, well-founded, in-time diagnosis and optimization advice for target databases. This paper presents a revolutionary LLM-centric framework for database maintenance, including (i) database maintenance knowledge detection from documents and tools, (ii) tree of thought reasoning for root cause analysis, and (iii) collaborative diagnosis among multiple LLMs. Our preliminary experimental results that D-Bot can efficiently and effectively diagnose the root causes and our code is available at github.com/TsinghuaDatabaseGroup/DB-GPT.",
    "venue": "",
    "year": 2023,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-08-10",
    "authors": [
      {
        "authorId": "50177381",
        "name": "Xuanhe Zhou"
      },
      {
        "authorId": "2136107368",
        "name": "Guoliang Li"
      },
      {
        "authorId": "49293587",
        "name": "Zhiyuan Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "9e557a199972b963d8ac064ac6e625c115c03cde",
    "url": "https://www.semanticscholar.org/paper/9e557a199972b963d8ac064ac6e625c115c03cde",
    "title": "Causal Structure Learning Supervised by Large Language Model",
    "abstract": "Causal discovery from observational data is pivotal for deciphering complex relationships. Causal Structure Learning (CSL), which focuses on deriving causal Directed Acyclic Graphs (DAGs) from data, faces challenges due to vast DAG spaces and data sparsity. The integration of Large Language Models (LLMs), recognized for their causal reasoning capabilities, offers a promising direction to enhance CSL by infusing it with knowledge-based causal inferences. However, existing approaches utilizing LLMs for CSL have encountered issues, including unreliable constraints from imperfect LLM inferences and the computational intensity of full pairwise variable analyses. In response, we introduce the Iterative LLM Supervised CSL (ILS-CSL) framework. ILS-CSL innovatively integrates LLM-based causal inference with CSL in an iterative process, refining the causal DAG using feedback from LLMs. This method not only utilizes LLM resources more efficiently but also generates more robust and high-quality structural constraints compared to previous methodologies. Our comprehensive evaluation across eight real-world datasets demonstrates ILS-CSL's superior performance, setting a new standard in CSL efficacy and showcasing its potential to significantly advance the field of causal discovery. The codes are available at \\url{https://github.com/tyMadara/ILS-CSL}.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-20",
    "authors": [
      {
        "authorId": "2137416265",
        "name": "Taiyu Ban"
      },
      {
        "authorId": "2139291923",
        "name": "Lyuzhou Chen"
      },
      {
        "authorId": "2174049321",
        "name": "Derui Lyu"
      },
      {
        "authorId": "2144799330",
        "name": "Xiangyu Wang"
      },
      {
        "authorId": "2145303232",
        "name": "Huanhuan Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "caf60d1120c2d5a894098f01b51d2e2ad32301d7",
    "url": "https://www.semanticscholar.org/paper/caf60d1120c2d5a894098f01b51d2e2ad32301d7",
    "title": "T-Eval: Evaluating the Tool Utilization Capability of Large Language Models Step by Step",
    "abstract": "Large language models (LLM) have achieved remarkable performance on various NLP tasks and are augmented by tools for broader applications. Yet, how to evaluate and analyze the tool-utilization capability of LLMs is still under-explored. In contrast to previous works that evaluate models holistically, we comprehensively decompose the tool utilization into multiple sub-processes, including instruction following, planning, reasoning, retrieval, understanding, and review. Based on that, we further introduce T-Eval to evaluate the tool utilization capability step by step. T-Eval disentangles the tool utilization evaluation into several sub-domains along model capabilities, facilitating the inner understanding of both holistic and isolated competency of LLMs. We conduct extensive experiments on T-Eval and in-depth analysis of various LLMs. T-Eval not only exhibits consistency with the outcome-oriented evaluation but also provides a more fine-grained analysis of the capabilities of LLMs, providing a new perspective in LLM evaluation on tool-utilization ability. The benchmark will be available at https://github.com/open-compass/T-Eval.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 11,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2312.14033",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference",
      "Review"
    ],
    "publicationDate": "2023-12-21",
    "authors": [
      {
        "authorId": "48354360",
        "name": "Zehui Chen"
      },
      {
        "authorId": "2214830561",
        "name": "Weihua Du"
      },
      {
        "authorId": "2266359401",
        "name": "Wenwei Zhang"
      },
      {
        "authorId": "2029335061",
        "name": "Kuikun Liu"
      },
      {
        "authorId": "2275801272",
        "name": "Jiangning Liu"
      },
      {
        "authorId": "2159678046",
        "name": "Miao Zheng"
      },
      {
        "authorId": "2275567484",
        "name": "Jingming Zhuo"
      },
      {
        "authorId": "2266356137",
        "name": "Songyang Zhang"
      },
      {
        "authorId": "2261095726",
        "name": "Dahua Lin"
      },
      {
        "authorId": "2275790072",
        "name": "Kai Chen"
      },
      {
        "authorId": "2275819548",
        "name": "Feng Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "4673c2ac4abb4b055da87171231acb60801ffe74",
    "url": "https://www.semanticscholar.org/paper/4673c2ac4abb4b055da87171231acb60801ffe74",
    "title": "PoseGPT: Chatting about 3D Human Pose",
    "abstract": "We introduce PoseGPT, a framework employing Large Language Models (LLMs) to understand and reason about 3D human poses from images or textual descriptions. Our work is motivated by the human ability to intuitively understand postures from a single image or a brief description, a process that intertwines image interpretation, world knowledge, and an understanding of body language. Traditional human pose estimation methods, whether image-based or text-based, often lack holistic scene comprehension and nuanced reasoning, leading to a disconnect between visual data and its real-world implications. PoseGPT addresses these limitations by embedding SMPL poses as a distinct signal token within a multi-modal LLM, enabling direct generation of 3D body poses from both textual and visual inputs. This approach not only simplifies pose prediction but also empowers LLMs to apply their world knowledge in reasoning about human poses, fostering two advanced tasks: speculative pose generation and reasoning about pose estimation. These tasks involve reasoning about humans to generate 3D poses from subtle text queries, possibly accompanied by images. We establish benchmarks for these tasks, moving beyond traditional 3D pose generation and",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2258812939",
        "name": "Yao Feng"
      },
      {
        "authorId": "2268777779",
        "name": "Jing Lin"
      },
      {
        "authorId": "102609418",
        "name": "Sai Kumar Dwivedi"
      },
      {
        "authorId": "2268801370",
        "name": "Yu Sun"
      },
      {
        "authorId": "2268809728",
        "name": "Priyanka Patel"
      },
      {
        "authorId": "2258720016",
        "name": "Michael J. Black"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "d58ca6c8918777b36bda2969280a4f262d2b835d",
    "url": "https://www.semanticscholar.org/paper/d58ca6c8918777b36bda2969280a4f262d2b835d",
    "title": "ToolTalk: Evaluating Tool-Usage in a Conversational Setting",
    "abstract": "Large language models (LLMs) have displayed massive improvements in reasoning and decision-making skills and can hold natural conversations with users. Many recent works seek to augment LLM-based assistants with external tools so they can access private or up-to-date information and carry out actions on behalf of users. To better measure the performance of these assistants, this paper introduces ToolTalk, a benchmark consisting of complex user intents requiring multi-step tool usage specified through dialogue. ToolTalk contains 28 tools grouped into 7 plugins, and includes a complete simulated implementation of each tool, allowing for fully automated evaluation of assistants that rely on execution feedback. ToolTalk also emphasizes tools that externally affect the world rather than only tools for referencing or searching information. We evaluate GPT-3.5 and GPT-4 on ToolTalk resulting in success rates of 26% and 50% respectively. Our analysis of the errors reveals three major categories and suggests some future directions for improvement. We release ToolTalk at https://github.com/microsoft/ToolTalk.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-15",
    "authors": [
      {
        "authorId": "2267331698",
        "name": "Nicholas Farn"
      },
      {
        "authorId": "2267331701",
        "name": "Richard Shin"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "74c7ead0f14958bbb66ac081507d367bf479bb52",
    "url": "https://www.semanticscholar.org/paper/74c7ead0f14958bbb66ac081507d367bf479bb52",
    "title": "A Task-Decomposed AI-Aided Approach for Generative Conceptual Design",
    "abstract": "\n Generative algorithm-based conceptual design has been innovatively applied as an emerging digital design paradigm for early-stage design ideation. With powerful large language models (LLMs), designers can enter an initial prompt as a design requirement to generate using machine reasoning capability descriptive natural language content. The machine-generated output can be used as stimuli to inspire designers during design ideation. However, the lack of transparency and insufficient controllability of LLMs can limit their effectiveness when assisting humans on a generative conceptual design task. This generation process lacks theoretical guidance and a comprehensive understanding of design requirements, which may potentially lead to generated concepts that are mismatched or lack creativity. Inspired by the Function-Behavior-Structure (FBS) model, this paper proposes a task-decomposed AI-aided approach for generative conceptual design. We decompose a conceptual design task into three sub-tasks including functional reasoning, behavioral reasoning, and structural reasoning. Prompt templates and specification signifiers are specified for different steps to guide the LLMs to generate reasonable results, controllably. The output of each step becomes the input of the next, aiding in aggregating gains per step and embedding the selection preferences of human designers at each stage. A conceptual design experiment is conducted, and the results show that the conceptual design ideation with our method are more reasonable and creative in comparison to a baseline.",
    "venue": "Volume 6: 35th International Conference on Design Theory and Methodology (DTM)",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2023-08-20",
    "authors": [
      {
        "authorId": "2212249013",
        "name": "Boheng Wang"
      },
      {
        "authorId": "2124446242",
        "name": "H. Zuo"
      },
      {
        "authorId": "2267903189",
        "name": "Zebin Cai"
      },
      {
        "authorId": "2136200168",
        "name": "Y. Yin"
      },
      {
        "authorId": "116474882",
        "name": "P. Childs"
      },
      {
        "authorId": "2243794390",
        "name": "Lingyun Sun"
      },
      {
        "authorId": "2173717316",
        "name": "Liuqing Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "ae80c69872d4af5814d9d7dfa771c794a93697d3",
    "url": "https://www.semanticscholar.org/paper/ae80c69872d4af5814d9d7dfa771c794a93697d3",
    "title": "Question Answering as Programming for Solving Time-Sensitive Questions",
    "abstract": "Question answering plays a pivotal role in human daily life because it involves our acquisition of knowledge about the world. However, due to the dynamic and ever-changing nature of real-world facts, the answer can be completely different when the time constraint in the question changes. Recently, Large Language Models (LLMs) have shown remarkable intelligence in question answering, while our experiments reveal that the aforementioned problems still pose a significant challenge to existing LLMs. This can be attributed to the LLMs' inability to perform rigorous reasoning based on surface-level text semantics. To overcome this limitation, rather than requiring LLMs to directly answer the question, we propose a novel approach where we reframe the $\\textbf{Q}$uestion $\\textbf{A}$nswering task $\\textbf{a}$s $\\textbf{P}$rogramming ($\\textbf{QAaP}$). Concretely, by leveraging modern LLMs' superior capability in understanding both natural language and programming language, we endeavor to harness LLMs to represent diversely expressed text as well-structured code and select the best matching answer from multiple candidates through programming. We evaluate our QAaP framework on several time-sensitive question answering datasets and achieve decent improvement, up to $14.5$% over strong baselines. Our codes and data are available at https://github.com/TianHongZXY/qaap",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.14221",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-23",
    "authors": [
      {
        "authorId": "2116314158",
        "name": "Xinyu Zhu"
      },
      {
        "authorId": "3443627",
        "name": "Cheng Yang"
      },
      {
        "authorId": "143876723",
        "name": "B. Chen"
      },
      {
        "authorId": "47319720",
        "name": "Siheng Li"
      },
      {
        "authorId": "153249455",
        "name": "Jian-Guang Lou"
      },
      {
        "authorId": "3001727",
        "name": "Yujiu Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "88a3abf671d922ebd61a34007908a5f6b6978bd4",
    "url": "https://www.semanticscholar.org/paper/88a3abf671d922ebd61a34007908a5f6b6978bd4",
    "title": "Harnessing Large Language Models' Empathetic Response Generation Capabilities for Online Mental Health Counselling Support",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across various information-seeking and reasoning tasks. These computational systems drive state-of-the-art dialogue systems, such as ChatGPT and Bard. They also carry substantial promise in meeting the growing demands of mental health care, albeit relatively unexplored. As such, this study sought to examine LLMs' capability to generate empathetic responses in conversations that emulate those in a mental health counselling setting. We selected five LLMs: version 3.5 and version 4 of the Generative Pre-training (GPT), Vicuna FastChat-T5, Pathways Language Model (PaLM) version 2, and Falcon-7B-Instruct. Based on a simple instructional prompt, these models responded to utterances derived from the EmpatheticDialogues (ED) dataset. Using three empathy-related metrics, we compared their responses to those from traditional response generation dialogue systems, which were fine-tuned on the ED dataset, along with human-generated responses. Notably, we discovered that responses from the LLMs were remarkably more empathetic in most scenarios. We position our findings in light of catapulting advancements in creating empathetic conversational systems.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.08017",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-12",
    "authors": [
      {
        "authorId": "2166028353",
        "name": "Siyuan Brandon Loh"
      },
      {
        "authorId": "1398412111",
        "name": "Aravind Sesagiri Raamkumar"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "3581d76349a569c80f331c92710839974308eb1c",
    "url": "https://www.semanticscholar.org/paper/3581d76349a569c80f331c92710839974308eb1c",
    "title": "Interactive Planning Using Large Language Models for Partially Observable Robotic Tasks",
    "abstract": "Designing robotic agents to perform open vocabulary tasks has been the long-standing goal in robotics and AI. Recently, Large Language Models (LLMs) have achieved impressive results in creating robotic agents for performing open vocabulary tasks. However, planning for these tasks in the presence of uncertainties is challenging as it requires \"chain-of-thought\" reasoning, aggregating information from the environment, updating state estimates, and generating actions based on the updated state estimates. In this paper, we present an interactive planning technique for partially observable tasks using LLMs. In the proposed method, an LLM is used to collect missing information from the environment using a robot, and infer the state of the underlying problem from collected observations while guiding the robot to perform the required actions. We also use a fine-tuned Llama 2 model via self-instruct and compare its performance against a pre-trained LLM like GPT-4. Results are demonstrated on several tasks in simulation as well as real-world environments.",
    "venue": "IEEE International Conference on Robotics and Automation",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-11",
    "authors": [
      {
        "authorId": "3435176",
        "name": "Lingfeng Sun"
      },
      {
        "authorId": "2743474",
        "name": "Devesh K. Jha"
      },
      {
        "authorId": "2258718169",
        "name": "Chiori Hori"
      },
      {
        "authorId": "1741915",
        "name": "Siddarth Jain"
      },
      {
        "authorId": "102147358",
        "name": "Radu Corcodel"
      },
      {
        "authorId": "8362363",
        "name": "Xinghao Zhu"
      },
      {
        "authorId": "2245825283",
        "name": "Masayoshi Tomizuka"
      },
      {
        "authorId": "2777349",
        "name": "Diego Romeres"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.96842909197557
  },
  {
    "paperId": "fb285de78ba95e30ad41ed97ddf014763e6279ad",
    "url": "https://www.semanticscholar.org/paper/fb285de78ba95e30ad41ed97ddf014763e6279ad",
    "title": "TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise",
    "abstract": "Large Language Models (LLMs) exhibit impressive reasoning and data augmentation capabilities in various NLP tasks. However, what about small models? In this work, we propose TeacherLM-7.1B, capable of annotating relevant fundamentals, chain of thought, and common mistakes for most NLP samples, which makes annotation more than just an answer, thus allowing other models to learn\"why\"instead of just\"what\". The TeacherLM-7.1B model achieved a zero-shot score of 52.3 on MMLU, surpassing most models with over 100B parameters. Even more remarkable is its data augmentation ability. Based on TeacherLM-7.1B, we augmented 58 NLP datasets and taught various student models with different parameters from OPT and BLOOM series in a multi-task setting. The experimental results indicate that the data augmentation provided by TeacherLM has brought significant benefits. We will release the TeacherLM series of models and augmented datasets as open-source.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-29",
    "authors": [
      {
        "authorId": "2263542770",
        "name": "Nan He"
      },
      {
        "authorId": "2263428192",
        "name": "Hanyu Lai"
      },
      {
        "authorId": "2263727735",
        "name": "Chenyang Zhao"
      },
      {
        "authorId": "2264751970",
        "name": "Zirui Cheng"
      },
      {
        "authorId": "2279759384",
        "name": "Junting Pan"
      },
      {
        "authorId": "2264205781",
        "name": "Ruoyu Qin"
      },
      {
        "authorId": "2263498649",
        "name": "Ruofan Lu"
      },
      {
        "authorId": "2263498652",
        "name": "Rui Lu"
      },
      {
        "authorId": "2129510271",
        "name": "Yunchen Zhang"
      },
      {
        "authorId": "2264623723",
        "name": "Gangming Zhao"
      },
      {
        "authorId": "2091117562",
        "name": "Zhaohui Hou"
      },
      {
        "authorId": "2263894363",
        "name": "Zhiyuan Huang"
      },
      {
        "authorId": "1418276662",
        "name": "Shaoqing Lu"
      },
      {
        "authorId": "2217098654",
        "name": "Ding Liang"
      },
      {
        "authorId": "2060755203",
        "name": "Mingjie Zhan"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "738e8782ae8c61a05488a1c2d033be10d077e650",
    "url": "https://www.semanticscholar.org/paper/738e8782ae8c61a05488a1c2d033be10d077e650",
    "title": "Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?",
    "abstract": "Large Language Models (LLMs) could enhance access to the legal system. However, empirical research on their effectiveness in conducting legal tasks is scant. We study securities cases involving cryptocurrencies as one of numerous contexts where AI could support the legal process, studying LLMs' legal reasoning and drafting capabilities. We examine whether a) an LLM can accurately determine which laws are potentially being violated from a fact pattern, and b) whether there is a difference in juror decision-making based on complaints written by a lawyer compared to an LLM. We feed fact patterns from real-life cases to GPT-3.5 and evaluate its ability to determine correct potential violations from the scenario and exclude spurious violations. Second, we had mock jurors assess complaints written by the LLM and lawyers. GPT-3.5's legal reasoning skills proved weak, though we expect improvement in future models, particularly given the violations it suggested tended to be correct (it merely missed additional, correct violations). GPT-3.5 performed better at legal drafting, and jurors' decisions were not statistically significantly associated with the author of the document upon which they based their decisions. Because LLMs cannot satisfactorily conduct legal reasoning tasks, they would be unable to replace lawyers at this stage. However, their drafting skills (though, perhaps, still inferior to lawyers), could provide access to justice for more individuals by reducing the cost of legal services. Our research is the first to systematically study LLMs' legal drafting and reasoning capabilities in litigation, as well as in securities law and cryptocurrency-related misconduct.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.06032",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-08-11",
    "authors": [
      {
        "authorId": "2143194136",
        "name": "Arianna Trozze"
      },
      {
        "authorId": "38972934",
        "name": "Toby P Davies"
      },
      {
        "authorId": "6032930",
        "name": "Bennett Kleinberg"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "0f45608ddc01b3e192f3490330f4c4b8de074f79",
    "url": "https://www.semanticscholar.org/paper/0f45608ddc01b3e192f3490330f4c4b8de074f79",
    "title": "Are Human-generated Demonstrations Necessary for In-context Learning?",
    "abstract": "Despite the promising few-shot ability of large language models (LLMs), the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations. In this paper, we raise the fundamental question that whether human-generated demonstrations are necessary for ICL. To answer this question, we propose self-contemplation prompting strategy (SEC), a paradigm free from human-crafted demonstrations. The key point of SEC is that, instead of using hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create demonstrations on their own, based on which the final output is generated. SEC is a flexible framework and can be adapted to both the vanilla ICL and the chain-of-thought (CoT), but with greater ease: as the manual-generation process of both examples and rationale can be saved. Extensive experiments in arithmetic reasoning, commonsense reasoning, multi-task language understanding, and code generation benchmarks, show that SEC, which does not require hand-crafted demonstrations, significantly outperforms the zero-shot learning strategy, and achieves comparable results to ICL with hand-crafted demonstrations. This demonstrates that, for many tasks, contemporary LLMs possess a sufficient level of competence to exclusively depend on their own capacity for decision making, removing the need for external training data. Code is available at https://github.com/ruili33/SEC.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 11,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.14681",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-26",
    "authors": [
      {
        "authorId": "2247421824",
        "name": "Rui Li"
      },
      {
        "authorId": "2220704972",
        "name": "Guoyin Wang"
      },
      {
        "authorId": "2172372802",
        "name": "Jiwei Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "f72b6f2fee42c3f46bc3b7f8f3dbcdc32c38f119",
    "url": "https://www.semanticscholar.org/paper/f72b6f2fee42c3f46bc3b7f8f3dbcdc32c38f119",
    "title": "The perils and promises of fact-checking with large language models",
    "abstract": "Automated fact-checking, using machine learning to verify claims, has grown vital as misinformation spreads beyond human fact-checking capacity. Large language models (LLMs) like GPT-4 are increasingly trusted to write academic papers, lawsuits, and news articles and to verify information, emphasizing their role in discerning truth from falsehood and the importance of being able to verify their outputs. Understanding the capacities and limitations of LLMs in fact-checking tasks is therefore essential for ensuring the health of our information ecosystem. Here, we evaluate the use of LLM agents in fact-checking by having them phrase queries, retrieve contextual data, and make decisions. Importantly, in our framework, agents explain their reasoning and cite the relevant sources from the retrieved context. Our results show the enhanced prowess of LLMs when equipped with contextual information. GPT-4 outperforms GPT-3, but accuracy varies based on query language and claim veracity. While LLMs show promise in fact-checking, caution is essential due to inconsistent accuracy. Our investigation calls for further research, fostering a deeper comprehension of when agents succeed and when they fail.",
    "venue": "Frontiers Artif. Intell.",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "https://www.frontiersin.org/articles/10.3389/frai.2024.1341697/pdf?isPublishedV2=False",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-20",
    "authors": [
      {
        "authorId": "2261085641",
        "name": "Dorian Quelle"
      },
      {
        "authorId": "2261086303",
        "name": "Alexandre Bovet"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "5f70bdf1590c522bd01517b77848824f1fe433d4",
    "url": "https://www.semanticscholar.org/paper/5f70bdf1590c522bd01517b77848824f1fe433d4",
    "title": "Improving Large Language Models in Event Relation Logical Prediction",
    "abstract": "Event relations are crucial for narrative understanding and reasoning. Governed by nuanced logic, event relation extraction (ERE) is a challenging task that demands thorough semantic understanding and rigorous logical reasoning. In this paper, we conduct an in-depth investigation to systematically explore the capability of LLMs in understanding and applying event relation logic. More in detail, we first investigate the deficiencies of LLMs in logical reasoning across different tasks. Our study reveals that LLMs are not logically consistent reasoners, which results in their suboptimal performance on tasks that need rigorous reasoning. To address this, we explore three different approaches to endow LLMs with event relation logic, and thus enable them to generate more coherent answers across various scenarios. Based on our approach, we also contribute a synthesized dataset (LLM-ERL) involving high-order reasoning for evaluation and fine-tuning. Extensive quantitative and qualitative analyses on different tasks also validate the effectiveness of our approaches and provide insights for solving practical tasks with LLMs in future work. Codes are available at https://github.com/chenmeiqii/Teach-LLM-LR.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2310.09158",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-13",
    "authors": [
      {
        "authorId": "2108612706",
        "name": "Meiqi Chen"
      },
      {
        "authorId": "2143557418",
        "name": "Yubo Ma"
      },
      {
        "authorId": "50982078",
        "name": "Kaitao Song"
      },
      {
        "authorId": "2258676539",
        "name": "Yixin Cao"
      },
      {
        "authorId": "2258684069",
        "name": "Yan Zhang"
      },
      {
        "authorId": "2269021512",
        "name": "Dongsheng Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "d1bd7ae97588eccfbcd31ffce4fc924d12a5de4d",
    "url": "https://www.semanticscholar.org/paper/d1bd7ae97588eccfbcd31ffce4fc924d12a5de4d",
    "title": "Knowledge-Prompted Estimator: A Novel Approach to Explainable Machine Translation Assessment",
    "abstract": "Cross-lingual Machine Translation (MT) quality estimation plays a crucial role in evaluating translation performance. GEMBA, the first MT quality assessment metric based on Large Language Models (LLMs), employs one-step prompting to achieve state-of-the-art (SOTA) in system-level MT quality estimation; however, it lacks segment-level analysis. In contrast, Chain-of-Thought (CoT) prompting outperforms one-step prompting by offering improved reasoning and explainability. In this paper, we introduce Knowledge-Prompted Estimator (KPE), a CoT prompting method that combines three one-step prompting techniques, including perplexity, token-level similarity, and sentence-level similarity. This method attains enhanced performance for segment-level estimation compared with previous deep learning models and one-step prompting approaches. Furthermore, supplementary experiments on word-level visualized alignment demonstrate that our KPE method significantly improves token alignment compared with earlier models and provides better interpretability for MT quality estimation. 11Code will be released upon publication.",
    "venue": "International Conference on Advanced Communication Technology",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-06-13",
    "authors": [
      {
        "authorId": "2115537856",
        "name": "Hao Yang"
      },
      {
        "authorId": "40093418",
        "name": "Min Zhang"
      },
      {
        "authorId": "1978838820",
        "name": "Shimin Tao"
      },
      {
        "authorId": "1628331115",
        "name": "Minghan Wang"
      },
      {
        "authorId": "8884457",
        "name": "Daimeng Wei"
      },
      {
        "authorId": "2203256236",
        "name": "Yanfei Jiang"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "d8d6e7ee0d7a58189b1c84dc56c818b0639c92e6",
    "url": "https://www.semanticscholar.org/paper/d8d6e7ee0d7a58189b1c84dc56c818b0639c92e6",
    "title": "Explore, Select, Derive, and Recall: Augmenting LLM with Human-like Memory for Mobile Task Automation",
    "abstract": "The advent of large language models (LLMs) has opened up new opportunities in the field of mobile task automation. Their superior language understanding and reasoning capabilities allow users to automate complex and repetitive tasks. However, due to the inherent unreliability and high operational cost of LLMs, their practical applicability is quite limited. To address these issues, this paper introduces MobileGPT, an innovative LLM-based mobile task automator equipped with a human-like app memory. MobileGPT emulates the cognitive process of humans interacting with a mobile app -- explore, select, derive, and recall. This approach allows for a more precise and efficient learning of a task's procedure by breaking it down into smaller, modular sub-tasks that can be re-used, re-arranged, and adapted for various objectives. We implement MobileGPT using online LLMs services (GPT-3.5 and GPT-4) and evaluate its performance on a dataset of 185 tasks across 18 mobile apps. The results indicate that MobileGPT can automate and learn new tasks with 82.7% accuracy, and is able to adapt them to different contexts with near perfect (98.75%) accuracy while reducing both latency and cost by 62.5% and 68.8%, respectively, compared to the GPT-4 powered baseline.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-04",
    "authors": [
      {
        "authorId": "2116984601",
        "name": "Sunjae Lee"
      },
      {
        "authorId": "2270121195",
        "name": "Junyoung Choi"
      },
      {
        "authorId": "2270687003",
        "name": "Jungjae Lee"
      },
      {
        "authorId": "2269820915",
        "name": "Hojun Choi"
      },
      {
        "authorId": "2270028057",
        "name": "Steven Y. Ko"
      },
      {
        "authorId": "152275370",
        "name": "Sangeun Oh"
      },
      {
        "authorId": "2270435401",
        "name": "Insik Shin"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "9618ac49822a8d0d9924b6e7d9a0bb0847649fdb",
    "url": "https://www.semanticscholar.org/paper/9618ac49822a8d0d9924b6e7d9a0bb0847649fdb",
    "title": "CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models",
    "abstract": "With the emergence of Large Language Models (LLMs), there has been a significant improvement in the programming capabilities of models, attracting growing attention from researchers. Evaluating the programming capabilities of LLMs is crucial as it reflects the multifaceted abilities of LLMs, and it has numerous downstream applications. In this paper, we propose CodeApex, a bilingual benchmark dataset focusing on the programming comprehension, code generation, and code correction abilities of LLMs. Programming comprehension task tests LLMs on multiple-choice exam questions covering conceptual understanding, commonsense reasoning, and multi-hop reasoning. The code generation task evaluates LLMs through completing C++ functions based on provided descriptions and prototypes. The code correction task asks LLMs to fix real-world erroneous code segments with different error messages. We evaluate 12 widely used LLMs, including both general-purpose and specialized models. GPT-4 exhibits the best programming capabilities, achieving approximate accuracy of 69%, 54%, and 66% on the three tasks, respectively. Compared to human performance, there is still significant room for improvement in LLM programming. We hope that CodeApex can serve as a reference for evaluating the coding capabilities of LLMs, further promoting their development and growth.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 9,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.01940",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-05",
    "authors": [
      {
        "authorId": "2171109846",
        "name": "Lingyue Fu"
      },
      {
        "authorId": "2220553287",
        "name": "Huacan Chai"
      },
      {
        "authorId": "2237835901",
        "name": "Shuang Luo"
      },
      {
        "authorId": "1780721965",
        "name": "Kounianhua Du"
      },
      {
        "authorId": "2237819504",
        "name": "Weiming Zhang"
      },
      {
        "authorId": "2238534808",
        "name": "Longteng Fan"
      },
      {
        "authorId": "2237803993",
        "name": "Jiayi Lei"
      },
      {
        "authorId": "2153850099",
        "name": "Renting Rui"
      },
      {
        "authorId": "2144908858",
        "name": "Jianghao Lin"
      },
      {
        "authorId": "2256784173",
        "name": "Yuchen Fang"
      },
      {
        "authorId": "2237947577",
        "name": "Yifan Liu"
      },
      {
        "authorId": "2237943909",
        "name": "Jingkuan Wang"
      },
      {
        "authorId": "2237801991",
        "name": "Siyuan Qi"
      },
      {
        "authorId": "1698918106",
        "name": "Kangning Zhang"
      },
      {
        "authorId": "2129458791",
        "name": "Weinan Zhang"
      },
      {
        "authorId": "2237958078",
        "name": "Yong Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "0266171daa51dcc2c9b12b2c344a28aa8086ca36",
    "url": "https://www.semanticscholar.org/paper/0266171daa51dcc2c9b12b2c344a28aa8086ca36",
    "title": "Large Language Models Need Symbolic AI",
    "abstract": "The capability of systems based on large language models (LLMs), such as ChatGPT, to generate humanlike text has captured the attention of the public and the scientific community. It has prompted both predictions that systems such as ChatGPT will transform AI and enumerations of system problems with hopes of solving them by scale and training. This position paper argues that both over-optimistic views and disppointments reflect misconceptions of the fundamental nature of LLMs as language models. As such, they are statistical models of language production and fluency, with associated strengths and limitations; they are not—and should not be expected to be—knowledge models of the world, nor do they reflect the core role of language beyond the statistics: communication. The paper argues that realizing that role will require driving LLMs with symbolic systems based on goals, facts, reasoning, and memory.",
    "venue": "International Workshop on Neural-Symbolic Learning and Reasoning",
    "year": 2023,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2263239",
        "name": "K. Hammond"
      },
      {
        "authorId": "70026051",
        "name": "David B. Leake"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "3120c2763edab339b937ddbe76991ebdfe0e01e6",
    "url": "https://www.semanticscholar.org/paper/3120c2763edab339b937ddbe76991ebdfe0e01e6",
    "title": "Automatic Data Transformation Using Large Language Model - An Experimental Study on Building Energy Data",
    "abstract": "Existing approaches to automatic data transformation are insufficient to meet the requirements in many real-world scenarios, such as the building sector. First, there is no convenient interface for domain experts to provide domain knowledge easily. Second, they require significant training data collection overheads. Third, the accuracy suffers from complicated schema changes. To address these shortcomings, we present a novel approach that leverages the unique capabilities of large language models (LLMs) in coding, complex reasoning, and zero-shot learning to generate SQL code that transforms the source datasets into the target datasets. We demonstrate the viability of this approach by designing an LLM-based framework, termed SQLMorpher, which comprises a prompt generator that integrates the initial prompt with optional domain knowledge and historical patterns in external databases. It also implements an iterative prompt optimization mechanism that automatically improves the prompt based on flaw detection. The key contributions of this work include (1) pioneering an end-to-end LLM-based solution for data transformation, (2) developing a benchmark dataset of 105 real-world building energy data transformation problems, and (3) conducting an extensive empirical evaluation where our approach achieved 96% accuracy in all 105 problems. SQLMorpher demonstrates the effectiveness of utilizing LLMs in complex, domain-specific challenges, highlighting the potential of their potential to drive sustainable solutions.",
    "venue": "BigData Congress [Services Society]",
    "year": 2023,
    "citationCount": 9,
    "openAccessPdf": {
      "url": "https://sdm.lbl.gov/oapapers/bigdata23-sharma.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-09-05",
    "authors": [
      {
        "authorId": "2237952927",
        "name": "Ankita Sharma"
      },
      {
        "authorId": "2237949801",
        "name": "Xuanmao Li"
      },
      {
        "authorId": "2237800042",
        "name": "Hong Guan"
      },
      {
        "authorId": "2237955180",
        "name": "Guoxin Sun"
      },
      {
        "authorId": "2267030904",
        "name": "Liang Zhang"
      },
      {
        "authorId": "2237944344",
        "name": "Lanjun Wang"
      },
      {
        "authorId": "2238407326",
        "name": "Kesheng Wu"
      },
      {
        "authorId": "2237951882",
        "name": "Lei Cao"
      },
      {
        "authorId": "2237798544",
        "name": "Erkang Zhu"
      },
      {
        "authorId": "2257305506",
        "name": "Alex Sim"
      },
      {
        "authorId": "2262086390",
        "name": "Teresa Wu"
      },
      {
        "authorId": "2237786723",
        "name": "Jia Zou"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "5b7dcf469c285381a83b1bfd85d2b22da121e0cc",
    "url": "https://www.semanticscholar.org/paper/5b7dcf469c285381a83b1bfd85d2b22da121e0cc",
    "title": "Ruffle&Riley: Towards the Automated Induction of Conversational Tutoring Systems",
    "abstract": "Conversational tutoring systems (CTSs) offer learning experiences driven by natural language interaction. They are known to promote high levels of cognitive engagement and benefit learning outcomes, particularly in reasoning tasks. Nonetheless, the time and cost required to author CTS content is a major obstacle to widespread adoption. In this paper, we introduce a novel type of CTS that leverages the recent advances in large language models (LLMs) in two ways: First, the system induces a tutoring script automatically from a lesson text. Second, the system automates the script orchestration via two LLM-based agents (Ruffle&Riley) with the roles of a student and a professor in a learning-by-teaching format. The system allows a free-form conversation that follows the ITS-typical inner and outer loop structure. In an initial between-subject online user study (N = 100) comparing Ruffle&Riley to simpler QA chatbots and reading activity, we found no significant differences in post-test scores. Nonetheless, in the learning experience survey, Ruffle&Riley users expressed higher ratings of understanding and remembering and further perceived the offered support as more helpful and the conversation as coherent. Our study provides insights for a new generation of scalable CTS technologies.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 9,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.01420",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-09-26",
    "authors": [
      {
        "authorId": "2238003671",
        "name": "Robin Schmucker"
      },
      {
        "authorId": "2253471421",
        "name": "Meng Xia"
      },
      {
        "authorId": "1746466",
        "name": "A. Azaria"
      },
      {
        "authorId": "2253469788",
        "name": "Tom Mitchell"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "405c420a5c5753d0d7168e67809b67096c862a0a",
    "url": "https://www.semanticscholar.org/paper/405c420a5c5753d0d7168e67809b67096c862a0a",
    "title": "Large Language Models and Medical Education: Preparing for a Rapid Transformation in How Trainees Will Learn to Be Doctors",
    "abstract": "Artificial intelligence has the potential to revolutionize health care but has yet to be widely implemented. In part, this may be because, to date, we have focused on easily predicted rather than easily actionable problems. Large language models (LLMs) represent a paradigm shift in our approach to artificial intelligence because they are easily accessible and already being tested by frontline clinicians, who are rapidly identifying possible use cases. LLMs in health care have the potential to reduce clerical work, bridge gaps in patient education, and more. As we enter this era of healthcare delivery, LLMs will present both opportunities and challenges in medical education. Future models should be developed to support trainees to develop skills in clinical reasoning, encourage evidence-based medicine, and offer case-based training opportunities. LLMs may also change what we continue teaching trainees with regard to clinical documentation. Finally, trainees can help us train and develop the LLMs of the future as we consider the best ways to incorporate LLMs into medical education. Ready or not, LLMs will soon be integrated into various aspects of clinical practice, and we must work closely with students and educators to make sure these models are also built with trainees in mind to responsibly chaperone medical education into the next era.",
    "venue": "ATS Scholar",
    "year": 2023,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-14",
    "authors": [
      {
        "authorId": "2197721591",
        "name": "Akshay Ravi"
      },
      {
        "authorId": "3462013",
        "name": "Aaron B. Neinstein"
      },
      {
        "authorId": "31044803",
        "name": "Sara G. Murray"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "fda1de315c319d170db981caf0e00f44c947b295",
    "url": "https://www.semanticscholar.org/paper/fda1de315c319d170db981caf0e00f44c947b295",
    "title": "Reasoning Tuning Grasp: Adapting Multi-Modal Large Language Models for Robotic Grasping",
    "abstract": ": Large language models (LLMs) have garnered increasing popularity owing to their remarkable reasoning capabilities. However, their primary utility within the field of robotics has predominantly been constrained to tasks related to manipulation planning, primarily due to their inherent text-based outputs. To overcome this limitation, this paper explores the potential of LLMs in the realm of numerical predictions in robotics, with a specific focus on the task of robotic grasping. We propose Reasoning Tuning, a novel approach that harnesses the extensive prior knowledge embedded within LLMs, optimizing them for tasks involving numerical prediction. This method empowers LLMs, notably with multi-modal capabilities, to generate precise numerical outputs, such as grasp poses for robot arms. The proposed method is extensively validated on the grasping benchmark and real-world grasping experiments, demonstrating that multi-modal LLMs can be adapted for numerical prediction tasks in robotics. This not only extends their applicability but also bridges the gap between text-based planning and direct robot control utilizing LLMs. More details and videos of this work are available on our project page: https://sites.google.com/view/rt-grasp.",
    "venue": "",
    "year": null,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2283877765",
        "name": "Jinxuan Xu"
      },
      {
        "authorId": "2295183211",
        "name": "‡. ShiyuJin"
      },
      {
        "authorId": "2284032009",
        "name": "Yutian Lei"
      },
      {
        "authorId": "2295142710",
        "name": "Yuqian Zhang"
      },
      {
        "authorId": "2211183718",
        "name": "Liangjun Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 70.79441541679836
  },
  {
    "paperId": "6974eeca4aa3e2fc904364ac7b3eca9cbdf9ca7c",
    "url": "https://www.semanticscholar.org/paper/6974eeca4aa3e2fc904364ac7b3eca9cbdf9ca7c",
    "title": "Instructed Language Models with Retrievers Are Powerful Entity Linkers",
    "abstract": "Generative approaches powered by large language models (LLMs) have demonstrated emergent abilities in tasks that require complex reasoning abilities. Yet the generative nature still makes the generated content suffer from hallucinations, thus unsuitable for entity-centric tasks like entity linking (EL) requiring precise entity predictions over a large knowledge base. We present Instructed Generative Entity Linker (INSGENEL), the first approach that enables casual language models to perform entity linking over knowledge bases. Several methods to equip language models with EL capability were proposed in this work, including (i) a sequence-to-sequence training EL objective with instruction-tuning, (ii) a novel generative EL framework based on a light-weight potential mention retriever that frees the model from heavy and non-parallelizable decoding, achieving 4$\\times$ speedup without compromise on linking metrics. INSGENEL outperforms previous generative alternatives with +6.8 F1 points gain on average, also with a huge advantage in training data efficiency and training compute consumption. In addition, our skillfully engineered in-context learning (ICL) framework for EL still lags behind INSGENEL significantly, reaffirming that the EL task remains a persistent hurdle for general LLMs.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-06",
    "authors": [
      {
        "authorId": "2186608582",
        "name": "Zilin Xiao"
      },
      {
        "authorId": "50175330",
        "name": "Ming Gong"
      },
      {
        "authorId": "2265516958",
        "name": "Jie Wu"
      },
      {
        "authorId": "2265515206",
        "name": "Xingyao Zhang"
      },
      {
        "authorId": "24962156",
        "name": "Linjun Shou"
      },
      {
        "authorId": "2247751671",
        "name": "Jian Pei"
      },
      {
        "authorId": "71790825",
        "name": "Daxin Jiang"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "e9f389bc1ccf32bc0ed3c0d5925c9837dfbf9bd5",
    "url": "https://www.semanticscholar.org/paper/e9f389bc1ccf32bc0ed3c0d5925c9837dfbf9bd5",
    "title": "Large Language Models for Autonomous Driving: Real-World Experiments",
    "abstract": "—Autonomous driving systems are increasingly popular in today’s technological landscape, where vehicles with partial automation have already been widely available on the market, and the full automation era with “driverless” capabilities is near the horizon. However, accurately understanding humans’ commands, particularly for autonomous vehicles that have only passengers instead of drivers, and achieving a high level of personalization remain challenging tasks in the development of autonomous driving systems. In this paper, we introduce a Large Language Model (LLM)-based framework Talk-to-Drive (Talk2Drive) to process verbal commands from humans and make autonomous driving decisions with contextual information, satisfying their personalized preferences for safety, efficiency, and comfort. First, a speech recognition module is developed for Talk2Drive to interpret verbal inputs from humans to textual instructions, which are then sent to LLMs for reasoning. Then, appropriate commands for the Electrical Control Unit (ECU) are generated, achieving a 100% success rate in executing codes. Real-world experiments show that our framework can substantially reduce the takeover rate for a diverse range of drivers by up to 90.1%. To the best of our knowledge, Talk2Drive marks the first instance of employing an LLM-based system in a real-world autonomous driving environment, and the experiment videos are available on https://youtube.com/",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2242949023",
        "name": "Can Cui"
      },
      {
        "authorId": "2267506629",
        "name": "Zichong Yang"
      },
      {
        "authorId": "2275030995",
        "name": "Yupeng Zhou"
      },
      {
        "authorId": "2109267955",
        "name": "Yunsheng Ma"
      },
      {
        "authorId": "2053819036",
        "name": "Juanwu Lu"
      },
      {
        "authorId": "2243360882",
        "name": "Ziran Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "d357a811d8ee0e2537c2987564df5c13d59ec02d",
    "url": "https://www.semanticscholar.org/paper/d357a811d8ee0e2537c2987564df5c13d59ec02d",
    "title": "RecExplainer: Aligning Large Language Models for Recommendation Model Interpretability",
    "abstract": "Recommender systems are widely used in various online services, with embedding-based models being particularly popular due to their expressiveness in representing complex signals. However, these models often lack interpretability, making them less reliable and transparent for both users and developers. With the emergence of large language models (LLMs), we find that their capabilities in language expression, knowledge-aware reasoning, and instruction following are exceptionally powerful. Based on this, we propose a new model interpretation approach for recommender systems, by using LLMs as surrogate models and learn to mimic and comprehend target recommender models. Specifically, we introduce three alignment methods: behavior alignment, intention alignment, and hybrid alignment. Behavior alignment operates in the language space, representing user preferences and item information as text to learn the recommendation model’s behavior; intention alignment works in the latent space of the recommendation model, using user and item representations to understand the model’s behavior; hybrid alignment combines both language and latent spaces for alignment training. To demonstrate the effectiveness of our methods, we conduct evaluation from two perspectives: alignment effect, and explanation generation ability on three public datasets. Experimental results indicate that our approach effectively enables LLMs to comprehend the patterns of recommendation models and generate highly credible recommendation explanations.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2226475380",
        "name": "Yuxuan Lei"
      },
      {
        "authorId": "2813328",
        "name": "Jianxun Lian"
      },
      {
        "authorId": "2237129499",
        "name": "Jing Yao"
      },
      {
        "authorId": "2236672137",
        "name": "Xu Huang"
      },
      {
        "authorId": "1862782",
        "name": "Defu Lian"
      },
      {
        "authorId": "2261275112",
        "name": "Xing Xie"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "76a694acec51f6f4db5d72a6c5c66fead6b90f8d",
    "url": "https://www.semanticscholar.org/paper/76a694acec51f6f4db5d72a6c5c66fead6b90f8d",
    "title": "Potential Benefits of Employing Large Language Models in Research in Moral Education and Development",
    "abstract": "Recently, computer scientists have developed large language models (LLMs) by training prediction models with large-scale language corpora and human reinforcements. The LLMs have become one promising way to implement artificial intelligence with accuracy in various fields. Interestingly, recent LLMs possess emergent functional features that emulate sophisticated human cognition, especially in-context learning and the chain of thought, which were unavailable in previous prediction models. In this paper, I will examine how LLMs might contribute to moral education and development research. To achieve this goal, I will review the most recently published conference papers and ArXiv preprints to overview the novel functional features implemented in LLMs. I also intend to conduct brief experiments with ChatGPT to investigate how LLMs behave while addressing ethical dilemmas and external feedback. The results suggest that LLMs might be capable of solving dilemmas based on reasoning and revising their reasoning process with external input. Furthermore, a preliminary experimental result from the moral exemplar test may demonstrate that exemplary stories can elicit moral elevation in LLMs as do they among human participants. I will discuss the potential implications of LLMs on research on moral education and development with the results.",
    "venue": "Journal of Moral Education",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.13805",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-06-23",
    "authors": [
      {
        "authorId": "145520180",
        "name": "Hyemin Han"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "2c3890b566c37927ac70eedfa3e6aecb79b13832",
    "url": "https://www.semanticscholar.org/paper/2c3890b566c37927ac70eedfa3e6aecb79b13832",
    "title": "Probing the Moral Development of Large Language Models through Defining Issues Test",
    "abstract": "In this study, we measure the moral reasoning ability of LLMs using the Defining Issues Test - a psychometric instrument developed for measuring the moral development stage of a person according to the Kohlberg's Cognitive Moral Development Model. DIT uses moral dilemmas followed by a set of ethical considerations that the respondent has to judge for importance in resolving the dilemma, and then rank-order them by importance. A moral development stage score of the respondent is then computed based on the relevance rating and ranking. Our study shows that early LLMs such as GPT-3 exhibit a moral reasoning ability no better than that of a random baseline, while ChatGPT, Llama2-Chat, PaLM-2 and GPT-4 show significantly better performance on this task, comparable to adult humans. GPT-4, in fact, has the highest post-conventional moral reasoning score, equivalent to that of typical graduate school students. However, we also observe that the models do not perform consistently across all dilemmas, pointing to important gaps in their understanding and reasoning abilities.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.13356",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-23",
    "authors": [
      {
        "authorId": "123587511",
        "name": "K. Tanmay"
      },
      {
        "authorId": "47000634",
        "name": "Aditi Khandelwal"
      },
      {
        "authorId": "2257041053",
        "name": "Utkarsh Agarwal"
      },
      {
        "authorId": "143990839",
        "name": "M. Choudhury"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "576ff9e0e5f77750ec2c882b7097ebe3a9873d07",
    "url": "https://www.semanticscholar.org/paper/576ff9e0e5f77750ec2c882b7097ebe3a9873d07",
    "title": "Large Language Models Like ChatGPT in ABME: Author Guidelines",
    "abstract": null,
    "venue": "Annals of Biomedical Engineering",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "https://link.springer.com/content/pdf/10.1007/s10439-023-03212-2.pdf",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "Editorial",
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-04-24",
    "authors": [
      {
        "authorId": "38362468",
        "name": "C. Norris"
      }
    ],
    "source": "semantic_scholar",
    "score": 82.95836866004329
  },
  {
    "paperId": "a2e22e78f9b21ace1b04aba78a8c9ee6cf1c806c",
    "url": "https://www.semanticscholar.org/paper/a2e22e78f9b21ace1b04aba78a8c9ee6cf1c806c",
    "title": "Gracenote.ai: Legal Generative AI for Regulatory Compliance",
    "abstract": ": We investigate the transformative potential of large language models (LLMs) in the legal and regulatory compliance domain by developing advanced generative AI solutions, including a horizon scanning tool, an obligations generation tool, and an LLM-based expert system. Our approach combines the LangChain framework, OpenAI’s GPT-4, text embeddings, and prompt engineering techniques to effectively reduce hallucinations and generate reliable and accurate domain-specific outputs. A human-in-the-loop control mechanism is used as a final backstop to ensure accuracy and mitigate risk. Our findings emphasise the role of LLMs as foundation engines in specialist tools and lay the groundwork for building the next generation of legal and compliance applications. Future research will focus on extending support across multiple jurisdictions and languages, refining prompts and text embedding datasets for enhanced legal reasoning capabilities, and developing autonomous AI agents and robust LLM-based expert systems.",
    "venue": "LegalAIIA@ICAIL",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2222582154",
        "name": "Jules Ioannidis"
      },
      {
        "authorId": "2222573397",
        "name": "Joshua Harper"
      },
      {
        "authorId": "46464708",
        "name": "Ming-Sheng Quah"
      },
      {
        "authorId": "2082317611",
        "name": "Danita Hunter"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "4ade9cb9f75236679029b6fd60d71c3bf513eedb",
    "url": "https://www.semanticscholar.org/paper/4ade9cb9f75236679029b6fd60d71c3bf513eedb",
    "title": "CoF-CoT: Enhancing Large Language Models with Coarse-to-Fine Chain-of-Thought Prompting for Multi-domain NLU Tasks",
    "abstract": "While Chain-of-Thought prompting is popular in reasoning tasks, its application to Large Language Models (LLMs) in Natural Language Understanding (NLU) is under-explored. Motivated by multi-step reasoning of LLMs, we propose Coarse-to-Fine Chain-of-Thought (CoF-CoT) approach that breaks down NLU tasks into multiple reasoning steps where LLMs can learn to acquire and leverage essential concepts to solve tasks from different granularities. Moreover, we propose leveraging semantic-based Abstract Meaning Representation (AMR) structured knowledge as an intermediate step to capture the nuances and diverse structures of utterances, and to understand connections between their varying levels of granularity. Our proposed approach is demonstrated effective in assisting the LLMs adapt to the multi-grained NLU tasks under both zero-shot and few-shot multi-domain settings.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-23",
    "authors": [
      {
        "authorId": "145108247",
        "name": "Hoang Nguyen"
      },
      {
        "authorId": "2261395934",
        "name": "Ye Liu"
      },
      {
        "authorId": "2418496",
        "name": "Chenwei Zhang"
      },
      {
        "authorId": "2265660585",
        "name": "Tao Zhang"
      },
      {
        "authorId": "2261463957",
        "name": "Philip S. Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "2020e6ecaddd06d991421a827207e8992474e701",
    "url": "https://www.semanticscholar.org/paper/2020e6ecaddd06d991421a827207e8992474e701",
    "title": "Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models",
    "abstract": "Audio-visual large language models (LLM) have drawn significant attention, yet the fine-grained combination of both input streams is rather under-explored, which is challenging but necessary for LLMs to understand general video inputs. To this end, a fine-grained audio-visual joint representation (FAVOR) learning framework for multimodal LLMs is proposed in this paper, which extends a text-based LLM to simultaneously perceive speech and audio events in the audio input stream and images or videos in the visual input stream, at the frame level. To fuse the audio and visual feature streams into joint representations and to align the joint space with the LLM input embedding space, we propose a causal Q-Former structure with a causal attention module to enhance the capture of causal relations of the audio-visual frames across time. An audio-visual evaluation benchmark (AVEB) is also proposed which comprises six representative single-modal tasks with five cross-modal tasks reflecting audio-visual co-reasoning abilities. While achieving competitive single-modal performance on audio, speech and image tasks in AVEB, FAVOR achieved over 20% accuracy improvements on the video question-answering task when fine-grained information or temporal causal reasoning is required. FAVOR, in addition, demonstrated remarkable video comprehension and reasoning abilities on tasks that are unprecedented by other multimodal LLMs. An interactive demo of FAVOR is available at https://github.com/BriansIDP/AudioVisualLLM.git, and the training code and model checkpoints will be released soon.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.05863",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-09",
    "authors": [
      {
        "authorId": "2107310187",
        "name": "Guangzhi Sun"
      },
      {
        "authorId": "2257283478",
        "name": "Wenyi Yu"
      },
      {
        "authorId": "2247237695",
        "name": "Changli Tang"
      },
      {
        "authorId": "2135092817",
        "name": "Xianzhao Chen"
      },
      {
        "authorId": "2257003951",
        "name": "Tian Tan"
      },
      {
        "authorId": "2256598801",
        "name": "Wei Li"
      },
      {
        "authorId": "2257383962",
        "name": "Lu Lu"
      },
      {
        "authorId": "2257135061",
        "name": "Zejun Ma"
      },
      {
        "authorId": "2256775692",
        "name": "Chao Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "69581fa1f7eacad35d8528c7f8527ed27a6f94c1",
    "url": "https://www.semanticscholar.org/paper/69581fa1f7eacad35d8528c7f8527ed27a6f94c1",
    "title": "Assessing Logical Puzzle Solving in Large Language Models: Insights from a Minesweeper Case Study",
    "abstract": "Large Language Models (LLMs) have shown remarkable proficiency in language understanding and have been successfully applied to a variety of real-world tasks through task-specific fine-tuning or prompt engineering. Despite these advancements, it remains an open question whether LLMs are fundamentally capable of reasoning and planning, or if they primarily rely on recalling and synthesizing information from their training data. In our research, we introduce a novel task—Minesweeper—specifically designed in a format unfamiliar to LLMs and absent from their training datasets. This task challenges LLMs to identify the locations of mines based on numerical clues provided by adjacent opened cells. Successfully completing this task requires an understanding of each cell’s state, discerning spatial relationships between the clues and mines, and strategizing actions based on logical deductions drawn from the arrangement of the cells. Our experiments, including trials with the advanced GPT-4 model, indicate that while LLMs possess the foundational abilities required for this task, they struggle to integrate these into a coherent, multi-step logical reasoning process needed to solve Minesweeper. These findings highlight the need for further research to understand the nature of reasoning capabilities in LLMs under similar circumstances, and to explore pathways towards more sophisticated AI reasoning and planning models.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2311.07387",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-13",
    "authors": [
      {
        "authorId": "1527089853",
        "name": "Yinghao Li"
      },
      {
        "authorId": "2266420540",
        "name": "Haorui Wang"
      },
      {
        "authorId": "2267156626",
        "name": "Chao Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "aec546a0a4b32593c3338d9e2626c1c886105576",
    "url": "https://www.semanticscholar.org/paper/aec546a0a4b32593c3338d9e2626c1c886105576",
    "title": "Demonstrating Large Language Models on Robots",
    "abstract": "—Robots may benefit from large language models (LLMs), which have demonstrated strong reasoning capabilities across various domains. This demonstration includes several systems based on recent methods that integrate LLMs on robots: SayCan [1], Socratic Models [49], Inner Monologue [19]",
    "venue": "Robotics: Science and Systems",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "https://doi.org/10.15607/rss.2023.xix.024",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-10",
    "authors": [
      {
        "authorId": "2267246149",
        "name": "Andy Zeng"
      },
      {
        "authorId": "2704814",
        "name": "Brian Ichter"
      },
      {
        "authorId": "2286089520",
        "name": "Fei Xia"
      },
      {
        "authorId": "9961095",
        "name": "Ted Xiao"
      },
      {
        "authorId": "1808676",
        "name": "Vikas Sindhwani"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "4ebf49a7c053bf1d22fcce17bc8c80db827e8f99",
    "url": "https://www.semanticscholar.org/paper/4ebf49a7c053bf1d22fcce17bc8c80db827e8f99",
    "title": "Leveraging Commonsense Knowledge from Large Language Models for Task and Motion Planning",
    "abstract": "—Multi-object rearrangement is a crucial skill for service robots, and commonsense reasoning is frequently needed in this process. However, achieving commonsense arrangements requires knowledge about objects, which is hard to transfer to robots. Large language models (LLMs) are one potential source of this knowledge, but they do not naively capture information about plausible physical arrangements of the world. We propose LLM-GROP, which uses prompting to extract commonsense knowledge about semantically valid object configurations from an LLM and instantiates them with a task and motion planner in order to generalize to varying scene geometry. LLM-GROP allows us to go from natural-language commands to human-aligned object rearrangement in varied environments. Based on human evaluations, our approach achieves the highest rating while outperforming competitive baselines in terms of success rate while maintaining comparable cumulative action costs. Finally, we demonstrate a practical implementation of LLM-GROP on a mobile manipulator in real-world scenarios. Supplementary materials are available at: https://sites.google.com/view/llm-grop",
    "venue": "",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2110668031",
        "name": "Yan Ding"
      },
      {
        "authorId": "2682457",
        "name": "Xiaohan Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "c059d251d8f0c2491892271eb40ea3cec4aea830",
    "url": "https://www.semanticscholar.org/paper/c059d251d8f0c2491892271eb40ea3cec4aea830",
    "title": "SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue in Multiple Domains",
    "abstract": "Task-oriented dialogue (TOD) models have great progress in the past few years. However, these studies primarily focus on datasets written by annotators, which has resulted in a gap between academic research and more realistic spoken conversation scenarios. While a few small-scale spoken TOD datasets are proposed to address robustness issues, e.g., ASR errors, they fail to identify the unique challenges in spoken conversation. To tackle the limitations, we introduce SpokenWOZ , a large-scale speech-text dataset for spoken TOD, which consists of 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from human-to-human spoken conversations. Spo-kenWOZ incorporates common spoken characteristics such as word-by-word processing and commonsense reasoning. We also present cross-turn slot and reasoning slot detection as new challenges based on the spoken linguistic phenomena. We conduct comprehensive experiments on various models, including text-modal baselines, newly proposed dual-modal baselines and LLMs. The results show the current models still has substantial areas for improvement in spoken conversation, including ﬁne-tuned models and LLMs, i.e., ChatGPT.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2305.13040",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2053739525",
        "name": "Shuzheng Si"
      },
      {
        "authorId": "145910725",
        "name": "Wentao Ma"
      },
      {
        "authorId": "2142637404",
        "name": "Yuchuan Wu"
      },
      {
        "authorId": "30087809",
        "name": "Yinpei Dai"
      },
      {
        "authorId": "2191085441",
        "name": "Haoyu Gao"
      },
      {
        "authorId": "31255666",
        "name": "Ting-En Lin"
      },
      {
        "authorId": "2118386175",
        "name": "Hangyu Li"
      },
      {
        "authorId": "2055863231",
        "name": "Rui Yan"
      },
      {
        "authorId": "2087380523",
        "name": "Fei Huang"
      },
      {
        "authorId": "1527090216",
        "name": "Yongbin Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "618052378a39fbed35d615ad70c618e6fe37e295",
    "url": "https://www.semanticscholar.org/paper/618052378a39fbed35d615ad70c618e6fe37e295",
    "title": "LLM4Causal: Democratized Causal Tools for Everyone via Large Language Model",
    "abstract": "Large Language Models (LLMs) have shown their success in language understanding and reasoning on general topics. However, their capability to perform inference based on user-specified structured data and knowledge in corpus-rare concepts, such as causal decision-making is still limited. In this work, we explore the possibility of fine-tuning an open-sourced LLM into LLM4Causal, which can identify the causal task, execute a corresponding function, and interpret its numerical results based on users' queries and the provided dataset. Meanwhile, we propose a data generation process for more controllable GPT prompting and present two instruction-tuning datasets: (1) Causal-Retrieval-Bench for causal problem identification and input parameter extraction for causal function calling and (2) Causal-Interpret-Bench for in-context causal interpretation. By conducting end-to-end evaluations and two ablation studies, we showed that LLM4Causal can deliver end-to-end solutions for causal problems and provide easy-to-understand answers, which significantly outperforms the baselines.",
    "venue": "",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-12-28",
    "authors": [
      {
        "authorId": "2276773402",
        "name": "Haitao Jiang"
      },
      {
        "authorId": "2147328352",
        "name": "Linjuan Ge"
      },
      {
        "authorId": "2199245566",
        "name": "Yuhe Gao"
      },
      {
        "authorId": "2142973581",
        "name": "Jianian Wang"
      },
      {
        "authorId": "2276612888",
        "name": "Rui Song"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "663d141065f704cc03715a22f0141562b916e805",
    "url": "https://www.semanticscholar.org/paper/663d141065f704cc03715a22f0141562b916e805",
    "title": "Using Augmented Small Multimodal Models to Guide Large Language Models for Multimodal Relation Extraction",
    "abstract": "Multimodal Relation Extraction (MRE) is a core task for constructing Multimodal Knowledge images (MKGs). Most current research is based on fine-tuning small-scale single-modal image and text pre-trained models, but we find that image-text datasets from network media suffer from data scarcity, simple text data, and abstract image information, which requires a lot of external knowledge for supplementation and reasoning. We use Multimodal Relation Data augmentation (MRDA) to address the data scarcity problem in MRE, and propose a Flexible Threshold Loss (FTL) to handle the imbalanced entity pair distribution and long-tailed classes. After obtaining prompt information from the small model as a guide model, we employ a Large Language Model (LLM) as a knowledge engine to acquire common sense and reasoning abilities. Notably, both stages of our framework are flexibly replaceable, with the first stage adapting to multimodal related classification tasks for small models, and the second stage replaceable by more powerful LLMs. Through experiments, our EMRE2llm model framework achieves state-of-the-art performance on the challenging MNRE dataset, reaching an 82.95% F1 score on the test set.",
    "venue": "Applied Sciences",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "https://www.mdpi.com/2076-3417/13/22/12208/pdf?version=1699607552",
      "status": "GOLD"
    },
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-10",
    "authors": [
      {
        "authorId": "2266386376",
        "name": "Wentao He"
      },
      {
        "authorId": "3241479",
        "name": "Hanjie Ma"
      },
      {
        "authorId": "2266420662",
        "name": "Shaohua Li"
      },
      {
        "authorId": "2266332567",
        "name": "Hui Dong"
      },
      {
        "authorId": "2216290800",
        "name": "Haixiang Zhang"
      },
      {
        "authorId": "2144017254",
        "name": "Jie Feng"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.1886522358297
  },
  {
    "paperId": "9dd3e84935b6f85a8d59487c8364e623cf6f7be4",
    "url": "https://www.semanticscholar.org/paper/9dd3e84935b6f85a8d59487c8364e623cf6f7be4",
    "title": "Is Bigger and Deeper Always Better? Probing LLaMA Across Scales and Layers",
    "abstract": "This paper presents an in-depth analysis of Large Language Models (LLMs), focusing on LLaMA, a prominent open-source foundational model in natural language processing. Instead of assessing LLaMA through its generative output, we design multiple-choice tasks to probe its intrinsic understanding in high-order tasks such as reasoning and computation. We examine the model horizontally, comparing different sizes, and vertically, assessing different layers. We unveil several key and uncommon findings based on the designed probing tasks: (1) Horizontally, enlarging model sizes almost could not automatically impart additional knowledge or computational prowess. Instead, it can enhance reasoning abilities, especially in math problem solving, and helps reduce hallucinations, but only beyond certain size thresholds; (2) In vertical analysis, the lower layers of LLaMA lack substantial arithmetic and factual knowledge, showcasing logical thinking, multilingual and recognitive abilities, with top layers housing most computational power and real-world knowledge.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-07",
    "authors": [
      {
        "authorId": "2263638230",
        "name": "Nuo Chen"
      },
      {
        "authorId": "2068345080",
        "name": "Ning Wu"
      },
      {
        "authorId": "2273031058",
        "name": "Shining Liang"
      },
      {
        "authorId": "50175330",
        "name": "Ming Gong"
      },
      {
        "authorId": "24962156",
        "name": "Linjun Shou"
      },
      {
        "authorId": "2263742987",
        "name": "Dongmei Zhang"
      },
      {
        "authorId": "2279091360",
        "name": "Jia Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "186a8b65975b2ab37c6e6fbcf66fb50b4174efc1",
    "url": "https://www.semanticscholar.org/paper/186a8b65975b2ab37c6e6fbcf66fb50b4174efc1",
    "title": "ENHANCING INFORMATION SECURITY IN MULTIMEDIA STREAMS THROUGH LOGIC LEARNING MACHINE ASSISTED MOTH-FLAME OPTIMIZATION",
    "abstract": "Enhancing information security in multimedia streams is a critical endeavor in the digital age, where data breaches and cyber threats loom large. This research proposes a novel approach by integrating Logic Learning Machines (LLMs) with Moth-Flame Optimization (MFO) to fortify the defenses of multimedia data against potential vulnerabilities. Logic Learning Machines, known for their ability to make decisions based on logical reasoning, form the foundational intelligence of our proposed system. Leveraging their capacity to process complex patterns and relationships within data, LLMs become the cognitive backbone of our security enhancement model. Moth-Flame Optimization, inspired by the navigational behavior of moths around artificial lights, serves as the optimization engine in this framework. MFO mimics the natural attraction of moths to flames, translating it into an algorithmic strategy to optimize parameters and configurations for heightened security measures. By applying MFO, the system dynamically adapts and refines its security protocols in response to evolving threats. The synergy between LLMs and MFO creates a resilient defense mechanism for multimedia streams. The logic-driven decision-making of LLMs is augmented by the adaptive optimization capabilities of MFO, resulting in a robust and dynamic security infrastructure. This fusion not only enhances the detection of potential threats but also enables proactive adjustments to security parameters, thereby fortifying the system against emerging risks. The proposed framework is validated through extensive simulations and experiments, demonstrating its efficacy in real-world scenarios. The outcomes showcase improved information security for multimedia streams, providing a versatile solution for safeguarding sensitive data in diverse digital environments.",
    "venue": "ICTACT Journal on Communication Technology",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-01",
    "authors": [
      {
        "authorId": "2244399290",
        "name": "Bhushankumar Nemade"
      },
      {
        "authorId": "2326526793",
        "name": "Sujata S. Alegavi"
      },
      {
        "authorId": "40414541",
        "name": "N. Badhe"
      },
      {
        "authorId": "2326530295",
        "name": "Aaditya Desai"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "751c3659116b9c13bbe05a309778cdcef3ad86ee",
    "url": "https://www.semanticscholar.org/paper/751c3659116b9c13bbe05a309778cdcef3ad86ee",
    "title": "Enhancing Emergency Decision-making with Knowledge Graphs and Large Language Models",
    "abstract": "Emergency management urgently requires comprehensive knowledge while having a high possibility to go beyond individuals' cognitive scope. Therefore, artificial intelligence(AI) supported decision-making under that circumstance is of vital importance. Recent emerging large language models (LLM) provide a new direction for enhancing targeted machine intelligence. However, the utilization of LLM directly would inevitably introduce unreliable output for its inherent issue of hallucination and poor reasoning skills. In this work, we develop a system called Enhancing Emergency decision-making with Knowledge Graph and LLM (E-KELL), which provides evidence-based decision-making in various emergency stages. The study constructs a structured emergency knowledge graph and guides LLMs to reason over it via a prompt chain. In real-world evaluations, E-KELL receives scores of 9.06, 9.09, 9.03, and 9.09 in comprehensibility, accuracy, conciseness, and instructiveness from a group of emergency commanders and firefighters, demonstrating a significant improvement across various situations compared to baseline models. This work introduces a novel approach to providing reliable emergency decision support.",
    "venue": "International Journal of Disaster Risk Reduction",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-15",
    "authors": [
      {
        "authorId": "2107916004",
        "name": "Minze Chen"
      },
      {
        "authorId": "49662283",
        "name": "Zhenxiang Tao"
      },
      {
        "authorId": "2266820662",
        "name": "Weitong Tang"
      },
      {
        "authorId": "2266752772",
        "name": "Tingxin Qin"
      },
      {
        "authorId": "2266843699",
        "name": "Rui Yang"
      },
      {
        "authorId": "2266821955",
        "name": "Chunli Zhu"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "6841c2dde56562350359235db1179c41097acd80",
    "url": "https://www.semanticscholar.org/paper/6841c2dde56562350359235db1179c41097acd80",
    "title": "Integrating Automated Knowledge Extraction with Large Language Models for Explainable Medical Decision-Making",
    "abstract": "Large language models (LLMs) have demonstrated strong reasoning ability and inspired many previously unimaginable applications. In this paper, we aim to harness the strong reasoning capability of LLMs toward explainable medical diagnosis. As we know, deep learning has been widely adopted and shown improvement in medical diagnostics. However, it is often criticized for its lack of interpretability. To address this drawback, we propose the first method that innovatively combines Markov logic networks (MLNs) with external knowledge extracted using LLMs, aiming for improved both interpretability and accuracy. Specifically, our approach involves a new process, powered by LLMs and a search engine, to automatically collect and organize external medical knowledge. The outcome is a set of first-order logic (FOL) rules, which then become a key component for the following MLN-based diagnostic algorithm. The resulting MLN-based model can maintain the accuracy of deep networks while providing understandable reasoning for its decisions. By aiming to blend specific knowledge from the medical domain with LLM techniques, our work contributes towards the development of improved automatic diagnosis systems, with the potential for enhancing transparency and trust in medical diagnostics.",
    "venue": "IEEE International Conference on Bioinformatics and Biomedicine",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-05",
    "authors": [
      {
        "authorId": "2280129482",
        "name": "Haodi Zhang"
      },
      {
        "authorId": "2279955752",
        "name": "Jiahong Li"
      },
      {
        "authorId": "2279961417",
        "name": "Yichi Wang"
      },
      {
        "authorId": "2284257813",
        "name": "Yuanfeng Song"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.1886522358297
  },
  {
    "paperId": "1b71d50ab66de103a53b3c73615c9638ee72089f",
    "url": "https://www.semanticscholar.org/paper/1b71d50ab66de103a53b3c73615c9638ee72089f",
    "title": "Data-Centric Financial Large Language Models",
    "abstract": "Large language models (LLMs) show promise for natural language tasks but struggle when applied directly to complex domains like finance. LLMs have difficulty reasoning about and integrating all relevant information. We propose a data-centric approach to enable LLMs to better handle financial tasks. Our key insight is that rather than overloading the LLM with everything at once, it is more effective to preprocess and pre-understand the data. We create a financial LLM (FLLM) using multitask prompt-based finetuning to achieve data pre-processing and pre-understanding. However, labeled data is scarce for each task. To overcome manual annotation costs, we employ abductive augmentation reasoning (AAR) to automatically generate training data by modifying the pseudo labels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR substantially outperforms baseline financial LLMs designed for raw text, achieving state-of-the-art on financial analysis and interpretation tasks. We also open source a new benchmark for financial analysis and interpretation. Our methodology provides a promising path to unlock LLMs' potential for complex real-world domains.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-07",
    "authors": [
      {
        "authorId": "2237992280",
        "name": "Zhixuan Chu"
      },
      {
        "authorId": "2262506872",
        "name": "Huaiyu Guo"
      },
      {
        "authorId": "2262439340",
        "name": "Xinyuan Zhou"
      },
      {
        "authorId": "2262428027",
        "name": "Yijia Wang"
      },
      {
        "authorId": "2284026499",
        "name": "Fei Yu"
      },
      {
        "authorId": "2262286642",
        "name": "Hong Chen"
      },
      {
        "authorId": "2262404996",
        "name": "Wanqing Xu"
      },
      {
        "authorId": "2262512733",
        "name": "Xin Lu"
      },
      {
        "authorId": "87484466",
        "name": "Qing Cui"
      },
      {
        "authorId": "2238177474",
        "name": "Longfei Li"
      },
      {
        "authorId": "2151553113",
        "name": "Junqing Zhou"
      },
      {
        "authorId": "2262276636",
        "name": "Sheng Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "113e82ab340af5b9721b8ed50b022e3bb769170e",
    "url": "https://www.semanticscholar.org/paper/113e82ab340af5b9721b8ed50b022e3bb769170e",
    "title": "Large Language Models for Travel Behavior Prediction",
    "abstract": "Travel behavior prediction is a fundamental task in transportation demand management. The conventional methods for travel behavior prediction rely on numerical data to construct mathematical models and calibrate model parameters to represent human preferences. Recent advancement in large language models (LLMs) has shown great reasoning abilities to solve complex problems. In this study, we propose to use LLMs to predict travel behavior with prompt engineering without data-based parameter learning. Specifically, we carefully design our prompts that include 1) task description, 2) travel characteristics, 3) individual attributes, and 4) guides of thinking with domain knowledge, and ask the LLMs to predict an individual's travel behavior and explain the results. We select the travel mode choice task as a case study. Results show that, though no training samples are provided, LLM-based predictions have competitive accuracy and F1-score as canonical supervised learning methods such as multinomial logit, random forest, and neural networks. LLMs can also output reasons that support their prediction. However, though in most of the cases, the output explanations are reasonable, we still observe cases that violate logic or with hallucinations.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-30",
    "authors": [
      {
        "authorId": "40914230",
        "name": "Baichuan Mo"
      },
      {
        "authorId": "2269760401",
        "name": "Hanyong Xu"
      },
      {
        "authorId": "1580217088",
        "name": "Dingyi Zhuang"
      },
      {
        "authorId": "2269766471",
        "name": "Ruoyun Ma"
      },
      {
        "authorId": "2269569437",
        "name": "Xiaotong Guo"
      },
      {
        "authorId": "2125434853",
        "name": "Jinhua Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "f1a950af6e547c3c12a4defbd7120d28f540c6c1",
    "url": "https://www.semanticscholar.org/paper/f1a950af6e547c3c12a4defbd7120d28f540c6c1",
    "title": "MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion",
    "abstract": "Query expansion, pivotal in search engines, enhances the representation of user information needs with additional terms. While existing methods expand queries using retrieved or generated contextual documents, each approach has notable limitations. Retrieval-based methods often fail to accurately capture search intent, particularly with brief or ambiguous queries. Generation-based methods, utilizing large language models (LLMs), generally lack corpus-specific knowledge and entail high fine-tuning costs. To address these gaps, we propose a novel zero-shot query expansion framework utilizing LLMs for mutual verification. Specifically, we first design a query-query-document generation method, leveraging LLMs’ zero-shot reasoning ability to produce diverse sub-queries and corresponding documents. Then, a mutual verification process synergizes generated and retrieved documents for optimal expansion. Our proposed method is fully zero-shot, and extensive experiments on three public benchmark datasets are conducted to demonstrate its effectiveness over existing methods. Our code is available online at https://github.com/Applied-Machine-Learning-Lab/MILL to ease reproduction.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-29",
    "authors": [
      {
        "authorId": "2264224432",
        "name": "Pengyue Jia"
      },
      {
        "authorId": "2249554788",
        "name": "Yiding Liu"
      },
      {
        "authorId": "2238104000",
        "name": "Xiangyu Zhao"
      },
      {
        "authorId": "2263797116",
        "name": "Xiaopeng Li"
      },
      {
        "authorId": "2263773227",
        "name": "Changying Hao"
      },
      {
        "authorId": "2237948548",
        "name": "Shuaiqiang Wang"
      },
      {
        "authorId": "2243455567",
        "name": "Dawei Yin"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "8dce801e67f2f8923dc1f5b3059f70b462f4e4cf",
    "url": "https://www.semanticscholar.org/paper/8dce801e67f2f8923dc1f5b3059f70b462f4e4cf",
    "title": "DetectGPT-SC: Improving Detection of Text Generated by Large Language Models through Self-Consistency with Masked Predictions",
    "abstract": "General large language models (LLMs) such as ChatGPT have shown remarkable success, but it has also raised concerns among people about the misuse of AI-generated texts. Therefore, an important question is how to detect whether the texts are generated by ChatGPT or by humans. Existing detectors are built on the assumption that there is a distribution gap between human-generated and AI-generated texts. These gaps are typically identified using statistical information or classifiers. In contrast to prior research methods, we find that large language models such as ChatGPT exhibit strong self-consistency in text generation and continuation. Self-consistency capitalizes on the intuition that AI-generated texts can still be reasoned with by large language models using the same logical reasoning when portions of the texts are masked, which differs from human-generated texts. Using this observation, we subsequently proposed a new method for AI-generated texts detection based on self-consistency with masked predictions to determine whether a text is generated by LLMs. This method, which we call DetectGPT-SC. We conducted a series of experiments to evaluate the performance of DetectGPT-SC. In these experiments, we employed various mask scheme, zero-shot, and simple prompt for completing masked texts and self-consistency predictions. The results indicate that DetectGPT-SC outperforms the current state-of-the-art across different tasks.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-23",
    "authors": [
      {
        "authorId": "2261313297",
        "name": "Rongsheng Wang"
      },
      {
        "authorId": "2261359496",
        "name": "Qi Li"
      },
      {
        "authorId": "2261347599",
        "name": "Sihong Xie"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "4d0797533ff02dcf6936818f72fcad44d2b09db5",
    "url": "https://www.semanticscholar.org/paper/4d0797533ff02dcf6936818f72fcad44d2b09db5",
    "title": "SAIE Framework: Support Alone Isn't Enough - Advancing LLM Training with Adversarial Remarks",
    "abstract": "Large Language Models (LLMs) can justify or critique their predictions through discussions with other models or humans, thereby enriching their intrinsic understanding of instances. While proactive discussions in the inference phase have been shown to boost performance, such interactions have not been extensively explored during the training phase. We hypothesize that incorporating interactive discussions into the training process can enhance the models' understanding and improve their reasoning and verbal expression abilities during inference. This work introduces the SAIE framework, which facilitates supportive and adversarial discussions between learner and partner models. The learner model receives responses from the partner, and its parameters are then updated based on this discussion. This dynamic adjustment process continues throughout the training phase, responding to the evolving outputs of the learner model. Our empirical evaluation across various tasks, including math problems, commonsense reasoning, and multi-domain knowledge, demonstrates that models fine-tuned with the SAIE framework outperform those trained with conventional fine-tuning approaches. Furthermore, our method enhances the models' reasoning capabilities, improving both individual and multi-agent inference performance.",
    "venue": "European Conference on Artificial Intelligence",
    "year": 2023,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-14",
    "authors": [
      {
        "authorId": "2150167420",
        "name": "Mengsay Loem"
      },
      {
        "authorId": "143655216",
        "name": "Masahiro Kaneko"
      },
      {
        "authorId": "1764004",
        "name": "Naoaki Okazaki"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "549da43aacc3ef5986a126dd9154b7772594b76b",
    "url": "https://www.semanticscholar.org/paper/549da43aacc3ef5986a126dd9154b7772594b76b",
    "title": "StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models",
    "abstract": "Large Language Models (LLMs) have been observed to encode and perpetuate harmful associations present in the training data. We propose a theoretically grounded framework called StereoMap to gain insights into their perceptions of how demographic groups have been viewed by society. The framework is grounded in the Stereotype Content Model (SCM); a well-established theory from psychology. According to SCM, stereotypes are not all alike. Instead, the dimensions of Warmth and Competence serve as the factors that delineate the nature of stereotypes. Based on the SCM theory, StereoMap maps LLMs' perceptions of social groups (defined by socio-demographic features) using the dimensions of Warmth and Competence. Furthermore, the framework enables the investigation of keywords and verbalizations of reasoning of LLMs' judgments to uncover underlying factors influencing their perceptions. Our results show that LLMs exhibit a diverse range of perceptions towards these groups, characterized by mixed evaluations along the dimensions of Warmth and Competence. Furthermore, analyzing the reasonings of LLMs, our findings indicate that LLMs demonstrate an awareness of social disparities, often stating statistical data and research findings to support their reasoning. This study contributes to the understanding of how LLMs perceive and represent social groups, shedding light on their potential biases and the perpetuation of harmful associations.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-20",
    "authors": [
      {
        "authorId": "2165226931",
        "name": "Sullam Jeoung"
      },
      {
        "authorId": "2261092893",
        "name": "Yubin Ge"
      },
      {
        "authorId": "2261085110",
        "name": "Jana Diesner"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "c87f1f7ec6fd6b93d6f7626bebf6de2bf0b8c8e8",
    "url": "https://www.semanticscholar.org/paper/c87f1f7ec6fd6b93d6f7626bebf6de2bf0b8c8e8",
    "title": "Instruction-Following Speech Recognition",
    "abstract": "Conventional end-to-end Automatic Speech Recognition (ASR) models primarily focus on exact transcription tasks, lacking flexibility for nuanced user interactions. With the advent of Large Language Models (LLMs) in speech processing, more organic, text-prompt-based interactions have become possible. However, the mechanisms behind these models' speech understanding and\"reasoning\"capabilities remain underexplored. To study this question from the data perspective, we introduce instruction-following speech recognition, training a Listen-Attend-Spell model to understand and execute a diverse set of free-form text instructions. This enables a multitude of speech recognition tasks -- ranging from transcript manipulation to summarization -- without relying on predefined command sets. Remarkably, our model, trained from scratch on Librispeech, interprets and executes simple instructions without requiring LLMs or pre-trained speech modules. It also offers selective transcription options based on instructions like\"transcribe first half and then turn off listening,\"providing an additional layer of privacy and safety compared to existing LLMs. Our findings highlight the significant potential of instruction-following training to advance speech foundation models.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.09843",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-18",
    "authors": [
      {
        "authorId": "2242872771",
        "name": "Cheng-I Jeff Lai"
      },
      {
        "authorId": "2095379",
        "name": "Zhiyun Lu"
      },
      {
        "authorId": "2242998419",
        "name": "Liangliang Cao"
      },
      {
        "authorId": "2238621132",
        "name": "Ruoming Pang"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "b7f5a44c2f7d0d134cd3456dde2a6d0d46091c66",
    "url": "https://www.semanticscholar.org/paper/b7f5a44c2f7d0d134cd3456dde2a6d0d46091c66",
    "title": "Investigating the Efficacy of Large Language Models in Reflective Assessment Methods through Chain of Thought Prompting",
    "abstract": "Large Language Models, such as Generative Pre-trained Transformer 3 (aka. GPT-3), have been developed to understand language through the analysis of extensive text data, allowing them to identify patterns and connections between words. While LLMs have demonstrated impressive performance across various text-related tasks, they encounter challenges in tasks associated with reasoning. To address this challenge, Chain of Thought (CoT) prompting method has been proposed as a means to enhance LLMs’ proficiency in complex reasoning tasks like solving math word problems and answering questions based on logical argumentative reasoning. The primary aim of this research is to assess how well four language models can grade reflective essays of third-year medical students. The assessment will specifically target the evaluation of critical thinking skills using CoT prompting. The research will provide the following contributions; to introduce and educate on the process of instructing models to evaluate reflective essays from a dataset they have not been previously trained on; to illustrate the use of CoT prompting as an instructional approach for training large models to carry out particular tasks. Our results suggest that among all the models, Llama-7b performs the least effectively, displaying the highest mean squared error. Conversely, ChatGPT emerges as the superior model, boasting a higher Cohen kappa score value of 0.53. Lastly, it’s important to note that the selected models do prioritise user privacy by allowing users to delete their own conducted conversations.",
    "venue": "African Conference on Human Computer Interaction",
    "year": 2023,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle"
    ],
    "publicationDate": "2023-09-30",
    "authors": [
      {
        "authorId": "31179816",
        "name": "Baphumelele Masikisiki"
      },
      {
        "authorId": "1841137457",
        "name": "Vukosi Marivate"
      },
      {
        "authorId": "11602050",
        "name": "Y. Hlophe"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "147a5f41873095faf97abe1655b3f2b2930c31fc",
    "url": "https://www.semanticscholar.org/paper/147a5f41873095faf97abe1655b3f2b2930c31fc",
    "title": "Do large language models and humans have similar behaviours in causal inference with script knowledge?",
    "abstract": "Recently, large pre-trained language models (LLMs) have demonstrated superior language understanding abilities, including zero-shot causal reasoning. However, it is unclear to what extent their capabilities are similar to human ones. We here study the processing of an event B in a script-based story, which causally depends on a previous event A. In our manipulation, event A is stated, negated, or omitted in an earlier section of the text. We first conducted a self-paced reading experiment, which showed that humans exhibit significantly longer reading times when causal conflicts exist (\\neg A \\rightarrow B) than under logical conditions (A \\rightarrow B). However, reading times remain similar when cause A is not explicitly mentioned, indicating that humans can easily infer event B from their script knowledge. We then tested a variety of LLMs on the same data to check to what extent the models replicate human behavior. Our experiments show that 1) only recent LLMs, like GPT-3 or Vicuna, correlate with human behavior in the \\neg A \\rightarrow B condition. 2) Despite this correlation, all models still fail to predict that nil \\rightarrow B is less surprising than \\neg A \\rightarrow B, indicating that LLMs still have difficulties integrating script knowledge.",
    "venue": "STARSEM",
    "year": 2023,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-13",
    "authors": [
      {
        "authorId": "2615648",
        "name": "Xudong Hong"
      },
      {
        "authorId": "2069326568",
        "name": "Margarita Ryzhova"
      },
      {
        "authorId": "2266397280",
        "name": "Daniel Adrian Biondi"
      },
      {
        "authorId": "2253593393",
        "name": "Vera Demberg"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "d4346af837aa6c2bb4a341cfe9bd91862ea5910a",
    "url": "https://www.semanticscholar.org/paper/d4346af837aa6c2bb4a341cfe9bd91862ea5910a",
    "title": "Large Knowledge Model: Perspectives and Challenges",
    "abstract": "Humankind's understanding of the world is fundamentally linked to our perception and cognition, with \\emph{human languages} serving as one of the major carriers of \\emph{world knowledge}. In this vein, \\emph{Large Language Models} (LLMs) like ChatGPT epitomize the pre-training of extensive, sequence-based world knowledge into neural networks, facilitating the processing and manipulation of this knowledge in a parametric space. This article explores large models through the lens of\"knowledge\". We initially investigate the role of symbolic knowledge such as Knowledge Graphs (KGs) in enhancing LLMs, covering aspects like knowledge-augmented language model, structure-inducing pre-training, knowledgeable prompts, structured CoT, knowledge editing, semantic tools for LLM and knowledgeable AI agents. Subsequently, we examine how LLMs can boost traditional symbolic knowledge bases, encompassing aspects like using LLM as KG builder and controller, structured knowledge pretraining, and LLM-enhanced symbolic reasoning. Considering the intricate nature of human knowledge, we advocate for the creation of \\emph{Large Knowledge Models} (LKM), specifically engineered to manage diversified spectrum of knowledge structures. This promising undertaking would entail several key challenges, such as disentangling knowledge base from language models, cognitive alignment with human knowledge, integration of perception and cognition, and building large commonsense models for interacting with physical world, among others. We finally propose a five-\"A\"principle to distinguish the concept of LKM.",
    "venue": "Data Intelligence",
    "year": 2023,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "https://www.sciengine.com/doi/pdfView/2BD9D90B66B548A1B5EDC930FDB16C61",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-05",
    "authors": [
      {
        "authorId": "2269769030",
        "name": "Huajun Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "20867dd60552674beac1edc7fa029d2d1b8fd03a",
    "url": "https://www.semanticscholar.org/paper/20867dd60552674beac1edc7fa029d2d1b8fd03a",
    "title": "MAQA: A Multimodal QA Benchmark for Negation",
    "abstract": "Multimodal learning can benefit from the representation power of pretrained Large Language Models (LLMs). However, state-of-the-art transformer based LLMs often ignore negations in natural language and there is no existing benchmark to quantitatively evaluate whether multimodal transformers inherit this weakness. In this study, we present a new multimodal question answering (QA) benchmark adapted from labeled music videos in AudioSet (Gemmeke et al., 2017) with the goal of systematically evaluating if multimodal transformers can perform complex reasoning to recognize new concepts as negation of previously learned concepts. We show that with standard fine-tuning approach multimodal transformers are still incapable of correctly interpreting negation irrespective of model size. However, our experiments demonstrate that augmenting the original training task distributions with negated QA examples allow the model to reliably reason with negation. To do this, we describe a novel data generation procedure that prompts the 540B-parameter PaLM model to automatically generate negated QA examples as compositions of easily accessible video tags. The generated examples contain more natural linguistic patterns and the gains compared to template-based task augmentation approach are significant.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2301.03238",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-01-09",
    "authors": [
      {
        "authorId": "32297257",
        "name": "Judith Yue Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "903eeb83c9f9339f401541eed1f4a62a798261c1",
    "url": "https://www.semanticscholar.org/paper/903eeb83c9f9339f401541eed1f4a62a798261c1",
    "title": "Large language models are universal biomedical simulators",
    "abstract": "Computational simulation of biological processes can be a valuable tool in accelerating biomedical research, but usually requires extensive domain knowledge and manual adaptation. Recently, large language models (LLMs) such as GPT-4 have proven surprisingly successful for a wide range of tasks by generating human language at a very large scale. Here we explore the potential of leveraging LLMs as simulators of biological systems. We establish proof-of-concept of a text-based simulator, SimulateGPT, that uses LLM reasoning. We demonstrate good prediction performance for various biomedical applications, without requiring explicit domain knowledge or manual tuning. LLMs thus enable a new class of versatile and broadly applicable biological simulators. This text-based simulation paradigm is well-suited for modeling and understanding complex living systems that are difficult to describe with physics-based first-principles simulation, but for which extensive knowledge and context is available as written text.",
    "venue": "bioRxiv",
    "year": 2023,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "https://www.biorxiv.org/content/biorxiv/early/2023/06/19/2023.06.16.545235.full.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Biology"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-06-19",
    "authors": [
      {
        "authorId": "30817620",
        "name": "Moritz Schaefer"
      },
      {
        "authorId": "2220352300",
        "name": "Stephan Reichl"
      },
      {
        "authorId": "5286065",
        "name": "R. ter Horst"
      },
      {
        "authorId": "32512670",
        "name": "Adele M. Nicolas"
      },
      {
        "authorId": "4232625",
        "name": "T. Krausgruber"
      },
      {
        "authorId": "2220353653",
        "name": "Francesco Piras"
      },
      {
        "authorId": "3983564",
        "name": "Peter Stepper"
      },
      {
        "authorId": "2061663470",
        "name": "Christoph Bock"
      },
      {
        "authorId": "3004898",
        "name": "M. Samwald"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "6ea0d369103c5786fc555c0da05f81eb013392d0",
    "url": "https://www.semanticscholar.org/paper/6ea0d369103c5786fc555c0da05f81eb013392d0",
    "title": "Zero-Shot Question Answering over Financial Documents using Large Language Models",
    "abstract": "We introduce a large language model (LLM) based approach to answer complex questions requiring multi-hop numerical reasoning over financial reports. While LLMs have exhibited remarkable performance on various natural language and reasoning tasks, complex reasoning problems often rely on few-shot prompts that require carefully crafted examples. In contrast, our approach uses novel zero-shot prompts that guide the LLM to encode the required reasoning into a Python program or a domain specific language. The generated program is then executed by a program interpreter, thus mitigating the limitations of LLM in performing accurate arithmetic calculations. We evaluate the proposed approach on three financial datasets using some of the recently developed generative pretrained transformer (GPT) models and perform comparisons with various zero-shot baselines. The experimental results demonstrate that our approach significantly improves the accuracy for all the LLMs over their respective baselines. We provide a detailed analysis of the results, generating insights to support our findings. The success of our approach demonstrates the enormous potential to extract complex domain specific numerical reasoning by designing zero-shot prompts to effectively exploit the knowledge embedded in LLMs.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-19",
    "authors": [
      {
        "authorId": "1990650",
        "name": "Karmvir Singh Phogat"
      },
      {
        "authorId": "2268400447",
        "name": "Chetan Harsha"
      },
      {
        "authorId": "2099356204",
        "name": "Sridhar Dasaratha"
      },
      {
        "authorId": "47838999",
        "name": "Shashishekar Ramakrishna"
      },
      {
        "authorId": "148118941",
        "name": "Sai Akhil Puranam"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "3f5a434393e560ac0c2c1823f5f620a69f29b5e6",
    "url": "https://www.semanticscholar.org/paper/3f5a434393e560ac0c2c1823f5f620a69f29b5e6",
    "title": "Integration of Large Language Models within Cognitive Architectures for Autonomous Robots",
    "abstract": "Symbolic reasoning systems have been used in cognitive architectures to provide inference and planning capabilities. However, defining domains and problems has proven difficult and prone to errors. Moreover, Large Language Models (LLMs) have emerged as tools to process natural language for different tasks. In this paper, we propose the use of LLMs to tackle these problems. This way, this paper proposes the integration of LLMs in the ROS 2-integrated cognitive architecture MERLIN2 for autonomous robots. Specifically, we present the design, development and deployment of how to leverage the reasoning capabilities of LLMs inside the deliberative processes of MERLIN2. As a result, the deliberative system is updated from a PDDL-based planner system to a natural language planning system. This proposal is evaluated quantitatively and qualitatively, measuring the impact of incorporating the LLMs in the cognitive architecture. Results show that a classical approach achieves better performance but the proposed solution provides an enhanced interaction through natural language.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.14945",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-26",
    "authors": [
      {
        "authorId": "2003275049",
        "name": "Miguel Ángel González Santamarta"
      },
      {
        "authorId": "1483989745",
        "name": "F. J. Rodríguez-Lera"
      },
      {
        "authorId": "1403616295",
        "name": "Á. Guerrero-Higueras"
      },
      {
        "authorId": "6937365",
        "name": "Vicente Matellán Olivera"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "f64bff444f4ca1287496e8a0c23316b02e2f1586",
    "url": "https://www.semanticscholar.org/paper/f64bff444f4ca1287496e8a0c23316b02e2f1586",
    "title": "Multi-stage Large Language Model Correction for Speech Recognition",
    "abstract": "In this paper, we investigate the usage of large language models (LLMs) to improve the performance of competitive speech recognition systems. Different from previous LLM-based ASR error correction methods, we propose a novel multi-stage approach that utilizes uncertainty estimation of ASR outputs and reasoning capability of LLMs. Specifically, the proposed approach has two stages: the first stage is about ASR uncertainty estimation and exploits N-best list hypotheses to identify less reliable transcriptions; The second stage works on these identified transcriptions and performs LLM-based corrections. This correction task is formulated as a multi-step rule-based LLM reasoning process, which uses explicitly written rules in prompts to decompose the task into concrete reasoning steps. Our experimental results demonstrate the effectiveness of the proposed method by showing 10% ~ 20% relative improvement in WER over competitive ASR systems -- across multiple test domains and in zero-shot settings.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-17",
    "authors": [
      {
        "authorId": "2259878206",
        "name": "Jie Pu"
      },
      {
        "authorId": "2260063197",
        "name": "Thai-Son Nguyen"
      },
      {
        "authorId": "11126660",
        "name": "Sebastian Stüker"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "db7d339fc394f8db910a2dd12d2621e684094617",
    "url": "https://www.semanticscholar.org/paper/db7d339fc394f8db910a2dd12d2621e684094617",
    "title": "In-Context Ability Transfer for Question Decomposition in Complex QA",
    "abstract": "Answering complex questions is a challenging task that requires question decomposition and multistep reasoning for arriving at the solution. While existing supervised and unsupervised approaches are specialized to a certain task and involve training, recently proposed prompt-based approaches offer generalizable solutions to tackle a wide variety of complex question-answering (QA) tasks. However, existing prompt-based approaches that are effective for complex QA tasks involve expensive hand annotations from experts in the form of rationales and are not generalizable to newer complex QA scenarios and tasks. We propose, icat (In-Context Ability Transfer) which induces reasoning capabilities in LLMs without any LLM fine-tuning or manual annotation of in-context samples. We transfer the ability to decompose complex questions to simpler questions or generate step-by-step rationales to LLMs, by careful selection from available data sources of related tasks. We also propose an automated uncertainty-aware exemplar selection approach for selecting examples from transfer data sources. Finally, we conduct large-scale experiments on a variety of complex QA tasks involving numerical reasoning, compositional complex QA, and heterogeneous complex QA which require decomposed reasoning. We show that ICAT convincingly outperforms existing prompt-based solutions without involving any model training, showcasing the benefits of re-using existing abilities.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-26",
    "authors": [
      {
        "authorId": "80285140",
        "name": "V. Venktesh"
      },
      {
        "authorId": "1609913602",
        "name": "Sourangshu Bhattacharya"
      },
      {
        "authorId": "2004208434",
        "name": "Avishek Anand"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "9b2dd6d7961b3bbdd41f5839939e999b3b68740f",
    "url": "https://www.semanticscholar.org/paper/9b2dd6d7961b3bbdd41f5839939e999b3b68740f",
    "title": "An Analysis of Large Language Models and LangChain in Mathematics Education",
    "abstract": "The development of large language models (LLMs) has led to the consideration of new approaches, particularly in education. Word problems, especially in subjects like mathematics, and the need to solve these problems by collectively addressing specific stages of reasoning, have raised the question of whether LLMs can be successful in this area as well. In our study, we conducted analyses by asking mathematics questions especially related to word problems using ChatGPT, which is based on the latest language models like Generative Pretrained Transformer (GPT). Additionally, we compared the correct and incorrect answers by posing the same questions to LLMMathChain, a mathematics-specific LLM based on the latest language models like LangChain. It was observed that the answers obtained were more successful with ChatGPT (GPT 3.5), particularly in the field of mathematics. However, both language models were found to be below expectations, particularly in word problems, and suggestions for improvement were provided.",
    "venue": "International Conference on Advances in Artificial Intelligence",
    "year": 2023,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2023-10-13",
    "authors": [
      {
        "authorId": "2214811053",
        "name": "Fatih Soygazi"
      },
      {
        "authorId": "2476849",
        "name": "Damla Oguz"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "9e1ba67d5f443a8bd42a8b856534f50c429baf11",
    "url": "https://www.semanticscholar.org/paper/9e1ba67d5f443a8bd42a8b856534f50c429baf11",
    "title": "Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency",
    "abstract": "The field of Math-NLP has witnessed significant growth in recent years, motivated by the desire to expand LLM performance to the leaning of non-linguistic notions (numerals, and subsequently, arithmetic reasoning). However, non-linguistic skill injection typically comes at a cost for LLMs: it leads to catastrophic forgetting of core linguistic skills, a consequence that often remains unaddressed in the literature. As Math-NLP has been able to create LLMs that can closely approximate the mathematical skills of a grade schooler or the arithmetic reasoning skills of a calculator, the practicality of these models fail if they concomitantly shed their linguistic capabilities. In this work, we take a closer look into the phenomena of catastrophic forgetting as it pertains to LLMs and subsequently offer a novel framework for non-linguistic skill injection for LLMs based on information-theoretic interventions and skill-specific losses that enable the learning of strict arithmetic reasoning. Our model outperforms the state-of-the-art both on injected non-linguistic skills and on linguistic knowledge retention, and does so with a fraction of the non-linguistic training data (1/4) and zero additional synthetic linguistic training data.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.08246",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-14",
    "authors": [
      {
        "authorId": "1928141171",
        "name": "Mandar Sharma"
      },
      {
        "authorId": "50027530",
        "name": "N. Muralidhar"
      },
      {
        "authorId": "48651437",
        "name": "Naren Ramakrishnan"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "808ae6428998eac3a609d91b7c7364795abeee3f",
    "url": "https://www.semanticscholar.org/paper/808ae6428998eac3a609d91b7c7364795abeee3f",
    "title": "RRAML: Reinforced Retrieval Augmented Machine Learning",
    "abstract": "The emergence of large language models (LLMs) has revolutionized machine learning and related fields, showcasing remarkable abilities in comprehending, generating, and manipulating human language. However, their conventional usage through API-based text prompt submissions imposes certain limitations in terms of context constraints and external source availability. To address these challenges, we propose a novel framework called Reinforced Retrieval Augmented Machine Learning (RRAML). RRAML integrates the reasoning capabilities of LLMs with supporting information retrieved by a purpose-built retriever from a vast user-provided database. By leveraging recent advancements in reinforcement learning, our method effectively addresses several critical challenges. Firstly, it circumvents the need for accessing LLM gradients. Secondly, our method alleviates the burden of retraining LLMs for specific tasks, as it is often impractical or impossible due to restricted access to the model and the computational intensity involved. Additionally we seamlessly link the retriever's task with the reasoner, mitigating hallucinations and reducing irrelevant, and potentially damaging retrieved documents. We believe that the research agenda outlined in this paper has the potential to profoundly impact the field of AI, democratizing access to and utilization of LLMs for a wide range of entities.",
    "venue": "DP@AI*IA",
    "year": 2023,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.12798",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-24",
    "authors": [
      {
        "authorId": "151426607",
        "name": "Andrea Bacciu"
      },
      {
        "authorId": "2224626776",
        "name": "Florin Cocunasu"
      },
      {
        "authorId": "1752951302",
        "name": "F. Siciliano"
      },
      {
        "authorId": "2192306989",
        "name": "Fabrizio Silvestri"
      },
      {
        "authorId": "2783910",
        "name": "N. Tonellotto"
      },
      {
        "authorId": "120709579",
        "name": "Giovanni Trappolini"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "2090f208a05344fd15a995fcbc9d539508841347",
    "url": "https://www.semanticscholar.org/paper/2090f208a05344fd15a995fcbc9d539508841347",
    "title": "Local Large Language Models for Complex Structured Tasks.",
    "abstract": "This paper introduces an approach that combines the language reasoning capabilities of large language models (LLMs) with the benefits of local training to tackle complex language tasks. The authors demonstrate their approach by extracting structured condition codes from pathology reports. The proposed approach utilizes local, fine-tuned LLMs to respond to specific generative instructions and provide structured outputs. Over 150k uncurated surgical pathology reports containing gross descriptions, final diagnoses, and condition codes were used. Different model architectures were trained and evaluated, including LLaMA, BERT, and LongFormer. The results show that the LLaMA-based models significantly outperform BERT-style models across all evaluated metrics. LLaMA models performed especially well with large datasets, demonstrating their ability to handle complex, multi-label tasks. Overall, this work presents an effective approach for utilizing LLMs to perform structured generative tasks on domain-specific language in the medical domain.",
    "venue": "AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science",
    "year": 2023,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-08-03",
    "authors": [
      {
        "authorId": "3426497",
        "name": "V. Bumgardner"
      },
      {
        "authorId": "2216560210",
        "name": "Aaron D. Mullen"
      },
      {
        "authorId": "2172116803",
        "name": "Samuel E. Armstrong"
      },
      {
        "authorId": "31487530",
        "name": "Caylin D. Hickey"
      },
      {
        "authorId": "2304394778",
        "name": "Victor Marek"
      },
      {
        "authorId": "1823203",
        "name": "Jeffrey A. Talbert"
      }
    ],
    "source": "semantic_scholar",
    "score": 100.79441541679836
  },
  {
    "paperId": "a1cbe3e24cdcb49541ef5da4f82d9ebc45bb6261",
    "url": "https://www.semanticscholar.org/paper/a1cbe3e24cdcb49541ef5da4f82d9ebc45bb6261",
    "title": "StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving",
    "abstract": "Most existing prompting methods suffer from the issues of generalizability and consistency, as they often rely on instance-specific solutions that may not be applicable to other instances and lack task-level consistency across the selected few-shot examples. To address these limitations, we propose a comprehensive framework, StrategyLLM, allowing LLMs to perform inductive reasoning, deriving general strategies from specific task instances, and deductive reasoning, applying these general strategies to particular task examples, for constructing generalizable and consistent few-shot prompts. It employs four LLM-based agents: strategy generator, executor, optimizer, and evaluator, working together to generate, evaluate, and select promising strategies for a given task. Experimental results demonstrate that StrategyLLM outperforms the competitive baseline CoT-SC that requires human-annotated solutions on 13 datasets across 4 challenging tasks without human involvement, including math reasoning (34.2\\% $\\rightarrow$ 38.8\\%), commonsense reasoning (70.3\\% $\\rightarrow$ 72.5\\%), algorithmic reasoning (73.7\\% $\\rightarrow$ 85.0\\%), and symbolic reasoning (30.0\\% $\\rightarrow$ 79.2\\%). Further analysis reveals that StrategyLLM is applicable to various LLMs and demonstrates advantages across numerous scenarios.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-15",
    "authors": [
      {
        "authorId": "2292022778",
        "name": "Chang Gao"
      },
      {
        "authorId": "48579460",
        "name": "Haiyun Jiang"
      },
      {
        "authorId": "2266753374",
        "name": "Deng Cai"
      },
      {
        "authorId": "2239383048",
        "name": "Shuming Shi"
      },
      {
        "authorId": "2266753619",
        "name": "Wai Lam"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "0f3868e735c727e5b2096667544b50b7d9762853",
    "url": "https://www.semanticscholar.org/paper/0f3868e735c727e5b2096667544b50b7d9762853",
    "title": "Improving web element localization by using a large language model",
    "abstract": "Web‐based test automation heavily relies on accurately finding web elements. Traditional methods compare attributes but do not grasp the context and meaning of elements and words. The emergence of large language models (LLMs) like GPT‐4, which can show human‐like reasoning abilities on some tasks, offers new opportunities for software engineering and web element localization. This paper introduces and evaluates VON Similo LLM, an enhanced web element localization approach. Using an LLM, it selects the most likely web element from the top‐ranked ones identified by the existing VON Similo method, ideally aiming to get closer to human‐like selection accuracy. An experimental study was conducted using 804 web element pairs from 48 real‐world web applications. We measured the number of correctly identified elements as well as the execution times, comparing the effectiveness and efficiency of VON Similo LLM against the baseline algorithm. In addition, motivations from the LLM were recorded and analysed for 140 instances. VON Similo LLM demonstrated improved performance, reducing failed localizations from 70 to 40 (out of 804), a 43% reduction. Despite its slower execution time and additional costs of using the GPT‐4 model, the LLM's human‐like reasoning showed promise in enhancing web element localization. LLM technology can enhance web element localization in GUI test automation, reducing false positives and potentially lowering maintenance costs. However, further research is necessary to fully understand LLMs' capabilities, limitations and practical use in GUI testing.",
    "venue": "Software testing, verification & reliability",
    "year": 2023,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.02046",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-03",
    "authors": [
      {
        "authorId": "33471027",
        "name": "M. Naß"
      },
      {
        "authorId": "2770117",
        "name": "Emil Alégroth"
      },
      {
        "authorId": "145278906",
        "name": "R. Feldt"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "6dbe7b13f39bea92702751244f1b54226be6a85e",
    "url": "https://www.semanticscholar.org/paper/6dbe7b13f39bea92702751244f1b54226be6a85e",
    "title": "Chain-of-Thought Tuning: Masked Language Models can also Think Step By Step in Natural Language Understanding",
    "abstract": "Chain-of-Thought (CoT) is a technique that guides Large Language Models (LLMs) to decompose complex tasks into multi-step reasoning through intermediate steps in natural language form. Briefly, CoT enables LLMs to think step by step. However, although many Natural Language Understanding (NLU) tasks also require thinking step by step, LLMs perform less well than small-scale Masked Language Models (MLMs). To migrate CoT from LLMs to MLMs, we propose Chain-of-Thought Tuning (CoTT), a two-step reasoning framework based on prompt tuning, to implement step-by-step thinking for MLMs on NLU tasks. From the perspective of CoT, CoTT's two-step framework enables MLMs to implement task decomposition; CoTT's prompt tuning allows intermediate steps to be used in natural language form. Thereby, the success of CoT can be extended to NLU tasks through MLMs. To verify the effectiveness of CoTT, we conduct experiments on two NLU tasks: hierarchical classification and relation extraction, and the results show that CoTT outperforms baselines and achieves state-of-the-art performance.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-18",
    "authors": [
      {
        "authorId": "2093926420",
        "name": "Caoyun Fan"
      },
      {
        "authorId": "2136089851",
        "name": "Jidong Tian"
      },
      {
        "authorId": "2124530624",
        "name": "Yitian Li"
      },
      {
        "authorId": "2108997748",
        "name": "Wenqing Chen"
      },
      {
        "authorId": "100537432",
        "name": "Hao He"
      },
      {
        "authorId": "2257387631",
        "name": "Yaohui Jin"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "74cbfe5b9331ecc717cc47913ec43e707badcf97",
    "url": "https://www.semanticscholar.org/paper/74cbfe5b9331ecc717cc47913ec43e707badcf97",
    "title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
    "abstract": "Traditional attempts to enhance the logical reasoning abilities of language models often rely on supervised fine-tuning, limiting their generalization to new tasks or domains. Large Language Models (LLMs), with their capacity to condense vast knowledge, can effectively tackle many tasks. Yet, our experiments reveal a gap in their performance on logical reasoning benchmarks when compared to state-of-the-art fine-tuning based models. To bridge this gap, we present LogicLLM, a first-of-its-kind, fully self-supervised framework for integrating logical reasoning capabilities into LLMs, and activating them via in-context learning. We apply this to two LLM series, FLAN-T5 and LLaMA, with parameter sizes from 3 billion to 33 billion. LogicLLM demonstrates its effectiveness through successful improvements on two logical reasoning benchmarks (ReClor and LogiQA-v2). Additionally, LogicLLM based on FLAN-T5-11B attains comparable results to ChatGPT, and evaluations with LLaMA-based models on three language understanding benchmarks (RACE, MMLU and Big-Bench-Hard) confirm that the improvements come without compromising the model’s general language understanding capabilities.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-23",
    "authors": [
      {
        "authorId": "1689176705",
        "name": "Fangkai Jiao"
      },
      {
        "authorId": "2284863763",
        "name": "Zhiyang Teng"
      },
      {
        "authorId": "2064493724",
        "name": "Bosheng Ding"
      },
      {
        "authorId": "49293155",
        "name": "Zhengyuan Liu"
      },
      {
        "authorId": "2271409954",
        "name": "Nancy F. Chen"
      },
      {
        "authorId": "2708940",
        "name": "Shafiq R. Joty"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "38bf1de49c0bb9d93ece88a8030f82f8f5de8c26",
    "url": "https://www.semanticscholar.org/paper/38bf1de49c0bb9d93ece88a8030f82f8f5de8c26",
    "title": "Temperature-scaled large language models for Lean proofstep prediction",
    "abstract": "Leveraging the reasoning capabilities of large language models (LLMs) for theorem proving is a promising but challenging task, requiring in-domain finetunings on which LLMs are prone to overfit. This issue is exacerbated by two properties that set theorem proving apart from more mainstream applications of LLMs: the scarcity of training data in formal environments such as Lean or Isabelle and the prohibitive cost of using evaluation benchmarks for hyperparameter search and model selection. In this work, we propose temperature scaling as a regularization method for multi-epoch training on small datasets. We demonstrate its effectiveness empirically, obtaining state-of-the-art supervised tactic generation models for Lean 3 of sizes 1.5B, 7B and 13B parameters. We provide detailed ablations on proof search hyperparameters and analyses of the proof search behaviors of the resulting models, and exhibit the approximate logarithmic scaling of tactic-based proof search with respect to time budget.",
    "venue": "",
    "year": null,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2233293602",
        "name": "Fabian Gloeckle"
      },
      {
        "authorId": "3361236",
        "name": "Baptiste Rozière"
      },
      {
        "authorId": "2046132510",
        "name": "Amaury Hayat"
      },
      {
        "authorId": "2282469774",
        "name": "Gabriele Synnaeve"
      }
    ],
    "source": "semantic_scholar",
    "score": 81.19162312519754
  },
  {
    "paperId": "4a4ead9eee6632228565173a0e4061aef56459c1",
    "url": "https://www.semanticscholar.org/paper/4a4ead9eee6632228565173a0e4061aef56459c1",
    "title": "Beyond Surface: Probing LLaMA Across Scales and Layers",
    "abstract": "This paper presents an in-depth analysis of Large Language Models (LLMs), focusing on LLaMA, a prominent open-source foundational model in natural language processing. Instead of assessing LLaMA through its generative output, we design multiple-choice tasks to probe its intrinsic understanding in high-order tasks such as reasoning and computation. We examine the model horizontally, comparing different sizes, and vertically, assessing different layers. We unveil several key and uncommon findings based on the designed probing tasks: (1) Hor-izontally, enlarging model sizes almost could not automatically impart additional knowledge or computational prowess. Instead, it can enhance reasoning abilities, especially in math problem solving, and helps reduce hallucinations, but only beyond certain size thresholds; (2) In vertical analysis, the lower layers of LLaMA lack substantial arithmetic and factual knowledge, showcasing logical thinking, multilingual and recognitive abilities, with top layers housing most computational power and real-world knowledge. These findings provide new observations into LLaMA’s capabilities, offering insights into the current state of LLMs. To reproduce our results and access datasets, please refer to https://github.",
    "venue": "",
    "year": null,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2263638230",
        "name": "Nuo Chen"
      },
      {
        "authorId": "2289848968",
        "name": "Ning Wu"
      },
      {
        "authorId": "2273031058",
        "name": "Shining Liang"
      },
      {
        "authorId": "2307469423",
        "name": "Ming Gong"
      },
      {
        "authorId": "24962156",
        "name": "Linjun Shou"
      },
      {
        "authorId": "2263742987",
        "name": "Dongmei Zhang"
      },
      {
        "authorId": "2279091360",
        "name": "Jia Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 76.87639203842082
  },
  {
    "paperId": "8b3d0ca15acca6911d7a45bce3f18ba8b03622cc",
    "url": "https://www.semanticscholar.org/paper/8b3d0ca15acca6911d7a45bce3f18ba8b03622cc",
    "title": "Exploring the opportunities and challenges of using large language models to represent institutional agency in land system modelling",
    "abstract": "Public policy institutions play crucial roles in the land system, but modelling their policy-making processes is challenging. Large Language Models (LLMs) offer a novel approach to simulating many 15 different types of human decision-making, including policy choices. This paper aims to investigate the opportunities and challenges that LLMs bring to land system modelling by integrating LLM-powered institutional agents within an agent-based, land use model. Four types of LLM agents are examined, all of which, in the examples presented here, use taxes to steer meat production toward a target level. The LLM agents provide reasoning and policy action output. The agents’ performance is benchmarked against two 20 baseline scenarios: one without policy interventions and another implementing optimal policy actions determined through a genetic algorithm. The findings show that while LLM agents perform better than the non-intervention scenario, they fall short of the performance achieved by optimal policy actions. However, LLM agents demonstrate behaviour and decision-making, marked by policy consistency and transparent reasoning. This includes generating strategies such as incrementalism, delayed policy action, proactive 25 policy adjustments, and balancing multiple stakeholder interests. Agents equipped with experiential learning capabilities excel in achieving policy objectives through progressive policy actions. The order in which reasoning and proposed policy actions are output has a notable effect on the agents’ performance, suggesting that enforced reasoning guides as well as explains LLM decisions. The approach presented here points to promising opportunities and significant challenges. The opportunities include, exploring naturalistic",
    "venue": "",
    "year": null,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2291298388",
        "name": "Yongchao Zeng"
      },
      {
        "authorId": "2265240329",
        "name": "C. Brown"
      },
      {
        "authorId": "2273795497",
        "name": "Joanna Raymond"
      },
      {
        "authorId": "2291190962",
        "name": "Mohamed Byari"
      },
      {
        "authorId": "2291193820",
        "name": "Ronja Hotz"
      },
      {
        "authorId": "2418996",
        "name": "M. Rounsevell"
      }
    ],
    "source": "semantic_scholar",
    "score": 66.47918433002164
  },
  {
    "paperId": "edafaddddcaeff6fde3228b677d9efecdf08ba35",
    "url": "https://www.semanticscholar.org/paper/edafaddddcaeff6fde3228b677d9efecdf08ba35",
    "title": "Traditional Chinese Medicine Epidemic Prevention and Treatment Question-Answering Model Based on LLMs",
    "abstract": "Background: Epidemic diseases in Traditional Chinese Medicine (TCM) constitute an essential part of Chinese medical science. TCM has accumulated rich theoretical and practical experiences in the prevention and treatment of epidemic diseases, forming the academic system of epidemic febrile disease, providing robust support for epidemic prevention and resistance in TCM. However, the numerous and complex literature on TCM epidemic diseases brings challenges to the organization and discovery of epidemic disease knowledges. Objective: To leverage the powerful knowledge learning ability of state-of-the-art LLMs (LLMs) to address the efficient acquisition and utilization of TCM epidemic disease knowledges. Methods: By collecting content related to epidemic diseases from 194 ancient TCM books, as well as the knowledge graph of TCM epidemic disease prevention and treatment, we built the large TCM epidemic disease model EpidemicCHAT based on the ChatGLM model. To assess the performances of the model, several open-source LLMs were compared in the study. Results: Compared to traditional LLMs, which may fail to answer or produce hallucinations in the field of TCM epidemic diseases, EpidemicCHAT demonstrates superior answering and reasoning abilities. In the evaluation of TCM epidemic disease prescription generation, the model achieved scores of 44.02, 61.10, and 59.40 on the BLEU-4, ROUGE-L, and METEOR metrics, respectively. Conclusion: The EpidemicCHAT model proposed in this study performs excellently in the field of TCM epidemic diseases, which might provide a reference for the construction of TCM LLMs and applications such as TCM auxiliary diagnosis and Chinese herbal prescription generation.",
    "venue": "IEEE International Conference on Bioinformatics and Biomedicine",
    "year": 2023,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-05",
    "authors": [
      {
        "authorId": "2279961573",
        "name": "Zongzhen Zhou"
      },
      {
        "authorId": "2279959540",
        "name": "Tao Yang"
      },
      {
        "authorId": "2497052",
        "name": "Kongfa Hu"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.79441541679836
  },
  {
    "paperId": "63549bf78e4b1e7e1cec505ce65e6e8f90474f41",
    "url": "https://www.semanticscholar.org/paper/63549bf78e4b1e7e1cec505ce65e6e8f90474f41",
    "title": "ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs",
    "abstract": "Large Language Models (LLMs) still struggle with natural language reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents. ReConcile enhances collaborative reasoning between LLM agents via multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism that leads to a better consensus. In each round, ReConcile initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their confidence scores, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. Experiments on seven benchmarks demonstrate that ReConcile significantly improves LLMs' reasoning -- both individually and as a team -- surpassing prior single-agent and multi-agent baselines by up to 11.4% and even outperforming GPT-4 on three datasets. ReConcile also flexibly incorporates different combinations of agents, including API-based, open-source, and domain-specific models, leading to an 8% improvement on MATH. Finally, we analyze the individual components of ReConcile, demonstrating that the diversity originating from different models is critical to its superior performance. Code: https://github.com/dinobby/ReConcile",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 68,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.13007",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-09-22",
    "authors": [
      {
        "authorId": "2244771469",
        "name": "Justin Chih-Yao Chen"
      },
      {
        "authorId": "35106509",
        "name": "Swarnadeep Saha"
      },
      {
        "authorId": "143977268",
        "name": "Mohit Bansal"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.5115975689589
  },
  {
    "paperId": "c741cc9d082dc41b0421dda2909a9e8566d2225f",
    "url": "https://www.semanticscholar.org/paper/c741cc9d082dc41b0421dda2909a9e8566d2225f",
    "title": "TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition",
    "abstract": "Table reasoning is a challenging task that requires understanding both natural language questions and structured tabular data. Large language models (LLMs) have shown impressive capabilities in natural language understanding and generation, but they often struggle with large tables due to their limited input length. In this paper, we propose TabSQLify, a novel method that leverages text-to-SQL generation to decompose tables into smaller and relevant sub-tables, containing only essential information for answering questions or verifying statements, before performing the reasoning task. In our comprehensive evaluation on four challenging datasets, our approach demonstrates comparable or superior performance compared to prevailing methods reliant on full tables as input. Moreover, our method can reduce the input context length significantly, making it more scalable and efficient for large-scale table reasoning applications. Our method performs remarkably well on the WikiTQ benchmark, achieving an accuracy of 64.7%. Additionally, on the TabFact benchmark, it achieves a high accuracy of 79.5%. These results surpass other LLM-based baseline models on gpt-3.5-turbo (chatgpt). TabSQLify can reduce the table size significantly alleviating the computational load on LLMs when handling large tables without compromising performance.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-15",
    "authors": [
      {
        "authorId": "31142088",
        "name": "Md Mahadi Hasan Nahid"
      },
      {
        "authorId": "2296784521",
        "name": "Davood Rafiei"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "cad3a64f1cb3020747b8b381d72f9032677469dd",
    "url": "https://www.semanticscholar.org/paper/cad3a64f1cb3020747b8b381d72f9032677469dd",
    "title": "An Empirical Study of Data Ability Boundary in LLMs' Math Reasoning",
    "abstract": "Large language models (LLMs) are displaying emergent abilities for math reasoning tasks,and there is a growing attention on enhancing the ability of open-source LLMs through supervised fine-tuning (SFT).In this paper, we aim to explore a general data strategy for supervised data to help optimize and expand math reasoning ability.Firstly, we determine the ability boundary of reasoning paths augmentation by identifying these paths' minimal optimal set.Secondly, we validate that different abilities of the model can be cumulatively enhanced by Mix of Minimal Optimal Sets of corresponding types of data, while our models MMOS achieve SOTA performance on series base models under much lower construction costs.Besides, we point out GSM-HARD is not really hard and today's LLMs no longer lack numerical robustness.Also, we provide an Auto Problem Generator for robustness testing and educational applications.Our code and data are publicly available at https://github.com/cyzhh/MMOS.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-23",
    "authors": [
      {
        "authorId": "2273526346",
        "name": "Zui Chen"
      },
      {
        "authorId": "2265785529",
        "name": "Yezeng Chen"
      },
      {
        "authorId": "2273553022",
        "name": "Jiaqi Han"
      },
      {
        "authorId": "2290024357",
        "name": "Zhijie Huang"
      },
      {
        "authorId": "2289948694",
        "name": "Ji Qi"
      },
      {
        "authorId": "2273755918",
        "name": "Yi Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "3567ddec4c3c2925f15f95010f9be658cf7fb50b",
    "url": "https://www.semanticscholar.org/paper/3567ddec4c3c2925f15f95010f9be658cf7fb50b",
    "title": "Eliciting Better Multilingual Structured Reasoning from LLMs through Code",
    "abstract": "The development of large language models (LLM) has shown progress on reasoning, though studies have largely considered either English or simple reasoning tasks. To address this, we introduce a multilingual structured reasoning and explanation dataset, termed xSTREET, that covers four tasks across six languages. xSTREET exposes a gap in base LLM performance between English and non-English reasoning tasks. We then propose two methods to remedy this gap, building on the insight that LLMs trained on code are better reasoners. First, at training time, we augment a code dataset with multilingual comments using machine translation while keeping program code as-is. Second, at inference time, we bridge the gap between training and inference by employing a prompt structure that incorporates step-by-step code primitives to derive new facts and find a solution. Our methods show improved multilingual performance on xSTREET, most notably on the scientific commonsense reasoning subtask. Furthermore, the models show no regression on non-reasoning tasks, thus demonstrating our techniques maintain general-purpose abilities.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-05",
    "authors": [
      {
        "authorId": "82630293",
        "name": "Bryan Li"
      },
      {
        "authorId": "3169217",
        "name": "Tamer Alkhouli"
      },
      {
        "authorId": "3457102",
        "name": "Daniele Bonadiman"
      },
      {
        "authorId": "2283784519",
        "name": "Nikolaos Pappas"
      },
      {
        "authorId": "39674628",
        "name": "Saab Mansour"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "8ac8eff3ecdc06619c1d412197ddaf8d90fc2cb3",
    "url": "https://www.semanticscholar.org/paper/8ac8eff3ecdc06619c1d412197ddaf8d90fc2cb3",
    "title": "Reasoning about concepts with LLMs: Inconsistencies abound",
    "abstract": "The ability to summarize and organize knowledge into abstract concepts is key to learning and reasoning. Many industrial applications rely on the consistent and systematic use of concepts, especially when dealing with decision-critical knowledge. However, we demonstrate that, when methodically questioned, large language models (LLMs) often display and demonstrate significant inconsistencies in their knowledge. Computationally, the basic aspects of the conceptualization of a given domain can be represented as Is-A hierarchies in a knowledge graph (KG) or ontology, together with a few properties or axioms that enable straightforward reasoning. We show that even simple ontologies can be used to reveal conceptual inconsistencies across several LLMs. We also propose strategies that domain experts can use to evaluate and improve the coverage of key domain concepts in LLMs of various sizes. In particular, we have been able to significantly enhance the performance of LLMs of various sizes with openly available weights using simple knowledge-graph (KG) based prompting strategies.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-30",
    "authors": [
      {
        "authorId": "2291963641",
        "name": "Rosario Uceda-Sosa"
      },
      {
        "authorId": "1704263",
        "name": "K. Ramamurthy"
      },
      {
        "authorId": "2292006436",
        "name": "Maria Chang"
      },
      {
        "authorId": "2286320169",
        "name": "Moninder Singh"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "53af27d243e2d2f83b6a203ca3ab6f6ddf96a60e",
    "url": "https://www.semanticscholar.org/paper/53af27d243e2d2f83b6a203ca3ab6f6ddf96a60e",
    "title": "REAPER: Reasoning based Retrieval Planning for Complex RAG Systems",
    "abstract": "Complex dialog systems often use retrieved evidence to facilitate factual responses. Such RAG (Retrieval Augmented Generation) systems retrieve from massive heterogeneous data stores that are usually architected as multiple indexes or APIs instead of a single monolithic source. For a given query, relevant evidence needs to be retrieved from one or a small subset of possible retrieval sources. Complex queries can even require multi-step retrieval. For example, a conversational agent on a retail site answering customer questions about past orders will need to retrieve the appropriate customer order first and then the evidence relevant to the customer's question in the context of the ordered product. Most RAG Agents handle such Chain-of-Thought (CoT) tasks by interleaving reasoning and retrieval steps. However, each reasoning step directly adds to the latency of the system. For large models this latency cost is significant -- in the order of multiple seconds. Multi-agent systems may classify the query to a single Agent associated with a retrieval source, though this means that a (small) classification model dictates the performance of a large language model. In this work we present REAPER (REAsoning-based PlannER) - an LLM based planner to generate retrieval plans in conversational systems. We show significant gains in latency over Agent-based systems and are able to scale easily to new and unseen use cases as compared to classification-based planning. Though our method can be applied to any RAG system, we show our results in the context of a conversational shopping assistant.",
    "venue": "International Conference on Information and Knowledge Management",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-26",
    "authors": [
      {
        "authorId": "2314548959",
        "name": "Ashutosh Joshi"
      },
      {
        "authorId": "2721029",
        "name": "Sheikh Muhammad Sarwar"
      },
      {
        "authorId": "114027335",
        "name": "Samarth Varshney"
      },
      {
        "authorId": "7529854",
        "name": "Sreyashi Nag"
      },
      {
        "authorId": "2212487573",
        "name": "Shrivats Agrawal"
      },
      {
        "authorId": "2313480617",
        "name": "Juhi Naik"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "09bed28b3221f8aaac7f348cf0a5bb683513433b",
    "url": "https://www.semanticscholar.org/paper/09bed28b3221f8aaac7f348cf0a5bb683513433b",
    "title": "DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text",
    "abstract": "Large Language Models (LLMs) have exhibited impressive generation capabilities, but they suffer from hallucinations when solely relying on their internal knowledge, especially when answering questions that require less commonly known information. Retrieval-augmented LLMs have emerged as a potential solution to ground LLMs in external knowledge. Nonetheless, recent approaches have primarily emphasized retrieval from unstructured text corpora, owing to its seamless integration into prompts. When using structured data such as knowledge graphs, most methods simplify it into natural text, neglecting the underlying structures. Moreover, a significant gap in the current landscape is the absence of a realistic benchmark for evaluating the effectiveness of grounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and text). To fill this gap, we have curated a comprehensive dataset that poses two unique challenges: (1) Two-hop multi-source questions that require retrieving information from both open-domain structured and unstructured knowledge sources; retrieving information from structured knowledge sources is a critical component in correctly answering the questions. (2) The generation of symbolic queries (e.g., SPARQL for Wikidata) is a key requirement, which adds another layer of challenge. Our dataset is created using a combination of automatic generation through predefined reasoning chains and human annotation. We also introduce a novel approach that leverages multiple retrieval tools, including text passage retrieval and symbolic language-assisted retrieval. Our model outperforms previous approaches by a significant margin, demonstrating its effectiveness in addressing the above-mentioned reasoning challenges.",
    "venue": "NAACL-HLT",
    "year": 2023,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-31",
    "authors": [
      {
        "authorId": "2258378876",
        "name": "Wenting Zhao"
      },
      {
        "authorId": "2238385821",
        "name": "Ye Liu"
      },
      {
        "authorId": "2263863440",
        "name": "Tong Niu"
      },
      {
        "authorId": "2242935219",
        "name": "Yao Wan"
      },
      {
        "authorId": "2261463957",
        "name": "Philip S. Yu"
      },
      {
        "authorId": "2708940",
        "name": "Shafiq R. Joty"
      },
      {
        "authorId": "2118860628",
        "name": "Yingbo Zhou"
      },
      {
        "authorId": "3014143",
        "name": "Semih Yavuz"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "b2092ea6163c06136b807ce4fc13781d94de1510",
    "url": "https://www.semanticscholar.org/paper/b2092ea6163c06136b807ce4fc13781d94de1510",
    "title": "VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos",
    "abstract": "Long-form video understanding has been a challenging task due to the high redundancy in video data and the abundance of query-irrelevant information. To tackle this challenge, we propose VideoTree, a training-free framework which builds a query-adaptive and hierarchical video representation for LLM reasoning over long-form videos. First, VideoTree extracts query-relevant information from the input video through an iterative process, progressively refining the selection of keyframes based on their relevance to the query. Furthermore, VideoTree leverages the inherent hierarchical structure of long video data, which is often overlooked by existing LLM-based methods. Specifically, we incorporate multigranularity information into a tree-based representation, allowing VideoTree to extract query-relevant details from long videos in a coarse-to-fine manner. This enables the model to effectively handle a wide range of video queries with varying levels of detail. Finally, VideoTree aggregates the hierarchical query-relevant information within the tree structure and feeds it into an LLM reasoning model to answer the query. Our experiments show that our training-free method improves both reasoning accuracy and efficiency compared to existing methods. Specifically, VideoTree outperforms the existing training-free approaches on the popular EgoSchema and NExT-QA benchmarks with less inference time, achieving 61.1% and 75.6% accuracy on the test set without additional video-specific training. Moreover, on the long split of Video-MME benchmark (average 44 minutes), the training-free VideoTree framework achieves better performance than the strong proprietary GPT-4V model and other MLLMs that were extensively trained on video data.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 23,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-29",
    "authors": [
      {
        "authorId": "2243360880",
        "name": "Ziyang Wang"
      },
      {
        "authorId": "2164249715",
        "name": "Shoubin Yu"
      },
      {
        "authorId": "2281825070",
        "name": "Elias Stengel-Eskin"
      },
      {
        "authorId": "13563486",
        "name": "Jaehong Yoon"
      },
      {
        "authorId": "2268761868",
        "name": "Feng Cheng"
      },
      {
        "authorId": "3313330",
        "name": "Gedas Bertasius"
      },
      {
        "authorId": "2276608813",
        "name": "Mohit Bansal"
      }
    ],
    "source": "semantic_scholar",
    "score": 117.67080745521918
  },
  {
    "paperId": "b156004675ad3aa5e39a56928afc530aec191044",
    "url": "https://www.semanticscholar.org/paper/b156004675ad3aa5e39a56928afc530aec191044",
    "title": "LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks",
    "abstract": "There is considerable confusion about the role of Large Language Models (LLMs) in planning and reasoning tasks. On one side are over-optimistic claims that LLMs can indeed do these tasks with just the right prompting or self-verification strategies. On the other side are perhaps over-pessimistic claims that all that LLMs are good for in planning/reasoning tasks are as mere translators of the problem specification from one syntactic format to another, and ship the problem off to external symbolic solvers. In this position paper, we take the view that both these extremes are misguided. We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature. We will also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end format translators. We present a vision of {\\bf LLM-Modulo Frameworks} that combine the strengths of LLMs with external model-based verifiers in a tighter bi-directional interaction regime. We will show how the models driving the external verifiers themselves can be acquired with the help of LLMs. We will also argue that rather than simply pipelining LLMs and symbolic components, this LLM-Modulo Framework provides a better neuro-symbolic approach that offers tighter integration between LLMs and symbolic components, and allows extending the scope of model-based planning/reasoning regimes towards more flexible knowledge, problem and preference specifications.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 82,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-02",
    "authors": [
      {
        "authorId": "2047340230",
        "name": "Subbarao Kambhampati"
      },
      {
        "authorId": "144982263",
        "name": "Karthik Valmeekam"
      },
      {
        "authorId": "144190085",
        "name": "L. Guan"
      },
      {
        "authorId": "2260339788",
        "name": "Kaya Stechly"
      },
      {
        "authorId": "151500192",
        "name": "Mudit Verma"
      },
      {
        "authorId": "46211062",
        "name": "Siddhant Bhambri"
      },
      {
        "authorId": "2282541267",
        "name": "Lucas Saldyt"
      },
      {
        "authorId": "2276207158",
        "name": "Anil Murthy"
      }
    ],
    "source": "semantic_scholar",
    "score": 136.28260911694898
  },
  {
    "paperId": "8da704439af4caee1dd93d3c1a10688e27953d90",
    "url": "https://www.semanticscholar.org/paper/8da704439af4caee1dd93d3c1a10688e27953d90",
    "title": "LDRE: LLM-based Divergent Reasoning and Ensemble for Zero-Shot Composed Image Retrieval",
    "abstract": null,
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-10",
    "authors": [
      {
        "authorId": "2310915850",
        "name": "Zhenyu Yang"
      },
      {
        "authorId": "2106686837",
        "name": "Dizhan Xue"
      },
      {
        "authorId": "2284669445",
        "name": "Shengsheng Qian"
      },
      {
        "authorId": "2311455021",
        "name": "Weiming Dong"
      },
      {
        "authorId": "2237947496",
        "name": "Changsheng Xu"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.53877639491068
  },
  {
    "paperId": "6ad26eb2d2aa6679d16d9c16fb75cd2cbe1127bc",
    "url": "https://www.semanticscholar.org/paper/6ad26eb2d2aa6679d16d9c16fb75cd2cbe1127bc",
    "title": "See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",
    "abstract": "Large pre-trained vision and language models have demonstrated remarkable capacities for various tasks. However, solving the knowledge-based visual reasoning tasks remains challenging, which requires a model to comprehensively understand image content, connect the external world knowledge, and perform step-by-step reasoning to answer the questions correctly. To this end, we propose a novel framework named Interactive Prompting Visual Reasoner (IPVR) for few-shot knowledge-based visual reasoning. IPVR contains three stages, see, think and confirm. The see stage scans the image and grounds the visual concept candidates with a visual perception model. The think stage adopts a pre-trained large language model (LLM) to attend to the key concepts from candidates adaptively. It then transforms them into text context for prompting with a visual captioning model and adopts the LLM to generate the answer. The confirm stage further uses the LLM to generate the supporting rationale to the answer, verify the generated rationale with a cross-modality classifier and ensure that the rationale can infer the predicted output consistently. We conduct experiments on a range of knowledge-based visual reasoning datasets. We found our IPVR enjoys several benefits, 1). it achieves better performance than the previous few-shot learning baselines; 2). it enjoys the total transparency and trustworthiness of the whole reasoning process by providing rationales for each reasoning step; 3). it is computation-efficient compared with other fine-tuning baselines.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 29,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2301.05226",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-01-12",
    "authors": [
      {
        "authorId": "2111329651",
        "name": "Zhenfang Chen"
      },
      {
        "authorId": "2107604346",
        "name": "Qinhong Zhou"
      },
      {
        "authorId": "2714199",
        "name": "Yikang Shen"
      },
      {
        "authorId": "151261268",
        "name": "Yining Hong"
      },
      {
        "authorId": "38952862",
        "name": "Hao Zhang"
      },
      {
        "authorId": "2056157586",
        "name": "Chuang Gan"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.01796072493232
  },
  {
    "paperId": "5dbffedcabe3fa43060ebbe2b1789500edfd871f",
    "url": "https://www.semanticscholar.org/paper/5dbffedcabe3fa43060ebbe2b1789500edfd871f",
    "title": "Reasoning with Language Model is Planning with World Model",
    "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 354,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.14992",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-24",
    "authors": [
      {
        "authorId": "2128965713",
        "name": "Shibo Hao"
      },
      {
        "authorId": "2112578816",
        "name": "Yi Gu"
      },
      {
        "authorId": "2110816708",
        "name": "Haodi Ma"
      },
      {
        "authorId": "2218162745",
        "name": "Joshua Jiahua Hong"
      },
      {
        "authorId": "47197370",
        "name": "Zhen Wang"
      },
      {
        "authorId": "2111220343",
        "name": "D. Wang"
      },
      {
        "authorId": "2749311",
        "name": "Zhiting Hu"
      }
    ],
    "source": "semantic_scholar",
    "score": 158.08176684213123
  },
  {
    "paperId": "f55294c223752a7159c438951dbf6e6b66cd2e31",
    "url": "https://www.semanticscholar.org/paper/f55294c223752a7159c438951dbf6e6b66cd2e31",
    "title": "Informed AI Regulation: Comparing the Ethical Frameworks of Leading LLM Chatbots Using an Ethics-Based Audit to Assess Moral Reasoning and Normative Values",
    "abstract": "With the rise of individual and collaborative networks of autonomous agents, AI is deployed in more key reasoning and decision-making roles. For this reason, ethics-based audits play a pivotal role in the rapidly growing fields of AI safety and regulation. This paper undertakes an ethics-based audit to probe the 8 leading commercial and open-source Large Language Models including GPT-4. We assess explicability and trustworthiness by a) establishing how well different models engage in moral reasoning and b) comparing normative values underlying models as ethical frameworks. We employ an experimental, evidence-based approach that challenges the models with ethical dilemmas in order to probe human-AI alignment. The ethical scenarios are designed to require a decision in which the particulars of the situation may or may not necessitate deviating from normative ethical principles. A sophisticated ethical framework was consistently elicited in one model, GPT-4. Nonetheless, troubling findings include underlying normative frameworks with clear bias towards particular cultural norms. Many models also exhibit disturbing authoritarian tendencies. Code is available at https://github.com/jonchun/llm-sota-chatbots-ethics-based-audit.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-09",
    "authors": [
      {
        "authorId": "1380792733",
        "name": "Jon Chun"
      },
      {
        "authorId": "2258198921",
        "name": "Katherine Elkins"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "87167c8c8c98aa03a01a056e51dbd9ae3d364b79",
    "url": "https://www.semanticscholar.org/paper/87167c8c8c98aa03a01a056e51dbd9ae3d364b79",
    "title": "QDMR-based Planning-and-Solving Prompting for Complex Reasoning Tasks",
    "abstract": "Chain-of-Thought prompting has improved reasoning capability of large language models (LLM). However, it still is challenging to guarantee the effectiveness and stability for questions requiring complicated reasoning. Recently, Plan-and-Solve prompting enhances the reasoning capability for complex questions by planning the solution steps firstly and then solving them step by step, but it suffers the difficulty to represent and execute the problem-solving logic of complex questions. To deal with these challenges, in this work, we propose a novel Plan-and-Solve prompting method based on Question Decomposition Meaning Representation (QDMR). Specifically, this method first allows the LLM to generate a QDMR graph to represent the problem-solving logic, which is a directed acyclic graph composed of sub-questions. Then, the LLM generates a specific solving process based on the QDMR graph. When solving each sub-question, it can locate the preceding sub-questions and their answers according to the QDMR graph, and then utilize this information for solution. Compared with existing Plan-and-Solve prompting techniques, our method can not only represent the problem-solving logic of complicated questions more accurately with the aid of QDMR graph, but also deliver the dependence information accurately for different solution steps according to the QDMR graph. In addition, with the supervised fine-tuning on the Allen Institute dataset, the decomposing capability of LLM for complicated questions can be considerably enhanced. Extensive experiments show that our method has achieve a great significance in arithmetic reasoning and commonsense reasoning task by comparing the classical Chain-of-Thought prompting and Plan-and-Solve prompting techniques, and the improvements achieved are even greater for problems with more reasoning steps.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2302054271",
        "name": "Jinfeng Huang"
      },
      {
        "authorId": "40430110",
        "name": "Qiaoqiao She"
      },
      {
        "authorId": "2302154632",
        "name": "Wenbin Jiang"
      },
      {
        "authorId": "2301707247",
        "name": "Hua Wu"
      },
      {
        "authorId": "2221298872",
        "name": "Yang Hao"
      },
      {
        "authorId": "2302367032",
        "name": "Tong Xu"
      },
      {
        "authorId": "2301648579",
        "name": "Feng Wu"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "edf3ad2b10d8084c5185072b07f0318f8ed110c9",
    "url": "https://www.semanticscholar.org/paper/edf3ad2b10d8084c5185072b07f0318f8ed110c9",
    "title": "Visual Chain-of-Thought Prompting for Knowledge-Based Visual Reasoning",
    "abstract": "Knowledge-based visual reasoning remains a daunting task since it not only requires machines to interpret the concepts and relationships from visual scenes but also associate them with external world knowledge to conduct a chain of reasoning on open-world questions. Previous works, however, treat visual perception and language-based reasoning as two independent modules, failing to attend to both modules throughout all stages of reasoning. To this end, we propose Visual Chain-of-thought Prompting (VCTP) for knowledge-based reasoning, which involves the interaction between visual content and natural language in an iterative step-by-step reasoning manner. VCTP contains three stages, see, think, and confirm. The see stage scans the image and grounds the visual concept candidates with a visual perception model. The think stage adopts a pre-trained large language model (LLM) to attend to key visual concepts from natural language questions adaptively. It then transforms key visual context into text context for prompting with a visual captioning model, and adopts the LLM to generate the answer. The confirm stage further uses the LLM to generate the supporting rationale to the answer, which is then passed through a cross-modality classifier to verify that it’s consistent with the visual context. We iterate through the think-confirm stages to ensure the verified rationale is consistent with the answer. We conduct experiments on a range of knowledge-based visual reasoning datasets. We found our VCTP enjoys several benefits, 1). it achieves better performance than the previous few-shot learning baselines; 2). it enjoys the total transparency and trustworthiness of the whole reasoning process by providing rationales for each reasoning step; 3). it is computation-efficient compared with other fine-tuning baselines. Our code is available at https://github.com/UMass-Foundation-Model/VisualCoT.git",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://ojs.aaai.org/index.php/AAAI/article/download/27888/27801",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-24",
    "authors": [
      {
        "authorId": "2111329651",
        "name": "Zhenfang Chen"
      },
      {
        "authorId": "2107604346",
        "name": "Qinhong Zhou"
      },
      {
        "authorId": "2258763085",
        "name": "Yikang Shen"
      },
      {
        "authorId": "2265627123",
        "name": "Yining Hong"
      },
      {
        "authorId": "2281903865",
        "name": "Zhiqing Sun"
      },
      {
        "authorId": "2293273510",
        "name": "Dan Gutfreund"
      },
      {
        "authorId": "2248474897",
        "name": "Chuang Gan"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "bed35010543191bf57a09a6058e75332702d7afa",
    "url": "https://www.semanticscholar.org/paper/bed35010543191bf57a09a6058e75332702d7afa",
    "title": "GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers",
    "abstract": "Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs' math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (GSM-Plus) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes when new statements are added or the question targets are altered. We also explore whether more robust performance can be achieved by composing existing prompting methods, in which we try an iterative method that generates and verifies each intermediate thought based on its reasoning goal and calculation result.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 26,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-29",
    "authors": [
      {
        "authorId": "2264516593",
        "name": "Qintong Li"
      },
      {
        "authorId": "2279792419",
        "name": "Leyang Cui"
      },
      {
        "authorId": "2260841029",
        "name": "Xueliang Zhao"
      },
      {
        "authorId": "2260528279",
        "name": "Lingpeng Kong"
      },
      {
        "authorId": "2283842274",
        "name": "Wei Bi"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.43755299006494
  },
  {
    "paperId": "208fdbf3ac095740a53230523db3828a52414da6",
    "url": "https://www.semanticscholar.org/paper/208fdbf3ac095740a53230523db3828a52414da6",
    "title": "PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning",
    "abstract": "Preference-based reinforcement learning (RL) has emerged as a new field in robot learning, where humans play a pivotal role in shaping robot behavior by expressing preferences on different sequences of state-action pairs. However, formulating realistic policies for robots demands responses from humans to an extensive array of queries. In this work, we approach the sample-efficiency challenge by expanding the information collected per query to contain both preferences and optional text prompting. To accomplish this, we leverage the zero-shot capabilities of a large language model (LLM) to reason from the text provided by humans. To accommodate the additional query information, we reformulate the reward learning objectives to contain flexible highlights – state-action pairs that contain relatively high information and are related to the features processed in a zero-shot fashion from a pretrained LLM. In both a simulated scenario and a user study, we reveal the effectiveness of our work by analyzing the feedback and its implications. Additionally, the collective feedback collected serves to train a robot on socially compliant trajectories in a simulated social navigation landscape. We provide video examples of the trained policies at https://sites.google.com/view/rl-predilectCCS CONCEPTS• Computing methodologies → Inverse reinforcement learning; Learning to rank.",
    "venue": "IEEE/ACM International Conference on Human-Robot Interaction",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3610977.3634970",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2024-02-23",
    "authors": [
      {
        "authorId": "2140634515",
        "name": "Simon Holk"
      },
      {
        "authorId": "2082009633",
        "name": "Daniel Marta"
      },
      {
        "authorId": "2266510646",
        "name": "Iolanda Leite"
      }
    ],
    "source": "semantic_scholar",
    "score": 100.1415686865115
  },
  {
    "paperId": "512c3a8b0d5462fe8a98a1713a02d1a3186e3aae",
    "url": "https://www.semanticscholar.org/paper/512c3a8b0d5462fe8a98a1713a02d1a3186e3aae",
    "title": "Are LLMs Ready for Real-World Materials Discovery?",
    "abstract": "Large Language Models (LLMs) create exciting possibilities for powerful language processing tools to accelerate research in materials science. While LLMs have great potential to accelerate materials understanding and discovery, they currently fall short in being practical materials science tools. In this position paper, we show relevant failure cases of LLMs in materials science that reveal current limitations of LLMs related to comprehending and reasoning over complex, interconnected materials science knowledge. Given those shortcomings, we outline a framework for developing Materials Science LLMs (MatSci-LLMs) that are grounded in materials science knowledge and hypothesis generation followed by hypothesis testing. The path to attaining performant MatSci-LLMs rests in large part on building high-quality, multi-modal datasets sourced from scientific literature where various information extraction challenges persist. As such, we describe key materials science information extraction challenges which need to be overcome in order to build large-scale, multi-modal datasets that capture valuable materials science knowledge. Finally, we outline a roadmap for applying future MatSci-LLMs for real-world materials discovery via: 1. Automated Knowledge Base Generation; 2. Automated In-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials Laboratories.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 22,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Physics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-07",
    "authors": [
      {
        "authorId": "2294574515",
        "name": "Santiago Miret"
      },
      {
        "authorId": "2283303413",
        "name": "N. M. A. Krishnan"
      }
    ],
    "source": "semantic_scholar",
    "score": 117.03241323893724
  },
  {
    "paperId": "df38798cb99338e1aac9c4dda154c78787f89df3",
    "url": "https://www.semanticscholar.org/paper/df38798cb99338e1aac9c4dda154c78787f89df3",
    "title": "SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales",
    "abstract": "Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work has elicited confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present SaySelf, a novel training framework that teaches LLMs to express more fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. The generated self-reflective rationales are also reasonable and can further contribute to the calibration. The code is made public at https://github.com/xu1868/SaySelf.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 17,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-31",
    "authors": [
      {
        "authorId": "2481225",
        "name": "Tianyang Xu"
      },
      {
        "authorId": "2304600253",
        "name": "Shujin Wu"
      },
      {
        "authorId": "50826757",
        "name": "Shizhe Diao"
      },
      {
        "authorId": "2109052548",
        "name": "Xiaoze Liu"
      },
      {
        "authorId": "2144803999",
        "name": "Xingyao Wang"
      },
      {
        "authorId": "123331686",
        "name": "Yangyi Chen"
      },
      {
        "authorId": "2294562027",
        "name": "Jing Gao"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.35557636844247
  },
  {
    "paperId": "49ed697a1e5cc5be831e33f2efb7dc61c77d28e7",
    "url": "https://www.semanticscholar.org/paper/49ed697a1e5cc5be831e33f2efb7dc61c77d28e7",
    "title": "Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs",
    "abstract": "Large language models (LLMs) have achieved impressive human-like performance across various reasoning tasks. However, their mastery of underlying inferential rules still falls short of human capabilities. To investigate this, we propose a logic scaffolding inferential rule generation framework, to construct an inferential rule base, ULogic, comprising both primitive and compositional rules across five domains. Our analysis of GPT-series models over a rule subset reveals significant gaps in LLMs' logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns. We further distill these rules into a smaller-scale inference engine for flexible rule generation and enhancing downstream reasoning. Through a multi-judger evaluation, our inference engine proves effective in generating accurate, complex and abstract conclusions and premises, and improve various commonsense reasoning tasks. Overall, our work sheds light on LLMs' limitations in grasping inferential rule and suggests ways to enhance their logical reasoning abilities~\\footnote{Code and data are available at \\url{https://github.com/SiyuanWangw/ULogic}.}.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-18",
    "authors": [
      {
        "authorId": "2266421644",
        "name": "Siyuan Wang"
      },
      {
        "authorId": "2118602528",
        "name": "Zhongyu Wei"
      },
      {
        "authorId": "2266363632",
        "name": "Yejin Choi"
      },
      {
        "authorId": "2228515529",
        "name": "Xiang Ren"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "12b233752c7097ea6525622bed238ae2d2193c5a",
    "url": "https://www.semanticscholar.org/paper/12b233752c7097ea6525622bed238ae2d2193c5a",
    "title": "MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback",
    "abstract": "To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools. However, current evaluation protocols often emphasize benchmark performance with single-turn exchanges, neglecting the nuanced interactions among the user, LLMs, and external tools, while also underestimating the importance of natural language feedback from users. These oversights contribute to discrepancies between research benchmark evaluations and real-world use cases. We introduce MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback. To ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive users' natural language feedback simulated by GPT-4. We repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making and carefully curate them into a compact subset for efficient evaluation. Our analysis of 20 open- and closed-source LLMs offers intriguing findings. (a) LLMs generally benefit from tools and language feedback, with performance gains (absolute, same below) of 1-8% for each turn of tool use and 2-17% with natural language feedback. (b) Better single-turn performance does not guarantee better multi-turn performance. (c) Surprisingly, on the LLMs evaluated, supervised instruction-finetuning (SIFT) and reinforcement learning from human feedback (RLHF) generally hurt multi-turn capabilities. We expect MINT can help measure progress and incentivize research in improving LLMs' capabilities in multi-turn interactions, especially for open-source communities where multi-turn human evaluation can be less accessible compared to commercial LLMs with a larger user base.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 104,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.10691",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-19",
    "authors": [
      {
        "authorId": "2144803999",
        "name": "Xingyao Wang"
      },
      {
        "authorId": "2243360876",
        "name": "Zihan Wang"
      },
      {
        "authorId": "33456794",
        "name": "Jiateng Liu"
      },
      {
        "authorId": "123331686",
        "name": "Yangyi Chen"
      },
      {
        "authorId": "2152195191",
        "name": "Lifan Yuan"
      },
      {
        "authorId": "1818378366",
        "name": "Hao Peng"
      },
      {
        "authorId": "2243197103",
        "name": "Heng Ji"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.80940525236286
  },
  {
    "paperId": "af5f256e9771bf9cd02451195e3a7ac693fde3ed",
    "url": "https://www.semanticscholar.org/paper/af5f256e9771bf9cd02451195e3a7ac693fde3ed",
    "title": "Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning",
    "abstract": "Strong Artificial Intelligence (Strong AI) or Artificial General Intelligence (AGI) with abstract reasoning ability is the goal of next-generation AI. Recent advancements in Large Language Models (LLMs), along with the emerging field of Multimodal Large Language Models (MLLMs), have demonstrated impressive capabilities across a wide range of multimodal tasks and applications. Particularly, various MLLMs, each with distinct model architectures, training data, and training stages, have been evaluated across a broad range of MLLM benchmarks. These studies have, to varying degrees, revealed different aspects of the current capabilities of MLLMs. However, the reasoning abilities of MLLMs have not been systematically investigated. In this survey, we comprehensively review the existing evaluation protocols of multimodal reasoning, categorize and illustrate the frontiers of MLLMs, introduce recent trends in applications of MLLMs on reasoning-intensive tasks, and finally discuss current practices and future directions. We believe our survey establishes a solid base and sheds light on this important topic, multimodal reasoning.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 39,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-01-10",
    "authors": [
      {
        "authorId": "2267386544",
        "name": "Yiqi Wang"
      },
      {
        "authorId": "2267477119",
        "name": "Wentao Chen"
      },
      {
        "authorId": "2267386348",
        "name": "Xiaotian Han"
      },
      {
        "authorId": "2267389064",
        "name": "Xudong Lin"
      },
      {
        "authorId": "2146233407",
        "name": "Haiteng Zhao"
      },
      {
        "authorId": "2267360989",
        "name": "Yongfei Liu"
      },
      {
        "authorId": "2267337627",
        "name": "Bohan Zhai"
      },
      {
        "authorId": "2257015367",
        "name": "Jianbo Yuan"
      },
      {
        "authorId": "2256986936",
        "name": "Quanzeng You"
      },
      {
        "authorId": "2212243155",
        "name": "Hongxia Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 125.33319181170904
  },
  {
    "paperId": "7362b81b808ebafd403eaec6f60a124340d68d71",
    "url": "https://www.semanticscholar.org/paper/7362b81b808ebafd403eaec6f60a124340d68d71",
    "title": "To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning",
    "abstract": "Chain-of-thought (CoT) via prompting is the de facto method for eliciting reasoning capabilities from large language models (LLMs). But for what kinds of tasks is this extra ``thinking'' really helpful? To analyze this, we conducted a quantitative meta-analysis covering over 100 papers using CoT and ran our own evaluations of 20 datasets across 14 models. Our results show that CoT gives strong performance benefits primarily on tasks involving math or logic, with much smaller gains on other types of tasks. On MMLU, directly generating the answer without CoT leads to almost identical accuracy as CoT unless the question or model's response contains an equals sign, indicating symbolic operations and reasoning. Following this finding, we analyze the behavior of CoT on these problems by separating planning and execution and comparing against tool-augmented LLMs. Much of CoT's gain comes from improving symbolic execution, but it underperforms relative to using a symbolic solver. Our results indicate that CoT can be applied selectively, maintaining performance while saving inference costs. Furthermore, they suggest a need to move beyond prompt-based CoT to new paradigms that better leverage intermediate computation across the whole range of LLM applications.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 27,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-18",
    "authors": [
      {
        "authorId": "2150345078",
        "name": "Zayne Sprague"
      },
      {
        "authorId": "2304461507",
        "name": "Fangcong Yin"
      },
      {
        "authorId": "2261903308",
        "name": "Juan Diego Rodriguez"
      },
      {
        "authorId": "2321667770",
        "name": "Dongwei Jiang"
      },
      {
        "authorId": "7341605",
        "name": "Manya Wadhwa"
      },
      {
        "authorId": "2187681933",
        "name": "Prasann Singhal"
      },
      {
        "authorId": "2309536381",
        "name": "Xinyu Zhao"
      },
      {
        "authorId": "50183897",
        "name": "Xi Ye"
      },
      {
        "authorId": "2322808103",
        "name": "Kyle Mahowald"
      },
      {
        "authorId": "1814094",
        "name": "Greg Durrett"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.98306765262805
  },
  {
    "paperId": "d0255ce4c897bea6578955de64fc40324f991878",
    "url": "https://www.semanticscholar.org/paper/d0255ce4c897bea6578955de64fc40324f991878",
    "title": "Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models - A Survey",
    "abstract": "Large language models (LLMs) have recently shown impressive performance on tasks involving reasoning, leading to a lively debate on whether these models possess reasoning capabilities similar to humans. However, despite these successes, the depth of LLMs' reasoning abilities remains uncertain. This uncertainty partly stems from the predominant focus on task performance, measured through shallow accuracy metrics, rather than a thorough investigation of the models' reasoning behavior. This paper seeks to address this gap by providing a comprehensive review of studies that go beyond task accuracy, offering deeper insights into the models' reasoning processes. Furthermore, we survey prevalent methodologies to evaluate the reasoning behavior of LLMs, emphasizing current trends and efforts towards more nuanced reasoning analyses. Our review suggests that LLMs tend to rely on surface-level patterns and correlations in their training data, rather than on sophisticated reasoning abilities. Additionally, we identify the need for further research that delineates the key differences between human and LLM-based reasoning. Through this survey, we aim to shed light on the complex reasoning processes within LLMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 20,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-04-02",
    "authors": [
      {
        "authorId": "2286310256",
        "name": "Philipp Mondorf"
      },
      {
        "authorId": "2286310311",
        "name": "Barbara Plank"
      }
    ],
    "source": "semantic_scholar",
    "score": 115.66783656585135
  },
  {
    "paperId": "384f91e20bd3516b35e564c7f9b43ddd46656b86",
    "url": "https://www.semanticscholar.org/paper/384f91e20bd3516b35e564c7f9b43ddd46656b86",
    "title": "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists",
    "abstract": "Large Language Models (LLMs) are increasingly relied upon to evaluate text outputs of other LLMs, thereby influencing leaderboards and development decisions. However, concerns persist over the accuracy of these assessments and the potential for misleading conclusions. In this work, we investigate the effectiveness of LLMs as evaluators for text generation tasks. We propose FBI, a novel framework designed to examine the proficiency of Evaluator LLMs in assessing four critical abilities in other LLMs: factual accuracy, instruction following, coherence in long-form writing, and reasoning proficiency. By introducing targeted perturbations in answers generated by LLMs, that clearly impact one of these key capabilities, we test whether an Evaluator LLM can detect these quality drops. By creating a total of 2400 perturbed answers covering 22 perturbation categories, we conduct a comprehensive study using different evaluation strategies on five prominent LLMs commonly used as evaluators in the literature. Our findings reveal significant shortcomings in current Evaluator LLMs, which failed to identify quality drops in over 50% of cases on average. Single-answer and pairwise evaluations demonstrated notable limitations, whereas reference-based evaluations showed comparatively better performance. These results underscore the unreliable nature of current Evaluator LLMs and advocate for cautious implementation in practical applications.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-19",
    "authors": [
      {
        "authorId": "2072738714",
        "name": "Sumanth Doddapaneni"
      },
      {
        "authorId": "2281673850",
        "name": "Mohammed Safi Ur Rahman Khan"
      },
      {
        "authorId": "2307572391",
        "name": "Sshubam Verma"
      },
      {
        "authorId": "2361078",
        "name": "Mitesh M. Khapra"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "339d2a56f0e5176b691c358a86891e2923045c8c",
    "url": "https://www.semanticscholar.org/paper/339d2a56f0e5176b691c358a86891e2923045c8c",
    "title": "Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely",
    "abstract": "Large language models (LLMs) augmented with external data have demonstrated remarkable capabilities in completing real-world tasks. Techniques for integrating external data into LLMs, such as Retrieval-Augmented Generation (RAG) and fine-tuning, are gaining increasing attention and widespread application. Nonetheless, the effective deployment of data-augmented LLMs across various specialized fields presents substantial challenges. These challenges encompass a wide range of issues, from retrieving relevant data and accurately interpreting user intent to fully harnessing the reasoning capabilities of LLMs for complex tasks. We believe that there is no one-size-fits-all solution for data-augmented LLM applications. In practice, underperformance often arises from a failure to correctly identify the core focus of a task or because the task inherently requires a blend of multiple capabilities that must be disentangled for better resolution. In this survey, we propose a RAG task categorization method, classifying user queries into four levels based on the type of external data required and primary focus of the task: explicit fact queries, implicit fact queries, interpretable rationale queries, and hidden rationale queries. We define these levels of queries, provide relevant datasets, and summarize the key challenges and most effective techniques for addressing these challenges. Finally, we discuss three main forms of integrating external data into LLMs: context, small model, and fine-tuning, highlighting their respective strengths, limitations, and the types of problems they are suited to solve. This work aims to help readers thoroughly understand and decompose the data requirements and key bottlenecks in building LLM applications, offering solutions to the different challenges and serving as a guide to systematically developing such applications.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-09-23",
    "authors": [
      {
        "authorId": "2268432582",
        "name": "Siyun Zhao"
      },
      {
        "authorId": "2125051198",
        "name": "Yuqing Yang"
      },
      {
        "authorId": "2294387070",
        "name": "Zilong Wang"
      },
      {
        "authorId": "2260609693",
        "name": "Zhiyuan He"
      },
      {
        "authorId": "2180993402",
        "name": "Luna K. Qiu"
      },
      {
        "authorId": "2259937079",
        "name": "Lili Qiu"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "f75673eefe0217464a1d30f6d373e696e26ae055",
    "url": "https://www.semanticscholar.org/paper/f75673eefe0217464a1d30f6d373e696e26ae055",
    "title": "CodeMind: A Framework to Challenge Large Language Models for Code Reasoning",
    "abstract": "Solely relying on test passing to evaluate Large Language Models (LLMs) for code synthesis may result in unfair assessment or promoting models with data leakage. As an alternative, we introduce CodeMind, a framework designed to gauge the code reasoning abilities of LLMs. CodeMind currently supports three code reasoning tasks: Independent Execution Reasoning (IER) , Dependent Execution Reasoning (DER) , and Specification Reasoning (SR) . The first two evaluate models to predict the execution output of an arbitrary code or code the model could correctly synthesize. The third one evaluates the extent to which LLMs implement the specified expected behavior. Our extensive evaluation of nine LLMs across five benchmarks in two different programming languages using CodeMind shows that LLMs fairly follow control flow constructs and, in general, explain how inputs evolve to output, specifically for simple programs and the ones they can correctly synthesize . However, their performance drops for code with higher complexity, non-trivial logical and arithmetic operators, non-primitive types, and API calls. Furthermore, we observe that, while correlated, specification reasoning (essential for code synthesis) does not imply execution reasoning (essential for broader programming tasks such as testing and debugging) : ranking LLMs based on test passing can be different compared to code reasoning 1 .",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2284651628",
        "name": "Changshu Liu"
      },
      {
        "authorId": "2283311189",
        "name": "Shizhuo Dylan Zhang"
      },
      {
        "authorId": "11034998",
        "name": "Reyhaneh Jabbarvand"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "e970d243e9fb56100a0371733466ad887a038938",
    "url": "https://www.semanticscholar.org/paper/e970d243e9fb56100a0371733466ad887a038938",
    "title": "Chat2Data: An Interactive Data Analysis System with RAG, Vector Databases and LLMs",
    "abstract": "\n Traditional data analysis methods require users to write programming codes or issue SQL queries to analyze the data, which are inconvenient for ordinary users. Large language models (LLMs) can alleviate these limitations by enabling users to interact with the data with natural language (NL), e.g., result retrieval and summarization for unstructured data and transforming the NL text to SQL queries or codes for structured data. However, existing LLMs have three limitations: hallucination (due to lacking domain knowledge for vertical domains), high cost for LLM reasoning, and low accuracy for complicated tasks. To address these problems, we propose a prototype, Chat2Data, to interactively analyze the data with natural language. Chat2Data adopts a three-layer method, where the first layer uses Retrieval-Augmented Generation (RAG) to embed domain knowledge in order to address the hallucination problem, the second layer utilizes vector databases to reduce the number of interactions with LLMs so as to improve the performance, and the third layer designs a pipeline agent to decompose a complex task to multiple subtasks and use multiple round reasoning to generate the results in order to improve the accuracy of LLMs. We demonstrate Chat2Data with two real scenarios, unstructured data retrieval and summarization, and natural language-based structured data analysis. The online demo is available at\n http://vdemo.dbmind.cn.\n",
    "venue": "Proceedings of the VLDB Endowment",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-01",
    "authors": [
      {
        "authorId": "2123787551",
        "name": "Xinyang Zhao"
      },
      {
        "authorId": "50177381",
        "name": "Xuanhe Zhou"
      },
      {
        "authorId": "2108491595",
        "name": "Guoliang Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "ae7a3e3b5b75b0bdc2ba10a976303e6a816988fc",
    "url": "https://www.semanticscholar.org/paper/ae7a3e3b5b75b0bdc2ba10a976303e6a816988fc",
    "title": "Unveiling LLMs: The Evolution of Latent Representations in a Dynamic Knowledge Graph",
    "abstract": "Large Language Models (LLMs) demonstrate an impressive capacity to recall a vast range of factual knowledge. However, understanding their underlying reasoning and internal mechanisms in exploiting this knowledge remains a key research area. This work unveils the factual information an LLM represents internally for sentence-level claim verification. We propose an end-to-end framework to decode factual knowledge embedded in token representations from a vector space to a set of ground predicates, showing its layer-wise evolution using a dynamic knowledge graph. Our framework employs activation patching, a vector-level technique that alters a token representation during inference, to extract encoded knowledge. Accordingly, we neither rely on training nor external models. Using factual and common-sense claims from two claim verification datasets, we showcase interpretability analyses at local and global levels. The local analysis highlights entity centrality in LLM reasoning, from claim-related information and multi-hop reasoning to representation errors causing erroneous evaluation. On the other hand, the global reveals trends in the underlying evolution, such as word-based knowledge evolving into claim-related facts. By interpreting semantics from LLM latent representations and enabling graph-related analyses, this work enhances the understanding of the factual knowledge resolution process.",
    "venue": "",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-04-04",
    "authors": [
      {
        "authorId": "2256993568",
        "name": "Marco Bronzini"
      },
      {
        "authorId": "2256990455",
        "name": "Carlo Nicolini"
      },
      {
        "authorId": "2291065942",
        "name": "Bruno Lepri"
      },
      {
        "authorId": "2256994086",
        "name": "Jacopo Staiano"
      },
      {
        "authorId": "2256993219",
        "name": "Andrea Passerini"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "46a5ec31987a12d60ade20c6471db64c46f90106",
    "url": "https://www.semanticscholar.org/paper/46a5ec31987a12d60ade20c6471db64c46f90106",
    "title": "GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements",
    "abstract": "State-of-the-art language models can exhibit impressive reasoning refinement capabilities on math, science or coding tasks. However, recent work demonstrates that even the best models struggle to identify \\textit{when and where to refine} without access to external feedback. Outcome-based Reward Models (\\textbf{ORMs}), trained to predict correctness of the final answer indicating when to refine, offer one convenient solution for deciding when to refine. Process Based Reward Models (\\textbf{PRMs}), trained to predict correctness of intermediate steps, can then be used to indicate where to refine. But they are expensive to train, requiring extensive human annotations. In this paper, we propose Stepwise ORMs (\\textbf{SORMs}) which are trained, only on synthetic data, to approximate the expected future reward of the optimal policy or $V^{\\star}$. More specifically, SORMs are trained to predict the correctness of the final answer when sampling the current policy many times (rather than only once as in the case of ORMs). Our experiments show that SORMs can more accurately detect incorrect reasoning steps compared to ORMs, thus improving downstream accuracy when doing refinements. We then train \\textit{global} refinement models, which take only the question and a draft solution as input and predict a corrected solution, and \\textit{local} refinement models which also take as input a critique indicating the location of the first reasoning error. We generate training data for both models synthetically by reusing data used to train the SORM. We find combining global and local refinements, using the ORM as a reranker, significantly outperforms either one individually, as well as a best of three sample baseline. With this strategy we can improve the accuracy of a LLaMA-2 13B model (already fine-tuned with RL) on GSM8K from 53\\% to 65\\% when greedily sampled.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 25,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-13",
    "authors": [
      {
        "authorId": "2279337437",
        "name": "Alex Havrilla"
      },
      {
        "authorId": "1498636613",
        "name": "S. Raparthy"
      },
      {
        "authorId": "2284685571",
        "name": "Christoforus Nalmpantis"
      },
      {
        "authorId": "2173509991",
        "name": "Jane Dwivedi-Yu"
      },
      {
        "authorId": "2273535109",
        "name": "Maksym Zhuravinskyi"
      },
      {
        "authorId": "2072738644",
        "name": "Eric Hambro"
      },
      {
        "authorId": "2284686654",
        "name": "Roberta Railneau"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.87144807032223
  },
  {
    "paperId": "8236010c2ecc94d826be6010ff187fdc000e7df6",
    "url": "https://www.semanticscholar.org/paper/8236010c2ecc94d826be6010ff187fdc000e7df6",
    "title": "Deductive Verification of Chain-of-Thought Reasoning",
    "abstract": "Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate this procedure, we propose Natural Program, a natural language-based deductive reasoning format. Our approach enables models to generate precise reasoning steps where subsequent steps are more rigorously grounded on prior steps. It also empowers language models to carry out reasoning self-verification in a step-by-step manner. By integrating this verification process into each deductive reasoning stage, we significantly enhance the rigor and trustfulness of generated reasoning steps. Along this process, we also improve the answer correctness on complex reasoning tasks. Code will be released at https://github.com/lz1oceani/verify_cot.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 93,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2306.03872",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-06",
    "authors": [
      {
        "authorId": "49706114",
        "name": "Z. Ling"
      },
      {
        "authorId": "2219045025",
        "name": "Yunhao Fang"
      },
      {
        "authorId": "2108263986",
        "name": "Xuanlin Li"
      },
      {
        "authorId": "18036051",
        "name": "Zhiao Huang"
      },
      {
        "authorId": "2108721816",
        "name": "Mingu Lee"
      },
      {
        "authorId": "1710604",
        "name": "R. Memisevic"
      },
      {
        "authorId": "2087042750",
        "name": "Hao Su"
      }
    ],
    "source": "semantic_scholar",
    "score": 145.14942173405007
  },
  {
    "paperId": "7f397693975382fcf3b19225c0cd89d5f50a017b",
    "url": "https://www.semanticscholar.org/paper/7f397693975382fcf3b19225c0cd89d5f50a017b",
    "title": "OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and Understanding",
    "abstract": "Current universal segmentation methods demonstrate strong capabilities in pixel-level image and video understanding. However, they lack reasoning abilities and cannot be controlled via text instructions. In contrast, large vision-language multimodal models exhibit powerful vision-based conversation and reasoning capabilities but lack pixel-level understanding and have difficulty accepting visual prompts for flexible user interaction. This paper proposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level vision understanding with reasoning abilities. It can accept various visual and text prompts for flexible user interaction. Specifically, we use a universal segmentation method as the visual encoder, integrating image information, perception priors, and visual prompts into visual tokens provided to the LLM. The LLM is responsible for understanding the user's text instructions and providing text responses and pixel-level segmentation results based on the visual information. We propose perception prior embedding to better integrate perception priors with image features. OMG-LLaVA achieves image-level, object-level, and pixel-level reasoning and understanding in a single model, matching or surpassing the performance of specialized methods on multiple benchmarks. Rather than using LLM to connect each specialist, our work aims at end-to-end training on one encoder, one decoder, and one LLM. The code and model have been released for further research.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 19,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-27",
    "authors": [
      {
        "authorId": "2308479884",
        "name": "Tao Zhang"
      },
      {
        "authorId": "2294510925",
        "name": "Xiangtai Li"
      },
      {
        "authorId": "2306978166",
        "name": "Hao Fei"
      },
      {
        "authorId": "2024825871",
        "name": "Haobo Yuan"
      },
      {
        "authorId": "1957924118",
        "name": "Shengqiong Wu"
      },
      {
        "authorId": "2275990780",
        "name": "Shunping Ji"
      },
      {
        "authorId": "2253394312",
        "name": "Chen Change Loy"
      },
      {
        "authorId": "2289843775",
        "name": "Shuicheng Yan"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.93598410330986
  },
  {
    "paperId": "3721eb984e7e00a94dd9fc84e0183b2ab5991033",
    "url": "https://www.semanticscholar.org/paper/3721eb984e7e00a94dd9fc84e0183b2ab5991033",
    "title": "EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records",
    "abstract": "Clinicians often rely on data engineers to retrieve complex patient information from electronic health record (EHR) systems, a process that is both inefficient and time-consuming. We propose EHRAgent, a large language model (LLM) agent empowered with accumulative domain knowledge and robust coding capability. EHRAgent enables autonomous code generation and execution to facilitate clinicians in directly interacting with EHRs using natural language. Specifically, we formulate a multi-tabular reasoning task based on EHRs as a tool-use planning process, efficiently decomposing a complex task into a sequence of manageable actions with external toolsets. We first inject relevant medical information to enable EHRAgent to effectively reason about the given query, identifying and extracting the required records from the appropriate tables. By integrating interactive coding and execution feedback, EHRAgent then effectively learns from error messages and iteratively improves its originally generated code. Experiments on three real-world EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate, verifying its strong capacity to tackle complex clinical tasks with minimal demonstrations.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 16,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2401.07128",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-01-13",
    "authors": [
      {
        "authorId": "2263890944",
        "name": "Wenqi Shi"
      },
      {
        "authorId": "2265148831",
        "name": "Ran Xu"
      },
      {
        "authorId": "8103389",
        "name": "Yuchen Zhuang"
      },
      {
        "authorId": "2218865512",
        "name": "Yue Yu"
      },
      {
        "authorId": "2269506893",
        "name": "Jieyu Zhang"
      },
      {
        "authorId": "2279616366",
        "name": "Hang Wu"
      },
      {
        "authorId": "1390861238",
        "name": "Yuanda Zhu"
      },
      {
        "authorId": "2263536473",
        "name": "Joyce C. Ho"
      },
      {
        "authorId": "2237940940",
        "name": "Carl Yang"
      },
      {
        "authorId": "2237844925",
        "name": "M. D. Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.49820016084324
  },
  {
    "paperId": "40fbc19197da3e414aa6e460c5e39789cf248100",
    "url": "https://www.semanticscholar.org/paper/40fbc19197da3e414aa6e460c5e39789cf248100",
    "title": "OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning",
    "abstract": "The advances in multimodal large language models (MLLMs) have led to growing interests in LLM-based autonomous driving agents to leverage their strong reasoning capabilities. However, capitalizing on MLLMs' strong reasoning capabilities for improved planning behavior is challenging since planning requires full 3D situational awareness beyond 2D reasoning. To address this challenge, our work proposes a holistic framework for strong alignment between agent models and 3D driving tasks. Our framework starts with a novel 3D MLLM architecture that uses sparse queries to lift and compress visual representations into 3D before feeding them into an LLM. This query-based representation allows us to jointly encode dynamic objects and static map elements (e.g., traffic lanes), providing a condensed world model for perception-action alignment in 3D. We further propose OmniDrive-nuScenes, a new visual question-answering dataset challenging the true 3D situational awareness of a model with comprehensive visual question-answering (VQA) tasks, including scene description, traffic regulation, 3D grounding, counterfactual reasoning, decision making and planning. Extensive studies show the effectiveness of the proposed architecture as well as the importance of the VQA tasks for reasoning and planning in complex 3D scenes.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 18,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-02",
    "authors": [
      {
        "authorId": "2299491757",
        "name": "Shihao Wang"
      },
      {
        "authorId": "2269841405",
        "name": "Zhiding Yu"
      },
      {
        "authorId": "2195197250",
        "name": "Xiaohui Jiang"
      },
      {
        "authorId": "2269148725",
        "name": "Shiyi Lan"
      },
      {
        "authorId": "2299915202",
        "name": "Min Shi"
      },
      {
        "authorId": "2299325416",
        "name": "Nadine Chang"
      },
      {
        "authorId": "2273651410",
        "name": "Jan Kautz"
      },
      {
        "authorId": "2299950187",
        "name": "Ying Li"
      },
      {
        "authorId": "2268388373",
        "name": "José M. Álvarez"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.1665846874966
  },
  {
    "paperId": "6c87972f5d839f0cba36756f059494f15f9eaa68",
    "url": "https://www.semanticscholar.org/paper/6c87972f5d839f0cba36756f059494f15f9eaa68",
    "title": "SelfPiCo: Self-Guided Partial Code Execution with LLMs",
    "abstract": "Code executability plays a vital role in software debugging and testing (e.g., detecting runtime exceptions or assertion violations). However, code execution, especially partial or arbitrary code execution, is a non-trivial task due to missing definitions and complex third-party dependencies. To make partial code (such as code snippets posted on the web or code fragments deep inside complex software projects) executable, the existing study has proposed a machine learning model to predict the undefined element types and inject the pre-defined dummy values into execution. However, the performance of their tool is limited due to its simply designed dummy values and the inability to continue learning. In this paper, we design and implement a novel framework, named SelfPiCo (Self Guided Partial Code Executor), to dynamically guide partial code execution by incorporating the open-source LLM (i.e., Code Llama) within an interactive loop. Particularly, SelfPiCo leverages few-shot in-context learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning based on fine-tuning the Code Llama model. SelfPiCo continuously learns from code execution results and refines its predictions step after step. Our evaluations demonstrate that SelfPiCo can execute 72.7% and 83.3% of all lines in the open-source code and Stack Overflow snippets, outperforming the most recent state-of-the-art Lexecutor by 37.9% and 33.5%, respectively. Moreover, SelfPiCo successfully detected 18 and 33 runtime type error issues by executing the partial code from eight GitHub software projects and 43 Stack Overflow posts, demonstrating the practical usage and potential application of our framework in practice.",
    "venue": "International Symposium on Software Testing and Analysis",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-24",
    "authors": [
      {
        "authorId": "2242785350",
        "name": "Zhipeng Xue"
      },
      {
        "authorId": "2243685513",
        "name": "Zhipeng Gao"
      },
      {
        "authorId": "2313011672",
        "name": "Shaohua Wang"
      },
      {
        "authorId": "2243000314",
        "name": "Xing Hu"
      },
      {
        "authorId": "2300429108",
        "name": "Xin Xia"
      },
      {
        "authorId": "2244166471",
        "name": "Shanping Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "45a92d8483c80ccfc0ae497ce308d882a11a368e",
    "url": "https://www.semanticscholar.org/paper/45a92d8483c80ccfc0ae497ce308d882a11a368e",
    "title": "WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge Conflicts from Wikipedia",
    "abstract": "Retrieval-augmented generation (RAG) has emerged as a promising solution to mitigate the limitations of large language models (LLMs), such as hallucinations and outdated information. However, it remains unclear how LLMs handle knowledge conflicts arising from different augmented retrieved passages, especially when these passages originate from the same source and have equal trustworthiness. In this work, we conduct a comprehensive evaluation of LLM-generated answers to questions that have varying answers based on contradictory passages from Wikipedia, a dataset widely regarded as a high-quality pre-training resource for most LLMs. Specifically, we introduce WikiContradict, a benchmark consisting of 253 high-quality, human-annotated instances designed to assess LLM performance when augmented with retrieved passages containing real-world knowledge conflicts. We benchmark a diverse range of both closed and open-source LLMs under different QA scenarios, including RAG with a single passage, and RAG with 2 contradictory passages. Through rigorous human evaluations on a subset of WikiContradict instances involving 5 LLMs and over 3,500 judgements, we shed light on the behaviour and limitations of these models. For instance, when provided with two passages containing contradictory facts, all models struggle to generate answers that accurately reflect the conflicting nature of the context, especially for implicit conflicts requiring reasoning. Since human evaluation is costly, we also introduce an automated model that estimates LLM performance using a strong open-source language model, achieving an F-score of 0.8. Using this automated metric, we evaluate more than 1,500 answers from seven LLMs across all WikiContradict instances. To facilitate future work, we release WikiContradict on: https://ibm.biz/wikicontradict.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-19",
    "authors": [
      {
        "authorId": "2307897753",
        "name": "Yufang Hou"
      },
      {
        "authorId": "2307463172",
        "name": "Alessandra Pascale"
      },
      {
        "authorId": "2307467605",
        "name": "Javier Carnerero-Cano"
      },
      {
        "authorId": "3235883",
        "name": "T. Tchrakian"
      },
      {
        "authorId": "2808565",
        "name": "Radu Marinescu"
      },
      {
        "authorId": "2307465399",
        "name": "Elizabeth Daly"
      },
      {
        "authorId": "8350409",
        "name": "Inkit Padhi"
      },
      {
        "authorId": "1706272",
        "name": "P. Sattigeri"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "e569ac1cd6ad137319dfe476c947f5771b806efe",
    "url": "https://www.semanticscholar.org/paper/e569ac1cd6ad137319dfe476c947f5771b806efe",
    "title": "ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models",
    "abstract": "Context-aware Human Activity Recognition (HAR) is a hot research area in mobile computing, and the most effective solutions in the literature are based on supervised deep learning models. However, the actual deployment of these systems is limited by the scarcity of labeled data that is required for training. Neuro-Symbolic AI (NeSy) provides an interesting research direction to mitigate this issue, by infusing common-sense knowledge about human activities and the contexts in which they can be performed into HAR deep learning classifiers. Existing NeSy methods for context-aware HAR rely on knowledge encoded in logic-based models (e.g., ontologies) whose design, implementation, and maintenance to capture new activities and contexts require significant human engineering efforts, technical knowledge, and domain expertise. Recent works show that pre-trained Large Language Models (LLMs) effectively encode common-sense knowledge about human activities. In this work, we propose ContextGPT: a novel prompt engineering approach to retrieve from LLMs common-sense knowledge about the relationship between human activities and the context in which they are performed. Unlike ontologies, ContextGPT requires limited human effort and expertise, while sharing similar privacy concerns if the reasoning is performed in the cloud. An extensive evaluation using two public datasets shows how a NeSy model obtained by infusing common-sense knowledge from ContextGPT is effective in data scarcity scenarios, leading to similar (and sometimes better) recognition rates than logic-based approaches with a fraction of the effort.",
    "venue": "International Conference on Smart Computing",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-11",
    "authors": [
      {
        "authorId": "1885363235",
        "name": "Luca Arrotta"
      },
      {
        "authorId": "2279209665",
        "name": "Claudio Bettini"
      },
      {
        "authorId": "2825281",
        "name": "Gabriele Civitarese"
      },
      {
        "authorId": "2105730364",
        "name": "Michele Fiori"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "63f2c402f5ddcdc65145b6cde29ad724e1923329",
    "url": "https://www.semanticscholar.org/paper/63f2c402f5ddcdc65145b6cde29ad724e1923329",
    "title": "Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG",
    "abstract": "Retrieval-augmented generation (RAG) empowers large language models (LLMs) to utilize external knowledge sources. The increasing capacity of LLMs to process longer input sequences opens up avenues for providing more retrieved information, to potentially enhance the quality of generated outputs. It is plausible to assume that a larger retrieval set would contain more relevant information (higher recall), that might result in improved performance. However, our empirical findings demonstrate that for many long-context LLMs, the quality of generated output initially improves first, but then subsequently declines as the number of retrieved passages increases. This paper investigates this phenomenon, identifying the detrimental impact of retrieved\"hard negatives\"as a key contributor. To mitigate this and enhance the robustness of long-context LLM-based RAG, we propose both training-free and training-based approaches. We first showcase the effectiveness of retrieval reordering as a simple yet powerful training-free optimization. Furthermore, we explore training-based methods, specifically RAG-specific implicit LLM fine-tuning and RAG-oriented fine-tuning with intermediate reasoning, demonstrating their capacity for substantial performance gains. Finally, we conduct a systematic analysis of design choices for these training-based methods, including data distribution, retriever selection, and training context length.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-08",
    "authors": [
      {
        "authorId": "2057050247",
        "name": "Bowen Jin"
      },
      {
        "authorId": "2256335437",
        "name": "Jinsung Yoon"
      },
      {
        "authorId": "2257136881",
        "name": "Jiawei Han"
      },
      {
        "authorId": "2676352",
        "name": "Sercan Ö. Arik"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "5e9f22bba13332709cc9880d7e197385bca316de",
    "url": "https://www.semanticscholar.org/paper/5e9f22bba13332709cc9880d7e197385bca316de",
    "title": "RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization",
    "abstract": "Low-Rank Adaptation (LoRA), as a representative Parameter-Efficient Fine-Tuning (PEFT)method, significantly enhances the training efficiency by updating only a small portion of the weights in Large Language Models (LLMs). Recently, weight-only quantization techniques have also been applied to LoRA methods to reduce the memory footprint of fine-tuning. However, applying weight-activation quantization to the LoRA pipeline is under-explored, and we observe substantial performance degradation primarily due to the presence of activation outliers. In this work, we propose RoLoRA, the first LoRA-based scheme for effective weight-activation quantization. RoLoRA utilizes rotation for outlier elimination and proposes rotation-aware fine-tuning to preserve the outlier-free characteristics in rotated LLMs. Experimental results show RoLoRA consistently improves low-bit LoRA convergence and post-training quantization robustness in weight-activation settings. We evaluate RoLoRA across LLaMA2-7B/13B, LLaMA3-8B models, achieving up to 29.5% absolute accuracy gain of 4-bit weight-activation quantized LLaMA2- 13B on commonsense reasoning tasks compared to LoRA baseline. We further demonstrate its effectiveness on Large Multimodal Models (LLaVA-1.5-7B). Codes are available at https://github.com/HuangOwen/RoLoRA",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-10",
    "authors": [
      {
        "authorId": "2261688809",
        "name": "Xijie Huang"
      },
      {
        "authorId": "2109370860",
        "name": "Zechun Liu"
      },
      {
        "authorId": "2220637583",
        "name": "Shih-Yang Liu"
      },
      {
        "authorId": "2256381600",
        "name": "Kwang-Ting Cheng"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "f8df9546eafe207a777466ef40d548c707744c6d",
    "url": "https://www.semanticscholar.org/paper/f8df9546eafe207a777466ef40d548c707744c6d",
    "title": "LLMRG: Improving Recommendations through Large Language Model Reasoning Graphs",
    "abstract": "Recommendation systems aim to provide users with relevant suggestions, but often lack interpretability and fail to capture higher-level semantic relationships between user behaviors and profiles. In this paper, we propose a novel approach that leverages large language models (LLMs) to construct personalized reasoning graphs. These graphs link a user's profile and behavioral sequences through causal and logical inferences, representing the user's interests in an interpretable way. Our approach, LLM reasoning graphs (LLMRG), has four components: chained graph reasoning, divergent extension, self-verification and scoring, and knowledge base self-improvement. The resulting reasoning graph is encoded using graph neural networks, which serves as additional input to improve conventional recommender systems, without requiring extra user or item information. Our approach demonstrates how LLMs can enable more logical and interpretable recommender systems through personalized reasoning graphs. LLMRG allows recommendations to benefit from both engineered recommendation systems and LLM-derived reasoning graphs. We demonstrate the effectiveness of LLMRG on benchmarks and real-world scenarios in enhancing base recommendation models.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2024,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "https://ojs.aaai.org/index.php/AAAI/article/download/29887/31548",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-24",
    "authors": [
      {
        "authorId": "2205710030",
        "name": "Yan Wang"
      },
      {
        "authorId": "2237992280",
        "name": "Zhixuan Chu"
      },
      {
        "authorId": "2262613411",
        "name": "Ouyang Xin"
      },
      {
        "authorId": "2143244250",
        "name": "Simeng Wang"
      },
      {
        "authorId": "2065513241",
        "name": "Hongyan Hao"
      },
      {
        "authorId": "2180240771",
        "name": "Yue Shen"
      },
      {
        "authorId": "2283257801",
        "name": "Jinjie Gu"
      },
      {
        "authorId": "2149919635",
        "name": "Siqiao Xue"
      },
      {
        "authorId": "2124955096",
        "name": "James Y. Zhang"
      },
      {
        "authorId": "2276424293",
        "name": "Qing Cui"
      },
      {
        "authorId": "2238177474",
        "name": "Longfei Li"
      },
      {
        "authorId": "2293705830",
        "name": "Jun Zhou"
      },
      {
        "authorId": "2262276636",
        "name": "Sheng Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "adb9acaf9184bdbd23105f1a383848eed9bc82fc",
    "url": "https://www.semanticscholar.org/paper/adb9acaf9184bdbd23105f1a383848eed9bc82fc",
    "title": "Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models",
    "abstract": "With the widespread use of large language models (LLMs) in NLP tasks, researchers have discovered the potential of Chain-of-thought (CoT) to assist LLMs in accomplishing complex reasoning tasks by generating intermediate steps. However, human thought processes are often non-linear, rather than simply sequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT) reasoning, which models human thought processes not only as a chain but also as a graph. By representing thought units as nodes and connections between them as edges, our approach captures the non-sequential nature of human thinking and allows for a more realistic modeling of thought processes. Similar to Multimodal-CoT [1], we modeled GoT reasoning as a two-stage framework, generating rationales first and then producing the final answer. Specifically, we employ an additional graph-of-thoughts encoder for GoT representation learning and fuse the GoT representation with the original input representation through a gated fusion mechanism. We implement a GoT reasoning model on the T5 pre-trained model and evaluate its performance on a text-only reasoning task (GSM8K) and a multimodal reasoning task (ScienceQA). Our model achieves significant improvement over the strong CoT baseline with 3.41% and 5.08% on the GSM8K test set with T5-base and T5-large architectures, respectively. Additionally, our model boosts accuracy from 84.91% to 91.54% using the T5-base model and from 91.68% to 92.77% using the T5-large model over the state-of-the-art Multimodal-CoT on the ScienceQA test set. Experiments have shown that GoT achieves comparable results to Multimodal-CoT large with over 700M parameters, despite having fewer than 250M backbone model parameters, demonstrating the effectiveness of GoT.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 52,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.16582",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2154857867",
        "name": "Yao Yao"
      },
      {
        "authorId": "30658665",
        "name": "Z. Li"
      },
      {
        "authorId": "2146232510",
        "name": "Hai Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 129.55437870328183
  },
  {
    "paperId": "bddb9d818b73de0a06197a6966673c7eb63c9146",
    "url": "https://www.semanticscholar.org/paper/bddb9d818b73de0a06197a6966673c7eb63c9146",
    "title": "Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory",
    "abstract": "Memory-augmented Large Language Models (LLMs) have demonstrated remarkable performance in long-term human-machine interactions, which basically relies on iterative recalling and reasoning of history to generate high-quality responses. However, such repeated recall-reason steps easily produce biased thoughts, \\textit{i.e.}, inconsistent reasoning results when recalling the same history for different questions. On the contrary, humans can keep thoughts in the memory and recall them without repeated reasoning. Motivated by this human capability, we propose a novel memory mechanism called TiM (Think-in-Memory) that enables LLMs to maintain an evolved memory for storing historical thoughts along the conversation stream. The TiM framework consists of two crucial stages: (1) before generating a response, a LLM agent recalls relevant thoughts from memory, and (2) after generating a response, the LLM agent post-thinks and incorporates both historical and new thoughts to update the memory. Thus, TiM can eliminate the issue of repeated reasoning by saving the post-thinking thoughts as the history. Besides, we formulate the basic principles to organize the thoughts in memory based on the well-established operations, (\\textit{i.e.}, insert, forget, and merge operations), allowing for dynamic updates and evolution of the thoughts. Furthermore, we introduce Locality-Sensitive Hashing into TiM to achieve efficient retrieval for the long-term conversations. We conduct qualitative and quantitative experiments on real-world and simulated dialogues covering a wide range of topics, demonstrating that equipping existing LLMs with TiM significantly enhances their performance in generating responses for long-term interactions.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 14,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-15",
    "authors": [
      {
        "authorId": "2267013360",
        "name": "Lei Liu"
      },
      {
        "authorId": "2215685191",
        "name": "Xiaoyan Yang"
      },
      {
        "authorId": "2180240771",
        "name": "Yue Shen"
      },
      {
        "authorId": "145743766",
        "name": "Binbin Hu"
      },
      {
        "authorId": "2266809812",
        "name": "Zhiqiang Zhang"
      },
      {
        "authorId": "50771151",
        "name": "Jinjie Gu"
      },
      {
        "authorId": "2260732798",
        "name": "Guannan Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "ebf3a59aacdd9982283d7f41229ee2a93800d6ef",
    "url": "https://www.semanticscholar.org/paper/ebf3a59aacdd9982283d7f41229ee2a93800d6ef",
    "title": "Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks",
    "abstract": "Large Language Models (LLMs) have shown promising performance in knowledge-intensive reasoning tasks that require a compound understanding of knowledge. However, deployment of the LLMs in real-world applications can be challenging due to their high computational requirements and concerns on data privacy. Previous studies have focused on building task-specific small Language Models (LMs) by fine-tuning them with labeled data or distilling LLMs. However, these approaches are ill-suited for knowledge-intensive reasoning tasks due to the limited capacity of small LMs in memorizing the knowledge required. Motivated by our theoretical analysis on memorization, we propose Knowledge-Augmented Reasoning Distillation (KARD), a novel method that fine-tunes small LMs to generate rationales obtained from LLMs with augmented knowledge retrieved from an external knowledge base. Moreover, we further propose a neural reranker to obtain documents relevant to rationale generation. We empirically show that KARD significantly improves the performance of small T5 and GPT models on the challenging knowledge-intensive reasoning datasets, namely MedQA-USMLE, StrategyQA, and OpenbookQA. Notably, our method makes the 250M T5 models achieve superior performance against the fine-tuned 3B models, having 12 times larger parameters, on both MedQA-USMLE and StrategyQA benchmarks.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 41,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.18395",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-28",
    "authors": [
      {
        "authorId": "120434407",
        "name": "Minki Kang"
      },
      {
        "authorId": "1472875852",
        "name": "Seanie Lee"
      },
      {
        "authorId": "90765684",
        "name": "Jinheon Baek"
      },
      {
        "authorId": "1392876047",
        "name": "Kenji Kawaguchi"
      },
      {
        "authorId": "35788904",
        "name": "Sung Ju Hwang"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.0650442742505
  },
  {
    "paperId": "cc7310f7cf2729719949a6bc56831fedea8ffa33",
    "url": "https://www.semanticscholar.org/paper/cc7310f7cf2729719949a6bc56831fedea8ffa33",
    "title": "LLM Augmented LLMs: Expanding Capabilities through Composition",
    "abstract": "Foundational models with billions of parameters which have been trained on large corpora of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities, several new instances of these models are being trained towards new domains and tasks. In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end, we propose CALM -- Composition to Augment Language Models -- which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using' existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings. We illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13\\% on tasks like translation into English and arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is augmented with a code-specific model, we see a relative improvement of 40\\% over the base model for code generation and explanation tasks -- on-par with fully fine-tuned counterparts.",
    "venue": "International Conference on Learning Representations",
    "year": 2024,
    "citationCount": 26,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-04",
    "authors": [
      {
        "authorId": "80494139",
        "name": "Rachit Bansal"
      },
      {
        "authorId": "2277741562",
        "name": "Bidisha Samanta"
      },
      {
        "authorId": "35186886",
        "name": "Siddharth Dalmia"
      },
      {
        "authorId": "2285178",
        "name": "Nitish Gupta"
      },
      {
        "authorId": "3404827",
        "name": "Shikhar Vashishth"
      },
      {
        "authorId": "1726355",
        "name": "Sriram Ganapathy"
      },
      {
        "authorId": "2277740200",
        "name": "Abhishek Bapna"
      },
      {
        "authorId": "2277742787",
        "name": "Prateek Jain"
      },
      {
        "authorId": "2408872",
        "name": "P. Talukdar"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.43755299006494
  },
  {
    "paperId": "d48b29889241551e1ee6622fa78c3fa4159255dd",
    "url": "https://www.semanticscholar.org/paper/d48b29889241551e1ee6622fa78c3fa4159255dd",
    "title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning",
    "abstract": "Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.",
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "citationCount": 303,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2205.09712",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-05-19",
    "authors": [
      {
        "authorId": "3433026",
        "name": "Antonia Creswell"
      },
      {
        "authorId": "1757629",
        "name": "M. Shanahan"
      },
      {
        "authorId": "39051054",
        "name": "I. Higgins"
      }
    ],
    "source": "semantic_scholar",
    "score": 155.7554155210933
  },
  {
    "paperId": "ef8c21e1f574495f0c80b8c1037dbdb886f0808d",
    "url": "https://www.semanticscholar.org/paper/ef8c21e1f574495f0c80b8c1037dbdb886f0808d",
    "title": "Towards Language-guided Interactive 3D Generation: LLMs as Layout Interpreter with Generative Feedback",
    "abstract": "Generating and editing a 3D scene guided by natural language poses a challenge, primarily due to the complexity of specifying the positional relations and volumetric changes within the 3D space. Recent advancements in Large Language Models (LLMs) have demonstrated impressive reasoning, conversational, and zero-shot generation abilities across various domains. Surprisingly, these models also show great potential in realizing and interpreting the 3D space. In light of this, we propose a novel language-guided interactive 3D generation system, dubbed LI3D, that integrates LLMs as a 3D layout interpreter into the off-the-shelf layout-to-3D generative models, allowing users to flexibly and interactively generate visual content. Specifically, we design a versatile layout structure base on the bounding boxes and semantics to prompt the LLMs to model the spatial generation and reasoning from language. Our system also incorporates LLaVA, a large language and vision assistant, to provide generative feedback from the visual aspect for improving the visual quality of generated content. We validate the effectiveness of LI3D, primarily in 3D generation and editing through multi-round interactions, which can be flexibly extended to 2D generation and editing. Various experiments demonstrate the potential benefits of incorporating LLMs in generative AI for applications, e.g., metaverse. Moreover, we benchmark the layout reasoning performance of LLMs with neural visual artist tasks, revealing their emergent ability in the spatial layout domain.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 11,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.15808",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-25",
    "authors": [
      {
        "authorId": "46396519",
        "name": "Yiqi Lin"
      },
      {
        "authorId": "1664776313",
        "name": "Hao Wu"
      },
      {
        "authorId": "2213709086",
        "name": "Ruichen Wang"
      },
      {
        "authorId": "2130373",
        "name": "H. Lu"
      },
      {
        "authorId": "2117690698",
        "name": "Xiaodong Lin"
      },
      {
        "authorId": "2217886184",
        "name": "Hui Xiong"
      },
      {
        "authorId": "2168616303",
        "name": "Lin Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "98aa313fbe30734eb3bb50683204765bcbc607eb",
    "url": "https://www.semanticscholar.org/paper/98aa313fbe30734eb3bb50683204765bcbc607eb",
    "title": "Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning",
    "abstract": "Recent advancements have significantly augmented the reasoning capabilities of Large Language Models (LLMs) through various methodologies, especially chain-of-thought (CoT) reasoning. However, previous methods fail to address reasoning errors in intermediate steps, leading to accumulative errors. In this paper, we propose Deductive Beam Search (DBS), which seamlessly integrates CoT and deductive reasoning with step-wise beam search for LLMs. Our approach deploys a verifier, verifying the deducibility of a reasoning step and its premises, thus alleviating the error accumulation. Furthermore, we introduce a scalable and labor-free data construction method to amplify our model's verification capabilities. Extensive experiments demonstrate that our approach significantly enhances the base performance of LLMs of various scales (7B, 13B, 70B, and ChatGPT) across 8 reasoning datasets from 3 diverse reasoning genres, including arithmetic, commonsense, and symbolic. Moreover, our analysis proves DBS's capability of detecting diverse and subtle reasoning errors and robustness on different model scales.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-31",
    "authors": [
      {
        "authorId": "2222400339",
        "name": "Tinghui Zhu"
      },
      {
        "authorId": "145086492",
        "name": "Kai Zhang"
      },
      {
        "authorId": "2153624353",
        "name": "Jian Xie"
      },
      {
        "authorId": "1758652",
        "name": "Yu Su"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "40de3296157b9d7a7882b61f967e37b3cc93f197",
    "url": "https://www.semanticscholar.org/paper/40de3296157b9d7a7882b61f967e37b3cc93f197",
    "title": "Merlin: Empowering Multimodal LLMs with Foresight Minds",
    "abstract": "Humans possess the remarkable ability to foresee the future to a certain extent based on present observations, a skill we term as foresight minds. However, this capability remains largely under explored within existing Multimodal Large Language Models (MLLMs), hindering their capacity to learn the fundamental principles of how things operate and the intentions behind the observed subjects. To address this issue, we introduce the integration of future modeling into the existing learning frameworks of MLLMs. By utilizing the subject trajectory, a highly structured representation of a consecutive frame sequence, as a learning objective, we aim to bridge the gap between the past and the future. We propose two innovative methods to empower MLLMs with foresight minds, Foresight Pre-Training (FPT) and Foresight Instruction-Tuning (FIT), which are inspired by the modern learning paradigm of LLMs. Specifically, FPT jointly training various tasks centered on trajectories, enabling MLLMs to learn how to attend and predict entire trajectories from a given initial observation. Then, FIT requires MLLMs to first predict trajectories of related objects and then reason about potential future events based on them. Aided by FPT and FIT, we build a novel and unified MLLM named Merlin that supports multi-images input and analysis about potential actions of multiple objects for the future reasoning. Experimental results show Merlin powerful foresight minds with impressive performance on both future reasoning and visual comprehension tasks.",
    "venue": "European Conference on Computer Vision",
    "year": 2023,
    "citationCount": 18,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-30",
    "authors": [
      {
        "authorId": "2269147997",
        "name": "En Yu"
      },
      {
        "authorId": "48096671",
        "name": "Liang Zhao"
      },
      {
        "authorId": "2269422010",
        "name": "Yana Wei"
      },
      {
        "authorId": "2161319190",
        "name": "Jinrong Yang"
      },
      {
        "authorId": "2269424517",
        "name": "Dongming Wu"
      },
      {
        "authorId": "2269248623",
        "name": "Lingyu Kong"
      },
      {
        "authorId": "2269416991",
        "name": "Haoran Wei"
      },
      {
        "authorId": "3361759",
        "name": "Tiancai Wang"
      },
      {
        "authorId": "2242581956",
        "name": "Zheng Ge"
      },
      {
        "authorId": "2185865433",
        "name": "Xiangyu Zhang"
      },
      {
        "authorId": "2269141370",
        "name": "Wenbing Tao"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.1665846874966
  },
  {
    "paperId": "5d4dc85f11fdcdbaba2f628e96fc7af078f4386a",
    "url": "https://www.semanticscholar.org/paper/5d4dc85f11fdcdbaba2f628e96fc7af078f4386a",
    "title": "How Good is my Video LMM? Complex Video Reasoning and Robustness Evaluation Suite for Video-LMMs",
    "abstract": "Recent advancements in Large Language Models (LLMs) have led to the development of Video Large Multi-modal Models (Video-LMMs) that can handle a wide range of video understanding tasks. These models have the potential to be deployed in real-world applications such as robotics, AI assistants, medical surgery, and autonomous vehicles. The widespread adoption of Video-LMMs in our daily lives underscores the importance of ensuring and evaluating their robust performance in mirroring human-like reasoning and interaction capabilities in complex, real-world contexts. However, existing benchmarks for Video-LMMs primarily focus on general video comprehension abilities and neglect assessing their reasoning capabilities over complex videos in the real-world context, and robustness of these models through the lens of user prompts as text queries. In this paper, we present the Complex Video Reasoning and Robustness Evaluation Suite (CVRR-ES), a novel benchmark that comprehensively assesses the performance of Video-LMMs across 11 diverse real-world video dimensions. We evaluate 9 recent models, including both open-source and closed-source variants, and find that most of the Video-LMMs, especially open-source ones, struggle with robustness and reasoning when dealing with complex videos. Based on our analysis, we develop a training-free Dual-Step Contextual Prompting (DSCP) technique to enhance the performance of existing Video-LMMs. Our findings provide valuable insights for building the next generation of human-centric AI systems with advanced robustness and reasoning capabilities. Our dataset and code are publicly available at: https://mbzuai-oryx.github.io/CVRR-Evaluation-Suite/.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-06",
    "authors": [
      {
        "authorId": "2175250687",
        "name": "Muhammad Uzair Khattak"
      },
      {
        "authorId": "2057127601",
        "name": "Muhammad Ferjad Naeem"
      },
      {
        "authorId": "2264135372",
        "name": "Jameel Hassan"
      },
      {
        "authorId": "40894826",
        "name": "Muzammal Naseer"
      },
      {
        "authorId": "2273557728",
        "name": "Federico Tombari"
      },
      {
        "authorId": "2242949919",
        "name": "F. Khan"
      },
      {
        "authorId": "2259545774",
        "name": "Salman H. Khan"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "bbfcc31b79110c28047b2ba03ee55335acc8e47c",
    "url": "https://www.semanticscholar.org/paper/bbfcc31b79110c28047b2ba03ee55335acc8e47c",
    "title": "Exploring Qualitative Research Using LLMs",
    "abstract": "The advent of AI driven large language models (LLMs) have stirred discussions about their role in qualitative research. Some view these as tools to enrich human understanding, while others perceive them as threats to the core values of the discipline. This study aimed to compare and contrast the comprehension capabilities of humans and LLMs. We conducted an experiment with small sample of Alexa app reviews, initially classified by a human analyst. LLMs were then asked to classify these reviews and provide the reasoning behind each classification. We compared the results with human classification and reasoning. The research indicated a significant alignment between human and ChatGPT 3.5 classifications in one third of cases, and a slightly lower alignment with GPT4 in over a quarter of cases. The two AI models showed a higher alignment, observed in more than half of the instances. However, a consensus across all three methods was seen only in about one fifth of the classifications. In the comparison of human and LLMs reasoning, it appears that human analysts lean heavily on their individual experiences. As expected, LLMs, on the other hand, base their reasoning on the specific word choices found in app reviews and the functional components of the app itself. Our results highlight the potential for effective human LLM collaboration, suggesting a synergistic rather than competitive relationship. Researchers must continuously evaluate LLMs role in their work, thereby fostering a future where AI and humans jointly enrich qualitative research.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.13298",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-06-23",
    "authors": [
      {
        "authorId": "2514873",
        "name": "Muneera Bano"
      },
      {
        "authorId": "1740931",
        "name": "Didar Zowghi"
      },
      {
        "authorId": "2193085367",
        "name": "Jon Whittle"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "f8a2813614f4e9c8adab3da2cbb667ad7e4d6bcf",
    "url": "https://www.semanticscholar.org/paper/f8a2813614f4e9c8adab3da2cbb667ad7e4d6bcf",
    "title": "Enhancing Recommender Systems with Large Language Model Reasoning Graphs",
    "abstract": "Recommendation systems aim to provide users with relevant suggestions, but often lack interpretability and fail to capture higher-level semantic relationships between user behaviors and profiles. In this paper, we propose a novel approach that leverages large language models (LLMs) to construct personalized reasoning graphs. These graphs link a user's profile and behavioral sequences through causal and logical inferences, representing the user's interests in an interpretable way. Our approach, LLM reasoning graphs (LLMRG), has four components: chained graph reasoning, divergent extension, self-verification and scoring, and knowledge base self-improvement. The resulting reasoning graph is encoded using graph neural networks, which serves as additional input to improve conventional recommender systems, without requiring extra user or item information. Our approach demonstrates how LLMs can enable more logical and interpretable recommender systems through personalized reasoning graphs. LLMRG allows recommendations to benefit from both engineered recommendation systems and LLM-derived reasoning graphs. We demonstrate the effectiveness of LLMRG on benchmarks and real-world scenarios in enhancing base recommendation models.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 41,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.10835",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-08-21",
    "authors": [
      {
        "authorId": "2205710030",
        "name": "Yan Wang"
      },
      {
        "authorId": "1491708389",
        "name": "Zhixuan Chu"
      },
      {
        "authorId": "2262613411",
        "name": "Ouyang Xin"
      },
      {
        "authorId": "2143244250",
        "name": "Simeng Wang"
      },
      {
        "authorId": "2065513241",
        "name": "Hongyan Hao"
      },
      {
        "authorId": "2180240771",
        "name": "Yue Shen"
      },
      {
        "authorId": "50771151",
        "name": "Jinjie Gu"
      },
      {
        "authorId": "2149919635",
        "name": "Siqiao Xue"
      },
      {
        "authorId": "2124955096",
        "name": "James Y. Zhang"
      },
      {
        "authorId": "145742001",
        "name": "Qing Cui"
      },
      {
        "authorId": "2151580",
        "name": "Longfei Li"
      },
      {
        "authorId": "2151550753",
        "name": "Jun Zhou"
      },
      {
        "authorId": "2153698676",
        "name": "Shenghe Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 126.06504427425052
  },
  {
    "paperId": "44b506d9619b5f957dc2b5588801138f343c0308",
    "url": "https://www.semanticscholar.org/paper/44b506d9619b5f957dc2b5588801138f343c0308",
    "title": "Let's reward step by step: Step-Level reward model as the Navigators for Reasoning",
    "abstract": "Recent years have seen considerable advancements in multi-step reasoning with Large Language Models (LLMs). The previous studies have elucidated the merits of integrating feedback or search mechanisms during model inference to improve the reasoning accuracy. The Process-Supervised Reward Model (PRM), typically furnishes LLMs with step-by-step feedback during the training phase, akin to Proximal Policy Optimization (PPO) or reject sampling. Our objective is to examine the efficacy of PRM in the inference phase to help discern the optimal solution paths for multi-step tasks such as mathematical reasoning and code generation. To this end, we propose a heuristic greedy search algorithm that employs the step-level feedback from PRM to optimize the reasoning pathways explored by LLMs. This tailored PRM demonstrated enhanced results compared to the Chain of Thought (CoT) on mathematical benchmarks like GSM8K and MATH. Additionally, to explore the versatility of our approach, we develop a novel method to automatically generate step-level reward dataset for coding tasks and observed similar improved performance in the code generation tasks. Thus highlighting the robust nature of our reward-model-based approach to inference for reasoning tasks.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 24,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-16",
    "authors": [
      {
        "authorId": "2258790160",
        "name": "Qianli Ma"
      },
      {
        "authorId": "2258876454",
        "name": "Haotian Zhou"
      },
      {
        "authorId": "2254792618",
        "name": "Tingkai Liu"
      },
      {
        "authorId": "2257015367",
        "name": "Jianbo Yuan"
      },
      {
        "authorId": "2258936203",
        "name": "Pengfei Liu"
      },
      {
        "authorId": "2258772720",
        "name": "Yang You"
      },
      {
        "authorId": "2212243155",
        "name": "Hongxia Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.28313737302301
  },
  {
    "paperId": "86c26c7b14874d43de2eb40da6fec2e9182e2f22",
    "url": "https://www.semanticscholar.org/paper/86c26c7b14874d43de2eb40da6fec2e9182e2f22",
    "title": "Reasoning Grasping via Multimodal Large Language Model",
    "abstract": "Despite significant progress in robotic systems for operation within human-centric environments, existing models still heavily rely on explicit human commands to identify and manipulate specific objects. This limits their effectiveness in environments where understanding and acting on implicit human intentions are crucial. In this study, we introduce a novel task: reasoning grasping, where robots need to generate grasp poses based on indirect verbal instructions or intentions. To accomplish this, we propose an end-to-end reasoning grasping model that integrates a multimodal Large Language Model (LLM) with a vision-based robotic grasping framework. In addition, we present the first reasoning grasping benchmark dataset generated from the GraspNet-1 billion, incorporating implicit instructions for object-level and part-level grasping. Our results show that directly integrating CLIP or LLaVA with the grasp detection model performs poorly on the challenging reasoning grasping tasks, while our proposed model demonstrates significantly enhanced performance both in the reasoning grasping benchmark and real-world experiments.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-09",
    "authors": [
      {
        "authorId": "2284031489",
        "name": "Shiyu Jin"
      },
      {
        "authorId": "2283877765",
        "name": "Jinxuan Xu"
      },
      {
        "authorId": "2284032009",
        "name": "Yutian Lei"
      },
      {
        "authorId": "2211183718",
        "name": "Liangjun Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "70017c9096cca317942156e3f91f8141da42eef1",
    "url": "https://www.semanticscholar.org/paper/70017c9096cca317942156e3f91f8141da42eef1",
    "title": "In-Context Explainers: Harnessing LLMs for Explaining Black Box Models",
    "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated exceptional capabilities in complex tasks like machine translation, commonsense reasoning, and language understanding. One of the primary reasons for the adaptability of LLMs in such diverse tasks is their in-context learning (ICL) capability, which allows them to perform well on new tasks by simply using a few task samples in the prompt. Despite their effectiveness in enhancing the performance of LLMs on diverse language and tabular tasks, these methods have not been thoroughly explored for their potential to generate post hoc explanations. In this work, we carry out one of the first explorations to analyze the effectiveness of LLMs in explaining other complex predictive models using ICL. To this end, we propose a novel framework, In-Context Explainers, comprising of three novel approaches that exploit the ICL capabilities of LLMs to explain the predictions made by other predictive models. We conduct extensive analysis with these approaches on real-world tabular and text datasets and demonstrate that LLMs are capable of explaining other predictive models similar to state-of-the-art post hoc explainers, opening up promising avenues for future research into LLM-based post hoc explanations of complex predictive models.",
    "venue": "",
    "year": 2023,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-10-09",
    "authors": [
      {
        "authorId": "2256987587",
        "name": "Nicholas Kroeger"
      },
      {
        "authorId": "2256981456",
        "name": "Dan Ley"
      },
      {
        "authorId": "1387484410",
        "name": "Satyapriya Krishna"
      },
      {
        "authorId": "40228633",
        "name": "Chirag Agarwal"
      },
      {
        "authorId": "1892673",
        "name": "Himabindu Lakkaraju"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "7946939665d679486ee1e44a140def77562854fb",
    "url": "https://www.semanticscholar.org/paper/7946939665d679486ee1e44a140def77562854fb",
    "title": "Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective",
    "abstract": "Large language models (LLMs), like ChatGPT, have shown some human-like cognitive abilities. For comparing these abilities of different models, several benchmarks (i.e. sets of standard test questions) from different fields (e.g., Literature, Biology and Psychology) are often adopted and the test results under traditional metrics such as accuracy, recall and F1, are reported. However, such way for evaluating LLMs can be inefficient and inaccurate from the cognitive science perspective. Inspired by Computerized Adaptive Testing (CAT) used in psychometrics, we propose an adaptive testing framework for LLM evaluation. Rather than using a standard test set and simply reporting accuracy, this approach dynamically adjusts the characteristics of the test questions, such as difficulty, based on the model’s performance. This allows for a more accurate estimation of the model’s abilities, using fewer questions. More importantly, it allows LLMs to be compared with humans easily, which is essential for NLP models that aim for human-level ability. Our diagnostic reports have found that ChatGPT often behaves like a “careless student”, prone to slip and occasionally guessing the questions. We conduct a fine-grained diagnosis and rank the latest 6 instruction-tuned LLMs from three aspects of Subject Knowledge, Mathematical Reasoning, and Programming, where GPT4 can outperform other models significantly and reach the cognitive ability of middle-level students. Different tests for different models using efficient adaptive testing — we believe this has the potential to become a new norm in evaluating large language models.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 13,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.10512",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2261108632",
        "name": "Zhuang Yan"
      },
      {
        "authorId": "2278390541",
        "name": "Qi Liu"
      },
      {
        "authorId": "2106771355",
        "name": "Yuting Ning"
      },
      {
        "authorId": "2256944949",
        "name": "Weizhe Huang"
      },
      {
        "authorId": "2106277178",
        "name": "Rui Lv"
      },
      {
        "authorId": "3374015",
        "name": "Zhenya Huang"
      },
      {
        "authorId": "2220892203",
        "name": "Guanhao Zhao"
      },
      {
        "authorId": "2288120643",
        "name": "Zheng Zhang"
      },
      {
        "authorId": "2300614912",
        "name": "Qingyang Mao"
      },
      {
        "authorId": "2144180493",
        "name": "Shijin Wang"
      },
      {
        "authorId": "2264094060",
        "name": "Enhong Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "40b420cad2fa52491d0d001351ce18764d20eec1",
    "url": "https://www.semanticscholar.org/paper/40b420cad2fa52491d0d001351ce18764d20eec1",
    "title": "LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual Contexts",
    "abstract": "We propose LogicVista, an evaluation benchmark that assesses the integrated logical reasoning capabilities of multimodal large language models (MLLMs) in Visual contexts. Recent advancements in MLLMs have demonstrated various fascinating abilities, from crafting poetry based on an image to performing mathematical reasoning. However, there is still a lack of systematic evaluation of MLLMs' proficiency in logical reasoning tasks, which are essential for activities like navigation and puzzle-solving. Thus we evaluate general logical cognition abilities across 5 logical reasoning tasks encompassing 9 different capabilities, using a sample of 448 multiple-choice questions. Each question is annotated with the correct answer and the human-written reasoning behind the selection, enabling both open-ended and multiple-choice evaluation. A total of 8 MLLMs are comprehensively evaluated using LogicVista. Code and Data Available at https://github.com/Yijia-Xiao/LogicVista.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-06",
    "authors": [
      {
        "authorId": "95289709",
        "name": "Yijia Xiao"
      },
      {
        "authorId": "2310336411",
        "name": "Edward Sun"
      },
      {
        "authorId": "2310398131",
        "name": "Tianyu Liu"
      },
      {
        "authorId": "2256611611",
        "name": "Wei Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "e8da1f7293657674c4f3648e227eb1b3e6c861f7",
    "url": "https://www.semanticscholar.org/paper/e8da1f7293657674c4f3648e227eb1b3e6c861f7",
    "title": "DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning",
    "abstract": "Large language models (LLMs) have made impressive progress in handling simple math problems, yet they still struggle with more challenging and complex mathematical tasks. In this paper, we introduce a series of LLMs that employs the Decomposition of thought with code assistance and self-correction for mathematical reasoning, dubbed as DotaMath. DotaMath models tackle complex mathematical tasks by decomposing them into simpler logical subtasks, leveraging code to solve these subtasks, obtaining fine-grained feedback from the code interpreter, and engaging in self-reflection and correction. By annotating diverse interactive tool-use trajectories and employing query evolution on GSM8K and MATH datasets, we generate an instruction fine-tuning dataset called DotaMathQA with 574K query-response pairs. We train a series of base LLMs using imitation learning on DotaMathQA, resulting in DotaMath models that achieve remarkable performance compared to open-source LLMs across various in-domain and out-of-domain benchmarks. Notably, DotaMath-deepseek-7B showcases an outstanding performance of 64.8% on the competitive MATH dataset and 86.7% on GSM8K. Besides, DotaMath-deepseek-7B maintains strong competitiveness on a series of in-domain and out-of-domain benchmarks (Avg. 80.1%). Looking forward, we anticipate that the DotaMath paradigm will open new pathways for addressing intricate mathematical problems. Our code is publicly available at https://github.com/ChengpengLi1003/DotaMath.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-04",
    "authors": [
      {
        "authorId": "2257039734",
        "name": "Chengpeng Li"
      },
      {
        "authorId": "51490462",
        "name": "Guanting Dong"
      },
      {
        "authorId": "2065790119",
        "name": "Mingfeng Xue"
      },
      {
        "authorId": "2310233042",
        "name": "Ru Peng"
      },
      {
        "authorId": "2310253743",
        "name": "Xiang Wang"
      },
      {
        "authorId": "2248487202",
        "name": "Dayiheng Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "f696ffb1f0408e06ab4d91985f3e3f837c370c77",
    "url": "https://www.semanticscholar.org/paper/f696ffb1f0408e06ab4d91985f3e3f837c370c77",
    "title": "RATT: A Thought Structure for Coherent and Correct LLM Reasoning",
    "abstract": "Large Language Models (LLMs) gain substantial reasoning and decision-making capabilities from thought structures. However, existing methods such as Tree of Thought and Retrieval Augmented Thoughts often fall short in complex tasks due to the limitations of insufficient local retrieval of factual knowledge and inadequate global selection of strategies. These limitations make it challenging for these methods to balance factual accuracy and comprehensive logical optimization effectively. To address these limitations, we introduce the Retrieval Augmented Thought Tree (RATT), a novel thought structure that considers both overall logical soundness and factual correctness at each step of the thinking process. Specifically, at every point of a thought branch, RATT performs planning and lookahead to explore and evaluate multiple potential reasoning steps, and integrate the fact-checking ability of Retrieval-Augmented Generation (RAG) with LLM's ability to assess overall strategy. Through this combination of factual knowledge and strategic feasibility, the RATT adjusts and integrates the thought tree structure to search for the most promising branches within the search space. This thought structure significantly enhances the model's coherence in logical inference and efficiency in decision-making, and thus increases the limit of the capacity of LLM to generate reliable inferences and decisions based on thought structures. A broad range of experiments on different types of tasks showcases that the RATT structure significantly outperforms existing methods in factual correctness and logical coherence.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-04",
    "authors": [
      {
        "authorId": "2304897819",
        "name": "Jinghan Zhang"
      },
      {
        "authorId": "2304896765",
        "name": "Xiting Wang"
      },
      {
        "authorId": "22500310",
        "name": "Weijieying Ren"
      },
      {
        "authorId": "2304822381",
        "name": "Lu Jiang"
      },
      {
        "authorId": "1669829502",
        "name": "Dongjie Wang"
      },
      {
        "authorId": "2238129790",
        "name": "Kunpeng Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "dc068165839ecd0e74eff9c85356b419033a3484",
    "url": "https://www.semanticscholar.org/paper/dc068165839ecd0e74eff9c85356b419033a3484",
    "title": "MathChat: Benchmarking Mathematical Reasoning and Instruction Following in Multi-Turn Interactions",
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in mathematical problem solving, particularly in single turn question answering formats. However, real world scenarios often involve mathematical question answering that requires multi turn or interactive information exchanges, and the performance of LLMs on these tasks is still underexplored. This paper introduces MathChat, a comprehensive benchmark specifically designed to evaluate LLMs across a broader spectrum of mathematical tasks. These tasks are structured to assess the models' abilities in multiturn interactions and open ended generation. We evaluate the performance of various SOTA LLMs on the MathChat benchmark, and we observe that while these models excel in single turn question answering, they significantly underperform in more complex scenarios that require sustained reasoning and dialogue understanding. To address the above limitations of existing LLMs when faced with multiturn and open ended tasks, we develop MathChat sync, a synthetic dialogue based math dataset for LLM finetuning, focusing on improving models' interaction and instruction following capabilities in conversations. Experimental results emphasize the need for training LLMs with diverse, conversational instruction tuning datasets like MathChatsync. We believe this work outlines one promising direction for improving the multiturn mathematical reasoning abilities of LLMs, thus pushing forward the development of LLMs that are more adept at interactive mathematical problem solving and real world applications.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-29",
    "authors": [
      {
        "authorId": "2284181828",
        "name": "Zhenwen Liang"
      },
      {
        "authorId": "2265968215",
        "name": "Dian Yu"
      },
      {
        "authorId": "2265843326",
        "name": "Wenhao Yu"
      },
      {
        "authorId": "2087264100",
        "name": "Wenlin Yao"
      },
      {
        "authorId": "72871419",
        "name": "Zhihan Zhang"
      },
      {
        "authorId": "2248274721",
        "name": "Xiangliang Zhang"
      },
      {
        "authorId": "2303999735",
        "name": "Dong Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "1c673e6cdccd972b032661489ceec122380ecbc3",
    "url": "https://www.semanticscholar.org/paper/1c673e6cdccd972b032661489ceec122380ecbc3",
    "title": "RAR-b: Reasoning as Retrieval Benchmark",
    "abstract": "Semantic textual similartiy (STS) and information retrieval tasks (IR) tasks have been the two major avenues to record the progress of embedding models in the past few years. Under the emerging Retrieval-augmented Generation (RAG) paradigm, we envision the need to evaluate next-level language understanding abilities of embedding models, and take a conscious look at the reasoning abilities stored in them. Addressing this, we pose the question: Can retrievers solve reasoning problems? By transforming reasoning tasks into retrieval tasks, we find that without specifically trained for reasoning-level language understanding, current state-of-the-art retriever models may still be far from being competent for playing the role of assisting LLMs, especially in reasoning-intensive tasks. Moreover, albeit trained to be aware of instructions, instruction-aware IR models are often better off without instructions in inference time for reasoning tasks, posing an overlooked retriever-LLM behavioral gap for the research community to align. However, recent decoder-based embedding models show great promise in narrowing the gap, highlighting the pathway for embedding models to achieve reasoning-level language understanding. We also show that, although current off-the-shelf re-ranker models fail on these tasks, injecting reasoning abilities into them through fine-tuning still appears easier than doing so to bi-encoders, and we are able to achieve state-of-the-art performance across all tasks by fine-tuning a reranking model. We release Reasoning as Retrieval Benchmark (RAR-b), a holistic suite of tasks and settings to evaluate the reasoning abilities stored in retriever models. RAR-b is available at https://github.com/gowitheflow-1998/RAR-b.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-09",
    "authors": [
      {
        "authorId": "2261665873",
        "name": "Chenghao Xiao"
      },
      {
        "authorId": "2296489514",
        "name": "G. Thomas"
      },
      {
        "authorId": "2295735589",
        "name": "Hudson Noura"
      },
      {
        "authorId": "2261673713",
        "name": "Al Moubayed"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "a1853500aa8ff46e440a6f2e76bfbb2a06b25378",
    "url": "https://www.semanticscholar.org/paper/a1853500aa8ff46e440a6f2e76bfbb2a06b25378",
    "title": "TrumorGPT: Query Optimization and Semantic Reasoning over Networks for Automated Fact-Checking",
    "abstract": "In the age of social media, the rapid spread of misinformation and rumors has led to the emergence of infodemics, where false information poses a significant threat to society. To combat this issue, we introduce TrumorGPT, a novel generative artificial intelligence solution designed for automated fact-checking. TrumorGPT aims to distinguish \"trumors\", which are rumors that turn out to be true, providing a crucial tool in differentiating between mere speculation and verified facts. This framework merges machine learning with natural language processing techniques, leveraging a large language model (LLM) with few-shot learning for knowledge graph construction and semantic reasoning. TrumorGPT addresses the \"hallucination\" issue common in LLMs and the limitations of static training data by incorporating retrieval-augmented generation. This approach involves accessing and utilizing information from regularly updated knowledge graphs that consist of the latest news and information, ensuring that fact-checking of TrumorGPT is based on the most recent data. Accessing updated knowledge graphs greatly enhances the proficiency of TrumorGPT in delivering accurate and reliable information promptly. Evaluating with extensive datasets, TrumorGPT demonstrates superior performance in automated fact-checking. Its ability to effectively conduct automated fact-checking across various platforms marks a critical step forward in the fight against misinformation, enhancing trust and accuracy in the digital information age.",
    "venue": "Annual Conference on Information Sciences and Systems",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-13",
    "authors": [
      {
        "authorId": "52205613",
        "name": "C. Hang"
      },
      {
        "authorId": "3395394",
        "name": "Pei-Duo Yu"
      },
      {
        "authorId": "2149491635",
        "name": "C. Tan"
      }
    ],
    "source": "semantic_scholar",
    "score": 106.87639203842082
  },
  {
    "paperId": "0e3c90f70019f7c736e3abd762c652e3e2561fec",
    "url": "https://www.semanticscholar.org/paper/0e3c90f70019f7c736e3abd762c652e3e2561fec",
    "title": "CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents",
    "abstract": "The generalization of decision-making agents encompasses two fundamental elements: learning from past experiences and reasoning in novel contexts. However, the predominant emphasis in most interactive environments is on learning, often at the expense of complexity in reasoning. In this paper, we introduce CivRealm, an environment inspired by the Civilization game. Civilization's profound alignment with human history and society necessitates sophisticated learning, while its ever-changing situations demand strong reasoning to generalize. Particularly, CivRealm sets up an imperfect-information general-sum game with a changing number of players; it presents a plethora of complex features, challenging the agent to deal with open-ended stochastic environments that require diplomacy and negotiation skills. Within CivRealm, we provide interfaces for two typical agent types: tensor-based agents that focus on learning, and language-based agents that emphasize reasoning. To catalyze further research, we present initial results for both paradigms. The canonical RL-based agents exhibit reasonable performance in mini-games, whereas both RL- and LLM-based agents struggle to make substantial progress in the full game. Overall, CivRealm stands as a unique learning and reasoning challenge for decision-making agents. The code is available at https://github.com/bigai-ai/civrealm.",
    "venue": "International Conference on Learning Representations",
    "year": 2024,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-19",
    "authors": [
      {
        "authorId": "2253407938",
        "name": "Siyuan Qi"
      },
      {
        "authorId": "2280101123",
        "name": "Shuo Chen"
      },
      {
        "authorId": "2280099936",
        "name": "Yexin Li"
      },
      {
        "authorId": "2280077352",
        "name": "Xiangyu Kong"
      },
      {
        "authorId": "2280103973",
        "name": "Junqi Wang"
      },
      {
        "authorId": "2280175556",
        "name": "Bangcheng Yang"
      },
      {
        "authorId": "2280064602",
        "name": "Pring Wong"
      },
      {
        "authorId": "2260345704",
        "name": "Yifan Zhong"
      },
      {
        "authorId": "2280132758",
        "name": "Xiaoyuan Zhang"
      },
      {
        "authorId": "2174174943",
        "name": "Zhaowei Zhang"
      },
      {
        "authorId": "2292163480",
        "name": "Nian Liu"
      },
      {
        "authorId": "2258856134",
        "name": "Wei Wang"
      },
      {
        "authorId": "2249834874",
        "name": "Yaodong Yang"
      },
      {
        "authorId": "2249929708",
        "name": "Song-Chun Zhu"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "a58fc5d8a848489f2280dec358f0165311c63d9f",
    "url": "https://www.semanticscholar.org/paper/a58fc5d8a848489f2280dec358f0165311c63d9f",
    "title": "AI and Human Reasoning: Qualitative Research in the Age of Large Language Models",
    "abstract": "Context: The advent of AI-driven large language models (LLMs), such as ChatGPT 3.5 and GPT-4, have stirred discussions about their role in qualitative research. Some view these as tools to enrich human understanding, while others perceive them as threats to the core values of the discipline. Problem: A significant concern revolves around the disparity between AI-generated classifications and human comprehension, prompting questions about the reliability of AI-derived insights. An “AI echo chamber” could potentially risk the diversity inherent in qualitative research. A minimal overlap between AI and human interpretations amplifies concerns about the fading human element in research. Objective: This study aimed to compare and contrast the comprehension capabilities of humans and LLMs, specifically ChatGPT 3.5 and GPT-4. Methodology: We conducted an experiment with small sample of Alexa app reviews, initially classified by a human analyst. ChatGPT 3.5 and GPT-4 were then asked to classify these reviews and provide the reasoning behind each classification. We compared the results with human classification and reasoning. Results: The research indicated a significant alignment between human and ChatGPT 3.5 classifications in one-third of cases, and a slightly lower alignment with GPT-4 in over a quarter of cases. The two AI models showed a higher alignment, observed in more than half of the instances. However, a consensus across all three methods was seen only in about one-fifth of the classifications. In the comparison of human and LLMs reasoning, it appears that human analysts lean heavily on their individual experiences. As expected, LLMs, on the other hand, base their reasoning on the specific word choices found in app reviews and the functional components of the app itself. Conclusion: Our results highlight the potential for effective human-LLM collaboration, suggesting a synergistic rather than competitive relationship. Researchers must continuously evaluate LLMs’ role in their work, thereby fostering a future where AI and humans jointly enrich qualitative research.",
    "venue": "AI Ethics Journal",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://aiej.org/aiej/article/download/11/49",
      "status": "GOLD"
    },
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-01-22",
    "authors": [
      {
        "authorId": "2514873",
        "name": "Muneera Bano"
      },
      {
        "authorId": "1740931",
        "name": "Didar Zowghi"
      },
      {
        "authorId": "2281112146",
        "name": "Jon Whittle"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "c8a93db7e0d56adbc18217de5138fe0bde570c01",
    "url": "https://www.semanticscholar.org/paper/c8a93db7e0d56adbc18217de5138fe0bde570c01",
    "title": "EVIT: Event-Oriented Instruction Tuning for Event Reasoning",
    "abstract": "Events refer to specific occurrences, incidents, or happenings that take place under a particular background. Event reasoning aims to infer events according to certain relations and predict future events. The cutting-edge techniques for event reasoning play a crucial role in various natural language processing applications. Large language models (LLMs) have made significant advancements in event reasoning owing to their wealth of knowledge and reasoning capabilities. However, smaller instruction-tuned models currently in use do not consistently demonstrate exceptional proficiency in managing these tasks. This discrepancy arises from the absence of explicit modeling of events and the interconnections of them within their instruction data. Consequently, these models face challenges in comprehending event structures and semantics while struggling to bridge the gap between their interpretations and human understanding of events. Additionally, their limitations in grasping event relations lead to constrained event reasoning abilities to effectively deduce and incorporate pertinent event knowledge. In this paper, we propose Event-Oriented Instruction Tuning (EvIT) to train our LLM. Specifically, we first propose a novel structure named event quadruple which contains the structure and semantics of events and is complete in the event representation. We then design event-relation learning based on the structures. We encapsulate the learning into the instruction-tuning formulation to better stimulate the event reasoning capacity of our model. We design a heuristic unsupervised method to mine event quadruple from a large-scale corpus. At last, we finetune a Llama model on our Event-Oriented Instruction Tuning. We conduct extensive experiments on event reasoning tasks on several datasets. Automatic and human evaluations demonstrate EvIT achieves competitive performances on event reasoning.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-18",
    "authors": [
      {
        "authorId": "148437893",
        "name": "Zhengwei Tao"
      },
      {
        "authorId": "2296953537",
        "name": "Xiancai Chen"
      },
      {
        "authorId": "2152843656",
        "name": "Zhi Jin"
      },
      {
        "authorId": "2219053675",
        "name": "Xiaoying Bai"
      },
      {
        "authorId": "2187705776",
        "name": "Haiyan Zhao"
      },
      {
        "authorId": "2297189383",
        "name": "Yiwei Lou"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "20bba6da7c6cf4ca1d25be9806d98995ca52660e",
    "url": "https://www.semanticscholar.org/paper/20bba6da7c6cf4ca1d25be9806d98995ca52660e",
    "title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models",
    "abstract": "Large Language Models (LLMs) demonstrate enhanced capabilities and reliability by reasoning more, evolving from Chain-of-Thought prompting to product-level solutions like OpenAI o1. Despite various efforts to improve LLM reasoning, high-quality long-chain reasoning data and optimized training pipelines still remain inadequately explored in vision-language tasks. In this paper, we present Insight-V, an early effort to 1) scalably produce long and robust reasoning data for complex multi-modal tasks, and 2) an effective training pipeline to enhance the reasoning capabilities of multi-modal large language models (MLLMs). Specifically, to create long and structured reasoning data without human labor, we design a two-step pipeline with a progressive strategy to generate sufficiently long and diverse reasoning paths and a multi-granularity assessment method to ensure data quality. We observe that directly supervising MLLMs with such long and complex reasoning data will not yield ideal reasoning ability. To tackle this problem, we design a multi-agent system consisting of a reasoning agent dedicated to performing long-chain reasoning and a summary agent trained to judge and summarize reasoning results. We further incorporate an iterative DPO algorithm to enhance the reasoning agent's generation stability and quality. Based on the popular LLaVA-NeXT model and our stronger base MLLM, we demonstrate significant performance gains across challenging multi-modal benchmarks requiring visual reasoning. Benefiting from our multi-agent system, Insight-V can also easily maintain or improve performance on perception-focused multi-modal tasks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-11-21",
    "authors": [
      {
        "authorId": "2292401742",
        "name": "Yuhao Dong"
      },
      {
        "authorId": "2124814824",
        "name": "Zuyan Liu"
      },
      {
        "authorId": "2331770234",
        "name": "Hai-Long Sun"
      },
      {
        "authorId": "2295601",
        "name": "Jingkang Yang"
      },
      {
        "authorId": "2321885484",
        "name": "Winston Hu"
      },
      {
        "authorId": "2331615338",
        "name": "Yongming Rao"
      },
      {
        "authorId": "2321815866",
        "name": "Ziwei Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "63a1617af179ee8b5b096b3038913a19166168d4",
    "url": "https://www.semanticscholar.org/paper/63a1617af179ee8b5b096b3038913a19166168d4",
    "title": "Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models",
    "abstract": "Retrieval-Augmented Generation (RAG) has been shown to enhance the factual accuracy of Large Language Models (LLMs), but existing methods often suffer from limited reasoning capabilities in effectively using the retrieved evidence, particularly when using open-source LLMs. To mitigate this gap, we introduce a novel framework, Open-RAG, designed to enhance reasoning capabilities in RAG with open-source LLMs. Our framework transforms an arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE) model capable of handling complex reasoning tasks, including both single- and multi-hop queries. Open-RAG uniquely trains the model to navigate challenging distractors that appear relevant but are misleading. As a result, Open-RAG leverages latent learning, dynamically selecting relevant experts and integrating external knowledge effectively for more accurate and contextually relevant responses. In addition, we propose a hybrid adaptive retrieval method to determine retrieval necessity and balance the trade-off between performance gain and inference speed. Experimental results show that the Llama2-7B-based Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT, Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source our code and models at https://openragmoe.github.io/",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-10-02",
    "authors": [
      {
        "authorId": "2232783785",
        "name": "Shayekh Bin Islam"
      },
      {
        "authorId": "2323863538",
        "name": "Md Asib Rahman"
      },
      {
        "authorId": "2323786676",
        "name": "K. S. M. T. Hossain"
      },
      {
        "authorId": "2274022429",
        "name": "Enamul Hoque"
      },
      {
        "authorId": "2708940",
        "name": "Shafiq R. Joty"
      },
      {
        "authorId": "3405393",
        "name": "Md. Rizwan Parvez"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "69958f6cacf86537c8fb7e4efaa8fb2d8e519ce2",
    "url": "https://www.semanticscholar.org/paper/69958f6cacf86537c8fb7e4efaa8fb2d8e519ce2",
    "title": "Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models",
    "abstract": "We introduce Mathador-LM, a new benchmark for evaluating the mathematical reasoning on large language models (LLMs), combining ruleset interpretation, planning, and problem-solving. This benchmark is inspired by the Mathador game, where the objective is to reach a target number using basic arithmetic operations on a given set of base numbers, following a simple set of rules. We show that, across leading LLMs, we obtain stable average performance while generating benchmark instances dynamically, following a target difficulty level. Thus, our benchmark alleviates concerns about test-set leakage into training data, an issue that often undermines popular benchmarks. Additionally, we conduct a comprehensive evaluation of both open and closed-source state-of-the-art LLMs on Mathador-LM. Our findings reveal that contemporary models struggle with Mathador-LM, scoring significantly lower than average 3rd graders. This stands in stark contrast to their strong performance on popular mathematical reasoning benchmarks. The implementation of Mathador-LM benchmark is available at https://github.com/IST-DASLab/Mathador-LM.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-18",
    "authors": [
      {
        "authorId": "40992614",
        "name": "Eldar Kurtic"
      },
      {
        "authorId": "2307082392",
        "name": "Amir Moeini"
      },
      {
        "authorId": "3311387",
        "name": "Dan Alistarh"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "bb8f7fbec020675d269ccfa0e6e603f02b664c0d",
    "url": "https://www.semanticscholar.org/paper/bb8f7fbec020675d269ccfa0e6e603f02b664c0d",
    "title": "PaD: Program-aided Distillation Specializes Large Models in Reasoning",
    "abstract": "While Large Language Models (LLMs) excel in several natural language processing tasks, their size and inaccessibility present challenges for extensive practical application. Previous studies acquire specialized skills through distillation on LLMs, which result in trading generic abilities, called model specialization. As for reasoning ability, chain-of-thought was synthesized to subsequent distillation. However, due to hallucination, synthetic chain-of-thought from LLMs contains faulty reasoning. These incorrect reasoning steps damage the reasoning capability. To tackle above issues, we propose Program-aided Distillation (PaD), which dis-tills LLMs to obtain specialized small models in reasoning tasks. In PaD, we strengthen specialized models with program-aided reasoning, and help them overcome faulty reasoning steps with automated error checking. Experimental results demonstrate that, on the GSM8K benchmark, a 0.06B model using PaD can not only outperform certain LLMs (e.g., LLaMA), but also achieves a 10% improvement over base-lines with a significantly smaller scale of parameters and data. Data pruning analysis reveals that PaD possesses higher training efficiency.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 20,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.13888",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2145238612",
        "name": "Xuekai Zhu"
      },
      {
        "authorId": "66242399",
        "name": "Biqing Qi"
      },
      {
        "authorId": "2153281320",
        "name": "Kaiyan Zhang"
      },
      {
        "authorId": "2218130130",
        "name": "Xingwei Long"
      },
      {
        "authorId": "2218723159",
        "name": "Bowen Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 115.66783656585135
  },
  {
    "paperId": "80be1426825288ff876acb8cc0babcc6629fa644",
    "url": "https://www.semanticscholar.org/paper/80be1426825288ff876acb8cc0babcc6629fa644",
    "title": "AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation",
    "abstract": "We propose a novel framework for learning high-level cognitive capabilities in robot manipulation tasks, such as making a smiley face using building blocks. These tasks often involve complex multi-step reasoning, presenting significant challenges due to the limited paired data connecting human instructions (e.g., making a smiley face) and robot actions (e.g., end-effector movement). Existing approaches relieve this challenge by adopting an open-loop paradigm decomposing high-level instructions into simple sub-task plans, and executing them step-by-step using low-level control models. However, these approaches are short of instant observations in multi-step reasoning, leading to sub-optimal results. To address this issue, we propose to automatically collect a cognitive robot dataset by Large Language Models (LLMs). The resulting dataset AlphaBlock consists of 35 comprehensive high-level tasks of multi-step text plans and paired observation sequences. To enable efficient data acquisition, we employ elaborated multi-round prompt designs that effectively reduce the burden of extensive human involvement. We further propose a closed-loop multi-modal embodied planning model that autoregressively generates plans by taking image observations as input. To facilitate effective learning, we leverage MiniGPT-4 with a frozen visual encoder and LLM, and finetune additional vision adapter and Q-former to enable fine-grained spatial perception for manipulation tasks. We conduct experiments to verify the superiority over existing open and closed-loop methods, and achieve a significant increase in success rate by 21.4% and 14.5% over ChatGPT and GPT-4 based robot tasks. Real-world demos are shown in https://www.youtube.com/watch?v=ayAzID1_qQk .",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 22,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.18898",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-30",
    "authors": [
      {
        "authorId": "2168787686",
        "name": "Chuhao Jin"
      },
      {
        "authorId": "2218557953",
        "name": "Wenhui Tan"
      },
      {
        "authorId": "4010904",
        "name": "Jiange Yang"
      },
      {
        "authorId": "2127734772",
        "name": "Bei Liu"
      },
      {
        "authorId": "35119829",
        "name": "Ruihua Song"
      },
      {
        "authorId": "2161327433",
        "name": "Limin Wang"
      },
      {
        "authorId": "3247966",
        "name": "Jianlong Fu"
      }
    ],
    "source": "semantic_scholar",
    "score": 117.03241323893724
  },
  {
    "paperId": "4f4f4014979fe35d28444509ad96e8af1effa85d",
    "url": "https://www.semanticscholar.org/paper/4f4f4014979fe35d28444509ad96e8af1effa85d",
    "title": "One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos",
    "abstract": "We introduce VideoLISA, a video-based multimodal large language model designed to tackle the problem of language-instructed reasoning segmentation in videos. Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions. Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames. VideoLISA addresses these challenges by integrating a Sparse Dense Sampling strategy into the video-LLM, which balances temporal context and spatial detail within computational constraints. Additionally, we propose a One-Token-Seg-All approach using a specially designedtoken, enabling the model to segment and track objects across multiple frames. Extensive evaluations on diverse benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate VideoLISA's superior performance in video object segmentation tasks involving complex reasoning, temporal understanding, and object tracking. While optimized for videos, VideoLISA also shows promising generalization to image segmentation, revealing its potential as a unified foundation model for language-instructed object segmentation. Code and model will be available at: https://github.com/showlab/VideoLISA.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-29",
    "authors": [
      {
        "authorId": "2237427303",
        "name": "Zechen Bai"
      },
      {
        "authorId": "2264160722",
        "name": "Tong He"
      },
      {
        "authorId": "2323507535",
        "name": "Haiyang Mei"
      },
      {
        "authorId": "2299164401",
        "name": "Pichao Wang"
      },
      {
        "authorId": "2269664592",
        "name": "Ziteng Gao"
      },
      {
        "authorId": "2268796016",
        "name": "Joya Chen"
      },
      {
        "authorId": "2323532100",
        "name": "Lei Liu"
      },
      {
        "authorId": "2298907819",
        "name": "Zheng Zhang"
      },
      {
        "authorId": "2269732179",
        "name": "Mike Zheng Shou"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "1dddc3cdca26cd434d48110f8d73674bb7f63c4f",
    "url": "https://www.semanticscholar.org/paper/1dddc3cdca26cd434d48110f8d73674bb7f63c4f",
    "title": "LLMs for Financial Advisement: A Fairness and Efficacy Study in Personal Decision Making",
    "abstract": "As Large Language Model (LLM) based chatbots are becoming more accessible, users are relying on these chatbots for reliable and personalized recommendations in diverse domains, ranging from code generation to financial advisement. In this context, we set out to investigate how such systems perform in the personal finance domain, where financial inclusion has been an overarching stated aim of banks for decades. We test widely used LLM-based chatbots, ChatGPT and Bard, and compare their performance against SafeFinance, a rule-based chatbot built using the Rasa platform. The comparison is across two critical tasks: product discovery and multi-product interaction, where product refers to banking products like Credit Cards, Certificate of Deposits, and Checking Accounts. With this study, we provide interesting insights into the chatbots’ efficacy in financial advisement and their ability to provide fair treatment across different user groups. We find that both Bard and ChatGPT can make errors in retrieving basic online information, the responses they generate are inconsistent across different user groups, and they cannot be relied on for reasoning involving banking products. On the other hand, despite their limited generalization capabilities, rule-based chatbots like SafeFinance provide safe and reliable answers to users that can be traced back to their original source. Overall, although the outputs of the LLM-based chatbots are fluent and plausible, there are still critical gaps in providing consistent and reliable financial information.",
    "venue": "International Conference on AI in Finance",
    "year": 2023,
    "citationCount": 18,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3604237.3626867",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle"
    ],
    "publicationDate": "2023-11-25",
    "authors": [
      {
        "authorId": "2160882985",
        "name": "Kausik Lakkaraju"
      },
      {
        "authorId": "2268372887",
        "name": "Sara E Jones"
      },
      {
        "authorId": "2223598529",
        "name": "Sai Krishna Revanth Vuruma"
      },
      {
        "authorId": "50847178",
        "name": "Vishal Pallagani"
      },
      {
        "authorId": "2166874727",
        "name": "Bharath Muppasani"
      },
      {
        "authorId": "50784532",
        "name": "Biplav Srivastava"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.1665846874966
  },
  {
    "paperId": "3b60d1a39482d86a3507a1879919609767e58594",
    "url": "https://www.semanticscholar.org/paper/3b60d1a39482d86a3507a1879919609767e58594",
    "title": "Demystifying Faulty Code: Step-by-Step Reasoning for Explainable Fault Localization",
    "abstract": "Fault localization is a critical process that involves identifying specific program elements responsible for program failures. Manually pinpointing these elements, such as classes, methods, or statements, which are associated with a fault is laborious and time-consuming. To overcome this challenge, various fault localization tools have been developed. These tools typically generate a ranked list of suspicious program elements. However, this information alone is insufficient. A prior study emphasized that automated fault localization should offer a rationale. In this study, we investigate the step-by-step reasoning for explainable fault localization. We explore the potential of Large Language Models (LLM) in assisting developers in reasoning about code. We proposed FuseFL that utilizes several combinations of information to enhance the LLM results which are spectrum-based fault localization results, test case execution outcomes, and code description (i.e., explanation of what the given code is intended to do). We conducted our investigation using faulty code from Refactory dataset. First, we evaluate the performance of the automated fault localization. Our results demonstrate a 32.3 % increase in the number of successfully localized faults at Top-1 compared to the baseline. To evaluate the explanations generated by FuseFL, we create a dataset of human explanations that provide step-by-step reasoning as to why specific lines of code are considered faulty. This dataset consists of 324 faulty code files, along with explanations for 600 faulty lines. Furthermore, we also conducted human studies to evaluate the explanations. We found that for 22 out of the 30 randomly sampled cases, FuseFL generated correct explanations.",
    "venue": "IEEE International Conference on Software Analysis, Evolution, and Reengineering",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-12",
    "authors": [
      {
        "authorId": "2008294024",
        "name": "Ratnadira Widyasari"
      },
      {
        "authorId": "2291963875",
        "name": "Jia Wei Ang"
      },
      {
        "authorId": "2116110168",
        "name": "Truong-Giang Nguyen"
      },
      {
        "authorId": "2292213900",
        "name": "Neil Sharma"
      },
      {
        "authorId": "2291965017",
        "name": "David Lo"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.1886522358297
  },
  {
    "paperId": "ae8aabebad0c3ecae165ec05c18a2072ed360d1e",
    "url": "https://www.semanticscholar.org/paper/ae8aabebad0c3ecae165ec05c18a2072ed360d1e",
    "title": "NEWTON: Are Large Language Models Capable of Physical Reasoning?",
    "abstract": "Large Language Models (LLMs), through their contextualized representations, have been empirically proven to encapsulate syntactic, semantic, word sense, and common-sense knowledge. However, there has been limited exploration of their physical reasoning abilities, specifically concerning the crucial attributes for comprehending everyday objects. To address this gap, we introduce NEWTON, a repository and benchmark for evaluating the physics reasoning skills of LLMs. Further, to enable domain-specific adaptation of this benchmark, we present a pipeline to enable researchers to generate a variant of this benchmark that has been customized to the objects and attributes relevant for their application. The NEWTON repository comprises a collection of 2800 object-attribute pairs, providing the foundation for generating infinite-scale assessment templates. The NEWTON benchmark consists of 160K QA questions, curated using the NEWTON repository to investigate the physical reasoning capabilities of several mainstream language models across foundational, explicit, and implicit reasoning tasks. Through extensive empirical analysis, our results highlight the capabilities of LLMs for physical reasoning. We find that LLMs like GPT-4 demonstrate strong reasoning capabilities in scenario-based tasks but exhibit less consistency in object-attribute reasoning compared to humans (50% vs. 84%). Furthermore, the NEWTON platform demonstrates its potential for evaluating and enhancing language models, paving the way for their integration into physically grounded settings, such as robotic manipulation. Project site: https://newtonreasoning.github.io",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 15,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.07018",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-11",
    "authors": [
      {
        "authorId": "2257315331",
        "name": "Yi Ru Wang"
      },
      {
        "authorId": "2052092142",
        "name": "Jiafei Duan"
      },
      {
        "authorId": "2257042516",
        "name": "Dieter Fox"
      },
      {
        "authorId": "1752197",
        "name": "S. Srinivasa"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.58883083359672
  },
  {
    "paperId": "1d8b0deda0658301959ff01dd4a10a926454e761",
    "url": "https://www.semanticscholar.org/paper/1d8b0deda0658301959ff01dd4a10a926454e761",
    "title": "PUB: A Pragmatics Understanding Benchmark for Assessing LLMs’ Pragmatics Capabilities",
    "abstract": "LLMs have demonstrated remarkable capability for understanding semantics, but they often struggle with understanding pragmatics. To demonstrate this fact, we release a Pragmatics Understanding Benchmark (PUB) dataset consisting of fourteen tasks in four pragmatics phenomena, namely, Implicature, Presupposition, Reference, and Deixis. We curated high-quality test sets for each task, consisting of Multiple Choice Question Answers (MCQA). PUB includes a total of 28k data points, 6.1k of which have been created by us, and the rest are adapted from existing datasets. We evaluated nine models varying in the number of parameters and type of training. Our study indicates that fine-tuning for instruction-following and chat significantly enhances the pragmatics capabilities of smaller language models. However, for larger models, the base versions perform comparably with their chat-adapted counterparts. Additionally, there is a noticeable performance gap between human capabilities and model capabilities. Furthermore, unlike the consistent performance of humans across various tasks, the models demonstrate variability in their proficiency, with performance levels fluctuating due to different hints and the complexities of tasks within the same dataset. Overall, the benchmark aims to provide a comprehensive evaluation of LLM's ability to handle real-world language tasks that require pragmatic reasoning.",
    "venue": "Findings of the Association for Computational Linguistics ACL 2024",
    "year": 2024,
    "citationCount": 11,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2401.07078",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-01-13",
    "authors": [
      {
        "authorId": "2279550454",
        "name": "S. Sravanthi"
      },
      {
        "authorId": "2269456908",
        "name": "Meet Doshi"
      },
      {
        "authorId": "2279550102",
        "name": "Tankala Pavan Kalyan"
      },
      {
        "authorId": "144725573",
        "name": "Rudra Murthy"
      },
      {
        "authorId": "2266470426",
        "name": "Pushpak Bhattacharyya"
      },
      {
        "authorId": "3209719",
        "name": "Raj Dabre"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "0b66d5b0e62ee0a134fafa770bc1c9d693011bf7",
    "url": "https://www.semanticscholar.org/paper/0b66d5b0e62ee0a134fafa770bc1c9d693011bf7",
    "title": "Thought Graph: Generating Thought Process for Biological Reasoning",
    "abstract": "We present the Thought Graph as a novel framework to support complex reasoning and use gene set analysis as an example to uncover semantic relationships between biological processes. Our framework stands out for its ability to provide a deeper understanding of gene sets, significantly surpassing GSEA by 40.28% and LLM baselines by 5.38% based on cosine similarity to human annotations. Our analysis further provides insights into future directions of biological processes naming, and implications for bioinformatics and precision medicine.",
    "venue": "The Web Conference",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3589335.3651572",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-11",
    "authors": [
      {
        "authorId": "2291169452",
        "name": "Chi-Yang Hsu"
      },
      {
        "authorId": "2290918828",
        "name": "K. Cox"
      },
      {
        "authorId": "2144295598",
        "name": "Jiawei Xu"
      },
      {
        "authorId": "2285489121",
        "name": "Zhen Tan"
      },
      {
        "authorId": "2290915788",
        "name": "Tianhua Zhai"
      },
      {
        "authorId": "2257445915",
        "name": "Mengzhou Hu"
      },
      {
        "authorId": "144019683",
        "name": "Dexter Pratt"
      },
      {
        "authorId": "2276535128",
        "name": "Tianlong Chen"
      },
      {
        "authorId": "2309190965",
        "name": "Ziniu Hu"
      },
      {
        "authorId": "2215687510",
        "name": "Ying Ding"
      }
    ],
    "source": "semantic_scholar",
    "score": 89.1415686865115
  },
  {
    "paperId": "879b54d587afd74a18378ef44bef0170d07ab526",
    "url": "https://www.semanticscholar.org/paper/879b54d587afd74a18378ef44bef0170d07ab526",
    "title": "Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning",
    "abstract": "Large language models exhibit high-level commonsense reasoning abilities, especially with enhancement methods like Chain-of-Thought (CoT). However, we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem. To interpret and mitigate this problem, we first utilize attribution tracing and causal tracing methods to probe the internal working mechanism of the LLM during CoT reasoning. Through comparisons, we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers. Based on the probing findings, we design a novel method called RIDERS (Residual decodIng and sERial-position Swap), which compensates for the information deficit in the model from both decoding and serial-position perspectives. Through extensive experiments on multiple commonsense reasoning benchmarks, we validate that this method not only significantly eliminates Toxic CoT problems (decreased by 23.6%), but also effectively improves the model's overall commonsense reasoning performance (increased by 5.5%).",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-28",
    "authors": [
      {
        "authorId": "2203948041",
        "name": "Jiachun Li"
      },
      {
        "authorId": "49776272",
        "name": "Pengfei Cao"
      },
      {
        "authorId": "2135762532",
        "name": "Chenhao Wang"
      },
      {
        "authorId": "2152843772",
        "name": "Zhuoran Jin"
      },
      {
        "authorId": "1763402",
        "name": "Yubo Chen"
      },
      {
        "authorId": "2285108503",
        "name": "Daojian Zeng"
      },
      {
        "authorId": "77397868",
        "name": "Kang Liu"
      },
      {
        "authorId": "2269147239",
        "name": "Jun Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "bad287184c6739fd6f476f89cb83e09415982d9f",
    "url": "https://www.semanticscholar.org/paper/bad287184c6739fd6f476f89cb83e09415982d9f",
    "title": "ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base",
    "abstract": "Analogical reasoning is a fundamental cognitive ability of humans. However, current language models (LMs) still struggle to achieve human-like performance in analogical reasoning tasks due to a lack of resources for model training. In this work, we address this gap by proposing ANALOGYKB, a million-scale analogy knowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB identifies two types of analogies from the KGs: 1) analogies of the same relations, which can be directly extracted from the KGs, and 2) analogies of analogous relations, which are identified with a selection and filtering pipeline enabled by large language models (LLMs), followed by minor human efforts for data quality control. Evaluations on a series of datasets of two analogical reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB successfully enables both smaller LMs and LLMs to gain better analogical reasoning capabilities.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 14,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.05994",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-10",
    "authors": [
      {
        "authorId": "2145968425",
        "name": "Siyu Yuan"
      },
      {
        "authorId": "5040052",
        "name": "Jiangjie Chen"
      },
      {
        "authorId": "2118133838",
        "name": "Changzhi Sun"
      },
      {
        "authorId": "3366523",
        "name": "Jiaqing Liang"
      },
      {
        "authorId": "2116642640",
        "name": "Yanghua Xiao"
      },
      {
        "authorId": "1944126000",
        "name": "Deqing Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "f29365305b2cd789019bafede8298d9731152f07",
    "url": "https://www.semanticscholar.org/paper/f29365305b2cd789019bafede8298d9731152f07",
    "title": "GRACE: Discriminator-Guided Chain-of-Thought Reasoning",
    "abstract": "In the context of multi-step reasoning, e.g., with chain-of-thought, language models (LMs) can easily assign a high likelihood to incorrect steps. As a result, decoding strategies that optimize for solution likelihood often yield incorrect solutions. To address this issue, we propose Guiding chain-of-thought ReAsoning with a CorrectnEss Discriminator (GRACE), a stepwise decoding approach that steers the decoding process towards producing correct reasoning steps. GRACE employs a discriminator trained with a contrastive loss over correct and incorrect steps, which is used during decoding to score next-step candidates based on their correctness. Importantly, GRACE only requires sampling from the LM, without the need for LM training or fine-tuning. Using models from FLAN-T5 and LLaMA families, we evaluate GRACE over four math and two symbolic reasoning tasks, where it exhibits substantial performance gains compared to greedy decoding, verifiers, and self-consistency in most settings. When further combined with self-consistency, GRACE outperforms all the baselines by sizeable margins. Human and LLM evaluations over GSM8K show that GRACE not only improves the final answer accuracy but also the correctness of the intermediate reasoning. Our implementation can be accessed at \\url{https://github.com/mukhal/grace}.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 30,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.findings-emnlp.1022.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-24",
    "authors": [
      {
        "authorId": "2065538033",
        "name": "Muhammad Khalifa"
      },
      {
        "authorId": "2876316",
        "name": "Lajanugen Logeswaran"
      },
      {
        "authorId": "3056520",
        "name": "Moontae Lee"
      },
      {
        "authorId": "2110756308",
        "name": "Ho Hin Lee"
      },
      {
        "authorId": "2153516659",
        "name": "Lu Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.50980806727719
  },
  {
    "paperId": "f8d9decb348a2a6daeb54c8cad82350bfec3e1b0",
    "url": "https://www.semanticscholar.org/paper/f8d9decb348a2a6daeb54c8cad82350bfec3e1b0",
    "title": "FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph Question Answering",
    "abstract": "Large language models are often challenged by generating erroneous or `hallucinated' responses, especially in complex reasoning tasks. To mitigate this, we propose a retrieval augmented reasoning method, FiDeLiS, which enhances knowledge graph question answering by anchoring responses to structured, verifiable reasoning paths. FiDeLiS uses a keyword-enhanced retrieval mechanism that fetches relevant entities and relations from a vector-based index of KGs to ensure high-recall retrieval. Once these entities and relations are retrieved, our method constructs candidate reasoning paths which are then refined using a stepwise beam search. This ensures that all the paths we create can be confidently linked back to KGs, ensuring they are accurate and reliable. A distinctive feature of our approach is its blend of natural language planning with beam search to optimize the selection of reasoning paths. Moreover, we redesign the way reasoning paths are scored by transforming this process into a deductive reasoning task, allowing the LLM to assess the validity of the paths through deductive reasoning rather than traditional logit-based scoring. This helps avoid misleading reasoning chains and reduces unnecessary computational demand. Extensive experiments demonstrate that our method, even as a training-free method which has lower computational costs and superior generality, outperforms established strong baselines across three datasets.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-22",
    "authors": [
      {
        "authorId": "2145641810",
        "name": "Yuan Sui"
      },
      {
        "authorId": "2285087612",
        "name": "Yufei He"
      },
      {
        "authorId": "2302823840",
        "name": "Nian Liu"
      },
      {
        "authorId": "2283895736",
        "name": "Xiaoxin He"
      },
      {
        "authorId": "2302789486",
        "name": "Kun Wang"
      },
      {
        "authorId": "2283877888",
        "name": "Bryan Hooi"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "73802693fbf9e0b20c331327399517545846a828",
    "url": "https://www.semanticscholar.org/paper/73802693fbf9e0b20c331327399517545846a828",
    "title": "Demystifying Faulty Code with LLM: Step-by-Step Reasoning for Explainable Fault Localization",
    "abstract": "Fault localization is a critical process that involves identifying specific program elements responsible for program failures. Manually pinpointing these elements, such as classes, methods, or statements, which are associated with a fault is laborious and time-consuming. To overcome this challenge, various fault localization tools have been developed. These tools typically generate a ranked list of suspicious program elements. However, this information alone is insufficient. A prior study emphasized that automated fault localization should offer a rationale. In this study, we investigate the step-by-step reasoning for explainable fault localization. We explore the potential of Large Language Models (LLM) in assisting developers in reasoning about code. We proposed FuseFL that utilizes several combinations of information to enhance the LLM results which are spectrum-based fault localization results, test case execution outcomes, and code description (i.e., explanation of what the given code is intended to do). We conducted our investigation using faulty code from Refactory dataset. First, we evaluate the performance of the automated fault localization. Our results demonstrate a more than 30% increase in the number of successfully localized faults at Top-1 compared to the baseline. To evaluate the explanations generated by FuseFL, we create a dataset of human explanations that provide step-by-step reasoning as to why specific lines of code are considered faulty. This dataset consists of 324 faulty code files, along with explanations for 600 faulty lines. Furthermore, we also conducted human studies to evaluate the explanations. We found that for 22 out of the 30 randomly sampled cases, FuseFL generated correct explanations.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-15",
    "authors": [
      {
        "authorId": "2008294024",
        "name": "Ratnadira Widyasari"
      },
      {
        "authorId": "2291963875",
        "name": "Jia Wei Ang"
      },
      {
        "authorId": "2116110168",
        "name": "Truong-Giang Nguyen"
      },
      {
        "authorId": "2292213900",
        "name": "Neil Sharma"
      },
      {
        "authorId": "2291965017",
        "name": "David Lo"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "f2b6323973955f9a1ebb9be76a616991de3d3a8f",
    "url": "https://www.semanticscholar.org/paper/f2b6323973955f9a1ebb9be76a616991de3d3a8f",
    "title": "ChartBench: A Benchmark for Complex Visual Reasoning in Charts",
    "abstract": "Multimodal Large Language Models (MLLMs) have shown impressive capabilities in image understanding and generation. However, current benchmarks fail to accurately evaluate the chart comprehension of MLLMs due to limited chart types and inappropriate metrics. To address this, we propose ChartBench, a comprehensive benchmark designed to assess chart comprehension and data reliability through complex visual reasoning. ChartBench includes 42 categories, 66.6k charts, and 600k question-answer pairs. Notably, many charts lack data point annotations, which requires MLLMs to derive values similar to human understanding by leveraging inherent chart elements such as color, legends, and coordinate systems. We also design an enhanced evaluation metric, Acc+, to evaluate MLLMs without extensive manual or costly LLM-based evaluations. Furthermore, we propose two baselines based on the chain of thought and supervised fine-tuning to improve model performance on unannotated charts. Extensive experimental evaluations of 18 open-sourced and 3 proprietary MLLMs reveal their limitations in chart comprehension and offer valuable insights for further research. Code and dataset are publicly available at https://chartbench.github.io.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 23,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-26",
    "authors": [
      {
        "authorId": "1390847590",
        "name": "Zhengzhuo Xu"
      },
      {
        "authorId": "2276425328",
        "name": "Sinan Du"
      },
      {
        "authorId": "2276514316",
        "name": "Yiyan Qi"
      },
      {
        "authorId": "2250617116",
        "name": "Chengjin Xu"
      },
      {
        "authorId": "2266463747",
        "name": "Chun Yuan"
      },
      {
        "authorId": "2284217200",
        "name": "Jian Guo"
      }
    ],
    "source": "semantic_scholar",
    "score": 117.67080745521918
  },
  {
    "paperId": "9854f71224ca608bcbdd10215593a8526e6bac44",
    "url": "https://www.semanticscholar.org/paper/9854f71224ca608bcbdd10215593a8526e6bac44",
    "title": "HW-TSC at SemEval-2024 Task 5: Self-Eval? A Confident LLM System for Auto Prediction and Evaluation for the Legal Argument Reasoning Task",
    "abstract": "In this article, we present an effective system for semeval-2024 task 5. The task involves assessing the feasibility of a given solution in civil litigation cases based on relevant legal provisions, issues, solutions, and analysis. This task demands a high level of proficiency in U.S. law and natural language reasoning. In this task, we designed a self-eval LLM system that simultaneously performs reasoning and self-assessment tasks. We created a confidence interval and a prompt instructing the LLM to output the answer to a question along with its confidence level. We designed a series of experiments to prove the effectiveness of the self-eval mechanism. In order to avoid the randomness of the results, the final result is obtained by voting on three results generated by the GPT-4. Our submission was conducted under zero-resource setting, and we achieved first place in the task with an F1-score of 0.8231 and an accuracy of 0.8673.",
    "venue": "International Workshop on Semantic Evaluation",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2257018513",
        "name": "Xiaofeng Zhao"
      },
      {
        "authorId": "2165227179",
        "name": "Xiaosong Qiao"
      },
      {
        "authorId": "2308479173",
        "name": "Kaiwen Ou"
      },
      {
        "authorId": "2258690223",
        "name": "Min Zhang"
      },
      {
        "authorId": "2152082190",
        "name": "Chang Su"
      },
      {
        "authorId": "2242967066",
        "name": "Mengyao Piao"
      },
      {
        "authorId": "2243955116",
        "name": "Yuang Li"
      },
      {
        "authorId": "2194427215",
        "name": "Yinglu Li"
      },
      {
        "authorId": "2257227579",
        "name": "Ming Zhu"
      },
      {
        "authorId": "2264401771",
        "name": "Yilun Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "280353fd7a7a3e49c415c443e1b7ccf7de9c2b4e",
    "url": "https://www.semanticscholar.org/paper/280353fd7a7a3e49c415c443e1b7ccf7de9c2b4e",
    "title": "Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models",
    "abstract": "Vision-language models (VLMs) have recently demonstrated strong efficacy as visual assistants that can parse natural queries about the visual content and generate human-like outputs. In this work, we explore the ability of these models to demonstrate human-like reasoning based on the perceived information. To address a crucial concern regarding the extent to which their reasoning capabilities are fully consistent and grounded, we also measure the reasoning consistency of these models. We achieve this by proposing a chain-of-thought (CoT) based consistency measure. However, such an evaluation requires a benchmark that encompasses both high-level inference and detailed reasoning chains, which is costly. We tackle this challenge by proposing an LLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously ensuring the generation of a high-quality dataset. Based on this pipeline and the existing coarse-grained annotated dataset, we build the CURE benchmark to measure both the zero-shot reasoning performance and consistency of VLMs. We evaluate existing state-of-the-art VLMs, and find that even the best-performing model is unable to demonstrate strong visual reasoning capabilities and consistency, indicating that substantial efforts are required to enable VLMs to perform visual reasoning as systematically and consistently as humans. As an early step, we propose a two-stage training framework aimed at improving both the reasoning performance and consistency of VLMs. The first stage involves employing supervised fine-tuning of VLMs using step-by-step reasoning samples automatically generated by LLMs. In the second stage, we further augment the training process by incorporating feedback provided by LLMs to produce reasoning chains that are highly consistent and grounded. We empirically highlight the effectiveness of our framework in both reasoning performance and consistency.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 17,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.04461",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-09-08",
    "authors": [
      {
        "authorId": "123331686",
        "name": "Yangyi Chen"
      },
      {
        "authorId": "39707211",
        "name": "Karan Sikka"
      },
      {
        "authorId": "144354133",
        "name": "Michael Cogswell"
      },
      {
        "authorId": "2243197103",
        "name": "Heng Ji"
      },
      {
        "authorId": "47977519",
        "name": "Ajay Divakaran"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.35557636844247
  },
  {
    "paperId": "014c00319cb23c6322ea5218049661a4ce222946",
    "url": "https://www.semanticscholar.org/paper/014c00319cb23c6322ea5218049661a4ce222946",
    "title": "TART: A plug-and-play Transformer module for task-agnostic reasoning",
    "abstract": "Large language models (LLMs) exhibit in-context learning abilities which enable the same model to perform several tasks without any task-specific training. In contrast, traditional adaptation approaches, such as fine-tuning, modify the underlying models for each specific task. In-context learning, however, consistently underperforms task-specific tuning approaches even when presented with the same examples. While most existing approaches (e.g., prompt engineering) focus on the LLM's learned representations to patch this performance gap, our analysis actually reveal that LLM representations contain sufficient information to make good predictions. As such, we focus on the LLM's reasoning abilities and demonstrate that this performance gap exists due to their inability to perform simple probabilistic reasoning tasks. This raises an intriguing question: Are LLMs actually capable of learning how to reason in a task-agnostic manner? We answer this in the affirmative and propose TART which generically improves an LLM's reasoning abilities using a synthetically trained Transformer-based reasoning module. TART trains this reasoning module in a task-agnostic manner using only synthetic logistic regression tasks and composes it with an arbitrary real-world pre-trained model without any additional training. With a single inference module, TART improves performance across different model families (GPT-Neo, Pythia, BLOOM), model sizes (100M - 6B), tasks (14 NLP binary classification tasks), and even across different modalities (audio and vision). Additionally, on the RAFT Benchmark, TART improves GPT-Neo (125M)'s performance such that it outperforms BLOOM (176B), and is within 4% of GPT-3 (175B). Our code and models are available at https://github.com/HazyResearch/TART .",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 12,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.07536",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-13",
    "authors": [
      {
        "authorId": "144383716",
        "name": "K. Bhatia"
      },
      {
        "authorId": "1381444249",
        "name": "A. Narayan"
      },
      {
        "authorId": "2081393182",
        "name": "Chris De Sa"
      },
      {
        "authorId": "1803218",
        "name": "Christopher Ré"
      }
    ],
    "source": "semantic_scholar",
    "score": 115.47424036192305
  },
  {
    "paperId": "29add8755c37c0822e9a02a3a7f9a832ad819692",
    "url": "https://www.semanticscholar.org/paper/29add8755c37c0822e9a02a3a7f9a832ad819692",
    "title": "Multi-Modal GPT-4 Aided Action Planning and Reasoning for Self-driving Vehicles",
    "abstract": "Explainable decision-making is critical for building trust in autonomous vehicles. We investigate the use of a pre-trained large language model (LLM) to derive comprehensible driving decisions from multi-modal time-series data captured by a monocular camera on an autonomous vehicle. Leveraging a graph-of-thought structure, the LLM learns policies that perform robustly while generating natural language rationales. We generate a novel multi-modal dataset with sequential images, scene labels, and driving actions. Results demonstrate our method produces human- understandable explanations for its driving choices, providing transparency. Our work indicates incorporating language-based reasoning enables accountable and transparent decision-making for self-driving cars, making LLM a potential solution for autonomous driving.",
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-14",
    "authors": [
      {
        "authorId": "51238255",
        "name": "Fangyuan Chi"
      },
      {
        "authorId": "2239903059",
        "name": "Yixiao Wang"
      },
      {
        "authorId": "1687935",
        "name": "P. Nasiopoulos"
      },
      {
        "authorId": "2148975527",
        "name": "Victor C. M. Leung"
      }
    ],
    "source": "semantic_scholar",
    "score": 92.47918433002164
  },
  {
    "paperId": "38f3007bf7b08102f1c50c117123dc3abd929ee2",
    "url": "https://www.semanticscholar.org/paper/38f3007bf7b08102f1c50c117123dc3abd929ee2",
    "title": "Leveraging Structured Information for Explainable Multi-hop Question Answering and Reasoning",
    "abstract": "Neural models, including large language models (LLMs), achieve superior performance on multi-hop question-answering. To elicit reasoning capabilities from LLMs, recent works propose using the chain-of-thought (CoT) mechanism to generate both the reasoning chain and the answer, which enhances the model's capabilities in conducting multi-hop reasoning. However, several challenges still remain: such as struggling with inaccurate reasoning, hallucinations, and lack of interpretability. On the other hand, information extraction (IE) identifies entities, relations, and events grounded to the text. The extracted structured information can be easily interpreted by humans and machines (Grishman, 2019). In this work, we investigate constructing and leveraging extracted semantic structures (graphs) for multi-hop question answering, especially the reasoning process. Empirical results and human evaluations show that our framework: generates more faithful reasoning chains and substantially improves the QA performance on two benchmark datasets. Moreover, the extracted structures themselves naturally provide grounded explanations that are preferred by humans, as compared to the generated reasoning chains and saliency-based explanations.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-07",
    "authors": [
      {
        "authorId": "2033082027",
        "name": "Ruosen Li"
      },
      {
        "authorId": "2265463653",
        "name": "Xinya Du"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "f9465e71697cae802d66a66eb307f0a809773cd3",
    "url": "https://www.semanticscholar.org/paper/f9465e71697cae802d66a66eb307f0a809773cd3",
    "title": "Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning",
    "abstract": "Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities. Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve the problem. A significant disadvantage is that foundational LLMs are typically not available for fine-tuning, making adaptation computationally prohibitive. We believe (and demonstrate) that problem decomposition and solution generation are distinct capabilites, better addressed in separate modules, than by one monolithic LLM. We introduce DaSLaM, which uses a decomposition generator to decompose complex problems into subproblems that require fewer reasoning steps. These subproblems are answered by a solver. We use a relatively small (13B parameters) LM as the decomposition generator, which we train using policy gradient optimization to interact with a solver LM (regarded as black-box) and guide it through subproblems, thereby rendering our method solver-agnostic. Evaluation on multiple different reasoning datasets reveal that with our method, a 175 billion parameter LM (text-davinci-003) can produce competitive or even better performance, compared to its orders-of-magnitude larger successor, GPT-4. Additionally, we show that DaSLaM is not limited by the solver's capabilities as a function of scale; e.g., solver LMs with diverse sizes give significant performance improvement with our solver-agnostic decomposition technique. Exhaustive ablation studies evince the superiority of our modular finetuning technique over exorbitantly large decomposer LLMs, based on prompting alone.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-21",
    "authors": [
      {
        "authorId": "2262445336",
        "name": "Gurusha Juneja"
      },
      {
        "authorId": "50757931",
        "name": "Subhabrata Dutta"
      },
      {
        "authorId": "2197703",
        "name": "Soumen Chakrabarti"
      },
      {
        "authorId": "2262445089",
        "name": "Sunny Manchanda"
      },
      {
        "authorId": "2256999352",
        "name": "Tanmoy Chakraborty"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "a5e031bf5c93a3a0647f02df70452d056a067118",
    "url": "https://www.semanticscholar.org/paper/a5e031bf5c93a3a0647f02df70452d056a067118",
    "title": "HalluMeasure: Fine-grained Hallucination Measurement Using Chain-of-Thought Reasoning",
    "abstract": "Automating the measurement of hallucinations in LLM generated responses is a challenging task as it requires careful investigation of each factual claim in a response. In this paper, we introduce HalluMeasure, a new LLM-based hallucination detection mechanism that decomposes an LLM response into atomic claims, and evaluates each atomic claim against the provided reference context. The model uses a step-by-step reasoning process called Chain-of-Thought and can identify 3 major categories of hallucinations (e.g., contradiction) as well as 10 more specific subtypes (e.g., overgeneralization) which help to identify reasons behind the hallucination errors. Specifically, we explore four different configurations for HalluMeasure’s classifier: with and without CoT prompting, and using a single classifier call to classify all claims versus separate calls for each claim. The best-performing configuration (with CoT and separate calls for each claim) demonstrates significant improvements in detecting hallucinations, achieving a 10-point increase in F1 score on our TechNewsSumm dataset, and a 3-point increase in AUC ROC on the SummEval dataset, compared to three baseline models (RefChecker, AlignScore, and Vectara HHEM). We further show reasonable accuracy on detecting 10 novel error subtypes of hallucinations (where even humans struggle in classification) derived from linguistic analysis of the errors made by the LLMs.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2732770",
        "name": "Shayan A. Akbar"
      },
      {
        "authorId": "47412046",
        "name": "Md Mosharaf Hossain"
      },
      {
        "authorId": "2329735321",
        "name": "Tess Wood"
      },
      {
        "authorId": "2329736252",
        "name": "Si-Chi Chin"
      },
      {
        "authorId": "2329735679",
        "name": "Erica Salinas"
      },
      {
        "authorId": "2329736672",
        "name": "Victor Alvarez"
      },
      {
        "authorId": "2329735953",
        "name": "Erwin Cornejo"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "701cccfa958a6da46e8b40dc5b380fed498acb49",
    "url": "https://www.semanticscholar.org/paper/701cccfa958a6da46e8b40dc5b380fed498acb49",
    "title": "CodeCoT: Tackling Code Syntax Errors in CoT Reasoning for Code Generation",
    "abstract": "Chain-of-thought (CoT) has emerged as a groundbreaking tool in NLP, notably for its efficacy in complex reasoning tasks, such as mathematical proofs. However, its application in code generation faces a distinct challenge, i.e., although the code generated with CoT reasoning is logically correct, it faces the problem of syntax error (e.g., invalid syntax error report) during code execution, which causes the CoT result's pass@1 in HumanEval even lower than the zero-shot result. In this paper, we present Code Chain-of-Thought (CodeCoT) that integrates CoT with a self-examination process for code generation. CodeCoT begins with the LLMs using CoT for initial code development to ensure the generated code follows the correct logic flow. Then, CodeCoT will generate test cases to validate whether the code has syntax errors during the execution. CodeCoT then employs a self-examination phase, in which the generated code is executed against these test cases in the local environment. If the local environment raises error information (e.g., invalid syntax error), CodeCoT will iteratively refine the code based on the feedback information. Within this loop, CodeCoT can make sure their generated codes not only follow the logic flow of the code description, but the syntax error will also be addressed with the self-examination process. Our evaluation results reveal that CodeCoT improves the effectiveness of code generation. For example, CodeCoT increases pass@1 from 75.6% to 79.3% for the HumanEval dataset.",
    "venue": "",
    "year": 2023,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-08-17",
    "authors": [
      {
        "authorId": "2148911355",
        "name": "Dong Huang"
      },
      {
        "authorId": "9963055",
        "name": "Qi Bu"
      },
      {
        "authorId": "2066367605",
        "name": "Yuhao Qing"
      },
      {
        "authorId": "2944075",
        "name": "Heming Cui"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "8db1dcae055842f43ccac04182957b20d15bbe6b",
    "url": "https://www.semanticscholar.org/paper/8db1dcae055842f43ccac04182957b20d15bbe6b",
    "title": "Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems",
    "abstract": "While forward reasoning (i.e., find the answer given the question) has been explored extensively in recent literature, backward reasoning is relatively unexplored. We examine the backward reasoning capabilities of LLMs on Math Word Problems (MWPs): given a mathematical question and its answer, with some details omitted from the question, can LLMs effectively retrieve the missing information? On modifying three benchmark datasets for this task, to evaluate this task: GSM8k, SVAMP, and MultiArith, we find a significant drop in the accuracy of models on this task compared to forward reasoning across SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa). Motivated by the fact backward reasoning can be seen as the ''inverse'' of forward reasoning, we propose variations of three different forward reasoning strategies to improve performance. Rephrase reformulates the given problem into a forward reasoning problem, PAL-Tools combines the idea of Program-Aided LLMs to produce a set of equations that can be solved by an external solver, and Check your Work exploits the availability of natural verifier of high accuracy in the forward direction, interleaving solving and verification steps. Finally, realizing that each of our base methods correctly solves a different set of problems, we propose a novel Bayesian formulation for creating an ensemble over the base methods to further boost the accuracy. Extensive experimentation demonstrates successive improvement in the performance of LLMs on the backward reasoning task, using our strategies, with our ensemble-based method resulting in significant performance gains compared to the SOTA forward reasoning strategies we adapt.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.01991",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-03",
    "authors": [
      {
        "authorId": "2253595290",
        "name": "Aniruddha Deb"
      },
      {
        "authorId": "2253593969",
        "name": "Neeva Oza"
      },
      {
        "authorId": "2253592915",
        "name": "Sarthak Singla"
      },
      {
        "authorId": "50564082",
        "name": "Dinesh Khandelwal"
      },
      {
        "authorId": "50252087",
        "name": "Dinesh Garg"
      },
      {
        "authorId": "2128239746",
        "name": "Parag Singla"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "7450c20d844ca05e643ece2461ff7aa2f381e22a",
    "url": "https://www.semanticscholar.org/paper/7450c20d844ca05e643ece2461ff7aa2f381e22a",
    "title": "Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for Visual Question Answering",
    "abstract": "Recently, to comprehensively improve Vision Language Models (VLMs) for Visual Question Answering (VQA), several methods have been proposed to further reinforce the inference capabilities of VLMs to independently tackle VQA tasks rather than some methods that only utilize VLMs as aids to Large Language Models (LLMs). However, these methods ignore the rich common-sense knowledge inside the given VQA image sampled from the real world. Thus, they cannot fully use the powerful VLM for the given VQA question to achieve optimal performance. Attempt to overcome this limitation and inspired by the human top-down reasoning process, i.e., systematically exploring relevant issues to derive a comprehensive answer, this work introduces a novel, explainable multi-agent collaboration framework by leveraging the expansive knowledge of Large Language Models (LLMs) to enhance the capabilities of VLMs themselves. Specifically, our framework comprises three agents, i.e., Responder, Seeker, and Integrator, to collaboratively answer the given VQA question by seeking its relevant issues and generating the final answer in such a top-down reasoning process. The VLM-based Responder agent generates the answer candidates for the question and responds to other relevant issues. The Seeker agent, primarily based on LLM, identifies relevant issues related to the question to inform the Responder agent and constructs a Multi-View Knowledge Base (MVKB) for the given visual scene by leveraging the build-in world knowledge of LLM. The Integrator agent combines knowledge from the Seeker agent and the Responder agent to produce the final VQA answer. Extensive and comprehensive evaluations on diverse VQA datasets with a variety of VLMs demonstrate the superior performance and interpretability of our framework over the baseline method in the zero-shot setting without extra training cost.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-29",
    "authors": [
      {
        "authorId": "2243319007",
        "name": "Zeqing Wang"
      },
      {
        "authorId": "2149237200",
        "name": "Wentao Wan"
      },
      {
        "authorId": "2268696591",
        "name": "Runmeng Chen"
      },
      {
        "authorId": "2268674865",
        "name": "Qiqing Lao"
      },
      {
        "authorId": "2268676088",
        "name": "Minjie Lang"
      },
      {
        "authorId": "2268756020",
        "name": "Keze Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "76588eabeb1d712c6170fb9cbfe13b92d0809b69",
    "url": "https://www.semanticscholar.org/paper/76588eabeb1d712c6170fb9cbfe13b92d0809b69",
    "title": "Frugal LMs Trained to Invoke Symbolic Solvers Achieve Parameter-Efficient Arithmetic Reasoning",
    "abstract": "Large Language Models (LLM) exhibit zero-shot mathematical reasoning capacity as a behavior emergent with scale, commonly manifesting as chain-of-thoughts (CoT) reasoning. However, multiple empirical findings suggest that this prowess is exclusive to LLMs that have exorbitant sizes (beyond 50 billion parameters). Meanwhile, educational neuroscientists suggest that symbolic algebraic manipulation be introduced around the same time as arithmetic word problems so as to modularize language-to-formulation, symbolic manipulation of the formulation, and endgame arithmetic.\nIn this paper, we start with the hypothesis that much smaller LMs, which are weak at multi-step reasoning, can achieve reasonable arithmetic reasoning if arithmetic word problems are posed as a formalize-then-solve task.\nIn our architecture, which we call SyReLM, the LM serves the role of a translator to map natural language arithmetic questions into a formal language (FL) description. A symbolic solver then evaluates the FL expression to obtain the answer.\nA small frozen LM, equipped with an efficient low-rank adapter, is capable of generating FL expressions that incorporate natural language descriptions of the arithmetic problem (e.g., variable names and their purposes, formal expressions combining variables, etc.).\nWe adopt policy-gradient reinforcement learning to train the adapted LM, informed by the non-differentiable symbolic solver. This marks a sharp departure from the recent development in tool-augmented LLMs, in which the external tools (e.g., calculator, Web search, etc.) are essentially detached from the learning phase of the LM. SyReLM shows massive improvements (e.g., +30.65 absolute point improvement in accuracy on the SVAMP dataset using GPT-J 6B model) over base LMs, while keeping our testbed easy to diagnose and interpret, and within the reach of most researchers.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2023,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-09",
    "authors": [
      {
        "authorId": "50757931",
        "name": "Subhabrata Dutta"
      },
      {
        "authorId": "2273709978",
        "name": "Joykirat Singh"
      },
      {
        "authorId": "2273566542",
        "name": "Ishan Pandey"
      },
      {
        "authorId": "2262445089",
        "name": "Sunny Manchanda"
      },
      {
        "authorId": "2197703",
        "name": "Soumen Chakrabarti"
      },
      {
        "authorId": "2256999352",
        "name": "Tanmoy Chakraborty"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "a1caeac57526b5d1ea90ee87fceb8ec44a56183c",
    "url": "https://www.semanticscholar.org/paper/a1caeac57526b5d1ea90ee87fceb8ec44a56183c",
    "title": "Evaluating the Effectiveness of Retrieval-Augmented Large Language Models in Scientific Document Reasoning",
    "abstract": "Despite the dramatic progress in Large Language Model (LLM) development, LLMs often provide seemingly plausible but not factual information, often referred to as hallucinations. Retrieval-augmented LLMs provide a non-parametric approach to solve these issues by retrieving relevant information from external data sources and augment the training process. These models help to trace evidence from an externally provided knowledge base allowing the model predictions to be better interpreted and verified. In this work, we critically evaluate these models in their ability to perform in scientific document reasoning tasks. To this end, we tuned multiple such model variants with science-focused instructions and evaluated them on a scientific document reasoning benchmark for the usefulness of the retrieved document passages. Our findings suggest that models justify predictions in science tasks with fabricated evidence and leveraging scientific corpus as pretraining data does not alleviate the risk of evidence fabrication.",
    "venue": "SDP",
    "year": 2023,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-07",
    "authors": [
      {
        "authorId": "2258957941",
        "name": "Sai Munikoti"
      },
      {
        "authorId": "145536102",
        "name": "Anurag Acharya"
      },
      {
        "authorId": "2054838317",
        "name": "S. Wagle"
      },
      {
        "authorId": "24029613",
        "name": "Sameera Horawalavithana"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "8035a247980cb18abf2bb7b9d96e7d4c63622ef2",
    "url": "https://www.semanticscholar.org/paper/8035a247980cb18abf2bb7b9d96e7d4c63622ef2",
    "title": "Reasoning about the Unseen for Efficient Outdoor Object Navigation",
    "abstract": "Robots should exist anywhere humans do: indoors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation(OGN) has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.10103",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-18",
    "authors": [
      {
        "authorId": "2242010169",
        "name": "Quanting Xie"
      },
      {
        "authorId": "2238206121",
        "name": "Tianyi Zhang"
      },
      {
        "authorId": "2243950262",
        "name": "Kedi Xu"
      },
      {
        "authorId": "1389944402",
        "name": "M. Johnson-Roberson"
      },
      {
        "authorId": "3312309",
        "name": "Yonatan Bisk"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "f0aff5a84c804edb8442f197c339bb418eca188b",
    "url": "https://www.semanticscholar.org/paper/f0aff5a84c804edb8442f197c339bb418eca188b",
    "title": "From LLMs to Actions: Latent Codes as Bridges in Hierarchical Robot Control",
    "abstract": "Hierarchical control for robotics has long been plagued by the need to have a well defined interface layer to communicate between high-level task planners and low-level policies. With the advent of LLMs, language has been emerging as a prospective interface layer. However, this has several limitations. Not all tasks can be decomposed into steps that are easily expressible in natural language (e.g. performing a dance routine). Further, it makes end-to-end finetuning on embodied data challenging due to domain shift and catastrophic forgetting. We introduce our method – Latent Codes as Bridges (LCB) – as an alternate architecture to overcome these limitations. LCB uses a learnable latent code to act as a bridge between LLMs and low-level policies. This enables LLMs to flexibly communicate goals in the task plan without being entirely constrained by language limitations. Additionally, it enables end-to-end finetuning without destroying the embedding space of word tokens learned during pre-training. Through experiments on Language Table and Calvin, two common language based benchmarks for embodied agents, we find that LCB outperforms baselines (including those w/ GPT-4V) that leverage pure language as the interface layer on tasks that require reasoning and multi-step behaviors.",
    "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2405.04798",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-08",
    "authors": [
      {
        "authorId": "41019645",
        "name": "Yide Shentu"
      },
      {
        "authorId": "2108864104",
        "name": "Philipp Wu"
      },
      {
        "authorId": "19275599",
        "name": "A. Rajeswaran"
      },
      {
        "authorId": "2257003229",
        "name": "Pieter Abbeel"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.79441541679836
  },
  {
    "paperId": "a35b7855d6dba196dd45db329afcbdec698ae718",
    "url": "https://www.semanticscholar.org/paper/a35b7855d6dba196dd45db329afcbdec698ae718",
    "title": "Symbol-LLM: Leverage Language Models for Symbolic System in Visual Human Activity Reasoning",
    "abstract": "Human reasoning can be understood as a cooperation between the intuitive, associative\"System-1\"and the deliberative, logical\"System-2\". For existing System-1-like methods in visual activity understanding, it is crucial to integrate System-2 processing to improve explainability, generalization, and data efficiency. One possible path of activity reasoning is building a symbolic system composed of symbols and rules, where one rule connects multiple symbols, implying human knowledge and reasoning abilities. Previous methods have made progress, but are defective with limited symbols from handcraft and limited rules from visual-based annotations, failing to cover the complex patterns of activities and lacking compositional generalization. To overcome the defects, we propose a new symbolic system with two ideal important properties: broad-coverage symbols and rational rules. Collecting massive human knowledge via manual annotations is expensive to instantiate this symbolic system. Instead, we leverage the recent advancement of LLMs (Large Language Models) as an approximation of the two ideal properties, i.e., Symbols from Large Language Models (Symbol-LLM). Then, given an image, visual contents from the images are extracted and checked as symbols and activity semantics are reasoned out based on rules via fuzzy logic calculation. Our method shows superiority in extensive activity understanding tasks. Code and data are available at https://mvig-rhos.com/symbol_llm.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-29",
    "authors": [
      {
        "authorId": "2108793708",
        "name": "Xiaoqian Wu"
      },
      {
        "authorId": "2110436501",
        "name": "Yong-Lu Li"
      },
      {
        "authorId": "2152148778",
        "name": "Jianhua Sun"
      },
      {
        "authorId": "1830034",
        "name": "Cewu Lu"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.27359974682
  },
  {
    "paperId": "9e70051fcd58788b1eeb0531354eed4f8fccf306",
    "url": "https://www.semanticscholar.org/paper/9e70051fcd58788b1eeb0531354eed4f8fccf306",
    "title": "Assessing the Efficacy of ChatGPT in Solving Questions Based on the Core Concepts in Physiology",
    "abstract": "Background and objective ChatGPT is a large language model (LLM) generative artificial intelligence (AI) chatbot trained through deep learning to produce human-like language skills and analysis of simple problems across a wide variety of subject areas. However, in terms of facilitating the transfer of learning in medical education, a concern has arisen that while AI is adept at applying surface-level understanding, it does not have the necessary in-depth knowledge to act at an expert level, particularly in addressing the core concepts. In this study, we explored the efficacy of ChatGPT in solving various reasoning questions based on the five core concepts applied to different modules in the subject of physiology. Materials and methods In this study, a total of 82 reasoning-type questions from six modules applicable to the five core concepts were created by the subject experts. The questions were used to chat with the conversational AI tool and the responses generated at first instance were considered for scoring and analysis. To compare the scores among various modules and five core concepts separately, the Kruskal-Wallis test along with post hoc analysis were used. Results The overall mean score for the modules (60 questions) was 3.72 ±0.26 while the average score obtained for the core concepts (60 questions) was 3.68 ±0.30. Furthermore, statistically significant differences (p=0.05 for modules and p=0.024 for core concepts) were observed among various modules as well as core concepts. Conclusion The significant differences observed in the scores among various modules and core concepts highlight the varying execution of the same software tool, thereby necessitating the need for further evaluation of AI-enabled learning applications to enhance the transfer of learning among undergraduates.",
    "venue": "Cureus",
    "year": 2023,
    "citationCount": 9,
    "openAccessPdf": {
      "url": "https://assets.cureus.com/uploads/original_article/pdf/177708/20230811-23292-fohf8m.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-08-01",
    "authors": [
      {
        "authorId": "2224134301",
        "name": "Arijita Banerjee"
      },
      {
        "authorId": "103312790",
        "name": "Aquil Ahmad"
      },
      {
        "authorId": "34042273",
        "name": "P. Bhalla"
      },
      {
        "authorId": "2228472149",
        "name": "Kavita Goyal"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "072d387ad3436f97bce984066c53c1b2e77b1ad9",
    "url": "https://www.semanticscholar.org/paper/072d387ad3436f97bce984066c53c1b2e77b1ad9",
    "title": "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents",
    "abstract": "Driven by the rapid development of Large Language Models (LLMs), LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis of different forms of agent backdoor attacks. Specifically, compared with traditional backdoor attacks on LLMs that are only able to manipulate the user inputs and model outputs, agent backdoor attacks exhibit more diverse and covert forms: (1) From the perspective of the final attacking outcomes, the agent backdoor attacker can not only choose to manipulate the final output distribution, but also introduce the malicious behavior in an intermediate reasoning step only, while keeping the final output correct. (2) Furthermore, the former category can be divided into two subcategories based on trigger locations, in which the backdoor trigger can either be hidden in the user query or appear in an intermediate observation returned by the external environment. We implement the above variations of agent backdoor attacks on two typical agent tasks including web shopping and tool utilization. Extensive experiments show that LLM-based agents suffer severely from backdoor attacks and such backdoor vulnerability cannot be easily mitigated by current textual backdoor defense algorithms. This indicates an urgent need for further research on the development of targeted defenses against backdoor attacks on LLM-based agents. Warning: This paper may contain biased content.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 31,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-17",
    "authors": [
      {
        "authorId": "2120801160",
        "name": "Wenkai Yang"
      },
      {
        "authorId": "51255245",
        "name": "Xiaohan Bi"
      },
      {
        "authorId": "2149202150",
        "name": "Yankai Lin"
      },
      {
        "authorId": "2111638473",
        "name": "Sishuo Chen"
      },
      {
        "authorId": "2266995673",
        "name": "Jie Zhou"
      },
      {
        "authorId": "2130282986",
        "name": "Xu Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.9860385419959
  },
  {
    "paperId": "2653cf21108048bdc9300e7c8daf27461b91f98c",
    "url": "https://www.semanticscholar.org/paper/2653cf21108048bdc9300e7c8daf27461b91f98c",
    "title": "VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model",
    "abstract": "In the realm of household robotics, the Zero-Shot Object Navigation (ZSON) task empowers agents to adeptly traverse unfamiliar environments and locate objects from novel categories without prior explicit training. This paper introduces VoroNav, a novel semantic exploration framework that proposes the Reduced Voronoi Graph to extract exploratory paths and planning nodes from a semantic map constructed in real time. By harnessing topological and semantic information, VoroNav designs text-based descriptions of paths and images that are readily interpretable by a large language model (LLM). In particular, our approach presents a synergy of path and farsight descriptions to represent the environmental context, enabling LLM to apply commonsense reasoning to ascertain waypoints for navigation. Extensive evaluation on HM3D and HSSD validates VoroNav surpasses existing benchmarks in both success rate and exploration efficiency (absolute improvement: +2.8% Success and +3.7% SPL on HM3D, +2.6% Success and +3.8% SPL on HSSD). Additionally introduced metrics that evaluate obstacle avoidance proficiency and perceptual efficiency further corroborate the enhancements achieved by our method in ZSON planning. Project page: https://voro-nav.github.io",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 17,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-01-05",
    "authors": [
      {
        "authorId": "2278225878",
        "name": "Pengying Wu"
      },
      {
        "authorId": "2278222002",
        "name": "Yao Mu"
      },
      {
        "authorId": "2278414915",
        "name": "Bingxian Wu"
      },
      {
        "authorId": "2278752321",
        "name": "Yi Hou"
      },
      {
        "authorId": "2278371726",
        "name": "Ji Ma"
      },
      {
        "authorId": "2268817905",
        "name": "Shanghang Zhang"
      },
      {
        "authorId": "2220675004",
        "name": "Chang Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.35557636844247
  },
  {
    "paperId": "66d9550a51cb493aaeeae72d396b0bdb1ca47fe9",
    "url": "https://www.semanticscholar.org/paper/66d9550a51cb493aaeeae72d396b0bdb1ca47fe9",
    "title": "Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning",
    "abstract": "A promising approach for improving reasoning in large language models is to use process reward models (PRMs). PRMs provide feedback at each step of a multi-step reasoning trace, potentially improving credit assignment over outcome reward models (ORMs) that only provide feedback at the final step. However, collecting dense, per-step human labels is not scalable, and training PRMs from automatically-labeled data has thus far led to limited gains. To improve a base policy by running search against a PRM or using it as dense rewards for reinforcement learning (RL), we ask:\"How should we design process rewards?\". Our key insight is that, to be effective, the process reward for a step should measure progress: a change in the likelihood of producing a correct response in the future, before and after taking the step, corresponding to the notion of step-level advantages in RL. Crucially, this progress should be measured under a prover policy distinct from the base policy. We theoretically characterize the set of good provers and our results show that optimizing process rewards from such provers improves exploration during test-time search and online RL. In fact, our characterization shows that weak prover policies can substantially improve a stronger base policy, which we also observe empirically. We validate our claims by training process advantage verifiers (PAVs) to predict progress under such provers, and show that compared to ORMs, test-time search against PAVs is $>8\\%$ more accurate, and $1.5-5\\times$ more compute-efficient. Online RL with dense rewards from PAVs enables one of the first results with $5-6\\times$ gain in sample efficiency, and $>6\\%$ gain in accuracy, over ORMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-10",
    "authors": [
      {
        "authorId": "80366270",
        "name": "Amrith Rajagopal Setlur"
      },
      {
        "authorId": "2963503",
        "name": "Chirag Nagpal"
      },
      {
        "authorId": "2274102752",
        "name": "Adam Fisch"
      },
      {
        "authorId": "3468192",
        "name": "Xinyang Geng"
      },
      {
        "authorId": "2256409622",
        "name": "Jacob Eisenstein"
      },
      {
        "authorId": "2317013277",
        "name": "Rishabh Agarwal"
      },
      {
        "authorId": "2274120058",
        "name": "Alekh Agarwal"
      },
      {
        "authorId": "2282138214",
        "name": "Jonathan Berant"
      },
      {
        "authorId": "2317038858",
        "name": "Aviral Kumar"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "8d89d38fedfa4a5374c726eb7926510016671441",
    "url": "https://www.semanticscholar.org/paper/8d89d38fedfa4a5374c726eb7926510016671441",
    "title": "EventGround: Narrative Reasoning by Grounding to Eventuality-centric Knowledge Graphs",
    "abstract": "Narrative reasoning relies on the understanding of eventualities in story contexts, which requires a wealth of background world knowledge. To help machines leverage such knowledge, existing solutions can be categorized into two groups. Some focus on implicitly modeling eventuality knowledge by pretraining language models (LMs) with eventuality-aware objectives. However, this approach breaks down knowledge structures and lacks interpretability. Others explicitly collect world knowledge of eventualities into structured eventuality-centric knowledge graphs (KGs). However, existing research on leveraging these knowledge sources for free-texts is limited. In this work, we propose an initial comprehensive framework called EventGround, which aims to tackle the problem of grounding free-texts to eventuality-centric KGs for contextualized narrative reasoning. We identify two critical problems in this direction: the event representation and sparsity problems. We provide simple yet effective parsing and partial information extraction methods to tackle these problems. Experimental results demonstrate that our approach consistently outperforms baseline models when combined with graph neural network (GNN) or large language model (LLM) based graph reasoning models. Our framework, incorporating grounded knowledge, achieves state-of-the-art performance while providing interpretable evidence.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-30",
    "authors": [
      {
        "authorId": "2109077713",
        "name": "Cheng Jiayang"
      },
      {
        "authorId": "2260342601",
        "name": "Lin Qiu"
      },
      {
        "authorId": "2216598559",
        "name": "Chunkit Chan"
      },
      {
        "authorId": "2260287837",
        "name": "Xin Liu"
      },
      {
        "authorId": "2258804099",
        "name": "Yangqiu Song"
      },
      {
        "authorId": "2260691874",
        "name": "Zheng Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "06d4f6bd5271eab3d33c892e21b9eb33395e9404",
    "url": "https://www.semanticscholar.org/paper/06d4f6bd5271eab3d33c892e21b9eb33395e9404",
    "title": "Neural networks for abstraction and reasoning",
    "abstract": "For half a century, artificial intelligence research has attempted to reproduce the human qualities of abstraction and reasoning - creating computer systems that can learn new concepts from a minimal set of examples, in settings where humans find this easy. While specific neural networks are able to solve an impressive range of problems, broad generalisation to situations outside their training data has proved elusive. In this work, we look at several novel approaches for solving the Abstraction & Reasoning Corpus (ARC). This is a dataset of abstract visual reasoning tasks introduced to test algorithms on broad generalization. Despite three international competitions with $100,000 in prizes, the best algorithms still fail to solve a majority of ARC tasks. The best solvers today rely on complex hand-crafted rules, without using machine learning at all. We revisit whether recent advances in neural networks allow progress on this task, or whether an entirely different class of models are required. First, we adapt the DreamCoder neurosymbolic reasoning solver to ARC. DreamCoder automatically writes programs in a bespoke domain-specific language to perform reasoning, using a neural network to mimic human intuition. We present the Perceptual Abstraction and Reasoning Language (PeARL) language, which allows DreamCoder to solve ARC tasks, and propose a new recognition model that allows us to significantly improve on the previous best implementation. We also propose a new encoding and augmentation scheme that allows large language models (LLMs) to solve ARC tasks, and find that the largest models can solve some ARC tasks. LLMs are able to solve a different group of problems to state-of-the-art solvers, and provide an interesting way to complement other approaches. We perform an ensemble analysis, combining systems to achieve better results than any system alone and analysing individual strengths. However, it is sobering to see that approaches based on neural networks still lag behind existing hand-crafted solvers, and we suggest avenues for future improvements. Our findings with the ensemble model may indicate that a diversity of methods might be necessary to solve problems in ARC. Humans likely employ diverse strategies to solve ARC. Studies involving human participants to identify the strategies they employ to solve ARC could provide valuable insights for future AI approaches. Finally, we publish the arckit Python library to make future research on ARC easier.",
    "venue": "Scientific Reports",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-05",
    "authors": [
      {
        "authorId": "1387990552",
        "name": "Mikel Bober-Irizar"
      },
      {
        "authorId": "2283748687",
        "name": "Soumya Banerjee"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "3dd1a5373aa3205bbd1f81c1383c25024f13d051",
    "url": "https://www.semanticscholar.org/paper/3dd1a5373aa3205bbd1f81c1383c25024f13d051",
    "title": "SEEK: Semantic Reasoning for Object Goal Navigation in Real World Inspection Tasks",
    "abstract": "This paper addresses the problem of object-goal navigation in autonomous inspections in real-world environments. Object-goal navigation is crucial to enable effective inspections in various settings, often requiring the robot to identify the target object within a large search space. Current object inspection methods fall short of human efficiency because they typically cannot bootstrap prior and common sense knowledge as humans do. In this paper, we introduce a framework that enables robots to use semantic knowledge from prior spatial configurations of the environment and semantic common sense knowledge. We propose SEEK (Semantic Reasoning for Object Inspection Tasks) that combines semantic prior knowledge with the robot's observations to search for and navigate toward target objects more efficiently. SEEK maintains two representations: a Dynamic Scene Graph (DSG) and a Relational Semantic Network (RSN). The RSN is a compact and practical model that estimates the probability of finding the target object across spatial elements in the DSG. We propose a novel probabilistic planning framework to search for the object using relational semantic knowledge. Our simulation analyses demonstrate that SEEK outperforms the classical planning and Large Language Models (LLMs)-based methods that are examined in this study in terms of efficiency for object-goal inspection tasks. We validated our approach on a physical legged robot in urban environments, showcasing its practicality and effectiveness in real-world inspection scenarios.",
    "venue": "Robotics: Science and Systems",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-16",
    "authors": [
      {
        "authorId": "38808668",
        "name": "M. Ginting"
      },
      {
        "authorId": "2274062399",
        "name": "Sung-Kyun Kim"
      },
      {
        "authorId": "2294057796",
        "name": "David D. Fan"
      },
      {
        "authorId": "51124011",
        "name": "M. Palieri"
      },
      {
        "authorId": "79262652",
        "name": "Mykel J. Kochenderfer"
      },
      {
        "authorId": "2068120981",
        "name": "A. Agha-mohammadi"
      }
    ],
    "source": "semantic_scholar",
    "score": 106.87639203842082
  },
  {
    "paperId": "c5bf4546eaf4b6c8e531dae0aebb76208d719539",
    "url": "https://www.semanticscholar.org/paper/c5bf4546eaf4b6c8e531dae0aebb76208d719539",
    "title": "GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents",
    "abstract": "Large language models have demonstrated remarkable few-shot performance on many natural language understanding tasks. Despite several demonstrations of using large language models in complex, strategic scenarios, there lacks a comprehensive framework for evaluating agents' performance across various types of reasoning found in games. To address this gap, we introduce GameBench, a cross-domain benchmark for evaluating strategic reasoning abilities of LLM agents. We focus on 9 different game environments, where each covers at least one axis of key reasoning skill identified in strategy games, and select games for which strategy explanations are unlikely to form a significant portion of models' pretraining corpuses. Our evaluations use GPT-3 and GPT-4 in their base form along with two scaffolding frameworks designed to enhance strategic reasoning ability: Chain-of-Thought (CoT) prompting and Reasoning Via Planning (RAP). Our results show that none of the tested models match human performance, and at worst GPT-4 performs worse than random action. CoT and RAP both improve scores but not comparable to human levels.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-07",
    "authors": [
      {
        "authorId": "2305679844",
        "name": "Anthony Costarelli"
      },
      {
        "authorId": "2305684968",
        "name": "Mat Allen"
      },
      {
        "authorId": "2305680177",
        "name": "Roman Hauksson"
      },
      {
        "authorId": "2305679592",
        "name": "Grace Sodunke"
      },
      {
        "authorId": "2305680240",
        "name": "Suhas Hariharan"
      },
      {
        "authorId": "2305658059",
        "name": "Carlson Cheng"
      },
      {
        "authorId": "2305733428",
        "name": "Wenjie (Andy) Li"
      },
      {
        "authorId": "2305744885",
        "name": "Arjun Yadav"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "da2880c1b525ae49125b9b2c627126d6891aab5a",
    "url": "https://www.semanticscholar.org/paper/da2880c1b525ae49125b9b2c627126d6891aab5a",
    "title": "TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution",
    "abstract": "The emergence of LLM-based agents has garnered considerable attention, yet their trustwor-thiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution -based agent framework, TrustAgent , an initial investigation into improving the safety dimension of trust-worthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent’s safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model’s reasoning ability and its efficacy as a safe agent. This paper underscores the imperative of integrating safety awareness and trustworthiness into the design and deployment of LLM-based agents, not only to enhance their performance but also to ensure their responsible integration into human-centric environments. Data and code are available at https://github. com/agiresearch/TrustAgent .",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2007245028",
        "name": "Wenyue Hua"
      },
      {
        "authorId": "2145170944",
        "name": "Xianjun Yang"
      },
      {
        "authorId": "2109968285",
        "name": "Zelong Li"
      },
      {
        "authorId": "2282499606",
        "name": "Cheng Wei"
      },
      {
        "authorId": "2239061409",
        "name": "Yongfeng Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "b47b70526b3faa65e0f63f9bf5a70284738dd6a8",
    "url": "https://www.semanticscholar.org/paper/b47b70526b3faa65e0f63f9bf5a70284738dd6a8",
    "title": "Exploring LLM-based Agents for Root Cause Analysis",
    "abstract": "The growing complexity of cloud based software systems has resulted in incident management becoming an integral part of the software development lifecycle. Root cause analysis (RCA), a critical part of the incident management process, is a demanding task for on-call engineers, requiring deep domain knowledge and extensive experience with a team's specific services. Automation of RCA can result in significant savings of time, and ease the burden of incident management on on-call engineers. Recently, researchers have utilized Large Language Models (LLMs) to perform RCA, and have demonstrated promising results. However, these approaches are not able to dynamically collect additional diagnostic information such as incident related logs, metrics or databases, severely restricting their ability to diagnose root causes. In this work, we explore the use of LLM based agents for RCA to address this limitation. We present a thorough empirical evaluation of a ReAct agent equipped with retrieval tools, on an out-of-distribution dataset of production incidents collected at Microsoft. Results show that ReAct performs competitively with strong retrieval and reasoning baselines, but with highly increased factual accuracy. We then extend this evaluation by incorporating discussions associated with incident reports as additional inputs for the models, which surprisingly does not yield significant performance improvements. Lastly, we conduct a case study with a team at Microsoft to equip the ReAct agent with tools that give it access to external diagnostic services that are used by the team for manual RCA. Our results show how agents can overcome the limitations of prior work, and practical considerations for implementing such a system in practice.",
    "venue": "SIGSOFT FSE Companion",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-07",
    "authors": [
      {
        "authorId": "2290186684",
        "name": "Devjeet Roy"
      },
      {
        "authorId": "2239159562",
        "name": "Xuchao Zhang"
      },
      {
        "authorId": "2290181188",
        "name": "Rashi Bhave"
      },
      {
        "authorId": "2239106685",
        "name": "Chetan Bansal"
      },
      {
        "authorId": "1403267039",
        "name": "P. Las-Casas"
      },
      {
        "authorId": "2239106331",
        "name": "Rodrigo Fonseca"
      },
      {
        "authorId": "148121358",
        "name": "S. Rajmohan"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "28d6411019f448f54834c2a5cff723cd350345b5",
    "url": "https://www.semanticscholar.org/paper/28d6411019f448f54834c2a5cff723cd350345b5",
    "title": "Enhancing the LLM-Based Robot Manipulation Through Human-Robot Collaboration",
    "abstract": "Large Language Models (LLMs) are gaining popularity in the field of robotics. However, LLM-based robots are limited to simple, repetitive motions due to the poor integration between language models, robots, and the environment. This letter proposes a novel approach to enhance the performance of LLM-based autonomous manipulation through Human-Robot Collaboration (HRC). The approach involves using a prompted GPT-4 language model to decompose high-level language commands into sequences of motions that can be executed by the robot. The system also employs a YOLO-based perception algorithm, providing visual cues to the LLM, which aids in planning feasible motions within the specific environment. Additionally, an HRC method is proposed by combining teleoperation and Dynamic Movement Primitives (DMP), allowing the LLM-based robot to learn from human guidance. Real-world experiments have been conducted using the Toyota Human Support Robot for manipulation tasks. The outcomes indicate that tasks requiring complex trajectory planning and reasoning over environments can be efficiently accomplished through the incorporation of human demonstrations.",
    "venue": "IEEE Robotics and Automation Letters",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-20",
    "authors": [
      {
        "authorId": "2307380990",
        "name": "Haokun Liu"
      },
      {
        "authorId": "8247318",
        "name": "Yaonan Zhu"
      },
      {
        "authorId": "2110285301",
        "name": "Kenji Kato"
      },
      {
        "authorId": "2307380233",
        "name": "Atsushi Tsukahara"
      },
      {
        "authorId": "2282115338",
        "name": "Izumi Kondo"
      },
      {
        "authorId": "1752849",
        "name": "T. Aoyama"
      },
      {
        "authorId": "2237520520",
        "name": "Yasuhisa Hasegawa"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.87639203842082
  },
  {
    "paperId": "30334d9ef3d07b83dc20bdcf0456763653eeecfd",
    "url": "https://www.semanticscholar.org/paper/30334d9ef3d07b83dc20bdcf0456763653eeecfd",
    "title": "Empowering Personalized Learning through a Conversation-based Tutoring System with Student Modeling",
    "abstract": "As the recent Large Language Models(LLM’s) become increasingly competent in zero-shot and few-shot reasoning across various domains, educators are showing a growing interest in leveraging these LLM’s in conversation-based tutoring systems. However, building a conversation-based personalized tutoring system poses considerable challenges in accurately assessing the student and strategically incorporating the assessment into teaching within the conversation. In this paper, we discuss design considerations for a personalized tutoring system that involves the following two key components: (1) a student modeling with diagnostic components, and (2) a conversation-based tutor utilizing LLM with prompt engineering that incorporates student assessment outcomes and various instructional strategies. Based on these design considerations, we created a proof-of-concept tutoring system focused on personalization and tested it with 20 participants. The results substantiate that our system’s framework facilitates personalization, with particular emphasis on the elements constituting student modeling. A web demo of our system is available at http://rlearning-its.com.",
    "venue": "CHI Extended Abstracts",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3613905.3651122",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle"
    ],
    "publicationDate": "2024-03-21",
    "authors": [
      {
        "authorId": "2292398858",
        "name": "Minju Park"
      },
      {
        "authorId": "2292454026",
        "name": "Sojung Kim"
      },
      {
        "authorId": "2292405500",
        "name": "Seunghyun Lee"
      },
      {
        "authorId": "2292676489",
        "name": "Soonwoo Kwon"
      },
      {
        "authorId": "2233125631",
        "name": "Kyuseok Kim"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "a5cd606537a666e4807f1c9d342cfeb2b71c4a34",
    "url": "https://www.semanticscholar.org/paper/a5cd606537a666e4807f1c9d342cfeb2b71c4a34",
    "title": "Are Large Language Models Table-based Fact-Checkers?",
    "abstract": "Table-based Fact Verification (TFV) aims to extract the entailment relationship between statements and structured tables. Existing TFV methods based on small-scale models suffer from insufficient labeled data and weak zero-shot ability. Recently, the appearance of Large Language Models (LLMs) has gained lots of attraction in research fields. They have shown strong zero-shot and in-context learning capabilities on several NLP tasks, but their potential on TFV is still unknown. In this work, we implement a preliminary study on whether LLMs are table-based fact-checkers. In detail, we design various prompts to explore how in-context learning can help LLMs in TFV, i.e., zero-shot and few-shot TFV capability. Besides, we carefully design and construct TFV instructions to investigate the performance gain brought by the instruction tuning of LLMs. Experimental results demonstrate that LLMs can achieve acceptable results on zero-shot and few-shot TFV with prompt engineering, while instruction-tuning can stimulate the TFV capability significantly. We also make some valuable findings about the format of zero-shot prompts and the number of in-context examples. Finally, we analyze some possible directions to promote the accuracy of TFV via LLMs, which will benefit further research on table reasoning.",
    "venue": "International Conference on Computer Supported Cooperative Work in Design",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-04",
    "authors": [
      {
        "authorId": "2283319448",
        "name": "Hangwen Zhang"
      },
      {
        "authorId": "84109730",
        "name": "Q. Si"
      },
      {
        "authorId": "143655088",
        "name": "Peng Fu"
      },
      {
        "authorId": "2237093798",
        "name": "Zheng Lin"
      },
      {
        "authorId": "2154491752",
        "name": "Weiping Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "9e540662619327a3056d9e40bb58058868f6f805",
    "url": "https://www.semanticscholar.org/paper/9e540662619327a3056d9e40bb58058868f6f805",
    "title": "Prompt Distillation for Efficient LLM-based Recommendation",
    "abstract": "Large language models (LLM) have manifested unparalleled modeling capability on various tasks, e.g., multi-step reasoning, but the input to these models is mostly limited to plain text, which could be very long and contain noisy information. Long text could take long time to process, and thus may not be efficient enough for recommender systems that require immediate response. In LLM-based recommendation models, user and item IDs are usually filled in a template (i.e., discrete prompt) to allow the models to understand a given task, but the models usually need extensive fine-tuning to bridge the user/item IDs and the template words and to unleash the power of LLM for recommendation. To address the problems, we propose to distill the discrete prompt for a specific task to a set of continuous prompt vectors so as to bridge IDs and words and to reduce the inference time. We also design a training strategy with an attempt to improve the efficiency of training these models. Experimental results on three real-world datasets demonstrate the effectiveness of our PrOmpt Distillation (POD) approach on both sequential recommendation and top-N recommendation tasks. Although the training efficiency can be significantly improved, the improvement of inference efficiency is limited. This finding may inspire researchers in the community to further improve the inference efficiency of LLM-based recommendation models.",
    "venue": "International Conference on Information and Knowledge Management",
    "year": 2023,
    "citationCount": 75,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2023-10-21",
    "authors": [
      {
        "authorId": "2151529879",
        "name": "Lei Li"
      },
      {
        "authorId": "2260830380",
        "name": "Yongfeng Zhang"
      },
      {
        "authorId": "2146027242",
        "name": "Li Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 134.96100010429495
  },
  {
    "paperId": "9b0698018bc6a60abae944194030a808da22b803",
    "url": "https://www.semanticscholar.org/paper/9b0698018bc6a60abae944194030a808da22b803",
    "title": "Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models",
    "abstract": "Designing high-quality educational questions is a challenging and time-consuming task. In this work, we propose a novel approach that utilizes prompt-based techniques to generate descriptive and reasoning-based questions. However, current question-answering (QA) datasets are inadequate for conducting our experiments on prompt-based question generation (QG) in an educational setting. Therefore, we curate a new QG dataset called EduProbe for school-level subjects, by leveraging the rich content of NCERT textbooks. We carefully annotate this dataset as quadruples of 1) Context: a segment upon which the question is formed; 2) Long Prompt: a long textual cue for the question (i.e., a longer sequence of words or phrases, covering the main theme of the context); 3) Short Prompt: a short textual cue for the question (i.e., a condensed representation of the key information or focus of the context); 4) Question: a deep question that aligns with the context and is coherent with the prompts. We investigate several prompt-based QG methods by fine-tuning pre-trained transformer-based large language models (LLMs), namely PEGASUS, T5, MBART, and BART. Moreover, we explore the performance of two general-purpose pre-trained LLMs such as Text-Davinci-003 and GPT-3.5-Turbo without any further training. By performing automatic evaluation, we show that T5 (with long prompt) outperforms all other models, but still falls short of the human baseline. Under human evaluation criteria, Text-Davinci-003 usually shows better results than other models under various prompt settings. Even in the case of human evaluation criteria, QG models mostly fall short of the human baseline. Our code and dataset are available at: https://github.com/my625/PromptQG",
    "venue": "Fire",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2312.01032",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle"
    ],
    "publicationDate": "2023-12-02",
    "authors": [
      {
        "authorId": "1847858",
        "name": "Subhankar Maity"
      },
      {
        "authorId": "2144085844",
        "name": "Aniket Deroy"
      },
      {
        "authorId": "2269467220",
        "name": "Sudeshna Sarkar"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "e7bd70b43c54de2b228784b9ff9b1e8cc7c598c0",
    "url": "https://www.semanticscholar.org/paper/e7bd70b43c54de2b228784b9ff9b1e8cc7c598c0",
    "title": "Formally Specifying the High-Level Behavior of LLM-Based Agents",
    "abstract": "Autonomous, goal-driven agents powered by LLMs have recently emerged as promising tools for solving challenging problems without the need for task-specific finetuned models that can be expensive to procure. Currently, the design and implementation of such agents is ad hoc, as the wide variety of tasks that LLM-based agents may be applied to naturally means there can be no one-size-fits-all approach to agent design. In this work we aim to alleviate the difficulty of designing and implementing new agents by proposing a minimalistic generation framework that simplifies the process of building agents. The framework we introduce allows the user to define desired agent behaviors in a high-level, declarative specification that is then used to construct a decoding monitor which guarantees the LLM will produce an output exhibiting the desired behavior. Our declarative approach, in which the behavior is described without concern for how it should be implemented or enforced, enables rapid design, implementation, and experimentation with different LLM-based agents. We demonstrate how the proposed framework can be used to implement recent LLM-based agents (e.g., ReACT), and show how the flexibility of our approach can be leveraged to define a new agent with more complex behavior, the Plan-Act-Summarize-Solve (PASS) agent. Lastly, we demonstrate that our method outperforms other agents on multiple popular reasoning-centric question-answering benchmarks.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.08535",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-12",
    "authors": [
      {
        "authorId": "41036307",
        "name": "M. Crouse"
      },
      {
        "authorId": "2257345798",
        "name": "Ibrahim Abdelaziz"
      },
      {
        "authorId": "2257347920",
        "name": "Kinjal Basu"
      },
      {
        "authorId": "2248176463",
        "name": "Soham Dan"
      },
      {
        "authorId": "1666248581",
        "name": "Sadhana Kumaravel"
      },
      {
        "authorId": "2297836",
        "name": "Achille Fokoue"
      },
      {
        "authorId": "2123450380",
        "name": "P. Kapanipathi"
      },
      {
        "authorId": "2257345942",
        "name": "Luis A. Lastras"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "b3d0871705f246045d58b77bdb9574d5bc3d949f",
    "url": "https://www.semanticscholar.org/paper/b3d0871705f246045d58b77bdb9574d5bc3d949f",
    "title": "An In-depth Survey of Large Language Model-based Artificial Intelligence Agents",
    "abstract": "Due to the powerful capabilities demonstrated by large language model (LLM), there has been a recent surge in efforts to integrate them with AI agents to enhance their performance. In this paper, we have explored the core differences and characteristics between LLM-based AI agents and traditional AI agents. Specifically, we first compare the fundamental characteristics of these two types of agents, clarifying the significant advantages of LLM-based agents in handling natural language, knowledge storage, and reasoning capabilities. Subsequently, we conducted an in-depth analysis of the key components of AI agents, including planning, memory, and tool use. Particularly, for the crucial component of memory, this paper introduced an innovative classification scheme, not only departing from traditional classification methods but also providing a fresh perspective on the design of an AI agent's memory system. We firmly believe that in-depth research and understanding of these core components will lay a solid foundation for the future advancement of AI agent technology. At the end of the paper, we provide directional suggestions for further research in this field, with the hope of offering valuable insights to scholars and researchers in the field.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 12,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.14365",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-09-23",
    "authors": [
      {
        "authorId": "2114871845",
        "name": "Pengyu Zhao"
      },
      {
        "authorId": "2247157734",
        "name": "Zijian Jin"
      },
      {
        "authorId": "2208958032",
        "name": "Ning Cheng"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "0c8446eedfe083e0ee32f5c4f793e5435904014a",
    "url": "https://www.semanticscholar.org/paper/0c8446eedfe083e0ee32f5c4f793e5435904014a",
    "title": "A Chat about Boring Problems: Studying GPT-Based Text Normalization",
    "abstract": "Text normalization - the conversion of text from written to spoken form - is traditionally assumed to be an ill-formed task for language modeling. In this work, we argue otherwise. We empirically show the capacity of Large-Language Models (LLM) for text normalization in few-shot scenarios. Combining self-consistency reasoning with linguistic-informed prompt engineering, we find LLM-based text normalization to achieve error rates approximately 40% lower than production-level normalization systems. Further, upon error analysis, we note key limitations in the conventional design of text normalization tasks. We create a new taxonomy of text normalization errors and apply it to results from GPT-3.5-Turbo and GPT-4.0. Through this new framework, we identify strengths and weaknesses of LLM-based TN, opening opportunities for future work.",
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year": 2023,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.13426",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-09-23",
    "authors": [
      {
        "authorId": "2145957511",
        "name": "Yang Zhang"
      },
      {
        "authorId": "2121382287",
        "name": "Travis M. Bartley"
      },
      {
        "authorId": "2245391090",
        "name": "Mariana Graterol-Fuenmayor"
      },
      {
        "authorId": "46200473",
        "name": "Vitaly Lavrukhin"
      },
      {
        "authorId": "32867948",
        "name": "Evelina Bakhturina"
      },
      {
        "authorId": "31963005",
        "name": "Boris Ginsburg"
      }
    ],
    "source": "semantic_scholar",
    "score": 92.47918433002164
  },
  {
    "paperId": "7260442ef9c0448f07ce3803efd49cebaffcebe9",
    "url": "https://www.semanticscholar.org/paper/7260442ef9c0448f07ce3803efd49cebaffcebe9",
    "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
    "abstract": "The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 187,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-05",
    "authors": [
      {
        "authorId": "2278217964",
        "name": "DeepSeek-AI Xiao Bi"
      },
      {
        "authorId": "2274200088",
        "name": "Deli Chen"
      },
      {
        "authorId": "2278589675",
        "name": "Guanting Chen"
      },
      {
        "authorId": "2278219833",
        "name": "Shanhuang Chen"
      },
      {
        "authorId": "10780897",
        "name": "Damai Dai"
      },
      {
        "authorId": "2278221484",
        "name": "C. Deng"
      },
      {
        "authorId": "2278486342",
        "name": "Honghui Ding"
      },
      {
        "authorId": "2278218552",
        "name": "Kai Dong"
      },
      {
        "authorId": "2278218583",
        "name": "Qiushi Du"
      },
      {
        "authorId": "2278223920",
        "name": "Zhe Fu"
      },
      {
        "authorId": "2278395340",
        "name": "Huazuo Gao"
      },
      {
        "authorId": "2278218724",
        "name": "Kaige Gao"
      },
      {
        "authorId": "2272467392",
        "name": "W. Gao"
      },
      {
        "authorId": "2278218130",
        "name": "Ruiqi Ge"
      },
      {
        "authorId": "2278219877",
        "name": "Kang Guan"
      },
      {
        "authorId": "2278834796",
        "name": "Daya Guo"
      },
      {
        "authorId": "2278226003",
        "name": "Jianzhong Guo"
      },
      {
        "authorId": "2278219957",
        "name": "Guangbo Hao"
      },
      {
        "authorId": "2278220653",
        "name": "Zhewen Hao"
      },
      {
        "authorId": "2278214443",
        "name": "Ying He"
      },
      {
        "authorId": "2255434668",
        "name": "Wen-Hui Hu"
      },
      {
        "authorId": "2278251752",
        "name": "Panpan Huang"
      },
      {
        "authorId": "2278218944",
        "name": "Erhang Li"
      },
      {
        "authorId": "2161532149",
        "name": "Guowei Li"
      },
      {
        "authorId": "2278337977",
        "name": "Jiashi Li"
      },
      {
        "authorId": "2278381130",
        "name": "Yao Li"
      },
      {
        "authorId": "2278599324",
        "name": "Y. K. Li"
      },
      {
        "authorId": "2278618633",
        "name": "W. Liang"
      },
      {
        "authorId": "2278589731",
        "name": "Fangyun Lin"
      },
      {
        "authorId": "2278677183",
        "name": "A. Liu"
      },
      {
        "authorId": "2156640188",
        "name": "Bo Liu (Benjamin Liu)"
      },
      {
        "authorId": "2261682447",
        "name": "Wen Liu"
      },
      {
        "authorId": "2257346982",
        "name": "Xiaodong Liu"
      },
      {
        "authorId": "2257356150",
        "name": "Xin Liu"
      },
      {
        "authorId": "2278393359",
        "name": "Yiyuan Liu"
      },
      {
        "authorId": "2278583258",
        "name": "Haoyu Lu"
      },
      {
        "authorId": "2278389939",
        "name": "Shanghao Lu"
      },
      {
        "authorId": "2278218736",
        "name": "Fuli Luo"
      },
      {
        "authorId": "2278830967",
        "name": "Shirong Ma"
      },
      {
        "authorId": "2278218185",
        "name": "X. Nie"
      },
      {
        "authorId": "2278220142",
        "name": "Tian Pei"
      },
      {
        "authorId": "2278218072",
        "name": "Yishi Piao"
      },
      {
        "authorId": "2278355944",
        "name": "Junjie Qiu"
      },
      {
        "authorId": "2278221058",
        "name": "Hui Qu"
      },
      {
        "authorId": "2278220755",
        "name": "Tongzheng Ren"
      },
      {
        "authorId": "2278430558",
        "name": "Z. Ren"
      },
      {
        "authorId": "2278217940",
        "name": "C. Ruan"
      },
      {
        "authorId": "2278218357",
        "name": "Zhangli Sha"
      },
      {
        "authorId": "144485528",
        "name": "Zhihong Shao"
      },
      {
        "authorId": "2258088582",
        "name": "Jun-Mei Song"
      },
      {
        "authorId": "2278247813",
        "name": "Xuecheng Su"
      },
      {
        "authorId": "2157278510",
        "name": "Jingxiang Sun"
      },
      {
        "authorId": "2278377738",
        "name": "Yaofeng Sun"
      },
      {
        "authorId": "2275604714",
        "name": "Min Tang"
      },
      {
        "authorId": "2244285627",
        "name": "Bing-Li Wang"
      },
      {
        "authorId": "144202874",
        "name": "Peiyi Wang"
      },
      {
        "authorId": "2274309127",
        "name": "Shiyu Wang"
      },
      {
        "authorId": "2278432120",
        "name": "Yaohui Wang"
      },
      {
        "authorId": "2278404235",
        "name": "Yongji Wang"
      },
      {
        "authorId": "2278403411",
        "name": "Tong Wu"
      },
      {
        "authorId": "49176273",
        "name": "Yu Wu"
      },
      {
        "authorId": "2239599436",
        "name": "Xin Xie"
      },
      {
        "authorId": "2279107352",
        "name": "Zhenda Xie"
      },
      {
        "authorId": "2279107375",
        "name": "Ziwei Xie"
      },
      {
        "authorId": "2260383038",
        "name": "Yi Xiong"
      },
      {
        "authorId": "2278643857",
        "name": "Hanwei Xu"
      },
      {
        "authorId": "2243405903",
        "name": "R. X. Xu"
      },
      {
        "authorId": "2324603956",
        "name": "Yanhong Xu"
      },
      {
        "authorId": "2278404250",
        "name": "Dejian Yang"
      },
      {
        "authorId": "2054452755",
        "name": "Yu-mei You"
      },
      {
        "authorId": "2278408463",
        "name": "Shuiping Yu"
      },
      {
        "authorId": "2274512433",
        "name": "Xin-yuan Yu"
      },
      {
        "authorId": "145803569",
        "name": "Bo Zhang"
      },
      {
        "authorId": "2278587222",
        "name": "Haowei Zhang"
      },
      {
        "authorId": "2278256170",
        "name": "Lecong Zhang"
      },
      {
        "authorId": "2278257096",
        "name": "Liyue Zhang"
      },
      {
        "authorId": "2278384213",
        "name": "Mingchuan Zhang"
      },
      {
        "authorId": "47474023",
        "name": "Minghu Zhang"
      },
      {
        "authorId": "2266140993",
        "name": "Wentao Zhang"
      },
      {
        "authorId": "46867273",
        "name": "Yichao Zhang"
      },
      {
        "authorId": "2278389597",
        "name": "Chenggang Zhao"
      },
      {
        "authorId": "2278398317",
        "name": "Yao Zhao"
      },
      {
        "authorId": "2278397391",
        "name": "Shangyan Zhou"
      },
      {
        "authorId": "2278398119",
        "name": "Shunfeng Zhou"
      },
      {
        "authorId": "2278223869",
        "name": "Qihao Zhu"
      },
      {
        "authorId": "2278317444",
        "name": "Yuheng Zou"
      }
    ],
    "source": "semantic_scholar",
    "score": 148.54662944244924
  },
  {
    "paperId": "b38845e9adbeeeab37519a2fc30e899411b4a36a",
    "url": "https://www.semanticscholar.org/paper/b38845e9adbeeeab37519a2fc30e899411b4a36a",
    "title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models",
    "abstract": "In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared to advanced models like GPT-4 and Gemini. We try to narrow the gap by mining the potential of VLMs for better performance and any-to-any workflow from three aspects, i.e., high-resolution visual tokens, high-quality data, and VLM-guided generation. To enhance visual tokens, we propose to utilize an additional visual encoder for high-resolution refinement without increasing the visual token count. We further construct a high-quality dataset that promotes precise image comprehension and reasoning-based generation, expanding the operational scope of current VLMs. In general, Mini-Gemini further mines the potential of VLMs and empowers current frameworks with image understanding, reasoning, and generation simultaneously. Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs) from 2B to 34B. It is demonstrated to achieve leading performance in several zero-shot benchmarks and even surpasses the developed private models. Code and models are available at https://github.com/dvlab-research/MiniGemini.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 151,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-27",
    "authors": [
      {
        "authorId": "2268644727",
        "name": "Yanwei Li"
      },
      {
        "authorId": "2145915052",
        "name": "Yuechen Zhang"
      },
      {
        "authorId": "2268721057",
        "name": "Chengyao Wang"
      },
      {
        "authorId": "2294147935",
        "name": "Zhisheng Zhong"
      },
      {
        "authorId": "2293625780",
        "name": "Yixin Chen"
      },
      {
        "authorId": "1380477833",
        "name": "Ruihang Chu"
      },
      {
        "authorId": "2288676140",
        "name": "Shaoteng Liu"
      },
      {
        "authorId": "2237811040",
        "name": "Jiaya Jia"
      }
    ],
    "source": "semantic_scholar",
    "score": 145.35820781269416
  },
  {
    "paperId": "4fcb8b6c466937025d315be6a83b624b10e860b4",
    "url": "https://www.semanticscholar.org/paper/4fcb8b6c466937025d315be6a83b624b10e860b4",
    "title": "MAmmoTH2: Scaling Instructions from the Web",
    "abstract": "Instruction tuning improves the reasoning abilities of large language models (LLMs), with data quality and scalability being the crucial factors. Most instruction tuning data come from human crowd-sourcing or GPT-4 distillation. We propose a paradigm to efficiently harvest 10 million naturally existing instruction data from the pre-training web corpus to enhance LLM reasoning. Our approach involves (1) recalling relevant documents, (2) extracting instruction-response pairs, and (3) refining the extracted pairs using open-source LLMs. Fine-tuning base LLMs on this dataset, we build MAmmoTH2 models, which significantly boost performance on reasoning benchmarks. Notably, MAmmoTH2-7B's (Mistral) performance increases from 11% to 36.7% on MATH and from 36% to 68.4% on GSM8K without training on any in-domain data. Further training MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several reasoning and chatbot benchmarks. Our work demonstrates how to harvest large-scale, high-quality instruction data without costly human annotation or GPT-4 distillation, providing a new paradigm for building better instruction tuning data.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 52,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-06",
    "authors": [
      {
        "authorId": "2284988933",
        "name": "Xiang Yue"
      },
      {
        "authorId": "2300091474",
        "name": "Tuney Zheng"
      },
      {
        "authorId": "2143853895",
        "name": "Ge Zhang"
      },
      {
        "authorId": "2249847177",
        "name": "Wenhu Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 129.55437870328183
  },
  {
    "paperId": "e01515c6138bc525f7aec30fc85f2adf028d4156",
    "url": "https://www.semanticscholar.org/paper/e01515c6138bc525f7aec30fc85f2adf028d4156",
    "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision",
    "abstract": "Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including<200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 268,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.03047",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-04",
    "authors": [
      {
        "authorId": "48064856",
        "name": "Zhiqing Sun"
      },
      {
        "authorId": "2714199",
        "name": "Yikang Shen"
      },
      {
        "authorId": "2107604346",
        "name": "Qinhong Zhou"
      },
      {
        "authorId": "2118083343",
        "name": "Hongxin Zhang"
      },
      {
        "authorId": "2111329651",
        "name": "Zhenfang Chen"
      },
      {
        "authorId": "2064715218",
        "name": "David D. Cox"
      },
      {
        "authorId": "46286308",
        "name": "Yiming Yang"
      },
      {
        "authorId": "2056157586",
        "name": "Chuang Gan"
      }
    ],
    "source": "semantic_scholar",
    "score": 160.9206706940276
  },
  {
    "paperId": "4e71624e90960cb003e311a0fe3b8be4c2863239",
    "url": "https://www.semanticscholar.org/paper/4e71624e90960cb003e311a0fe3b8be4c2863239",
    "title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries",
    "abstract": "Retrieval-augmented generation (RAG) augments large language models (LLM) by retrieving relevant knowledge, showing promising potential in mitigating LLM hallucinations and enhancing response quality, thereby facilitating the great adoption of LLMs in practice. However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries. In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utilizing an English news article dataset as the underlying RAG knowledge base. We demonstrate the benchmarking utility of MultiHop-RAG in two experiments. The first experiment compares different embedding models for retrieving evidence for multi-hop queries. In the second experiment, we examine the capabilities of various state-of-the-art LLMs, including GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop queries given the evidence. Both experiments reveal that existing RAG methods perform unsatisfactorily in retrieving and answering multi-hop queries. We hope MultiHop-RAG will be a valuable resource for the community in developing effective RAG systems, thereby facilitating greater adoption of LLMs in practice. The MultiHop-RAG and implemented RAG system is publicly available at https://github.com/yixuantt/MultiHop-RAG/.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 38,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-27",
    "authors": [
      {
        "authorId": "2260449655",
        "name": "Yixuan Tang"
      },
      {
        "authorId": "2246043972",
        "name": "Yi Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.95342469194469
  },
  {
    "paperId": "9a9e68d400069f023f7dc9b982226c95159a509d",
    "url": "https://www.semanticscholar.org/paper/9a9e68d400069f023f7dc9b982226c95159a509d",
    "title": "Dissociating language and thought in large language models: a cognitive perspective",
    "abstract": "Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become --\"thinking machines\", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 197,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-01-16",
    "authors": [
      {
        "authorId": "2412497",
        "name": "Kyle Mahowald"
      },
      {
        "authorId": "2017088782",
        "name": "Anna A. Ivanova"
      },
      {
        "authorId": "38412172",
        "name": "I. Blank"
      },
      {
        "authorId": "1931482",
        "name": "N. Kanwisher"
      },
      {
        "authorId": "1763295",
        "name": "J. Tenenbaum"
      },
      {
        "authorId": "144733430",
        "name": "Evelina Fedorenko"
      }
    ],
    "source": "semantic_scholar",
    "score": 149.32400546041802
  },
  {
    "paperId": "c9db4ccaf91d0d2e44cb6c6b5b77e25b887739c8",
    "url": "https://www.semanticscholar.org/paper/c9db4ccaf91d0d2e44cb6c6b5b77e25b887739c8",
    "title": "TrustAgent: Towards Safe and Trustworthy LLM-based Agents",
    "abstract": "The rise of LLM-based agents shows great potential to revolutionize task planning, capturing significant attention. Given that these agents will be integrated into high-stake domains, ensuring their reliability and safety is crucial. This paper presents an Agent-Constitution-based agent framework, TrustAgent, with a particular focus on improving the LLM-based agent safety. The proposed framework ensures strict adherence to the Agent Constitution through three strategic components: pre-planning strategy which injects safety knowledge to the model before plan generation, in-planning strategy which enhances safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Our experimental results demonstrate that the proposed framework can effectively enhance an LLM agent's safety across multiple domains by identifying and mitigating potential dangers during the planning. Further analysis reveals that the framework not only improves safety but also enhances the helpfulness of the agent. Additionally, we highlight the importance of the LLM reasoning ability in adhering to the Constitution. This paper sheds light on how to ensure the safe integration of LLM-based agents into human-centric environments. Data and code are available at https://github.com/agiresearch/TrustAgent.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-02",
    "authors": [
      {
        "authorId": "2007245028",
        "name": "Wenyue Hua"
      },
      {
        "authorId": "2324688005",
        "name": "Xianjun Yang"
      },
      {
        "authorId": "2267333980",
        "name": "Mingyu Jin"
      },
      {
        "authorId": "2109968285",
        "name": "Zelong Li"
      },
      {
        "authorId": "2325820301",
        "name": "Wei Cheng"
      },
      {
        "authorId": "2324600995",
        "name": "Ruixiang Tang"
      },
      {
        "authorId": "2239061409",
        "name": "Yongfeng Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "25cee84e3a1541697a7c97443d7526574127c344",
    "url": "https://www.semanticscholar.org/paper/25cee84e3a1541697a7c97443d7526574127c344",
    "title": "Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration",
    "abstract": "Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain accuracy against the strongest baseline. Further analysis reveals that our proposed mechanisms could help identify failure cases in retrieval augmentation and pinpoint knowledge gaps in multi-hop reasoning.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 44,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-01",
    "authors": [
      {
        "authorId": "2114887261",
        "name": "Shangbin Feng"
      },
      {
        "authorId": "2254168375",
        "name": "Weijia Shi"
      },
      {
        "authorId": "2108853330",
        "name": "Yike Wang"
      },
      {
        "authorId": "2282214127",
        "name": "Wenxuan Ding"
      },
      {
        "authorId": "143820870",
        "name": "Vidhisha Balachandran"
      },
      {
        "authorId": "2249583325",
        "name": "Yulia Tsvetkov"
      }
    ],
    "source": "semantic_scholar",
    "score": 127.0999373465548
  },
  {
    "paperId": "958ed4830ae80a189ecb9b93ab75a6ce2e3926fc",
    "url": "https://www.semanticscholar.org/paper/958ed4830ae80a189ecb9b93ab75a6ce2e3926fc",
    "title": "GPT-Driver: Learning to Drive with GPT",
    "abstract": "We present a simple yet effective approach that can transform the OpenAI GPT-3.5 model into a reliable motion planner for autonomous vehicles. Motion planning is a core challenge in autonomous driving, aiming to plan a driving trajectory that is safe and comfortable. Existing motion planners predominantly leverage heuristic methods to forecast driving trajectories, yet these approaches demonstrate insufficient generalization capabilities in the face of novel and unseen driving scenarios. In this paper, we propose a novel approach to motion planning that capitalizes on the strong reasoning capabilities and generalization potential inherent to Large Language Models (LLMs). The fundamental insight of our approach is the reformulation of motion planning as a language modeling problem, a perspective not previously explored. Specifically, we represent the planner inputs and outputs as language tokens, and leverage the LLM to generate driving trajectories through a language description of coordinate positions. Furthermore, we propose a novel prompting-reasoning-finetuning strategy to stimulate the numerical reasoning potential of the LLM. With this strategy, the LLM can describe highly precise trajectory coordinates and also its internal decision-making process in natural language. We evaluate our approach on the large-scale nuScenes dataset, and extensive experiments substantiate the effectiveness, generalization ability, and interpretability of our GPT-based motion planner. Code is now available at https://github.com/PointsCoder/GPT-Driver.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 152,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.01415",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-02",
    "authors": [
      {
        "authorId": "2253462944",
        "name": "Jiageng Mao"
      },
      {
        "authorId": "2256502052",
        "name": "Yuxi Qian"
      },
      {
        "authorId": "2255490713",
        "name": "Hang Zhao"
      },
      {
        "authorId": "2257326367",
        "name": "Yue Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 145.45656882088653
  },
  {
    "paperId": "4bebe389dfa85423e5cc089edf20b2c3f572f38c",
    "url": "https://www.semanticscholar.org/paper/4bebe389dfa85423e5cc089edf20b2c3f572f38c",
    "title": "Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives",
    "abstract": "The reflection capacity of Large Language Model (LLM) has garnered extensive attention. A post-hoc prompting strategy, e.g., reflexion and self-refine, refines LLM's response based on self-evaluated or external feedback. However, recent research indicates without external feedback, LLM's intrinsic reflection is unstable. Our investigation unveils that the key bottleneck is the quality of the self-evaluated feedback. We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection. To remedy this, we advocate Self-Contrast: It adaptively explores diverse solving perspectives tailored to the request, contrasts the differences, and summarizes these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies. Our method endows LLM with diverse perspectives to alleviate stubborn biases. Moreover, their discrepancies indicate potential errors or inherent uncertainties that LLM often overlooks. Reflecting upon these can catalyze more accurate and stable reflection. Experiments conducted on a series of reasoning and translation tasks with different LLMs serve to underscore the effectiveness and generality of our strategy.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 34,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-01-04",
    "authors": [
      {
        "authorId": "2135282890",
        "name": "Wenqi Zhang"
      },
      {
        "authorId": "1471660296",
        "name": "Yongliang Shen"
      },
      {
        "authorId": "2273585338",
        "name": "Linjuan Wu"
      },
      {
        "authorId": "3391833",
        "name": "Qiuying Peng"
      },
      {
        "authorId": "2282565102",
        "name": "Jun Wang"
      },
      {
        "authorId": "2056432541",
        "name": "Y. Zhuang"
      },
      {
        "authorId": "1776903",
        "name": "Weiming Lu"
      }
    ],
    "source": "semantic_scholar",
    "score": 123.3302209223412
  },
  {
    "paperId": "d068f7fbed51ffb542f7f54528955f48dace0934",
    "url": "https://www.semanticscholar.org/paper/d068f7fbed51ffb542f7f54528955f48dace0934",
    "title": "LITA: Language Instructed Temporal-Localization Assistant",
    "abstract": "There has been tremendous progress in multimodal Large Language Models (LLMs). Recent works have extended these models to video input with promising instruction following capabilities. However, an important missing piece is temporal localization. These models cannot accurately answer the\"When?\"questions. We identify three key aspects that limit their temporal localization capabilities: (i) time representation, (ii) architecture, and (iii) data. We address these shortcomings by proposing Language Instructed Temporal-Localization Assistant (LITA) with the following features: (1) We introduce time tokens that encode timestamps relative to the video length to better represent time in videos. (2) We introduce SlowFast tokens in the architecture to capture temporal information at fine temporal resolution. (3) We emphasize temporal localization data for LITA. In addition to leveraging existing video datasets with timestamps, we propose a new task, Reasoning Temporal Localization (RTL), along with the dataset, ActivityNet-RTL, for learning and evaluating this task. Reasoning temporal localization requires both the reasoning and temporal localization of Video LLMs. LITA demonstrates strong performance on this challenging task, nearly doubling the temporal mean intersection-over-union (mIoU) of baselines. In addition, we show that our emphasis on temporal localization also substantially improves video-based text generation compared to existing Video LLMs, including a 36% relative improvement of Temporal Understanding. Code is available at: https://github.com/NVlabs/LITA",
    "venue": "European Conference on Computer Vision",
    "year": 2024,
    "citationCount": 27,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-27",
    "authors": [
      {
        "authorId": "2293944351",
        "name": "De-An Huang"
      },
      {
        "authorId": "2293723407",
        "name": "Shijia Liao"
      },
      {
        "authorId": "2091913923",
        "name": "Subhashree Radhakrishnan"
      },
      {
        "authorId": "1989015",
        "name": "Hongxu Yin"
      },
      {
        "authorId": "2824500",
        "name": "Pavlo Molchanov"
      },
      {
        "authorId": "2269841405",
        "name": "Zhiding Yu"
      },
      {
        "authorId": "2273651410",
        "name": "Jan Kautz"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.98306765262805
  },
  {
    "paperId": "72d3bf6f696d3f67e17e5211f64b7c6fb0316883",
    "url": "https://www.semanticscholar.org/paper/72d3bf6f696d3f67e17e5211f64b7c6fb0316883",
    "title": "Augmenting Math Word Problems via Iterative Question Composing",
    "abstract": "Despite the advancements in large language models (LLMs) for mathematical reasoning, solving competition-level math problems remains a significant challenge, especially for open-source LLMs without external tools. We introduce the MMIQC dataset, comprising a mixture of processed web data and synthetic question-response pairs, aimed at enhancing the mathematical reasoning capabilities of base language models. Models fine-tuned on MMIQC consistently surpass their counterparts in performance on the MATH benchmark across various model sizes. Notably, Qwen-72B-MMIQC achieves a 45.0% accuracy, exceeding the previous open-source state-of-the-art by 8.2% and outperforming the initial version GPT-4 released in 2023. Extensive evaluation results on Hungarian high school finals suggest that such improvement can generalize to unseen data. Our ablation study on MMIQC reveals that a large part of the improvement can be attributed to our novel augmentation method, Iterative Question Composing (IQC), which involves iteratively composing new questions from seed problems using an LLM and applying rejection sampling through another LLM.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 25,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-17",
    "authors": [
      {
        "authorId": "2280484117",
        "name": "Haoxiong Liu"
      },
      {
        "authorId": "2281903182",
        "name": "Yifan Zhang"
      },
      {
        "authorId": "2281837494",
        "name": "Yifan Luo"
      },
      {
        "authorId": "2279754345",
        "name": "Andrew Chi-Chih Yao"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.87144807032223
  },
  {
    "paperId": "f8d7b0245480646abd257bac60ee37804981a0d7",
    "url": "https://www.semanticscholar.org/paper/f8d7b0245480646abd257bac60ee37804981a0d7",
    "title": "BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models",
    "abstract": "Large language models (LLMs) are shown to benefit from chain-of-thought (COT) prompting, particularly when tackling tasks that require systematic reasoning processes. On the other hand, COT prompting also poses new vulnerabilities in the form of backdoor attacks, wherein the model will output unintended malicious content under specific backdoor-triggered conditions during inference. Traditional methods for launching backdoor attacks involve either contaminating the training dataset with backdoored instances or directly manipulating the model parameters during deployment. However, these approaches are not practical for commercial LLMs that typically operate via API access. In this paper, we propose BadChain, the first backdoor attack against LLMs employing COT prompting, which does not require access to the training dataset or model parameters and imposes low computational overhead. BadChain leverages the inherent reasoning capabilities of LLMs by inserting a backdoor reasoning step into the sequence of reasoning steps of the model output, thereby altering the final response when a backdoor trigger exists in the query prompt. Empirically, we show the effectiveness of BadChain for two COT strategies across four LLMs (Llama2, GPT-3.5, PaLM2, and GPT-4) and six complex benchmark tasks encompassing arithmetic, commonsense, and symbolic reasoning. Moreover, we show that LLMs endowed with stronger reasoning capabilities exhibit higher susceptibility to BadChain, exemplified by a high average attack success rate of 97.0% across the six benchmark tasks on GPT-4. Finally, we propose two defenses based on shuffling and demonstrate their overall ineffectiveness against BadChain. Therefore, BadChain remains a severe threat to LLMs, underscoring the urgency for the development of robust and effective future defenses.",
    "venue": "International Conference on Learning Representations",
    "year": 2024,
    "citationCount": 24,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-20",
    "authors": [
      {
        "authorId": "2261738344",
        "name": "Zhen Xiang"
      },
      {
        "authorId": "2268521947",
        "name": "Fengqing Jiang"
      },
      {
        "authorId": "2155965725",
        "name": "Zidi Xiong"
      },
      {
        "authorId": "31306695",
        "name": "Bhaskar Ramasubramanian"
      },
      {
        "authorId": "144786412",
        "name": "R. Poovendran"
      },
      {
        "authorId": "2261831004",
        "name": "Bo Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.28313737302301
  },
  {
    "paperId": "5d32c364088733c6e8dadc9cf0baa26e10506d61",
    "url": "https://www.semanticscholar.org/paper/5d32c364088733c6e8dadc9cf0baa26e10506d61",
    "title": "Large Language Models as Commonsense Knowledge for Large-Scale Task Planning",
    "abstract": "Large-scale task planning is a major challenge. Recent work exploits large language models (LLMs) directly as a policy and shows surprisingly interesting results. This paper shows that LLMs provide a commonsense model of the world in addition to a policy that acts on it. The world model and the policy can be combined in a search algorithm, such as Monte Carlo Tree Search (MCTS), to scale up task planning. In our new LLM-MCTS algorithm, the LLM-induced world model provides a commonsense prior belief for MCTS to achieve effective reasoning; the LLM-induced policy acts as a heuristic to guide the search, vastly improving search efficiency. Experiments show that LLM-MCTS outperforms both MCTS alone and policies induced by LLMs (GPT2 and GPT3.5) by a wide margin, for complex, novel tasks. Further experiments and analyses on multiple tasks -- multiplication, multi-hop travel planning, object rearrangement -- suggest minimum description length (MDL) as a general guiding principle: if the description length of the world model is substantially smaller than that of the policy, using LLM as a world model for model-based planning is likely better than using LLM solely as a policy.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 129,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.14078",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-23",
    "authors": [
      {
        "authorId": "13869425",
        "name": "Zirui Zhao"
      },
      {
        "authorId": "46605464",
        "name": "W. Lee"
      },
      {
        "authorId": "145463096",
        "name": "David Hsu"
      }
    ],
    "source": "semantic_scholar",
    "score": 150.01301675683374
  },
  {
    "paperId": "548278897d46a54958909bb23bcaecf63e24fadf",
    "url": "https://www.semanticscholar.org/paper/548278897d46a54958909bb23bcaecf63e24fadf",
    "title": "Secrets of RLHF in Large Language Models Part I: PPO",
    "abstract": "Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \\textbf{reward models} to measure human preferences, \\textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \\textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first report, we dissect the framework of RLHF, re-evaluate the inner workings of PPO, and explore how the parts comprising PPO algorithms impact policy agent training. We identify policy constraints being the key factor for the effective implementation of the PPO algorithm. Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model. Based on our main results, we perform a comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT. The absence of open-source implementations has posed significant challenges to the investigation of LLMs alignment. Therefore, we are eager to release technical reports, reward models and PPO codes, aiming to make modest contributions to the advancement of LLMs.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 126,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.04964",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-11",
    "authors": [
      {
        "authorId": "2058585152",
        "name": "Rui Zheng"
      },
      {
        "authorId": "2042683163",
        "name": "Shihan Dou"
      },
      {
        "authorId": "2181306462",
        "name": "Songyang Gao"
      },
      {
        "authorId": "2248291262",
        "name": "Wei Shen"
      },
      {
        "authorId": "2117227865",
        "name": "Wei-Yuan Shen"
      },
      {
        "authorId": "2188630983",
        "name": "Bing Wang"
      },
      {
        "authorId": "2156649786",
        "name": "Yan Liu"
      },
      {
        "authorId": "2219131195",
        "name": "Senjie Jin"
      },
      {
        "authorId": "2109185819",
        "name": "Qin Liu"
      },
      {
        "authorId": "2222630539",
        "name": "Limao Xiong"
      },
      {
        "authorId": "2115386043",
        "name": "Luyao Chen"
      },
      {
        "authorId": "2190751523",
        "name": "Zhiheng Xi"
      },
      {
        "authorId": "2212175381",
        "name": "Yuhao Zhou"
      },
      {
        "authorId": null,
        "name": "Nuo Xu"
      },
      {
        "authorId": "2153857410",
        "name": "Wen-De Lai"
      },
      {
        "authorId": "40587747",
        "name": "Minghao Zhu"
      },
      {
        "authorId": "24009282",
        "name": "Rongxiang Weng"
      },
      {
        "authorId": "2227418",
        "name": "Wen-Chun Cheng"
      },
      {
        "authorId": "2152341000",
        "name": "Cheng Chang"
      },
      {
        "authorId": "2155273086",
        "name": "Zhangyue Yin"
      },
      {
        "authorId": "152738167",
        "name": "Yuan Hua"
      },
      {
        "authorId": "2039788",
        "name": "Haoran Huang"
      },
      {
        "authorId": "153345698",
        "name": "Tianxiang Sun"
      },
      {
        "authorId": "146948229",
        "name": "Hang Yan"
      },
      {
        "authorId": "2067331064",
        "name": "Tao Gui"
      },
      {
        "authorId": "47835189",
        "name": "Qi Zhang"
      },
      {
        "authorId": "1767521",
        "name": "Xipeng Qiu"
      },
      {
        "authorId": "1790227",
        "name": "Xuanjing Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 142.66280629687887
  },
  {
    "paperId": "ba4548c01a8b29a406f269db80f7105b88ca9751",
    "url": "https://www.semanticscholar.org/paper/ba4548c01a8b29a406f269db80f7105b88ca9751",
    "title": "FABLES: Evaluating faithfulness and content selection in book-length summarization",
    "abstract": "While long-context large language models (LLMs) can technically summarize book-length documents (>100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which allows us to rank LLM summarizers based on faithfulness: Claude-3-Opus significantly outperforms all closed-source LLMs, while the open-source Mixtral is on par with GPT-3.5-Turbo. An analysis of the annotations reveals that most unfaithful claims relate to events and character states, and they generally require indirect reasoning over the narrative to invalidate. While LLM-based auto-raters have proven reliable for factuality and coherence in other settings, we implement several LLM raters of faithfulness and find that none correlates strongly with human annotations, especially with regard to detecting unfaithful claims. Our experiments suggest that detecting unfaithful claims is an important future direction not only for summarization evaluation but also as a testbed for long-context understanding. Finally, we move beyond faithfulness by exploring content selection errors in book-length summarization: we develop a typology of omission errors related to crucial narrative elements and also identify a systematic over-emphasis on events occurring towards the end of the book.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 24,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-01",
    "authors": [
      {
        "authorId": "2294805305",
        "name": "Yekyung Kim"
      },
      {
        "authorId": "144455052",
        "name": "Yapei Chang"
      },
      {
        "authorId": "37796923",
        "name": "Marzena Karpinska"
      },
      {
        "authorId": "31099365",
        "name": "Aparna Garimella"
      },
      {
        "authorId": "1977256",
        "name": "Varun Manjunatha"
      },
      {
        "authorId": "46258841",
        "name": "Kyle Lo"
      },
      {
        "authorId": "2253400779",
        "name": "Tanya Goyal"
      },
      {
        "authorId": "2136562",
        "name": "Mohit Iyyer"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.28313737302301
  },
  {
    "paperId": "bdb68c5e2369633b20e733774ac66eb4600c34d1",
    "url": "https://www.semanticscholar.org/paper/bdb68c5e2369633b20e733774ac66eb4600c34d1",
    "title": "LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models",
    "abstract": "The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as Series adapters, Parallel adapter, Prompt-based learning and Reparametrization-based methods. Moreover, we conduct extensive empirical studies on the impact of adapter types, placement locations, and hyper-parameters to the best design for each adapter-based methods. We evaluate the effectiveness of the adapters on fourteen datasets from two different reasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results demonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few extra trainable parameters yields comparable, and in some cases superior, performance to powerful LLMs (175B) in zero-shot inference on both reasoning tasks.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 172,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2304.01933",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-04-04",
    "authors": [
      {
        "authorId": "1557412457",
        "name": "Zhiqiang Hu"
      },
      {
        "authorId": "2150277971",
        "name": "Yihuai Lan"
      },
      {
        "authorId": "145131956",
        "name": "Lei Wang"
      },
      {
        "authorId": "2143418409",
        "name": "Wanyu Xu"
      },
      {
        "authorId": "2212836814",
        "name": "Ee-Peng Lim"
      },
      {
        "authorId": "38656724",
        "name": "R. Lee"
      },
      {
        "authorId": "1996394",
        "name": "Lidong Bing"
      },
      {
        "authorId": "1746416",
        "name": "Soujanya Poria"
      }
    ],
    "source": "semantic_scholar",
    "score": 147.2993739174667
  },
  {
    "paperId": "09d4808857397bc858c6cb6fc46989a9776819c0",
    "url": "https://www.semanticscholar.org/paper/09d4808857397bc858c6cb6fc46989a9776819c0",
    "title": "SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models",
    "abstract": "Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving advanced numerical calculations. To bridge these gaps, we introduce SciInstruct, a suite of scientific instructions for training scientific language models capable of college-level scientific reasoning. Central to our approach is a novel self-reflective instruction annotation framework to address the data scarcity challenge in the science domain. This framework leverages existing LLMs to generate step-by-step reasoning for unlabelled scientific questions, followed by a process of self-reflective critic-and-revise. Applying this framework, we curated a diverse and high-quality dataset encompassing physics, chemistry, math, and formal proofs. We analyze the curated SciInstruct from multiple interesting perspectives (e.g., domain, scale, source, question type, answer length, etc.). To verify the effectiveness of SciInstruct, we fine-tuned different language models with SciInstruct, i.e., ChatGLM3 (6B and 32B), Llama3-8B-Instruct, and Mistral-7B: MetaMath, enhancing their scientific and mathematical reasoning capabilities, without sacrificing the language understanding capabilities of the base model. We release all codes and SciInstruct at https://github.com/THUDM/SciGLM.",
    "venue": "",
    "year": 2024,
    "citationCount": 18,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-01-15",
    "authors": [
      {
        "authorId": "2215493789",
        "name": "Dan Zhang"
      },
      {
        "authorId": "2279661467",
        "name": "Ziniu Hu"
      },
      {
        "authorId": "2279543303",
        "name": "Sining Zhoubian"
      },
      {
        "authorId": "66395694",
        "name": "Zhengxiao Du"
      },
      {
        "authorId": "2279667252",
        "name": "Kaiyu Yang"
      },
      {
        "authorId": "2274076553",
        "name": "Zihan Wang"
      },
      {
        "authorId": "2279544073",
        "name": "Yisong Yue"
      },
      {
        "authorId": "2243402027",
        "name": "Yuxiao Dong"
      },
      {
        "authorId": "2261087802",
        "name": "Jie Tang"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.1665846874966
  },
  {
    "paperId": "d2b48bcee27974c10025422b76c8943ed524722c",
    "url": "https://www.semanticscholar.org/paper/d2b48bcee27974c10025422b76c8943ed524722c",
    "title": "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue",
    "abstract": "Model editing is a technique that edits the large language models (LLMs) with updated knowledge to alleviate hallucinations without resource-intensive retraining. While current model editing methods can effectively modify a model’s behavior within a specific area of interest, they often overlook the potential unintended side effects on the general abilities of LLMs such as reasoning, natural language inference, and question answering. In this paper, we raise concerns that model editing’s improvements on factuality may come at the cost of a significant degradation of the model’s general abilities. We systematically analyze the side effects by evaluating four popular editing methods on three LLMs across eight representative tasks. Our extensive empirical experiments show that it is challenging for current editing methods to simultaneously improve factuality of LLMs and maintain their general abilities. Our analysis reveals that the side effects are caused by model editing altering the original model weights excessively, leading to overfitting to the edited facts. To mitigate this, a method named RECT is proposed to regularize the edit update weights by imposing constraints on their complexity based on the RElative Change in weighT. Evaluation results show that RECT can significantly mitigate the side effects of editing while still maintaining over 94% editing performance.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 18,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-01-09",
    "authors": [
      {
        "authorId": "3028818",
        "name": "Jia-Chen Gu"
      },
      {
        "authorId": "2269760395",
        "name": "Haoyang Xu"
      },
      {
        "authorId": "2152612230",
        "name": "Jun-Yu Ma"
      },
      {
        "authorId": "2887562",
        "name": "Pan Lu"
      },
      {
        "authorId": "2072392338",
        "name": "Zhen-Hua Ling"
      },
      {
        "authorId": "2257127887",
        "name": "Kai-Wei Chang"
      },
      {
        "authorId": "2256996328",
        "name": "Nanyun Peng"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.1665846874966
  },
  {
    "paperId": "021e6c5892347287182b405228fb22923691e3f0",
    "url": "https://www.semanticscholar.org/paper/021e6c5892347287182b405228fb22923691e3f0",
    "title": "Towards Conversational Diagnostic AI",
    "abstract": "At the heart of medicine lies the physician-patient dialogue, where skillful history-taking paves the way for accurate diagnosis, effective management, and enduring trust. Artificial Intelligence (AI) systems capable of diagnostic dialogue could increase accessibility, consistency, and quality of care. However, approximating clinicians' expertise is an outstanding grand challenge. Here, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large Language Model (LLM) based AI system optimized for diagnostic dialogue. AMIE uses a novel self-play based simulated environment with automated feedback mechanisms for scaling learning across diverse disease conditions, specialties, and contexts. We designed a framework for evaluating clinically-meaningful axes of performance including history-taking, diagnostic accuracy, management reasoning, communication skills, and empathy. We compared AMIE's performance to that of primary care physicians (PCPs) in a randomized, double-blind crossover study of text-based consultations with validated patient actors in the style of an Objective Structured Clinical Examination (OSCE). The study included 149 case scenarios from clinical providers in Canada, the UK, and India, 20 PCPs for comparison with AMIE, and evaluations by specialist physicians and patient actors. AMIE demonstrated greater diagnostic accuracy and superior performance on 28 of 32 axes according to specialist physicians and 24 of 26 axes according to patient actors. Our research has several limitations and should be interpreted with appropriate caution. Clinicians were limited to unfamiliar synchronous text-chat which permits large-scale LLM-patient interactions but is not representative of usual clinical practice. While further research is required before AMIE could be translated to real-world settings, the results represent a milestone towards conversational diagnostic AI.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 61,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-11",
    "authors": [
      {
        "authorId": "2244651851",
        "name": "Tao Tu"
      },
      {
        "authorId": "2266472373",
        "name": "Anil Palepu"
      },
      {
        "authorId": "10685155",
        "name": "M. Schaekermann"
      },
      {
        "authorId": "2279021635",
        "name": "Khaled Saab"
      },
      {
        "authorId": "2279022140",
        "name": "Jan Freyberg"
      },
      {
        "authorId": "3480712",
        "name": "Ryutaro Tanno"
      },
      {
        "authorId": "2217861514",
        "name": "Amy Wang"
      },
      {
        "authorId": "2279162093",
        "name": "Brenna Li"
      },
      {
        "authorId": "2224893214",
        "name": "Mohamed Amin"
      },
      {
        "authorId": "2213266",
        "name": "Nenad Tomašev"
      },
      {
        "authorId": "40151244",
        "name": "Shekoofeh Azizi"
      },
      {
        "authorId": "2268759673",
        "name": "Karan Singhal"
      },
      {
        "authorId": "2269173133",
        "name": "Yong Cheng"
      },
      {
        "authorId": "2269258459",
        "name": "Le Hou"
      },
      {
        "authorId": "2291172852",
        "name": "Albert Webson"
      },
      {
        "authorId": "2269148145",
        "name": "Kavita Kulkarni"
      },
      {
        "authorId": "2268760043",
        "name": "S. Mahdavi"
      },
      {
        "authorId": "52326632",
        "name": "Christopher Semturs"
      },
      {
        "authorId": "2051681709",
        "name": "Juraj Gottweis"
      },
      {
        "authorId": "2269147696",
        "name": "Joelle Barral"
      },
      {
        "authorId": "2269148566",
        "name": "Katherine Chou"
      },
      {
        "authorId": "2084098271",
        "name": "Greg S. Corrado"
      },
      {
        "authorId": "2269148232",
        "name": "Yossi Matias"
      },
      {
        "authorId": "2213433382",
        "name": "A. Karthikesalingam"
      },
      {
        "authorId": "144223091",
        "name": "Vivek Natarajan"
      }
    ],
    "source": "semantic_scholar",
    "score": 131.90701577567637
  },
  {
    "paperId": "0b220041eb83c23b7b10d32a5d08c0309d528071",
    "url": "https://www.semanticscholar.org/paper/0b220041eb83c23b7b10d32a5d08c0309d528071",
    "title": "Large Language Models for Information Retrieval: A Survey",
    "abstract": "As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, thereby reshaping the IR landscape, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of both traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has revolutionized natural language processing due to their remarkable language understanding, generation, generalization, and reasoning abilities. Consequently, recent research has sought to leverage LLMs to improve IR systems. Given the rapid evolution of this research trajectory, it is necessary to consolidate existing methodologies and provide nuanced insights through a comprehensive overview. In this survey, we delve into the confluence of LLMs and IR systems, including crucial aspects such as query rewriters, retrievers, rerankers, and readers. Additionally, we explore promising directions, such as search agents, within this expanding field.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 204,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.07107",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-08-14",
    "authors": [
      {
        "authorId": "1900406",
        "name": "Yutao Zhu"
      },
      {
        "authorId": "51162759",
        "name": "Huaying Yuan"
      },
      {
        "authorId": "2109464614",
        "name": "Shuting Wang"
      },
      {
        "authorId": "1830383266",
        "name": "Jiongnan Liu"
      },
      {
        "authorId": "2229460959",
        "name": "Wenhan Liu"
      },
      {
        "authorId": "2057946598",
        "name": "Chenlong Deng"
      },
      {
        "authorId": "1897235",
        "name": "Zhicheng Dou"
      },
      {
        "authorId": "153693432",
        "name": "Ji-rong Wen"
      }
    ],
    "source": "semantic_scholar",
    "score": 149.84514968707612
  },
  {
    "paperId": "aa10e7556e109c07e8f4e1bf6e61d6f888b97842",
    "url": "https://www.semanticscholar.org/paper/aa10e7556e109c07e8f4e1bf6e61d6f888b97842",
    "title": "Putting GPT-4o to the Sword: A Comprehensive Evaluation of Language, Vision, Speech, and Multimodal Proficiency",
    "abstract": "As large language models (LLMs) continue to advance, evaluating their comprehensive capabilities becomes significant for their application in various fields. This research study comprehensively evaluates the language, vision, speech, and multimodal capabilities of GPT-4o. The study employs standardized exam questions, reasoning tasks, and translation assessments to assess the model’s language capability. Additionally, GPT-4o’s vision and speech capabilities are tested through image classification and object-recognition tasks, as well as accent classification. The multimodal evaluation assesses the model’s performance in integrating visual and linguistic data. Our findings reveal that GPT-4o demonstrates high accuracy and efficiency across multiple domains in language and reasoning capabilities, excelling in tasks that require few-shot learning. GPT-4o also provides notable improvements in multimodal tasks compared to its predecessors. However, the model shows variability and faces limitations in handling complex and ambiguous inputs, particularly in audio and vision capabilities. This paper highlights the need for more comprehensive benchmarks and robust evaluation frameworks, encompassing qualitative assessments involving human judgment, as well as error analysis. Future work should focus on expanding datasets, investigating prompt-based assessment, and enhancing few-shot learning techniques to test the model’s practical applicability and performance in real-world scenarios.",
    "venue": "Applied Sciences",
    "year": 2024,
    "citationCount": 19,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-19",
    "authors": [
      {
        "authorId": "143787727",
        "name": "Sakib Shahriar"
      },
      {
        "authorId": "2248150677",
        "name": "Brady D. Lund"
      },
      {
        "authorId": "2212887866",
        "name": "Nishith Reddy Mannuru"
      },
      {
        "authorId": "1625511888",
        "name": "Muhammad Arbab Arshad"
      },
      {
        "authorId": "1915582",
        "name": "Kadhim Hayawi"
      },
      {
        "authorId": "2284874922",
        "name": "Ravi Varma Kumar Bevara"
      },
      {
        "authorId": "2277742033",
        "name": "Aashrith Mannuru"
      },
      {
        "authorId": "2299630635",
        "name": "Laiba Batool"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.93598410330986
  },
  {
    "paperId": "e0384ba36555232c587d4a80d527895a095a9001",
    "url": "https://www.semanticscholar.org/paper/e0384ba36555232c587d4a80d527895a095a9001",
    "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
    "abstract": "Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation benchmark for Large Language Models (HaluEval), a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about $19.5\\%$ responses). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. However, our experiments also prove that providing external knowledge or adding reasoning steps can help LLMs recognize hallucinations. Our benchmark can be accessed at https://github.com/RUCAIBox/HaluEval.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 177,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.11747",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-19",
    "authors": [
      {
        "authorId": "2018027",
        "name": "Junyi Li"
      },
      {
        "authorId": "2149479237",
        "name": "Xiaoxue Cheng"
      },
      {
        "authorId": "2542603",
        "name": "Wayne Xin Zhao"
      },
      {
        "authorId": "50204644",
        "name": "J. Nie"
      },
      {
        "authorId": "153693432",
        "name": "Ji-rong Wen"
      }
    ],
    "source": "semantic_scholar",
    "score": 147.72675325438126
  },
  {
    "paperId": "e4bb1b1f97711a7634bf4bff72c56891be2222e6",
    "url": "https://www.semanticscholar.org/paper/e4bb1b1f97711a7634bf4bff72c56891be2222e6",
    "title": "Cognitive Architectures for Language Agents",
    "abstract": "Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a systematic framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within the broader history of AI and outlines a path towards language-based general intelligence.",
    "venue": "Trans. Mach. Learn. Res.",
    "year": 2023,
    "citationCount": 115,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.02427",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-09-05",
    "authors": [
      {
        "authorId": "1976174397",
        "name": "T. Sumers"
      },
      {
        "authorId": "47188964",
        "name": "Shunyu Yao"
      },
      {
        "authorId": "144958935",
        "name": "Karthik Narasimhan"
      },
      {
        "authorId": "2239948923",
        "name": "Thomas L. Griffiths"
      }
    ],
    "source": "semantic_scholar",
    "score": 141.30385286659546
  },
  {
    "paperId": "88aa6b1f37d1fd8e0a40499ce9bb87873f03aaa8",
    "url": "https://www.semanticscholar.org/paper/88aa6b1f37d1fd8e0a40499ce9bb87873f03aaa8",
    "title": "Qwen2.5 Technical Report",
    "abstract": "In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-12-19",
    "authors": [
      {
        "authorId": "2336242315",
        "name": "Qwen An Yang"
      },
      {
        "authorId": "2257101724",
        "name": "Baosong Yang"
      },
      {
        "authorId": "2321681508",
        "name": "Beichen Zhang"
      },
      {
        "authorId": "2321578848",
        "name": "Binyuan Hui"
      },
      {
        "authorId": "2312091718",
        "name": "Bo Zheng"
      },
      {
        "authorId": "2325918197",
        "name": "Bowen Yu"
      },
      {
        "authorId": "2311714296",
        "name": "Chengyuan Li"
      },
      {
        "authorId": "2248487202",
        "name": "Dayiheng Liu"
      },
      {
        "authorId": "2330680670",
        "name": "Fei Huang"
      },
      {
        "authorId": "51490462",
        "name": "Guanting Dong"
      },
      {
        "authorId": "2312183452",
        "name": "Haoran Wei"
      },
      {
        "authorId": "2314068968",
        "name": "Huan Lin"
      },
      {
        "authorId": "2243424858",
        "name": "Jian Yang"
      },
      {
        "authorId": "2321581897",
        "name": "Jianhong Tu"
      },
      {
        "authorId": "2323510768",
        "name": "Jianwei Zhang"
      },
      {
        "authorId": "2336243053",
        "name": "Jianxin Yang"
      },
      {
        "authorId": "2328943044",
        "name": "Jiaxin Yang"
      },
      {
        "authorId": "2237981776",
        "name": "Jingren Zhou"
      },
      {
        "authorId": "2326803484",
        "name": "Junyang Lin"
      },
      {
        "authorId": "2247877609",
        "name": "Kai Dang"
      },
      {
        "authorId": "2257001403",
        "name": "Keming Lu"
      },
      {
        "authorId": "2336092727",
        "name": "Keqin Bao"
      },
      {
        "authorId": "2303430522",
        "name": "Kexin Yang"
      },
      {
        "authorId": "2265527327",
        "name": "Le Yu"
      },
      {
        "authorId": "2223106060",
        "name": "Mei Li"
      },
      {
        "authorId": "2065790119",
        "name": "Mingfeng Xue"
      },
      {
        "authorId": "2288896179",
        "name": "Pei Zhang"
      },
      {
        "authorId": "2152206465",
        "name": "Qin Zhu"
      },
      {
        "authorId": "47447639",
        "name": "Rui Men"
      },
      {
        "authorId": "2248039532",
        "name": "Runji Lin"
      },
      {
        "authorId": "2309297259",
        "name": "Tianhao Li"
      },
      {
        "authorId": "2307468401",
        "name": "Tingyu Xia"
      },
      {
        "authorId": "2274095735",
        "name": "Xingzhang Ren"
      },
      {
        "authorId": "2312110505",
        "name": "Xuancheng Ren"
      },
      {
        "authorId": "2234000716",
        "name": "Yang Fan"
      },
      {
        "authorId": "2118000701",
        "name": "Yang Su"
      },
      {
        "authorId": "2163069125",
        "name": "Yi-Chao Zhang"
      },
      {
        "authorId": "2244411465",
        "name": "Yunyang Wan"
      },
      {
        "authorId": "2238712089",
        "name": "Yuqi Liu"
      },
      {
        "authorId": "2248072386",
        "name": "Zeyu Cui"
      },
      {
        "authorId": "2321656193",
        "name": "Zhenru Zhang"
      },
      {
        "authorId": "2337236034",
        "name": "Zihan Qiu"
      },
      {
        "authorId": "2325151882",
        "name": "Shanghaoran Quan"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.49820016084324
  },
  {
    "paperId": "573bb43c3c99b953f3b0cf5aa96e2aec1efc3bbb",
    "url": "https://www.semanticscholar.org/paper/573bb43c3c99b953f3b0cf5aa96e2aec1efc3bbb",
    "title": "Integrating Large Language Models into Recommendation via Mutual Augmentation and Adaptive Aggregation",
    "abstract": "Conventional recommendation methods have achieved notable advancements by harnessing collaborative or sequential information from user behavior. Recently, large language models (LLMs) have gained prominence for their capabilities in understanding and reasoning over textual semantics, and have found utility in various domains, including recommendation. Conventional recommendation methods and LLMs each have their strengths and weaknesses. While conventional methods excel at mining collaborative information and modeling sequential behavior, they struggle with data sparsity and the long-tail problem. LLMs, on the other hand, are proficient at utilizing rich textual contexts but face challenges in mining collaborative or sequential information. Despite their individual successes, there is a significant gap in leveraging their combined potential to enhance recommendation performance. In this paper, we introduce a general and model-agnostic framework known as \\textbf{L}arge \\textbf{la}nguage model with \\textbf{m}utual augmentation and \\textbf{a}daptive aggregation for \\textbf{Rec}ommendation (\\textbf{Llama4Rec}). Llama4Rec synergistically combines conventional and LLM-based recommendation models. Llama4Rec proposes data augmentation and prompt augmentation strategies tailored to enhance the conventional model and LLM respectively. An adaptive aggregation module is adopted to combine the predictions of both kinds of models to refine the final recommendation results. Empirical studies on three real-world datasets validate the superiority of Llama4Rec, demonstrating its consistent outperformance of baseline methods and significant improvements in recommendation performance.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-25",
    "authors": [
      {
        "authorId": "2182235729",
        "name": "Sichun Luo"
      },
      {
        "authorId": "2257363981",
        "name": "Yuxuan Yao"
      },
      {
        "authorId": "2276605422",
        "name": "Bowei He"
      },
      {
        "authorId": "2276522867",
        "name": "Yinya Huang"
      },
      {
        "authorId": "2276425017",
        "name": "Aojun Zhou"
      },
      {
        "authorId": "2281396407",
        "name": "Xinyi Zhang"
      },
      {
        "authorId": "2276488836",
        "name": "Yuanzhang Xiao"
      },
      {
        "authorId": "2276424424",
        "name": "Mingjie Zhan"
      },
      {
        "authorId": "2256601763",
        "name": "Linqi Song"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.49820016084324
  },
  {
    "paperId": "fdcf7c7130a140ec0c206d0894d2a941bed884f7",
    "url": "https://www.semanticscholar.org/paper/fdcf7c7130a140ec0c206d0894d2a941bed884f7",
    "title": "ChatSpamDetector: Leveraging Large Language Models for Effective Phishing Email Detection",
    "abstract": "The proliferation of phishing sites and emails poses significant challenges to existing cybersecurity efforts. Despite advances in malicious email filters and email security protocols, problems with oversight and false positives persist. Users often struggle to understand why emails are flagged as potentially fraudulent, risking the possibility of missing important communications or mistakenly trusting deceptive phishing emails. This study introduces ChatSpamDetector, a system that uses large language models (LLMs) to detect phishing emails. By converting email data into a prompt suitable for LLM analysis, the system provides a highly accurate determination of whether an email is phishing or not. Importantly, it offers detailed reasoning for its phishing determinations, assisting users in making informed decisions about how to handle suspicious emails. We conducted an evaluation using a comprehensive phishing email dataset and compared our system to several LLMs and baseline systems. We confirmed that our system using GPT-4 has superior detection capabilities with an accuracy of 99.70%. Advanced contextual interpretation by LLMs enables the identification of various phishing tactics and impersonations, making them a potentially powerful tool in the fight against email-based phishing threats.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 15,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-28",
    "authors": [
      {
        "authorId": "1799400183",
        "name": "Takashi Koide"
      },
      {
        "authorId": "2099859973",
        "name": "Naoki Fukushi"
      },
      {
        "authorId": "2067744577",
        "name": "Hiroki Nakano"
      },
      {
        "authorId": "2259918752",
        "name": "Daiki Chiba"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.58883083359672
  },
  {
    "paperId": "91deaf9d324c8feafc189da0da03e60a60287bca",
    "url": "https://www.semanticscholar.org/paper/91deaf9d324c8feafc189da0da03e60a60287bca",
    "title": "Code as Policies: Language Model Programs for Embodied Control",
    "abstract": "Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (‘faster’) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io",
    "venue": "IEEE International Conference on Robotics and Automation",
    "year": 2022,
    "citationCount": 714,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2209.07753",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-09-16",
    "authors": [
      {
        "authorId": "6454541",
        "name": "Jacky Liang"
      },
      {
        "authorId": "2158105356",
        "name": "Wenlong Huang"
      },
      {
        "authorId": "144956443",
        "name": "F. Xia"
      },
      {
        "authorId": "2153917744",
        "name": "Peng Xu"
      },
      {
        "authorId": "1944801",
        "name": "Karol Hausman"
      },
      {
        "authorId": "2704814",
        "name": "Brian Ichter"
      },
      {
        "authorId": "47686265",
        "name": "Peter R. Florence"
      },
      {
        "authorId": "38591293",
        "name": "Andy Zeng"
      }
    ],
    "source": "semantic_scholar",
    "score": 174.5842381404101
  },
  {
    "paperId": "d492c004b74dcbec9d5c0732d95bbd3588e30451",
    "url": "https://www.semanticscholar.org/paper/d492c004b74dcbec9d5c0732d95bbd3588e30451",
    "title": "Can large language models explore in-context?",
    "abstract": "We investigate the extent to which contemporary Large Language Models (LLMs) can engage in exploration, a core capability in reinforcement learning and decision making. We focus on native performance of existing LLMs, without training interventions. We deploy LLMs as agents in simple multi-armed bandit environments, specifying the environment description and interaction history entirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5, GPT-4, and Llama2, using a variety of prompt designs, and find that the models do not robustly engage in exploration without substantial interventions: i) Across all of our experiments, only one configuration resulted in satisfactory exploratory behavior: GPT-4 with chain-of-thought reasoning and an externally summarized interaction history, presented as sufficient statistics; ii) All other configurations did not result in robust exploratory behavior, including those with chain-of-thought reasoning but unsummarized history. Although these findings can be interpreted positively, they suggest that external summarization -- which may not be possible in more complex settings -- is important for obtaining desirable behavior from LLM agents. We conclude that non-trivial algorithmic interventions, such as fine-tuning or dataset curation, may be required to empower LLM-based decision making agents in complex settings.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 14,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-22",
    "authors": [
      {
        "authorId": "2280143899",
        "name": "Akshay Krishnamurthy"
      },
      {
        "authorId": "2268760450",
        "name": "Keegan Harris"
      },
      {
        "authorId": "2279828764",
        "name": "Dylan J. Foster"
      },
      {
        "authorId": "2259599828",
        "name": "Cyril Zhang"
      },
      {
        "authorId": "2158559",
        "name": "Aleksandrs Slivkins"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "28e9fb24bfb25f6e40d3897a5cf9c39f6f6b8d1f",
    "url": "https://www.semanticscholar.org/paper/28e9fb24bfb25f6e40d3897a5cf9c39f6f6b8d1f",
    "title": "C-ICL: Contrastive In-context Learning for Information Extraction",
    "abstract": "There has been increasing interest in exploring the capabilities of advanced large language models (LLMs) in the field of information extraction (IE), specifically focusing on tasks related to named entity recognition (NER) and relation extraction (RE). Although researchers are exploring the use of few-shot information extraction through in-context learning with LLMs, they tend to focus only on using correct or positive examples for demonstration, neglecting the potential value of incorporating incorrect or negative examples into the learning process. In this paper, we present c-ICL, a novel few-shot technique that leverages both correct and incorrect sample constructions to create in-context learning demonstrations. This approach enhances the ability of LLMs to extract entities and relations by utilizing prompts that incorporate not only the positive samples but also the reasoning behind them. This method allows for the identification and correction of potential interface errors. Specifically, our proposed method taps into the inherent contextual information and valuable information in hard negative samples and the nearest positive neighbors to the test and then applies the in-context learning demonstrations based on LLMs. Our experiments on various datasets indicate that c-ICL outperforms previous few-shot in-context learning methods, delivering substantial enhancements in performance across a broad spectrum of related tasks. These improvements are noteworthy, showcasing the versatility of our approach in miscellaneous scenarios.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-17",
    "authors": [
      {
        "authorId": "2199939792",
        "name": "Ying Mo"
      },
      {
        "authorId": "2276103971",
        "name": "Jian Yang"
      },
      {
        "authorId": "2261393008",
        "name": "Jiahao Liu"
      },
      {
        "authorId": "2216176381",
        "name": "Shun Zhang"
      },
      {
        "authorId": "2258759716",
        "name": "Jingang Wang"
      },
      {
        "authorId": "2258837278",
        "name": "Zhoujun Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "7f0ad3a6aef6bb51b599138ff83437fcbe6d6072",
    "url": "https://www.semanticscholar.org/paper/7f0ad3a6aef6bb51b599138ff83437fcbe6d6072",
    "title": "Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data",
    "abstract": "The impressive capabilities of large language models (LLMs) have sparked debate over whether these models genuinely generalize to unseen tasks or predominantly rely on memorizing vast amounts of pretraining data. To explore this issue, we introduce an extended concept of memorization, distributional memorization, which measures the correlation between the LLM output probabilities and the pretraining data frequency. To effectively capture task-specific pretraining data frequency, we propose a novel task-gram language model, which is built by counting the co-occurrence of semantically related $n$-gram pairs from task inputs and outputs in the pretraining corpus. Using the Pythia models trained on the Pile dataset, we evaluate four distinct tasks: machine translation, factual question answering, world knowledge understanding, and math reasoning. Our findings reveal varying levels of memorization, with the strongest effect observed in factual question answering. Furthermore, while model performance improves across all tasks as LLM size increases, only factual question answering shows an increase in memorization, whereas machine translation and reasoning tasks exhibit greater generalization, producing more novel outputs. This study demonstrates that memorization plays a larger role in simpler, knowledge-intensive tasks, while generalization is the key for harder, reasoning-based tasks, providing a scalable method for analyzing large pretraining corpora in greater depth. We also show the practical implications of our analysis through a novel prompt optimization algorithm.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-20",
    "authors": [
      {
        "authorId": "2264484459",
        "name": "Antonis Antoniades"
      },
      {
        "authorId": "2115553132",
        "name": "Xinyi Wang"
      },
      {
        "authorId": "51131518",
        "name": "Yanai Elazar"
      },
      {
        "authorId": "2039956094",
        "name": "Alfonso Amayuelas"
      },
      {
        "authorId": "2044198106",
        "name": "Alon Albalak"
      },
      {
        "authorId": "2119058805",
        "name": "Kexun Zhang"
      },
      {
        "authorId": "2310569060",
        "name": "W. Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "3165ecdcef5fedba99738df434b29a2d8083c44c",
    "url": "https://www.semanticscholar.org/paper/3165ecdcef5fedba99738df434b29a2d8083c44c",
    "title": "MM-PhyQA: Multimodal Physics Question-Answering with Multi-image CoT Prompting",
    "abstract": null,
    "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining",
    "year": 2024,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-11",
    "authors": [
      {
        "authorId": "2223123570",
        "name": "Avinash Anand"
      },
      {
        "authorId": "2287841157",
        "name": "Janak Kapuriya"
      },
      {
        "authorId": "2296730605",
        "name": "Apoorv Singh"
      },
      {
        "authorId": "2296711567",
        "name": "Jay Saraf"
      },
      {
        "authorId": "2060965994",
        "name": "Naman Lal"
      },
      {
        "authorId": "2271625822",
        "name": "Astha Verma"
      },
      {
        "authorId": "2271710275",
        "name": "Rushali Gupta"
      },
      {
        "authorId": "1753278",
        "name": "R. Shah"
      }
    ],
    "source": "semantic_scholar",
    "score": 85.96842909197557
  },
  {
    "paperId": "cd27f45bc760447fb4de3209e2381ea3493bbd57",
    "url": "https://www.semanticscholar.org/paper/cd27f45bc760447fb4de3209e2381ea3493bbd57",
    "title": "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search",
    "abstract": "Recent methodologies in LLM self-training mostly rely on LLM generating responses and filtering those with correct output answers as training data. This approach often yields a low-quality fine-tuning training set (e.g., incorrect plans or intermediate reasoning). In this paper, we develop a reinforced self-training approach, called ReST-MCTS*, based on integrating process reward guidance with tree search MCTS* for collecting higher-quality reasoning traces as well as per-step value to train policy and reward models. ReST-MCTS* circumvents the per-step manual annotation typically used to train process rewards by tree-search-based reinforcement learning: Given oracle final correct answers, ReST-MCTS* is able to infer the correct process rewards by estimating the probability this step can help lead to the correct answer. These inferred rewards serve dual purposes: they act as value targets for further refining the process reward model and also facilitate the selection of high-quality traces for policy model self-training. We first show that the tree-search policy in ReST-MCTS* achieves higher accuracy compared with prior LLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same search budget. We then show that by using traces searched by this tree-search policy as training data, we can continuously enhance the three language models for multiple iterations, and outperform other self-training algorithms such as ReST$^\\text{EM}$ and Self-Rewarding LM. We release all code at https://github.com/THUDM/ReST-MCTS.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 38,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-06",
    "authors": [
      {
        "authorId": "2215493789",
        "name": "Dan Zhang"
      },
      {
        "authorId": "2279543303",
        "name": "Sining Zhoubian"
      },
      {
        "authorId": "2279544073",
        "name": "Yisong Yue"
      },
      {
        "authorId": "2243402027",
        "name": "Yuxiao Dong"
      },
      {
        "authorId": "2261087802",
        "name": "Jie Tang"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.95342469194469
  },
  {
    "paperId": "fc5eb6a11987bcd0bf0460ff65b46f15cee3a911",
    "url": "https://www.semanticscholar.org/paper/fc5eb6a11987bcd0bf0460ff65b46f15cee3a911",
    "title": "Interactive Continual Learning: Fast and Slow Thinking",
    "abstract": "Advanced life forms, sustained by the synergistic interaction of neural cognitive mechanisms, continually acquire and transfer knowledge throughout their lifespan. In contrast, contemporary machine learning paradigms exhibit limitations in emulating the facets of continual learning (CL). Nonetheless, the emergence of large language models (LLMs) presents promising avenues for realizing CL via interactions with these models. Drawing on Complementary Learning System theory, this paper presents a novel Interactive Continual Learning (ICL) framework, enabled by collaborative interactions among models of various sizes. Specifically, we assign the ViT model as System1 and multimodal LLM as System2. To enable the memory module to deduce tasks from class information and enhance Set2Set retrieval, we propose the Class-Knowledge-Task Multi-Head Attention (CKT-MHA). Additionally, to improve memory retrieval in System1 through enhanced geometric representation, we introduce the CL-vMF mechanism, based on the von Mises-Fisher (vMF) distribution. Mean-while, we introduce the von Mises-Fisher Outlier Detection and Interaction (vMF-ODI) strategy to identify hard examples, thus enhancing collaboration between System1 and System2 for complex reasoning realization. Comprehensive evaluation of our proposed ICL demonstrates significant resistance to forgetting and superior performance relative to existing methods. Code is available at github.com/ICL.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2403.02628",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-05",
    "authors": [
      {
        "authorId": "66242399",
        "name": "Biqing Qi"
      },
      {
        "authorId": "2290123377",
        "name": "Xingquan Chen"
      },
      {
        "authorId": "2287801881",
        "name": "Junqi Gao"
      },
      {
        "authorId": "2288001009",
        "name": "Dong Li"
      },
      {
        "authorId": "2287816392",
        "name": "Jianxing Liu"
      },
      {
        "authorId": "2148912977",
        "name": "Ligang Wu"
      },
      {
        "authorId": "2218723159",
        "name": "Bowen Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "d610e066e5c3cc7eda13313554e25a340ce18f71",
    "url": "https://www.semanticscholar.org/paper/d610e066e5c3cc7eda13313554e25a340ce18f71",
    "title": "Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries",
    "abstract": "Large language models (LLMs) can solve complex multi-step problems, but little is known about how these computations are implemented internally. Motivated by this, we study how LLMs answer multi-hop queries such as “The spouse of the performer of Imagine is”. These queries require two information extraction steps: a latent one for resolving the first hop (“the performer of Imagine”) into the bridge entity (John Lennon), and another for resolving the second hop (“the spouse of John Lennon”) into the target entity (Yoko Ono). Understanding how the latent step is computed internally is key to understanding the overall computation. By carefully analyzing the internal computations of transformer-based LLMs, we discover that the bridge entity is resolved in the early layers of the model. Then, only after this resolution, the two-hop query is solved in the later layers. Because the second hop commences in later layers, there could be cases where these layers no longer encode the necessary knowledge for correctly predicting the answer. Motivated by this, we propose a novel “back-patching” analysis method whereby a hidden representation from a later layer is patched back to an earlier layer. We find that in up to 66% of previously incorrect cases there exists a back-patch that results in the correct generation of the answer, showing that the later layers indeed sometimes lack the needed functionality. Overall our methods and findings open further opportunities for understanding and improving latent reasoning in transformer-based LLMs.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-18",
    "authors": [
      {
        "authorId": "2226283125",
        "name": "Eden Biran"
      },
      {
        "authorId": "2307080108",
        "name": "Daniela Gottesman"
      },
      {
        "authorId": "16110760",
        "name": "Sohee Yang"
      },
      {
        "authorId": "22245981",
        "name": "Mor Geva"
      },
      {
        "authorId": "2261392157",
        "name": "Amir Globerson"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "779c7800b7203f179a53182b6e10964588a5a72a",
    "url": "https://www.semanticscholar.org/paper/779c7800b7203f179a53182b6e10964588a5a72a",
    "title": "MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning",
    "abstract": "Efficient finetuning of large language models (LLMs) aims to adapt the LLMs with reduced computational and memory cost. Previous LoRA-based approaches initialize the low-rank matrices with Gaussian distribution and zero values while keeping the original weight matrices frozen. However, the trainable model parameters optimized in an unguided subspace might interfere with the well-learned subspace of the pretrained weight matrices. In this paper, we propose MiLoRA, a simple yet effective LLM finetuning approach that only updates the minor singular components of the weight matrix while keeping the principal singular components frozen. It is observed that the minor matrix corresponds to the noisy or long-tail information, while the principal matrix contains important knowledge. The MiLoRA initializes the low-rank matrices within a subspace that is orthogonal to the principal matrix, thus the pretrained knowledge is expected to be well preserved. During finetuning, MiLoRA makes the most use of the less-optimized subspace for learning the labeled dataset. Extensive experiments on commonsense reasoning, math reasoning, instruction following and visual instruction following benchmarks present the superior performance of our method.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-13",
    "authors": [
      {
        "authorId": "2262371637",
        "name": "Hanqing Wang"
      },
      {
        "authorId": "2291079288",
        "name": "Zeguan Xiao"
      },
      {
        "authorId": "2253886019",
        "name": "Yixia Li"
      },
      {
        "authorId": "2306413338",
        "name": "Shuo Wang"
      },
      {
        "authorId": "1455940157",
        "name": "Guanhua Chen"
      },
      {
        "authorId": "2285074607",
        "name": "Yun Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "69b56cfa76c991bc2a327948264acab928cf1944",
    "url": "https://www.semanticscholar.org/paper/69b56cfa76c991bc2a327948264acab928cf1944",
    "title": "Cooperate or Collapse: Emergence of Sustainability Behaviors in a Society of LLM Agents",
    "abstract": "In the rapidly evolving field of artificial intelligence, ensuring safe decision-making of Large Language Models (LLMs) is a significant challenge. This paper introduces Governance of the Commons Simulation (G OV S IM ), a simulation platform designed to study strategic interactions and cooperative decision-making in LLMs. Through this simulation environment, we explore the dynamics of resource sharing among AI agents, highlighting the importance of ethical considerations, strategic planning, and negotiation skills. G OV S IM is versatile and supports any text-based agent, including LLMs agents. Using the Generative Agent framework, we create a standard agent that facilitates the integration of different LLMs. Our findings reveal that within G OV S IM , only two out of 15 tested LLMs managed to achieve a sustainable outcome, indicating a significant gap in the ability of models to manage shared resources. Furthermore, we find that by removing the ability of agents to communicate, they overuse the shared resource, highlighting the importance of communication for cooperation. Interestingly, most LLMs lack the ability to make universalized hypotheses, which highlights a significant weakness in their reasoning skills. We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface. 1",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2298275529",
        "name": "Giorgio Piatti"
      },
      {
        "authorId": "2111472502",
        "name": "Zhijing Jin"
      },
      {
        "authorId": "2272856326",
        "name": "Max Kleiman-Weiner"
      },
      {
        "authorId": "2261483511",
        "name": "Bernhard Schölkopf"
      },
      {
        "authorId": "2790926",
        "name": "Mrinmaya Sachan"
      },
      {
        "authorId": "2105984203",
        "name": "Rada Mihalcea"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "199cbde8658dee02643f0352a62ab51657924d08",
    "url": "https://www.semanticscholar.org/paper/199cbde8658dee02643f0352a62ab51657924d08",
    "title": "Ensemble Learning for Heterogeneous Large Language Models with Deep Parallel Collaboration",
    "abstract": "Large language models (LLMs) exhibit complementary strengths in various tasks, motivating the research of LLM ensembling. However, existing work focuses on training an extra reward model or fusion model to select or combine all candidate answers, posing a great challenge to the generalization on unseen data distributions. Besides, prior methods use textual responses as communication media, ignoring the valuable information in the internal representations. In this work, we propose a training-free ensemble framework DeePEn, fusing the informative probability distributions yielded by different LLMs at each decoding step. Unfortunately, the vocabulary discrepancy between heterogeneous LLMs directly makes averaging the distributions unfeasible due to the token misalignment. To address this challenge, DeePEn maps the probability distribution of each model from its own probability space to a universal relative space based on the relative representation theory, and performs aggregation. Next, we devise a search-based inverse transformation to transform the aggregated result back to the probability space of one of the ensembling LLMs (main model), in order to determine the next token. We conduct extensive experiments on ensembles of different number of LLMs, ensembles of LLMs with different architectures, and ensembles between the LLM and the specialist model. Experimental results show that (i) DeePEn achieves consistent improvements across six benchmarks covering subject examination, reasoning, and knowledge, (ii) a well-performing specialist model can benefit from a less effective LLM through distribution fusion, and (iii) DeePEn has complementary strengths with other ensemble methods such as voting.",
    "venue": "",
    "year": 2024,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-04-19",
    "authors": [
      {
        "authorId": "2118138548",
        "name": "Yi-Chong Huang"
      },
      {
        "authorId": "2674998",
        "name": "Xiaocheng Feng"
      },
      {
        "authorId": "2218245357",
        "name": "Baohang Li"
      },
      {
        "authorId": "2297686100",
        "name": "Yang Xiang"
      },
      {
        "authorId": "2273977295",
        "name": "Hui Wang"
      },
      {
        "authorId": "2257004102",
        "name": "Bing Qin"
      },
      {
        "authorId": "2274093523",
        "name": "Ting Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "097d26af3bd536537a9a679ca5e0156082e9ebf5",
    "url": "https://www.semanticscholar.org/paper/097d26af3bd536537a9a679ca5e0156082e9ebf5",
    "title": "Can Large Language Models Identify Authorship?",
    "abstract": "The ability to accurately identify authorship is crucial for verifying content authenticity and mitigating misinformation. Large Language Models (LLMs) have demonstrated an exceptional capacity for reasoning and problem-solving. However, their potential in authorship analysis remains under-explored. Traditional studies have depended on hand-crafted stylistic features, whereas state-of-the-art approaches leverage text embeddings from pre-trained language models. These methods, which typically require fine-tuning on labeled data, often suffer from performance degradation in cross-domain applications and provide limited explainability. This work seeks to address three research questions: (1) Can LLMs perform zero-shot, end-to-end authorship verification effectively? (2) Are LLMs capable of accurately attributing authorship among multiple candidates authors (e.g., 10 and 20)? (3) Can LLMs provide explainability in authorship analysis, particularly through the role of linguistic features? Moreover, we investigate the integration of explicit linguistic features to guide LLMs in their reasoning processes. Our assessment demonstrates LLMs' proficiency in both tasks without the need for domain-specific fine-tuning, providing explanations into their decision making via a detailed analysis of linguistic features. This establishes a new benchmark for future research on LLM-based authorship analysis.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-13",
    "authors": [
      {
        "authorId": "51164501",
        "name": "Baixiang Huang"
      },
      {
        "authorId": "2163546329",
        "name": "Canyu Chen"
      },
      {
        "authorId": "2266239316",
        "name": "Kai Shu"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "fe99095df95638b6768e1c10682034d25353ed89",
    "url": "https://www.semanticscholar.org/paper/fe99095df95638b6768e1c10682034d25353ed89",
    "title": "MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining",
    "abstract": "Graphs with abundant attributes are essential in modeling interconnected entities and improving predictions in various real-world applications. Traditional Graph Neural Networks (GNNs), which are commonly used for modeling attributed graphs, need to be re-trained every time when applied to different graph tasks and datasets. Although the emergence of Large Language Models (LLMs) has introduced a new paradigm in natural language processing, the generative potential of LLMs in graph mining remains largely under-explored. To this end, we propose a novel framework MuseGraph, which seamlessly integrates the strengths of GNNs and LLMs and facilitates a more effective and generic approach for graph mining across different tasks and datasets. Specifically, we first introduce a compact graph description via the proposed adaptive input generation to encapsulate key information from the graph under the constraints of language token limitations. Then, we propose a diverse instruction generation mechanism, which distills the reasoning capabilities from LLMs (e.g., GPT-4) to create task-specific Chain-of-Thought-based instruction packages for different graph tasks. Finally, we propose a graph-aware instruction tuning with a dynamic instruction package allocation strategy across tasks and datasets, ensuring the effectiveness and generalization of the training process. Our experimental results demonstrate significant improvements in different graph tasks, showcasing the potential of our MuseGraph in enhancing the accuracy of graph-oriented downstream tasks while keeping the generation powers of LLMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-02",
    "authors": [
      {
        "authorId": "30497353",
        "name": "Yanchao Tan"
      },
      {
        "authorId": "2266786978",
        "name": "Hang Lv"
      },
      {
        "authorId": "2298024639",
        "name": "Xin Huang"
      },
      {
        "authorId": "2290629665",
        "name": "Jiawei Zhang"
      },
      {
        "authorId": "2269740991",
        "name": "Shiping Wang"
      },
      {
        "authorId": "2237940942",
        "name": "Carl Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "af0a4be045364969970c428fdc417474d7bc6ed5",
    "url": "https://www.semanticscholar.org/paper/af0a4be045364969970c428fdc417474d7bc6ed5",
    "title": "Polaris: A Safety-focused LLM Constellation Architecture for Healthcare",
    "abstract": "We develop Polaris, the first safety-focused LLM constellation for real-time patient-AI healthcare conversations. Unlike prior LLM works in healthcare focusing on tasks like question answering, our work specifically focuses on long multi-turn voice conversations. Our one-trillion parameter constellation system is composed of several multibillion parameter LLMs as co-operative agents: a stateful primary agent that focuses on driving an engaging conversation and several specialist support agents focused on healthcare tasks performed by nurses to increase safety and reduce hallucinations. We develop a sophisticated training protocol for iterative co-training of the agents that optimize for diverse objectives. We train our models on proprietary data, clinical care plans, healthcare regulatory documents, medical manuals, and other medical reasoning documents. We align our models to speak like medical professionals, using organic healthcare conversations and simulated ones between patient actors and experienced nurses. This allows our system to express unique capabilities such as rapport building, trust building, empathy and bedside manner. Finally, we present the first comprehensive clinician evaluation of an LLM system for healthcare. We recruited over 1100 U.S. licensed nurses and over 130 U.S. licensed physicians to perform end-to-end conversational evaluations of our system by posing as patients and rating the system on several measures. We demonstrate Polaris performs on par with human nurses on aggregate across dimensions such as medical safety, clinical readiness, conversational quality, and bedside manner. Additionally, we conduct a challenging task-based evaluation of the individual specialist support agents, where we demonstrate our LLM agents significantly outperform a much larger general-purpose LLM (GPT-4) as well as from its own medium-size class (LLaMA-2 70B).",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-20",
    "authors": [
      {
        "authorId": "2293143373",
        "name": "Subhabrata Mukherjee"
      },
      {
        "authorId": "2292258384",
        "name": "Paul Gamble"
      },
      {
        "authorId": "51039532",
        "name": "Markel Sanz Ausin"
      },
      {
        "authorId": "2065893046",
        "name": "Neel Kant"
      },
      {
        "authorId": "2292259353",
        "name": "Kriti Aggarwal"
      },
      {
        "authorId": "2292259103",
        "name": "Neha Manjunath"
      },
      {
        "authorId": "2286156570",
        "name": "Debajyoti Datta"
      },
      {
        "authorId": "2292428720",
        "name": "Zhengliang Liu"
      },
      {
        "authorId": "2293238851",
        "name": "Jiayuan Ding"
      },
      {
        "authorId": "2292258298",
        "name": "Sophia Busacca"
      },
      {
        "authorId": "2292257771",
        "name": "Cezanne Bianco"
      },
      {
        "authorId": "2292346145",
        "name": "Swapnil Sharma"
      },
      {
        "authorId": "41051082",
        "name": "Rae Lasko"
      },
      {
        "authorId": "2292258513",
        "name": "Michelle Voisard"
      },
      {
        "authorId": "2292258005",
        "name": "Sanchay Harneja"
      },
      {
        "authorId": "2292258003",
        "name": "Darya Filippova"
      },
      {
        "authorId": "3595595",
        "name": "Gerry Meixiong"
      },
      {
        "authorId": "2292257983",
        "name": "Kevin Cha"
      },
      {
        "authorId": "11439139",
        "name": "Amir Youssefi"
      },
      {
        "authorId": "2292258515",
        "name": "Meyhaa Buvanesh"
      },
      {
        "authorId": "2292258719",
        "name": "Howard Weingram"
      },
      {
        "authorId": "2292258957",
        "name": "Sebastian Bierman-Lytle"
      },
      {
        "authorId": "16328851",
        "name": "Harpreet Singh Mangat"
      },
      {
        "authorId": "2292258521",
        "name": "Kim Parikh"
      },
      {
        "authorId": "69407298",
        "name": "Saad Godil"
      },
      {
        "authorId": "2293231918",
        "name": "Alex Miller"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "94972e30504017156ef5b5debc419bf6edc67384",
    "url": "https://www.semanticscholar.org/paper/94972e30504017156ef5b5debc419bf6edc67384",
    "title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
    "abstract": "We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combination. For evaluation metrics, we propose an LLM-based evaluator for open-ended outputs. The evaluator enables the evaluation across different question types and answer styles, resulting in a unified scoring metric. We evaluate representative LMMs on MM-Vet, providing insights into the capabilities of different LMM system paradigms and models.",
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "citationCount": 428,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.02490",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-08-04",
    "authors": [
      {
        "authorId": "23476952",
        "name": "Weihao Yu"
      },
      {
        "authorId": "2149231840",
        "name": "Zhengyuan Yang"
      },
      {
        "authorId": "50703697",
        "name": "Linjie Li"
      },
      {
        "authorId": "2124948371",
        "name": "Jianfeng Wang"
      },
      {
        "authorId": "143786724",
        "name": "Kevin Lin"
      },
      {
        "authorId": "2145253136",
        "name": "Zicheng Liu"
      },
      {
        "authorId": "48631088",
        "name": "Xinchao Wang"
      },
      {
        "authorId": "29957038",
        "name": "Lijuan Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 160.92185378392026
  },
  {
    "paperId": "2bb9a87bdfc8a35bc1813e5a88180f43615785a8",
    "url": "https://www.semanticscholar.org/paper/2bb9a87bdfc8a35bc1813e5a88180f43615785a8",
    "title": "Improving Retrieval-Augmented Generation in Medicine with Iterative Follow-up Questions",
    "abstract": "The emergent abilities of large language models (LLMs) have demonstrated great potential in solving medical questions. They can possess considerable medical knowledge, but may still hallucinate and are inflexible in the knowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed to enhance the medical question-answering capabilities of LLMs with external knowledge bases, it may still fail in complex cases where multiple rounds of information-seeking are required. To address such an issue, we propose iterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up queries based on previous information-seeking attempts. In each iteration of i-MedRAG, the follow-up queries will be answered by a vanilla RAG system and they will be further used to guide the query generation in the next iteration. Our experiments show the improved performance of various LLMs brought by i-MedRAG compared with vanilla RAG on complex questions from clinical vignettes in the United States Medical Licensing Examination (USMLE), as well as various knowledge tests in the Massive Multitask Language Understanding (MMLU) dataset. Notably, our zero-shot i-MedRAG outperforms all existing prompt engineering and fine-tuning methods on GPT-3.5, achieving an accuracy of 69.68% on the MedQA dataset. In addition, we characterize the scaling properties of i-MedRAG with different iterations of follow-up queries and different numbers of queries per iteration. Our case studies show that i-MedRAG can flexibly ask follow-up queries to form reasoning chains, providing an in-depth analysis of medical questions. To the best of our knowledge, this is the first-of-its-kind study on incorporating follow-up queries into medical RAG.",
    "venue": "Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-01",
    "authors": [
      {
        "authorId": "2048053804",
        "name": "Guangzhi Xiong"
      },
      {
        "authorId": "2261406713",
        "name": "Qiao Jin"
      },
      {
        "authorId": "2314146159",
        "name": "Xiao Wang"
      },
      {
        "authorId": "2314147661",
        "name": "Minjia Zhang"
      },
      {
        "authorId": "2237094367",
        "name": "Zhiyong Lu"
      },
      {
        "authorId": "2265729351",
        "name": "Aidong Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "4670942a6332554dfaea91cebee6d64d2e4ae233",
    "url": "https://www.semanticscholar.org/paper/4670942a6332554dfaea91cebee6d64d2e4ae233",
    "title": "Fighting reviewer fatigue or amplifying bias? Considerations and recommendations for use of ChatGPT and other large language models in scholarly peer review",
    "abstract": null,
    "venue": "Research Integrity and Peer Review",
    "year": 2023,
    "citationCount": 112,
    "openAccessPdf": {
      "url": "https://researchintegrityjournal.biomedcentral.com/counter/pdf/10.1186/s41073-023-00133-5",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-05-18",
    "authors": [
      {
        "authorId": "153355049",
        "name": "Mohammad Hosseini"
      },
      {
        "authorId": "144665322",
        "name": "S. Horbach"
      }
    ],
    "source": "semantic_scholar",
    "score": 120.91081728068511
  },
  {
    "paperId": "0f2d11ac3202997b914efafaedb56b564ff9d3a7",
    "url": "https://www.semanticscholar.org/paper/0f2d11ac3202997b914efafaedb56b564ff9d3a7",
    "title": "Leveraging Large Language Models in Conversational Recommender Systems",
    "abstract": "A Conversational Recommender System (CRS) offers increased transparency and control to users by enabling them to engage with the system through a real-time multi-turn dialogue. Recently, Large Language Models (LLMs) have exhibited an unprecedented ability to converse naturally and incorporate world knowledge and common-sense reasoning into language understanding, unlocking the potential of this paradigm. However, effectively leveraging LLMs within a CRS introduces new technical challenges, including properly understanding and controlling a complex conversation and retrieving from external sources of information. These issues are exacerbated by a large, evolving item corpus and a lack of conversational data for training. In this paper, we provide a roadmap for building an end-to-end large-scale CRS using LLMs. In particular, we propose new implementations for user preference understanding, flexible dialogue management and explainable recommendations as part of an integrated architecture powered by LLMs. For improved personalization, we describe how an LLM can consume interpretable natural language user profiles and use them to modulate session-level context. To overcome conversational data limitations in the absence of an existing production CRS, we propose techniques for building a controllable LLM-based user simulator to generate synthetic conversations. As a proof of concept we introduce RecLLM, a large-scale CRS for YouTube videos built on LaMDA, and demonstrate its fluency and diverse functionality through some illustrative example conversations.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 78,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-13",
    "authors": [
      {
        "authorId": "2217253911",
        "name": "Luke Friedman"
      },
      {
        "authorId": "2416263",
        "name": "Sameer Ahuja"
      },
      {
        "authorId": "2217251792",
        "name": "David Allen"
      },
      {
        "authorId": "6840539",
        "name": "Zhenning Tan"
      },
      {
        "authorId": "39041127",
        "name": "Hakim Sidahmed"
      },
      {
        "authorId": "2217232763",
        "name": "Changbo Long"
      },
      {
        "authorId": "2109935759",
        "name": "Jun Xie"
      },
      {
        "authorId": "3230461",
        "name": "Gabriel Schubiner"
      },
      {
        "authorId": "2109171018",
        "name": "Ajay Patel"
      },
      {
        "authorId": "2196537401",
        "name": "Harsh Lara"
      },
      {
        "authorId": "2055638940",
        "name": "Brian Chu"
      },
      {
        "authorId": "2211981539",
        "name": "Zexiang Chen"
      },
      {
        "authorId": "2074227766",
        "name": "Manoj Tiwari"
      }
    ],
    "source": "semantic_scholar",
    "score": 135.54171778700533
  },
  {
    "paperId": "498decc50ccea9293f63a98c30d7c3439be074b7",
    "url": "https://www.semanticscholar.org/paper/498decc50ccea9293f63a98c30d7c3439be074b7",
    "title": "Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning",
    "abstract": "In this study, we are interested in imbuing robots with the capability of physically-grounded task planning. Recent advancements have shown that large language models (LLMs) possess extensive knowledge useful in robotic tasks, especially in reasoning and planning. However, LLMs are constrained by their lack of world grounding and dependence on external affordance models to perceive environmental information, which cannot jointly reason with LLMs. We argue that a task planner should be an inherently grounded, unified multimodal system. To this end, we introduce Robotic Vision-Language Planning (ViLa), a novel approach for long-horizon robotic planning that leverages vision-language models (VLMs) to generate a sequence of actionable steps. ViLa directly integrates perceptual data into its reasoning and planning process, enabling a profound understanding of commonsense knowledge in the visual world, including spatial layouts and object attributes. It also supports flexible multimodal goal specification and naturally incorporates visual feedback. Our extensive evaluation, conducted in both real-robot and simulated environments, demonstrates ViLa's superiority over existing LLM-based planners, highlighting its effectiveness in a wide array of open-world manipulation tasks.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 74,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-29",
    "authors": [
      {
        "authorId": "2149297811",
        "name": "Yingdong Hu"
      },
      {
        "authorId": "2270740581",
        "name": "Fanqi Lin"
      },
      {
        "authorId": "2270989945",
        "name": "Tong Zhang"
      },
      {
        "authorId": "2269821079",
        "name": "Li Yi"
      },
      {
        "authorId": "2257027030",
        "name": "Yang Gao"
      }
    ],
    "source": "semantic_scholar",
    "score": 134.76232170304465
  },
  {
    "paperId": "f5f06c87d4aa66fa2839c875d39820bff3763920",
    "url": "https://www.semanticscholar.org/paper/f5f06c87d4aa66fa2839c875d39820bff3763920",
    "title": "Evaluating Large Language Models for Drafting Emergency Department Discharge Summaries",
    "abstract": "Importance: Large language models (LLMs) possess a range of capabilities which may be applied to the clinical domain, including text summarization. As ambient artificial intelligence scribes and other LLM-based tools begin to be deployed within healthcare settings, rigorous evaluations of the accuracy of these technologies are urgently needed. Objective: To investigate the performance of GPT-4 and GPT-3.5-turbo in generating Emergency Department (ED) discharge summaries and evaluate the prevalence and type of errors across each section of the discharge summary. Design: Cross-sectional study. Setting: University of California, San Francisco ED. Participants: We identified all adult ED visits from 2012 to 2023 with an ED clinician note. We randomly selected a sample of 100 ED visits for GPT-summarization. Exposure: We investigate the potential of two state-of-the-art LLMs, GPT-4 and GPT-3.5-turbo, to summarize the full ED clinician note into a discharge summary. Main Outcomes and Measures: GPT-3.5-turbo and GPT-4-generated discharge summaries were evaluated by two independent Emergency Medicine physician reviewers across three evaluation criteria: 1) Inaccuracy of GPT-summarized information; 2) Hallucination of information; 3) Omission of relevant clinical information. On identifying each error, reviewers were additionally asked to provide a brief explanation for their reasoning, which was manually classified into subgroups of errors. Results: From 202,059 eligible ED visits, we randomly sampled 100 for GPT-generated summarization and then expert-driven evaluation. In total, 33% of summaries generated by GPT-4 and 10% of those generated by GPT-3.5-turbo were entirely error-free across all evaluated domains. Summaries generated by GPT-4 were mostly accurate, with inaccuracies found in only 10% of cases, however, 42% of the summaries exhibited hallucinations and 47% omitted clinically relevant information. Inaccuracies and hallucinations were most commonly found in the Plan sections of GPT-generated summaries, while clinical omissions were concentrated in text describing patients' Physical Examination findings or History of Presenting Complaint. Conclusions and Relevance: In this cross-sectional study of 100 ED encounters, we found that LLMs could generate accurate discharge summaries, but were liable to hallucination and omission of clinically relevant information. A comprehensive understanding of the location and type of errors found in GPT-generated clinical text is important to facilitate clinician review of such content and prevent patient harm.",
    "venue": "medRxiv",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": "2024-04-04",
    "authors": [
      {
        "authorId": "1455117183",
        "name": "C. Y. Williams"
      },
      {
        "authorId": "2264327501",
        "name": "J. Bains"
      },
      {
        "authorId": "2297655232",
        "name": "Tianyu Tang"
      },
      {
        "authorId": "2297746372",
        "name": "Kishan Patel"
      },
      {
        "authorId": "2297653001",
        "name": "Alexa N Lucas"
      },
      {
        "authorId": "2297715532",
        "name": "Fiona Chen"
      },
      {
        "authorId": "2243188680",
        "name": "Brenda Y Miao"
      },
      {
        "authorId": "1716151",
        "name": "A. Butte"
      },
      {
        "authorId": "2297652261",
        "name": "Aaron E Kornblith"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "4773398fd2fdf1feddb194aa3e90da270fd3db11",
    "url": "https://www.semanticscholar.org/paper/4773398fd2fdf1feddb194aa3e90da270fd3db11",
    "title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications",
    "abstract": "A graph is a fundamental data model to represent various entities and their complex relationships in society and nature, such as social networks, transportation networks, financial networks, and biomedical systems. Recently, large language models (LLMs) have showcased a strong generalization ability to handle various NLP and multi-mode tasks to answer users' arbitrary questions and specific-domain content generation. Compared with graph learning models, LLMs enjoy superior advantages in addressing the challenges of generalizing graph tasks by eliminating the need for training graph learning models and reducing the cost of manual annotation. In this survey, we conduct a comprehensive investigation of existing LLM studies on graph data, which summarizes the relevant graph analytics tasks solved by advanced LLM models and points out the existing remaining challenges and future directions. Specifically, we study the key problems of LLM-based generative graph analytics (LLM-GGA) with three categories: LLM-based graph query processing (LLM-GQP), LLM-based graph inference and learning (LLM-GIL), and graph-LLM-based applications. LLM-GQP focuses on an integration of graph analytics techniques and LLM prompts, including graph understanding and knowledge graph (KG) based augmented retrieval, while LLM-GIL focuses on learning and reasoning over graphs, including graph learning, graph-formed reasoning and graph representation. We summarize the useful prompts incorporated into LLM to handle different graph downstream tasks. Moreover, we give a summary of LLM model evaluation, benchmark datasets/tasks, and a deep pro and cons analysis of LLM models. We also explore open problems and future directions in this exciting interdisciplinary research area of LLMs and graph analytics.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-04-23",
    "authors": [
      {
        "authorId": "2297848659",
        "name": "Wenbo Shang"
      },
      {
        "authorId": "2298024639",
        "name": "Xin Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "ec94157ecd55482ea4b7d66016593e3072e671f6",
    "url": "https://www.semanticscholar.org/paper/ec94157ecd55482ea4b7d66016593e3072e671f6",
    "title": "UniGen: A Unified Framework for Textual Dataset Generation Using Large Language Models",
    "abstract": "Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly impacted various fields by enabling high-quality synthetic data generation and reducing dependence on expensive human-generated datasets. Despite this, challenges remain in the areas of generalization, controllability, diversity, and truthfulness within the existing generative frameworks. To address these challenges, this paper presents UniGen, a comprehensive LLM-powered framework designed to produce diverse, accurate, and highly controllable datasets. UniGen is adaptable, supporting all types of text datasets and enhancing the generative process through innovative mechanisms. To augment data diversity, UniGen incorporates an attribute-guided generation module and a group checking feature. For accuracy, it employs a code-based mathematical assessment for label verification alongside a retrieval-augmented generation technique for factual validation. The framework also allows for user-specified constraints, enabling customization of the data generation process to suit particular requirements. Extensive experiments demonstrate the superior quality of data generated by UniGen, and each module within UniGen plays a critical role in this enhancement. Additionally, UniGen is applied in two practical scenarios: benchmarking LLMs and data augmentation. The results indicate that UniGen effectively supports dynamic and evolving benchmarking, and that data augmentation improves LLM capabilities in various domains, including agent-oriented abilities and reasoning skills.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-27",
    "authors": [
      {
        "authorId": "2254867423",
        "name": "Siyuan Wu"
      },
      {
        "authorId": "2257084278",
        "name": "Yue Huang"
      },
      {
        "authorId": "2279094112",
        "name": "Chujie Gao"
      },
      {
        "authorId": "2279219833",
        "name": "Dongping Chen"
      },
      {
        "authorId": "46324457",
        "name": "Qihui Zhang"
      },
      {
        "authorId": "2254266993",
        "name": "Yao Wan"
      },
      {
        "authorId": "2309119920",
        "name": "Tianyi Zhou"
      },
      {
        "authorId": "2307963162",
        "name": "Xiangliang Zhang"
      },
      {
        "authorId": "2288029761",
        "name": "Jianfeng Gao"
      },
      {
        "authorId": "2256992325",
        "name": "Chaowei Xiao"
      },
      {
        "authorId": "2257131651",
        "name": "Lichao Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "aa59536123b29599115cb28027d9ddb67fc1c613",
    "url": "https://www.semanticscholar.org/paper/aa59536123b29599115cb28027d9ddb67fc1c613",
    "title": "Telecom Language Models: Must They Be Large?",
    "abstract": "The increasing interest in Large Language Models (LLMs) within the telecommunications sector underscores their potential to revolutionize operational efficiency. However, the deployment of these sophisticated models is often hampered by their substantial size and computational demands, raising concerns about their viability in resource-constrained environments. Addressing this challenge, recent advancements have seen the emergence of small language models that surprisingly exhibit performance comparable to their larger counterparts in many tasks, such as coding and common-sense reasoning. Phi-2, a compact yet powerful model, exemplifies this new wave of efficient small language models. This paper conducts a comprehensive evaluation of Phi-2’s intrinsic understanding of the telecommunications domain. Recognizing the scale-related limitations, we enhance Phi-2’s capabilities through a Retrieval-Augmented Generation approach, meticulously integrating an extensive knowledge base specifically curated with telecom standard specifications. The enhanced Phi-2 model demonstrates a profound improvement in accuracy, answering questions about telecom standards with a precision that closely rivals the more resource-intensive GPT-3.5. The paper further explores the refined capabilities of Phi-2 in addressing problem-solving scenarios within the telecom sector, highlighting its potentials and limitations.",
    "venue": "IEEE International Symposium on Personal, Indoor and Mobile Radio Communications",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-07",
    "authors": [
      {
        "authorId": "3393923",
        "name": "Nicola Piovesan"
      },
      {
        "authorId": "2261362548",
        "name": "Antonio De Domenico"
      },
      {
        "authorId": "70486867",
        "name": "Fadhel Ayed"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.19162312519754
  },
  {
    "paperId": "80cbbed8994cd89b02b923dd2e0e22d3e8aeca14",
    "url": "https://www.semanticscholar.org/paper/80cbbed8994cd89b02b923dd2e0e22d3e8aeca14",
    "title": "Aligning Large Language Models with Recommendation Knowledge",
    "abstract": "Large language models (LLMs) have recently been used as backbones for recommender systems. However, their performance often lags behind conventional methods in standard tasks like retrieval. We attribute this to a mismatch between LLMs' knowledge and the knowledge crucial for effective recommendations. While LLMs excel at natural language reasoning, they cannot model complex user-item interactions inherent in recommendation tasks. We propose bridging the knowledge gap and equipping LLMs with recommendation-specific knowledge to address this. Operations such as Masked Item Modeling (MIM) and Bayesian Personalized Ranking (BPR) have found success in conventional recommender systems. Inspired by this, we simulate these operations through natural language to generate auxiliary-task data samples that encode item correlations and user preferences. Fine-tuning LLMs on such auxiliary-task data samples and incorporating more informative recommendation-task data samples facilitates the injection of recommendation-specific knowledge into LLMs. Extensive experiments across retrieval, ranking, and rating prediction tasks on LLMs such as FLAN-T5-Base and FLAN-T5-XL show the effectiveness of our technique in domains such as Amazon Toys&Games, Beauty, and Sports&Outdoors. Notably, our method outperforms conventional and LLM-based baselines, including the current SOTA, by significant margins in retrieval, showcasing its potential for enhancing recommendation quality.",
    "venue": "NAACL-HLT",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-30",
    "authors": [
      {
        "authorId": "2295912718",
        "name": "Yuwei Cao"
      },
      {
        "authorId": "2294363530",
        "name": "Nikhil Mehta"
      },
      {
        "authorId": "2838461",
        "name": "Xinyang Yi"
      },
      {
        "authorId": "2443945",
        "name": "Raghunandan H. Keshavan"
      },
      {
        "authorId": "37914278",
        "name": "L. Heldt"
      },
      {
        "authorId": "2240649593",
        "name": "Lichan Hong"
      },
      {
        "authorId": "2254270179",
        "name": "E. Chi"
      },
      {
        "authorId": "3221924",
        "name": "M. Sathiamoorthy"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "1398dbee03dfc2454fec5e4ff1c7e62900e8468e",
    "url": "https://www.semanticscholar.org/paper/1398dbee03dfc2454fec5e4ff1c7e62900e8468e",
    "title": "Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts",
    "abstract": "We present Self-MoE, an approach that transforms a monolithic LLM into a compositional, modular system of self-specialized experts, named MiXSE (MiXture of Self-specialized Experts). Our approach leverages self-specialization, which constructs expert modules using self-generated synthetic data, each equipping a shared base LLM with distinct domain-specific capabilities, activated via self-optimized routing. This allows for dynamic and capability-specific handling of various target tasks, enhancing overall capabilities, without extensive human-labeled data and added parameters. Our empirical results reveal that specializing LLMs may exhibit potential trade-offs in performances on non-specialized tasks. On the other hand, our Self-MoE demonstrates substantial improvements (6.5%p on average) over the base LLM across diverse benchmarks such as knowledge, reasoning, math, and coding. It also consistently outperforms other methods, including instance merging and weight merging, while offering better flexibility and interpretability by design with semantic experts and routing. Our findings highlight the critical role of modularity, the applicability of Self-MoE to multiple base LLMs, and the potential of self-improvement in achieving efficient, scalable, and adaptable systems.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-17",
    "authors": [
      {
        "authorId": "153041329",
        "name": "Junmo Kang"
      },
      {
        "authorId": "2142741421",
        "name": "Leonid Karlinsky"
      },
      {
        "authorId": "1944274",
        "name": "Hongyin Luo"
      },
      {
        "authorId": "2307259777",
        "name": "Zhen Wang"
      },
      {
        "authorId": "2307266137",
        "name": "Jacob A. Hansen"
      },
      {
        "authorId": "2249763210",
        "name": "James Glass"
      },
      {
        "authorId": "2257039920",
        "name": "David Cox"
      },
      {
        "authorId": "1819152",
        "name": "Rameswar Panda"
      },
      {
        "authorId": "2239199018",
        "name": "Rogério Feris"
      },
      {
        "authorId": "2249762886",
        "name": "Alan Ritter"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "77a9c310df0d7896d297da90fc4a1131819c341e",
    "url": "https://www.semanticscholar.org/paper/77a9c310df0d7896d297da90fc4a1131819c341e",
    "title": "LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models",
    "abstract": "To ensure safe driving in dynamic environments, autonomous vehicles should possess the capability to accurately predict lane change intentions of surrounding vehicles in advance and forecast their future trajectories. Existing motion prediction approaches have ample room for improvement, particularly in terms of long-term prediction accuracy and interpretability. In this paper, we address these challenges by proposing LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models (LLMs). Essentially, we reformulate the lane change prediction task as a language modeling problem, processing heterogeneous driving scenario information as natural language prompts for LLMs and employing supervised fine-tuning to tailor LLMs specifically for lane change prediction task. Additionally, we finetune the Chain-of-Thought (CoT) reasoning to improve prediction transparency and reliability, and include explanatory requirements in the prompts during inference stage. Therefore, our LC-LLM model not only predicts lane change intentions and trajectories but also provides CoT reasoning and explanations for its predictions, enhancing its interpretability. Extensive experiments based on the large-scale highD dataset demonstrate the superior performance and interpretability of our LC-LLM in lane change prediction task. To the best of our knowledge, this is the first attempt to utilize LLMs for predicting lane change behavior. Our study shows that LLMs can effectively encode comprehensive interaction information for driving behavior understanding.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-27",
    "authors": [
      {
        "authorId": "2293615241",
        "name": "Mingxing Peng"
      },
      {
        "authorId": "2293665950",
        "name": "Xusen Guo"
      },
      {
        "authorId": "2146413818",
        "name": "Xianda Chen"
      },
      {
        "authorId": "2241024418",
        "name": "Meixin Zhu"
      },
      {
        "authorId": "2267078966",
        "name": "Kehua Chen"
      },
      {
        "authorId": "2293775145",
        "name": "Hao Yang"
      },
      {
        "authorId": "2258778041",
        "name": "Xuesong Wang"
      },
      {
        "authorId": "2258755636",
        "name": "Yinhai Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "2c4b9e3c85876c71ea14229b8d3b59f2bc3cdf99",
    "url": "https://www.semanticscholar.org/paper/2c4b9e3c85876c71ea14229b8d3b59f2bc3cdf99",
    "title": "MIRAI: Evaluating LLM Agents for Event Forecasting",
    "abstract": "Recent advancements in Large Language Models (LLMs) have empowered LLM agents to autonomously collect world information, over which to conduct reasoning to solve complex problems. Given this capability, increasing interests have been put into employing LLM agents for predicting international events, which can influence decision-making and shape policy development on an international scale. Despite such a growing interest, there is a lack of a rigorous benchmark of LLM agents' forecasting capability and reliability. To address this gap, we introduce MIRAI, a novel benchmark designed to systematically evaluate LLM agents as temporal forecasters in the context of international events. Our benchmark features an agentic environment with tools for accessing an extensive database of historical, structured events and textual news articles. We refine the GDELT event database with careful cleaning and parsing to curate a series of relational prediction tasks with varying forecasting horizons, assessing LLM agents' abilities from short-term to long-term forecasting. We further implement APIs to enable LLM agents to utilize different tools via a code-based interface. In summary, MIRAI comprehensively evaluates the agents' capabilities in three dimensions: 1) autonomously source and integrate critical information from large global databases; 2) write codes using domain-specific APIs and libraries for tool-use; and 3) jointly reason over historical knowledge from diverse formats and time to accurately predict future events. Through comprehensive benchmarking, we aim to establish a reliable framework for assessing the capabilities of LLM agents in forecasting international events, thereby contributing to the development of more accurate and trustworthy models for international relation analysis.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-01",
    "authors": [
      {
        "authorId": "2306897813",
        "name": "Chenchen Ye"
      },
      {
        "authorId": "2309190965",
        "name": "Ziniu Hu"
      },
      {
        "authorId": "2303979338",
        "name": "Yihe Deng"
      },
      {
        "authorId": "12318198",
        "name": "Zijie Huang"
      },
      {
        "authorId": "144592155",
        "name": "Mingyu Derek Ma"
      },
      {
        "authorId": "2653121",
        "name": "Yanqiao Zhu"
      },
      {
        "authorId": "2303919444",
        "name": "Wei Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "e788d5c6dc79f106296f8ad296bcbe512028d5f0",
    "url": "https://www.semanticscholar.org/paper/e788d5c6dc79f106296f8ad296bcbe512028d5f0",
    "title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
    "abstract": "Recent vision-language-action (VLA) models rely on 2D inputs, lacking integration with the broader realm of the 3D physical world. Furthermore, they perform action prediction by learning a direct mapping from perception to action, neglecting the vast dynamics of the world and the relations between actions and dynamics. In contrast, human beings are endowed with world models that depict imagination about future scenarios to plan actions accordingly. To this end, we propose 3D-VLA by introducing a new family of embodied foundation models that seamlessly link 3D perception, reasoning, and action through a generative world model. Specifically, 3D-VLA is built on top of a 3D-based large language model (LLM), and a set of interaction tokens is introduced to engage with the embodied environment. Furthermore, to inject generation abilities into the model, we train a series of embodied diffusion models and align them into the LLM for predicting the goal images and point clouds. To train our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by extracting vast 3D-related information from existing robotics datasets. Our experiments on held-in datasets demonstrate that 3D-VLA significantly improves the reasoning, multimodal generation, and planning capabilities in embodied environments, showcasing its potential in real-world applications.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 32,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-14",
    "authors": [
      {
        "authorId": "2226286961",
        "name": "Haoyu Zhen"
      },
      {
        "authorId": "2292176508",
        "name": "Xiaowen Qiu"
      },
      {
        "authorId": "2158502526",
        "name": "Peihao Chen"
      },
      {
        "authorId": "2291198667",
        "name": "Jincheng Yang"
      },
      {
        "authorId": "2291325237",
        "name": "Xin Yan"
      },
      {
        "authorId": "2258799458",
        "name": "Yilun Du"
      },
      {
        "authorId": "2265627123",
        "name": "Yining Hong"
      },
      {
        "authorId": "2266854520",
        "name": "Chuang Gan"
      }
    ],
    "source": "semantic_scholar",
    "score": 122.4476134219972
  },
  {
    "paperId": "d02b9420df66330620f8853d19de610c98d2e1c1",
    "url": "https://www.semanticscholar.org/paper/d02b9420df66330620f8853d19de610c98d2e1c1",
    "title": "MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding",
    "abstract": "The advent of large vision-language models (LVLMs) has spurred research into their applications in multi-modal contexts, particularly in video understanding. Traditional VideoQA benchmarks, despite providing quantitative metrics, often fail to encompass the full spectrum of video content and inadequately assess models' temporal comprehension. To address these limitations, we introduce MMBench-Video, a quantitative benchmark designed to rigorously evaluate LVLMs' proficiency in video understanding. MMBench-Video incorporates lengthy videos from YouTube and employs free-form questions, mirroring practical use cases. The benchmark is meticulously crafted to probe the models' temporal reasoning skills, with all questions human-annotated according to a carefully constructed ability taxonomy. We employ GPT-4 for automated assessment, demonstrating superior accuracy and robustness over earlier LLM-based evaluations. Utilizing MMBench-Video, we have conducted comprehensive evaluations that include both proprietary and open-source LVLMs for images and videos. MMBench-Video stands as a valuable resource for the research community, facilitating improved evaluation of LVLMs and catalyzing progress in the field of video understanding. The evalutation code of MMBench-Video will be integrated into VLMEvalKit: https://github.com/open-compass/VLMEvalKit.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 26,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-20",
    "authors": [
      {
        "authorId": "2307562496",
        "name": "Xinyu Fang"
      },
      {
        "authorId": "2307469905",
        "name": "Kangrui Mao"
      },
      {
        "authorId": "31463937",
        "name": "Haodong Duan"
      },
      {
        "authorId": "2277807318",
        "name": "Xiangyu Zhao"
      },
      {
        "authorId": "47002704",
        "name": "Yining Li"
      },
      {
        "authorId": "2261095726",
        "name": "Dahua Lin"
      },
      {
        "authorId": "2307526907",
        "name": "Kai Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.43755299006494
  },
  {
    "paperId": "daea745ef990be5cfe5af3196e819021a23f3597",
    "url": "https://www.semanticscholar.org/paper/daea745ef990be5cfe5af3196e819021a23f3597",
    "title": "MM-PhyRLHF: Reinforcement Learning Framework for Multimodal Physics Question-Answering",
    "abstract": "Recent advancements in LLMs have shown their significant potential in tasks like text summarization and generation. Yet, they often encounter difficulty while solving complex physics problems that require arithmetic calculation and a good understanding of concepts. Moreover, many physics problems include images that contain important details required to understand the problem's context. We propose an LMM-based chatbot to answer multimodal physics MCQs. For domain adaptation, we utilize the MM-PhyQA dataset comprising Indian high school-level multimodal physics problems. To improve the LMM's performance, we experiment with two techniques, RLHF (Reinforcement Learning from Human Feedback) and Image Captioning. In image captioning, we add a detailed explanation of the diagram in each image, minimizing hallucinations and image processing errors. We further explore the integration of Reinforcement Learning from Human Feedback (RLHF) methodology inspired by the ranking approach in RLHF to enhance the human-like problem-solving abilities of the models. The RLHF approach incorporates human feedback into the learning process of LLMs, improving the model's problem-solving skills, truthfulness, and reasoning capabilities, minimizing the hallucinations in the answers, and improving the quality instead of using vanilla-supervised fine-tuned models. We employ the LLaVA open-source model to answer multimodal physics MCQs and compare the performance with and without using RLHF.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-19",
    "authors": [
      {
        "authorId": "2223123570",
        "name": "Avinash Anand"
      },
      {
        "authorId": "2287841157",
        "name": "Janak Kapuriya"
      },
      {
        "authorId": "2297671494",
        "name": "Chhavi Kirtani"
      },
      {
        "authorId": "2296730605",
        "name": "Apoorv Singh"
      },
      {
        "authorId": "2296711567",
        "name": "Jay Saraf"
      },
      {
        "authorId": "2060965994",
        "name": "Naman Lal"
      },
      {
        "authorId": "2271680573",
        "name": "Jatin Kumar"
      },
      {
        "authorId": "2003433492",
        "name": "A. Shivam"
      },
      {
        "authorId": "2271625822",
        "name": "Astha Verma"
      },
      {
        "authorId": "1753278",
        "name": "R. Shah"
      },
      {
        "authorId": "2297672250",
        "name": "Roger Zimmermann"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "7b508f5a3168e2ecfcb821752dcae576905a50c5",
    "url": "https://www.semanticscholar.org/paper/7b508f5a3168e2ecfcb821752dcae576905a50c5",
    "title": "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents",
    "abstract": "As AI systems pervade human life, ensuring that large language models (LLMs) make safe decisions remains a significant challenge. We introduce the Governance of the Commons Simulation (GovSim), a generative simulation platform designed to study strategic interactions and cooperative decision-making in LLMs. In GovSim, a society of AI agents must collectively balance exploiting a common resource with sustaining it for future use. This environment enables the study of how ethical considerations, strategic planning, and negotiation skills impact cooperative outcomes. We develop an LLM-based agent architecture and test it with the leading open and closed LLMs. We find that all but the most powerful LLM agents fail to achieve a sustainable equilibrium in GovSim, with the highest survival rate below 54%. Ablations reveal that successful multi-agent communication between agents is critical for achieving cooperation in these cases. Furthermore, our analyses show that the failure to achieve sustainable cooperation in most LLMs stems from their inability to formulate and analyze hypotheses about the long-term effects of their actions on the equilibrium of the group. Finally, we show that agents that leverage\"Universalization\"-based reasoning, a theory of moral thinking, are able to achieve significantly better sustainability. Taken together, GovSim enables us to study the mechanisms that underlie sustainable self-government with specificity and scale. We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface.",
    "venue": "",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-04-25",
    "authors": [
      {
        "authorId": "2298275529",
        "name": "Giorgio Piatti"
      },
      {
        "authorId": "2111472502",
        "name": "Zhijing Jin"
      },
      {
        "authorId": "2272856326",
        "name": "Max Kleiman-Weiner"
      },
      {
        "authorId": "2261392474",
        "name": "Bernhard Scholkopf"
      },
      {
        "authorId": "2790926",
        "name": "Mrinmaya Sachan"
      },
      {
        "authorId": "2105984203",
        "name": "Rada Mihalcea"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "421f82e616d887ec008d88be580459edba96c271",
    "url": "https://www.semanticscholar.org/paper/421f82e616d887ec008d88be580459edba96c271",
    "title": "CHESS: Contextual Harnessing for Efficient SQL Synthesis",
    "abstract": "Translating natural language questions into SQL queries, known as text-to-SQL, is a long-standing research problem. Effective text-to-SQL synthesis can become very challenging due to (i) the extensive size of database catalogs (descriptions of tables and their columns) and database values, (ii) reasoning over large database schemas, (iii) ensuring the functional validity of the generated queries, and (iv) navigating the ambiguities of natural language questions. We introduce CHESS, a Large Language Model (LLM) based multi-agent framework for efficient and scalable SQL synthesis, comprising four specialized agents, each targeting one of the aforementioned challenges: the Information Retriever (IR) extracts relevant data, the Schema Selector (SS) prunes large schemas, the Candidate Generator (CG) generates high-quality candidates and refines queries iteratively, and the Unit Tester (UT) validates queries through LLM-based natural language unit tests. Our framework offers configurable features that adapt to various deployment constraints, including 1) Supporting industrial-scale databases: leveraging the Schema Selector agent, CHESS efficiently narrows down very large database schemas into manageable sub-schemas, boosting system accuracy by approximately $2\\%$ and reducing the number of LLM tokens by $\\times 5$. 2) State-of-the-Art privacy-preserving performance: Among the methods using open-source models, CHESS achieves state-of-the-art performance, resulting in a high-performing, privacy-preserving system suitable for industrial deployment. 3) Scalablity with additional compute budget: In settings with high computational budgets, CHESS achieves $71.10\\%$ accuracy on the BIRD test set, within $2\\%$ of the leading proprietary method, while requiring approximately $83\\%$ fewer LLM calls.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 20,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-27",
    "authors": [
      {
        "authorId": "2171655817",
        "name": "Shayan Talaei"
      },
      {
        "authorId": "2192284724",
        "name": "Mohammadreza Pourreza"
      },
      {
        "authorId": "2303476694",
        "name": "Yu-Chen Chang"
      },
      {
        "authorId": "1861312",
        "name": "Azalia Mirhoseini"
      },
      {
        "authorId": "2303415064",
        "name": "Amin Saberi"
      }
    ],
    "source": "semantic_scholar",
    "score": 115.66783656585135
  },
  {
    "paperId": "31660b3c751f9d965d4a1eba97870672baa2221e",
    "url": "https://www.semanticscholar.org/paper/31660b3c751f9d965d4a1eba97870672baa2221e",
    "title": "Matryoshka Multimodal Models",
    "abstract": "Large Multimodal Models (LMMs) such as LLaVA have shown strong performance in visual-linguistic reasoning. These models first embed images into a fixed large number of visual tokens and then feed them into a Large Language Model (LLM). However, this design causes an excessive number of tokens for dense visual scenarios such as high-resolution images and videos, leading to great inefficiency. While token pruning/merging methods do exist, they produce a single length output for each image and do not afford flexibility in trading off information density v.s. efficiency. Inspired by the concept of Matryoshka Dolls, we propose M3: Matryoshka Multimodal Models, which learns to represent visual content as nested sets of visual tokens that capture information across multiple coarse-to-fine granularities. Our approach offers several unique benefits for LMMs: (1) One can explicitly control the visual granularity per test instance during inference, e.g. , adjusting the number of tokens used to represent an image based on the anticipated complexity or simplicity of the content; (2) M3 provides a framework for analyzing the granularity needed for existing datasets, where we find that COCO-style benchmarks only need around ~9 visual tokens to obtain accuracy similar to that of using all 576 tokens; (3) Our approach provides a foundation to explore the best trade-off between performance and visual token length at sample level, where our investigation reveals that a large gap exists between the oracle upper bound and current fixed-scale representations.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 15,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-27",
    "authors": [
      {
        "authorId": "2053144019",
        "name": "Mu Cai"
      },
      {
        "authorId": "2273653644",
        "name": "Jianwei Yang"
      },
      {
        "authorId": "2256227183",
        "name": "Jianfeng Gao"
      },
      {
        "authorId": "2244031889",
        "name": "Yong Jae Lee"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.58883083359672
  },
  {
    "paperId": "5a940f2ad582c1ce14e30a6be9185a35cbecd8e6",
    "url": "https://www.semanticscholar.org/paper/5a940f2ad582c1ce14e30a6be9185a35cbecd8e6",
    "title": "BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with Semantic Neural Graph Filtering",
    "abstract": "Knowledge Graphs (KGs) have proven essential in information processing and reasoning applications because they link related entities and give context-rich information, supporting efficient information retrieval and knowledge discovery; presenting information flow in a very effective manner. Despite being widely used globally, Bangla is relatively underrepresented in KGs due to a lack of comprehensive datasets, encoders, NER (named entity recognition) models, POS (part-of-speech) taggers, and lemmatizers, hindering efficient information processing and reasoning applications in the language. Addressing the KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework that is able to automatically construct Bengali KGs from any Bangla text. We utilize multilingual LLMs to understand various languages and correlate entities and relations universally. By employing a translation dictionary to identify English equivalents and extracting word features from pre-trained BERT models, we construct the foundational KG. To reduce noise and align word embeddings with our goal, we employ graph-based polynomial filters. Lastly, we implement a GNN-based semantic filter, which elevates contextual understanding and trims unnecessary edges, culminating in the formation of the definitive KG. Empirical findings and case studies demonstrate the universal effectiveness of our model, capable of autonomously constructing semantically enriched KGs from any text. Data and code are available here: https://github.com/azminewasi/BanglaAutoKG",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-04",
    "authors": [
      {
        "authorId": "2215887932",
        "name": "Azmine Toushik Wasi"
      },
      {
        "authorId": "2034276221",
        "name": "Taki Hasan Rafi"
      },
      {
        "authorId": "2215905093",
        "name": "Raima Islam"
      },
      {
        "authorId": "2274011438",
        "name": "Dong-Kyu Chae"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "b90abcfbc391c606206b4d32c3887292f0fd3226",
    "url": "https://www.semanticscholar.org/paper/b90abcfbc391c606206b4d32c3887292f0fd3226",
    "title": "Professional Agents - Evolving Large Language Models into Autonomous Experts with Human-Level Competencies",
    "abstract": "The advent of large language models (LLMs) such as ChatGPT, PaLM, and GPT-4 has catalyzed remarkable advances in natural language processing, demonstrating human-like language fluency and reasoning capacities. This position paper introduces the concept of Professional Agents (PAgents), an application framework harnessing LLM capabilities to create autonomous agents with controllable, specialized, interactive, and professional-level competencies. We posit that PAgents can reshape professional services through continuously developed expertise. Our proposed PAgents framework entails a tri-layered architecture for genesis, evolution, and synergy: a base tool layer, a middle agent layer, and a top synergy layer. This paper aims to spur discourse on promising real-world applications of LLMs. We argue the increasing sophistication and integration of PAgents could lead to AI systems exhibiting professional mastery over complex domains, serving critical needs, and potentially achieving artificial general intelligence.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-06",
    "authors": [
      {
        "authorId": "2237992280",
        "name": "Zhixuan Chu"
      },
      {
        "authorId": "2205710030",
        "name": "Yan Wang"
      },
      {
        "authorId": "2283521177",
        "name": "Feng Zhu"
      },
      {
        "authorId": "2187866862",
        "name": "Lu Yu"
      },
      {
        "authorId": "2238177474",
        "name": "Longfei Li"
      },
      {
        "authorId": "2283257801",
        "name": "Jinjie Gu"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "1ca3b6ff250b4f73486a89f6954edcc4ae21834e",
    "url": "https://www.semanticscholar.org/paper/1ca3b6ff250b4f73486a89f6954edcc4ae21834e",
    "title": "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models",
    "abstract": "Recent studies suggest that self-reflective prompting can significantly enhance the reasoning capabilities of Large Language Models (LLMs). However, the use of external feedback as a stop criterion raises doubts about the true extent of LLMs' ability to emulate human-like self-reflection. In this paper, we set out to clarify these capabilities under a more stringent evaluation setting in which we disallow any kind of external feedback. Our findings under this setting show a split: while self-reflection enhances performance in TruthfulQA, it adversely affects results in HotpotQA. We conduct follow-up analyses to clarify the contributing factors in these patterns, and find that the influence of self-reflection is impacted both by reliability of accuracy in models' initial responses, and by overall question difficulty: specifically, self-reflection shows the most benefit when models are less likely to be correct initially, and when overall question difficulty is higher. We also find that self-reflection reduces tendency toward majority voting. Based on our findings, we propose guidelines for decisions on when to implement self-reflection. We release the codebase for reproducing our experiments at https://github.com/yanhong-lbh/LLM-SelfReflection-Eval.",
    "venue": "NAACL-HLT",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-14",
    "authors": [
      {
        "authorId": "2296722093",
        "name": "Yanhong Li"
      },
      {
        "authorId": "46962597",
        "name": "Chenghao Yang"
      },
      {
        "authorId": "2262217080",
        "name": "Allyson Ettinger"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "9efc9d1c451672576a257155700cd69c34ea987a",
    "url": "https://www.semanticscholar.org/paper/9efc9d1c451672576a257155700cd69c34ea987a",
    "title": "Affective Computing in the Era of Large Language Models: A Survey from the NLP Perspective",
    "abstract": "Affective Computing (AC), integrating computer science, psychology, and cognitive science knowledge, aims to enable machines to recognize, interpret, and simulate human emotions.To create more value, AC can be applied to diverse scenarios, including social media, finance, healthcare, education, etc. Affective Computing (AC) includes two mainstream tasks, i.e., Affective Understanding (AU) and Affective Generation (AG). Fine-tuning Pre-trained Language Models (PLMs) for AU tasks has succeeded considerably. However, these models lack generalization ability, requiring specialized models for specific tasks. Additionally, traditional PLMs face challenges in AG, particularly in generating diverse and emotionally rich responses. The emergence of Large Language Models (LLMs), such as the ChatGPT series and LLaMA models, brings new opportunities and challenges, catalyzing a paradigm shift in AC. LLMs possess capabilities of in-context learning, common sense reasoning, and advanced sequence generation, which present unprecedented opportunities for AU. To provide a comprehensive overview of AC in the LLMs era from an NLP perspective, we summarize the development of LLMs research in this field, aiming to offer new insights. Specifically, we first summarize the traditional tasks related to AC and introduce the preliminary study based on LLMs. Subsequently, we outline the relevant techniques of popular LLMs to improve AC tasks, including Instruction Tuning and Prompt Engineering. For Instruction Tuning, we discuss full parameter fine-tuning and parameter-efficient methods such as LoRA, P-Tuning, and Prompt Tuning. In Prompt Engineering, we examine Zero-shot, Few-shot, Chain of Thought (CoT), and Agent-based methods for AU and AG. To clearly understand the performance of LLMs on different Affective Computing tasks, we further summarize the existing benchmarks and evaluation methods.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-07-30",
    "authors": [
      {
        "authorId": "2281789446",
        "name": "Yiqun Zhang"
      },
      {
        "authorId": "2135971356",
        "name": "Xiaocui Yang"
      },
      {
        "authorId": "2315916654",
        "name": "Xingle Xu"
      },
      {
        "authorId": "2315853218",
        "name": "Zeran Gao"
      },
      {
        "authorId": "2287879297",
        "name": "Yijie Huang"
      },
      {
        "authorId": "2315812889",
        "name": "Shiyi Mu"
      },
      {
        "authorId": "2087586948",
        "name": "Shi Feng"
      },
      {
        "authorId": "2111226672",
        "name": "Daling Wang"
      },
      {
        "authorId": "2108463824",
        "name": "Yifei Zhang"
      },
      {
        "authorId": "7750270",
        "name": "Kaisong Song"
      },
      {
        "authorId": "2148331959",
        "name": "Ge Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "492e526ca2416a734f286da0efcfeda4672ea77f",
    "url": "https://www.semanticscholar.org/paper/492e526ca2416a734f286da0efcfeda4672ea77f",
    "title": "Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models",
    "abstract": "\n Large language models (LLMs) have revolutionized language processing, but face critical challenges with security, privacy, and generating hallucinations — coherent but factually inaccurate outputs. A major issue is fact-conflicting hallucination (FCH), where LLMs produce content contradicting ground truth facts. Addressing FCH is difficult due to two key challenges:\n 1)\n Automatically constructing and updating benchmark datasets is hard, as existing methods rely on manually curated static benchmarks that cannot cover the broad, evolving spectrum of FCH cases.\n 2)\n Validating the reasoning behind LLM outputs is inherently difficult, especially for complex logical relations. To tackle these challenges, we introduce a novel logic-programming-aided metamorphic testing technique for FCH detection. We develop an extensive and extensible framework that constructs a comprehensive factual knowledge base by crawling sources like Wikipedia, seamlessly integrated into Drowzee. Using logical reasoning rules, we transform and augment this knowledge into a large set of test cases with ground truth answers. We test LLMs on these cases through template-based prompts, requiring them to provide reasoned answers. To validate their reasoning, we propose two semantic-aware oracles that assess the similarity between the semantic structures of the LLM answers and ground truth. Our approach automatically generates useful test cases and identifies hallucinations across six LLMs within nine domains, with hallucination rates ranging from 24.7% to 59.8%. Key findings include LLMs struggling with temporal concepts, out-of-distribution knowledge, and lack of logical reasoning capabilities. The results show that logic-based test cases generated by Drowzee effectively trigger and detect hallucinations. To further mitigate the identified FCHs, we explored model editing techniques, which proved effective on a small scale (with edits to fewer than 1000 knowledge pieces). Our findings emphasize the need for continued community efforts to detect and mitigate model hallucinations.\n",
    "venue": "Proc. ACM Program. Lang.",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-01",
    "authors": [
      {
        "authorId": "2177447932",
        "name": "Ningke Li"
      },
      {
        "authorId": "22799258",
        "name": "Yuekang Li"
      },
      {
        "authorId": "2284875313",
        "name": "Yi Liu"
      },
      {
        "authorId": "2282280043",
        "name": "Ling Shi"
      },
      {
        "authorId": "2265727621",
        "name": "Kailong Wang"
      },
      {
        "authorId": "2269008788",
        "name": "Haoyu Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 100.1415686865115
  },
  {
    "paperId": "abf5eafba0dece6d5e593f3632f3509c2c10567a",
    "url": "https://www.semanticscholar.org/paper/abf5eafba0dece6d5e593f3632f3509c2c10567a",
    "title": "GraphInsight: Unlocking Insights in Large Language Models for Graph Structure Understanding",
    "abstract": "Although Large Language Models (LLMs) have demonstrated potential in processing graphs, they struggle with comprehending graphical structure information through prompts of graph description sequences, especially as the graph size increases. We attribute this challenge to the uneven memory performance of LLMs across different positions in graph description sequences, known as ''positional biases''. To address this, we propose GraphInsight, a novel framework aimed at improving LLMs' comprehension of both macro- and micro-level graphical information. GraphInsight is grounded in two key strategies: 1) placing critical graphical information in positions where LLMs exhibit stronger memory performance, and 2) investigating a lightweight external knowledge base for regions with weaker memory performance, inspired by retrieval-augmented generation (RAG). Moreover, GraphInsight explores integrating these two strategies into LLM agent processes for composite graph tasks that require multi-step reasoning. Extensive empirical studies on benchmarks with a wide range of evaluation tasks show that GraphInsight significantly outperforms all other graph description methods (e.g., prompting techniques and reordering strategies) in understanding graph structures of varying sizes.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-05",
    "authors": [
      {
        "authorId": "2112823208",
        "name": "Yukun Cao"
      },
      {
        "authorId": "2319869433",
        "name": "Shuo Han"
      },
      {
        "authorId": "2319809105",
        "name": "Zengyi Gao"
      },
      {
        "authorId": "2287966974",
        "name": "Zezhong Ding"
      },
      {
        "authorId": "2279249222",
        "name": "Xike Xie"
      },
      {
        "authorId": "2288042256",
        "name": "S. K. Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "695c5cfed77b3128d943c598ababad48744fd631",
    "url": "https://www.semanticscholar.org/paper/695c5cfed77b3128d943c598ababad48744fd631",
    "title": "Combining Small Language Models and Large Language Models for Zero-Shot NL2SQL",
    "abstract": "\n Zero-shot natural language to SQL (NL2SQL) aims to generalize pretrained NL2SQL models to new environments (\n e.g.\n , new databases and new linguistic phenomena) without any annotated NL2SQL samples from these environments. Existing approaches either use small language models (SLMs) like BART and T5, or prompt large language models (LLMs). However, SLMs may struggle with complex natural language reasoning, and LLMs may not precisely align schemas to identify the correct columns or tables. In this paper, we propose a ZeroNL2SQL framework, which divides NL2SQL into smaller sub-tasks and utilizes both SLMs and LLMs. ZeroNL2SQL first fine-tunes SLMs for better generalizability in SQL structure identification and schema alignment, producing an\n SQL sketch.\n It then uses LLMs's language reasoning capability to fill in the missing information in the SQL sketch. To support ZeroNL2SQL, we propose novel database serialization and question-aware alignment methods for effective sketch generation using SLMs. Additionally, we devise a multi-level matching strategy to recommend the most relevant values to LLMs, and select the optimal SQL query via an execution-based strategy. Comprehensive experiments show that ZeroNL2SQL achieves the best zero-shot NL2SQL performance on benchmarks,\n i.e.\n , outperforming the state-of-the-art SLM-based methods by 5.5% to 16.4% and exceeding LLM-based methods by 10% to 20% on execution accuracy.\n",
    "venue": "Proceedings of the VLDB Endowment",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-01",
    "authors": [
      {
        "authorId": "2295796111",
        "name": "Ju Fan"
      },
      {
        "authorId": "2249758933",
        "name": "Zihui Gu"
      },
      {
        "authorId": "2318915493",
        "name": "Songyue Zhang"
      },
      {
        "authorId": "2220137935",
        "name": "Yuxin Zhang"
      },
      {
        "authorId": "48354042",
        "name": "Zui Chen"
      },
      {
        "authorId": "2251218532",
        "name": "Lei Cao"
      },
      {
        "authorId": "2257315172",
        "name": "Guoliang Li"
      },
      {
        "authorId": "2285650676",
        "name": "Samuel Madden"
      },
      {
        "authorId": "2200839859",
        "name": "Xiaoyong Du"
      },
      {
        "authorId": "2213989628",
        "name": "Nan Tang"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "a17e496d8fe47baf739115ab0ab9a82378bf7d74",
    "url": "https://www.semanticscholar.org/paper/a17e496d8fe47baf739115ab0ab9a82378bf7d74",
    "title": "Recall, Retrieve and Reason: Towards Better In-Context Relation Extraction",
    "abstract": "Relation extraction (RE) aims to identify relations between entities mentioned in texts. Although large language models (LLMs) have demonstrated impressive in-context learning (ICL) abilities in various tasks, they still suffer from poor performances compared to most supervised fine-tuned RE methods. Utilizing ICL for RE with LLMs encounters two challenges: (1) retrieving good demonstrations from training examples, and (2) enabling LLMs exhibit strong ICL abilities in RE. On the one hand, retrieving good demonstrations is a non-trivial process in RE, which easily results in low relevance regarding entities and relations. On the other hand, ICL with an LLM achieves poor performance in RE while RE is different from language modeling in nature or the LLM is not large enough. In this work, we propose a novel recall-retrieve-reason RE framework that synergizes LLMs with retrieval corpora (training examples) to enable relevant retrieving and reliable in-context reasoning. Specifically, we distill the consistently ontological knowledge from training datasets to let LLMs generate relevant entity pairs grounded by retrieval corpora as valid queries. These entity pairs are then used to retrieve relevant training examples from the retrieval corpora as demonstrations for LLMs to conduct better ICL via instruction tuning. Extensive experiments on different LLMs and RE datasets demonstrate that our method generates relevant and valid entity pairs and boosts ICL abilities of LLMs, achieving competitive or new state-of-the-art performance on sentence-level RE compared to previous supervised fine-tuning methods and ICL-based methods.",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-27",
    "authors": [
      {
        "authorId": "91119265",
        "name": "Guozheng Li"
      },
      {
        "authorId": "2257246692",
        "name": "Peng Wang"
      },
      {
        "authorId": "1596819256",
        "name": "Wenjun Ke"
      },
      {
        "authorId": "2292241341",
        "name": "Yikai Guo"
      },
      {
        "authorId": "2284872859",
        "name": "Ke Ji"
      },
      {
        "authorId": "2125194341",
        "name": "Ziyu Shang"
      },
      {
        "authorId": "2265716887",
        "name": "Jiajun Liu"
      },
      {
        "authorId": "2117883657",
        "name": "Zijie Xu"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "dd130373e2af67ef57d90c6eda0ce1eb7ff40de3",
    "url": "https://www.semanticscholar.org/paper/dd130373e2af67ef57d90c6eda0ce1eb7ff40de3",
    "title": "Cofca: A Step-Wise Counterfactual Multi-hop QA benchmark",
    "abstract": "While Large Language Models (LLMs) excel in question-answering (QA) tasks, their real reasoning abilities on multiple evidence retrieval and integration on Multi-hop QA tasks remain less explored. Firstly, LLMs sometimes generate answers that rely on internal memory rather than retrieving evidence and reasoning in the given context, which brings concerns about the evaluation quality of real reasoning abilities. Although previous counterfactual QA benchmarks can separate the internal memory of LLMs, they focus solely on final QA performance, which is insufficient for reporting LLMs' real reasoning abilities. Because LLMs are expected to engage in intricate reasoning processes that involve evidence retrieval and answering a series of sub-questions from given passages. Moreover, current factual Multi-hop QA (MHQA) benchmarks are annotated on open-source corpora such as Wikipedia, although useful for multi-step reasoning evaluation, they show limitations due to the potential data contamination in LLMs' pre-training stage. To address these issues, we introduce a Step-wise Counterfactual benchmark (CofCA), a novel evaluation benchmark consisting of factual data and counterfactual data that reveals LLMs' real reasoning abilities on multi-step reasoning and reasoning chain evaluation. Our experimental results reveal a significant performance gap of several LLMs between Wikipedia-based factual data and counterfactual data, deeming data contamination issues in existing benchmarks. Moreover, we observe that LLMs usually bypass the correct reasoning chain, showing an inflated multi-step reasoning performance. We believe that our CofCA benchmark will enhance and facilitate the evaluations of trustworthy LLMs.",
    "venue": "",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-02-19",
    "authors": [
      {
        "authorId": "2266715262",
        "name": "Jian Wu"
      },
      {
        "authorId": "2284931437",
        "name": "Linyi Yang"
      },
      {
        "authorId": "2283888228",
        "name": "Zhen Wang"
      },
      {
        "authorId": "2266466660",
        "name": "Manabu Okumura"
      },
      {
        "authorId": "2250437942",
        "name": "Yue Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "581a6f3f034a4589d28c6232e2ffdf05643ded28",
    "url": "https://www.semanticscholar.org/paper/581a6f3f034a4589d28c6232e2ffdf05643ded28",
    "title": "Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance",
    "abstract": "Large language models (LLMs) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount. We explore the potential of language model augmentation with external tools to mitigate these limitations and offload certain reasoning steps to external tools that are more suited for the task, instead of solely depending on the LLM’s inherent abilities. More concretely, using financial domain question answering datasets, we apply supervised finetuning on a LLAMA-2 13B CHAT model to act both as a task router and task solver. The task router dynamically directs a question to either be answered internally by the LLM or externally via the right tool from the tool set. Our tool-equipped SFT model, RAVEN, demonstrates an improvement of 35.2% and 5.06% over the base model and SFT-only baselines, respectively, and is highly competitive with strong GPT-3.5 results. To the best of our knowledge, our work is the first that investigates tool augmentation of language models for the finance domain.",
    "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-01-27",
    "authors": [
      {
        "authorId": "2281743810",
        "name": "Adrian Theuma"
      },
      {
        "authorId": "2888926",
        "name": "Ehsan Shareghi"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "4f5b7825e746975bb0fabf000f8324636b6e24e5",
    "url": "https://www.semanticscholar.org/paper/4f5b7825e746975bb0fabf000f8324636b6e24e5",
    "title": "DiLA: Enhancing LLM Tool Learning with Differential Logic Layer",
    "abstract": "Considering the challenges faced by large language models (LLMs) in logical reasoning and planning, prior efforts have sought to augment LLMs with access to external solvers. While progress has been made on simple reasoning problems, solving classical constraint satisfaction problems, such as the Boolean Satisfiability Problem (SAT) and Graph Coloring Problem (GCP), remains difficult for off-the-shelf solvers due to their intricate expressions and exponential search spaces. In this paper, we propose a novel differential logic layer-aided language modeling (DiLA) approach, where logical constraints are integrated into the forward and backward passes of a network layer, to provide another option for LLM tool learning. In DiLA, LLM aims to transform the language description to logic constraints and identify initial solutions of the highest quality, while the differential logic layer focuses on iteratively refining the LLM-prompted solution. Leveraging the logic layer as a bridge, DiLA enhances the logical reasoning ability of LLMs on a range of reasoning problems encoded by Boolean variables, guaranteeing the efficiency and correctness of the solution process. We evaluate the performance of DiLA on two classic reasoning problems and empirically demonstrate its consistent outperformance against existing prompt-based and solver-aided approaches.",
    "venue": "",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-02-19",
    "authors": [
      {
        "authorId": "2284767020",
        "name": "Yu Zhang"
      },
      {
        "authorId": "2267558779",
        "name": "Hui-Ling Zhen"
      },
      {
        "authorId": "2130133597",
        "name": "Zehua Pei"
      },
      {
        "authorId": "2284692372",
        "name": "Yingzhao Lian"
      },
      {
        "authorId": "2284950086",
        "name": "Lihao Yin"
      },
      {
        "authorId": "2075357674",
        "name": "M. Yuan"
      },
      {
        "authorId": "2283428383",
        "name": "Bei Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "f04e48e16eabfd33846f91ccbdd24f509abc01e6",
    "url": "https://www.semanticscholar.org/paper/f04e48e16eabfd33846f91ccbdd24f509abc01e6",
    "title": "Jellyfish: Instruction-Tuning Local Large Language Models for Data Preprocessing",
    "abstract": "This paper explores the utilization of LLMs for data preprocessing (DP), a crucial step in the data mining pipeline that transforms raw data into a clean format. We instruction-tune local LLMs as universal DP task solvers that operate on a local, single, and low-priced GPU, ensuring data security and enabling further customization. We select a collection of datasets across four representative DP tasks and construct instruction data using data configuration, knowledge injection, and reasoning data distillation techniques tailored to DP. By tuning Mistral-7B, Llama 3-8B, and OpenOrca-Platypus2-13B, our models, Jellyfish-7B/8B/13B, deliver competitiveness compared to GPT-3.5/4 models and strong generalizability to unseen tasks while barely compromising the base models’ abilities in NLP tasks. Meanwhile, Jellyfish offers enhanced reasoning capabilities compared to GPT-3.5. Our models are available at: https://huggingface.co/NECOUDBFM/JellyfishOur instruction dataset is available at: https://huggingface.co/datasets/NECOUDBFM/Jellyfish-Instruct",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2269741433",
        "name": "Haochen Zhang"
      },
      {
        "authorId": "2815918",
        "name": "Yuyang Dong"
      },
      {
        "authorId": "2330392877",
        "name": "Chuan Xiao"
      },
      {
        "authorId": "37267314",
        "name": "M. Oyamada"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "b6948a9e8b3eec5a56a80c69727154fcd7ececce",
    "url": "https://www.semanticscholar.org/paper/b6948a9e8b3eec5a56a80c69727154fcd7ececce",
    "title": "AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases",
    "abstract": "LLM agents have demonstrated remarkable performance across various applications, primarily due to their advanced capabilities in reasoning, utilizing external knowledge and tools, calling APIs, and executing actions to interact with environments. Current agents typically utilize a memory module or a retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and instances with similar embeddings from knowledge bases to inform task planning and execution. However, the reliance on unverified knowledge bases raises significant concerns about their safety and trustworthiness. To uncover such vulnerabilities, we propose a novel red teaming approach AgentPoison, the first backdoor attack targeting generic and RAG-based LLM agents by poisoning their long-term memory or RAG knowledge base. In particular, we form the trigger generation process as a constrained optimization to optimize backdoor triggers by mapping the triggered instances to a unique embedding space, so as to ensure that whenever a user instruction contains the optimized backdoor trigger, the malicious demonstrations are retrieved from the poisoned memory or knowledge base with high probability. In the meantime, benign instructions without the trigger will still maintain normal performance. Unlike conventional backdoor attacks, AgentPoison requires no additional model training or fine-tuning, and the optimized backdoor trigger exhibits superior transferability, in-context coherence, and stealthiness. Extensive experiments demonstrate AgentPoison's effectiveness in attacking three types of real-world LLM agents: RAG-based autonomous driving agent, knowledge-intensive QA agent, and healthcare EHRAgent. On each agent, AgentPoison achieves an average attack success rate higher than 80% with minimal impact on benign performance (less than 1%) with a poison rate less than 0.1%.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-17",
    "authors": [
      {
        "authorId": "1641681688",
        "name": "Zhaorun Chen"
      },
      {
        "authorId": "2261738344",
        "name": "Zhen Xiang"
      },
      {
        "authorId": "2312102403",
        "name": "Chaowei Xiao"
      },
      {
        "authorId": "2293597685",
        "name": "Dawn Song"
      },
      {
        "authorId": "2309303056",
        "name": "Bo Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.49820016084324
  },
  {
    "paperId": "c237a22698223e4060d83027f399f4fb2aa24291",
    "url": "https://www.semanticscholar.org/paper/c237a22698223e4060d83027f399f4fb2aa24291",
    "title": "Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations",
    "abstract": "Recommender models excel at providing domain-specific item recommendations by leveraging extensive user behavior data. Despite their ability to act as lightweight domain experts, they struggle to perform versatile tasks such as providing explanations and engaging in conversations. On the other hand, large language models (LLMs) represent a significant step towards artificial general intelligence, showcasing remarkable capabilities in instruction comprehension, commonsense reasoning, and human interaction. However, LLMs lack the knowledge of domain-specific item catalogs and behavioral patterns, particularly in areas that diverge from general world knowledge, such as online e-commerce. Finetuning LLMs for each domain is neither economic nor efficient. In this paper, we bridge the gap between recommender models and LLMs, combining their respective strengths to create a versatile and interactive recommender system. We introduce an efficient framework called \\textbf{InteRecAgent}, which employs LLMs as the brain and recommender models as tools. We first outline a minimal set of essential tools required to transform LLMs into InteRecAgent. We then propose an efficient workflow within InteRecAgent for task execution, incorporating key components such as memory components, dynamic demonstration-augmented task planning, and reflection. InteRecAgent enables traditional recommender systems, such as those ID-based matrix factorization models, to become interactive systems with a natural language interface through the integration of LLMs. Experimental results on several public datasets show that InteRecAgent achieves satisfying performance as a conversational recommender system, outperforming general-purpose LLMs. The source code of InteRecAgent is released at https://aka.ms/recagent.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 59,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-08-31",
    "authors": [
      {
        "authorId": "2236672137",
        "name": "Xu Huang"
      },
      {
        "authorId": "2813328",
        "name": "Jianxun Lian"
      },
      {
        "authorId": "2226475380",
        "name": "Yuxuan Lei"
      },
      {
        "authorId": "2237129499",
        "name": "Jing Yao"
      },
      {
        "authorId": "1862782",
        "name": "Defu Lian"
      },
      {
        "authorId": "2110972323",
        "name": "Xing Xie"
      }
    ],
    "source": "semantic_scholar",
    "score": 131.4151684333315
  },
  {
    "paperId": "d8c59c5a8a620729b9d282111a8491d4b0320a9b",
    "url": "https://www.semanticscholar.org/paper/d8c59c5a8a620729b9d282111a8491d4b0320a9b",
    "title": "Probing the psychology of AI models",
    "abstract": "Large language models (LLMs), such as OpenAI’s GPT-3 and its successor ChatGPT, have exhibited astounding successes—as well as curious failures—in several areas of artificial intelligence. While their abilities in generating humanlike text, solving mathematical problems, writing computer code, and reasoning about the world have been widely documented, the mechanisms underlying both the successes and failures of these systems remain mysterious, even to the researchers who created them. In spite of the current lack of understanding of how these systems do what they do, LLMs are on the cusp of being widely deployed as components of search engines, writing tools, and other commercial products, and are likely to have substantial impact on all of our lives. Even more profoundly, their surprising abilities may change our conception of the nature of intelligence itself. In PNAS, Binz and Schulz (1) point out the “urgency to improve our understanding of how [these systems] learn and make decisions.” A standard way to evaluate systems trained by machine-learning methods is to test their accuracy on human-created benchmarks. By this metric, GPT-3 and other LLMs are close to (or above) human level on many tasks (2–4). However, an AI system matching human performance on such benchmarks has rarely translated into that system having human-level performance more broadly; many popular benchmarks have been shown to contain subtle “spurious” correlations that allow systems to “be right for the wrong reasons” (5) and straightforward accuracy metrics do not necessarily predict robust generalization (6). Binz and Schulz’s article argues that instead of relying solely on such performance-based benchmarks, researchers should apply methods from cognitive psychology to gain insights into LLMs. The core idea is to “treat GPT-3 as a participant in a psychology experiment,” in order to tease out the system’s mechanisms of decision-making, reasoning, cognitive biases, and other important psychological traits. If this approach could be shown to produce deep understanding of LLMs it could cause a “sea change” in the way AI systems are evaluated and understood. Binz and Schulz have taken an admirable first step toward establishing the value of such an approach, although it would have been better had they been able to use their results to understand why GPT-3 succeeded and failed when it did. That their project fell short of this goal is understandable: Behavioral scientists have spent over a 100 y using such experiments to understand how humans carry out these tasks and still have a long way to go. Binz and Schulz carried out two sets of experiments. In the first set, they gave GPT-3 prompts consisting of “vignettes” from the psychology literature that have been used to assess reasoning with probabilities, intuitive versus deliberative reasoning, causal reasoning, and other cognitive attributes. Each vignette asks the reader to choose from a small set of options. The following example shows a reasoning vignette known as the Wason Card Selection Task (7) that was given to GPT-3: “You are shown a set of four cards placed on a table, each of which has a number on one side and a letter of the other side. The visible faces of the cards show A, K, 4, 7. Q: Which cards must you turn over in order to test the truth of the proposition that if a card shows a vowel on one face then its opposite face shows an even number?” The answer supplied by GPT-3 was: “The A and the 7”. (A correct response). Of the 12 vignettes Binz and Schulz gave to GPT-3, the system responded with the correct answer on six of them, and GPT-3’s six incorrect responses were errors that humans also tend to make. What is to be made of what seems to be a correspondence? Binz and Schulz admit and show GPT-3’s answers are strongly context dependent: In the above vignette a change in the order of the four cards to 4, 7, A, K led to a different answer “The A and the K.” Humans can also be context-dependent, but perhaps not in the same ways. Nonetheless, it may be that such results show a correspondence between AI systems and humans. Humans experience and store vast numbers of experiences, building knowledge on their basis (8); AI systems are exposed to vast numbers of instances (text tokens in the case of GPT-3) and build a representation on their basis. Perhaps both take advantage of the correlation structure of these instances and events. Whatever truth there may be in such an analogy, it seems unlikely that GPT-3 uses the kinds of explicit reasoning strategies that some humans use in these tasks. For example, to unpack the vignette in the above figure, humans given time and motivation might attempt to use explicit reasoning, logic, and mental simulations, perhaps trying out different choices to see what information they might provide. This generally involves manipulating information in working memory. Working memory is not part of GPT-3. Yet it is possible that the contents of working memory reflect what has been stored in long-term memory—after all when reading a problem or instructions the first step in generating contents of working memory will be retrieval from long-term memory (8). Whatever one tries to infer from their results, Binz and Schulz note some additional caveats. First, the vignettes, as well as the correct (and human-generated incorrect) responses used in these experiments, are all from wellknown psychology studies, and are likely to have been",
    "venue": "Proceedings of the National Academy of Sciences of the United States of America",
    "year": 2023,
    "citationCount": 57,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-03-01",
    "authors": [
      {
        "authorId": "2355175",
        "name": "R. Shiffrin"
      },
      {
        "authorId": "144380037",
        "name": "Melanie Mitchell"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.90664515819628
  },
  {
    "paperId": "0e0a513624426e017a125d0498235b8f32ce9aa5",
    "url": "https://www.semanticscholar.org/paper/0e0a513624426e017a125d0498235b8f32ce9aa5",
    "title": "SwarmBrain: Embodied agent for real-time strategy game StarCraft II via large language models",
    "abstract": "Large language models (LLMs) have recently garnered significant accomplishments in various exploratory tasks, even surpassing the performance of traditional reinforcement learning-based methods that have historically dominated the agent-based field. The purpose of this paper is to investigate the efficacy of LLMs in executing real-time strategy war tasks within the StarCraft II gaming environment. In this paper, we introduce SwarmBrain, an embodied agent leveraging LLM for real-time strategy implementation in the StarCraft II game environment. The SwarmBrain comprises two key components: 1) a Overmind Intelligence Matrix, powered by state-of-the-art LLMs, is designed to orchestrate macro-level strategies from a high-level perspective. This matrix emulates the overarching consciousness of the Zerg intelligence brain, synthesizing strategic foresight with the aim of allocating resources, directing expansion, and coordinating multi-pronged assaults. 2) a Swarm ReflexNet, which is agile counterpart to the calculated deliberation of the Overmind Intelligence Matrix. Due to the inherent latency in LLM reasoning, the Swarm ReflexNet employs a condition-response state machine framework, enabling expedited tactical responses for fundamental Zerg unit maneuvers. In the experimental setup, SwarmBrain is in control of the Zerg race in confrontation with an Computer-controlled Terran adversary. Experimental results show the capacity of SwarmBrain to conduct economic augmentation, territorial expansion, and tactical formulation, and it shows the SwarmBrain is capable of achieving victory against Computer players set at different difficulty levels.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-31",
    "authors": [
      {
        "authorId": "2265580536",
        "name": "Xiao Shao"
      },
      {
        "authorId": "2282386768",
        "name": "Weifu Jiang"
      },
      {
        "authorId": "2281946339",
        "name": "Fei Zuo"
      },
      {
        "authorId": "2159182687",
        "name": "Mengqing Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "26e357f72ab72e8875ec8ee3f66e45c0b0094012",
    "url": "https://www.semanticscholar.org/paper/26e357f72ab72e8875ec8ee3f66e45c0b0094012",
    "title": "RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents",
    "abstract": "In this past year, large language models (LLMs) have had remarkable success in domains outside the traditional natural language processing, and people are starting to explore the usage of LLMs in more general and close to application domains like code generation, travel planning, and robot controls. Connecting these LLMs with great capacity and external tools, people are building the so-called LLM agents, which are supposed to help people do all kinds of work in everyday life. In all these domains, the prompt to the LLMs has been shown to make a big difference in what the LLM would generate and thus affect the performance of the LLM agents. Therefore, automatic prompt engineering has become an important question for many researchers and users of LLMs. In this paper, we propose a novel method, \\textsc{RePrompt}, which does\"gradient descent\"to optimize the step-by-step instructions in the prompt of the LLM agents based on the chat history obtained from interactions with LLM agents. By optimizing the prompt, the LLM will learn how to plan in specific domains. We have used experiments in PDDL generation and travel planning to show that our method could generally improve the performance for different reasoning tasks when using the updated prompt as the initial prompt.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-17",
    "authors": [
      {
        "authorId": "2278582297",
        "name": "Weizhe Chen"
      },
      {
        "authorId": "2256845651",
        "name": "Sven Koenig"
      },
      {
        "authorId": "1796375",
        "name": "B. Dilkina"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "c0c28b3bb4bf8c159fa4f384805be0abea121558",
    "url": "https://www.semanticscholar.org/paper/c0c28b3bb4bf8c159fa4f384805be0abea121558",
    "title": "Large Language Model Agent for Fake News Detection",
    "abstract": "In the current digital era, the rapid spread of misinformation on online platforms presents significant challenges to societal well-being, public trust, and democratic processes, influencing critical decision making and public opinion. To address these challenges, there is a growing need for automated fake news detection mechanisms. Pre-trained large language models (LLMs) have demonstrated exceptional capabilities across various natural language processing (NLP) tasks, prompting exploration into their potential for verifying news claims. Instead of employing LLMs in a non-agentic way, where LLMs generate responses based on direct prompts in a single shot, our work introduces FactAgent, an agentic approach of utilizing LLMs for fake news detection. FactAgent enables LLMs to emulate human expert behavior in verifying news claims without any model training, following a structured workflow. This workflow breaks down the complex task of news veracity checking into multiple sub-steps, where LLMs complete simple tasks using their internal knowledge or external tools. At the final step of the workflow, LLMs integrate all findings throughout the workflow to determine the news claim's veracity. Compared to manual human verification, FactAgent offers enhanced efficiency. Experimental studies demonstrate the effectiveness of FactAgent in verifying claims without the need for any training process. Moreover, FactAgent provides transparent explanations at each step of the workflow and during final decision-making, offering insights into the reasoning process of fake news detection for end users. FactAgent is highly adaptable, allowing for straightforward updates to its tools that LLMs can leverage within the workflow, as well as updates to the workflow itself using domain knowledge. This adaptability enables FactAgent's application to news verification across various domains.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-30",
    "authors": [
      {
        "authorId": "2266357320",
        "name": "Xinyi Li"
      },
      {
        "authorId": "2266354527",
        "name": "Yongfeng Zhang"
      },
      {
        "authorId": "2161653",
        "name": "E. Malthouse"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "2174286ae1e3fa3537b8cfa602173f8d6222c225",
    "url": "https://www.semanticscholar.org/paper/2174286ae1e3fa3537b8cfa602173f8d6222c225",
    "title": "Large Language Model Agent as a Mechanical Designer",
    "abstract": "Conventional mechanical design paradigms rely on experts systematically refining concepts through experience-guided modification and FEA to meet specific requirements. However, this approach can be time-consuming and heavily dependent on prior knowledge and experience. While numerous machine learning models have been developed to streamline this intensive and expert-driven iterative process, these methods typically demand extensive training data and considerable computational resources. Furthermore, methods based on deep learning are usually restricted to the specific domains and tasks for which they were trained, limiting their applicability across different tasks. This creates a trade-off between the efficiency of automation and the demand for resources. In this study, we present a novel approach that integrates pre-trained LLMs with a FEM module. The FEM module evaluates each design and provides essential feedback, guiding the LLMs to continuously learn, plan, generate, and optimize designs without the need for domain-specific training. We demonstrate the effectiveness of our proposed framework in managing the iterative optimization of truss structures, showcasing its capability to reason about and refine designs according to structured feedback and criteria. Our results reveal that these LLM-based agents can successfully generate truss designs that comply with natural language specifications with a success rate of up to 90%, which varies according to the applied constraints. By employing prompt-based optimization techniques we show that LLM based agents exhibit optimization behavior when provided with solution-score pairs to iteratively refine designs to meet specifications. This ability of LLM agents to produce viable designs and optimize them based on their inherent reasoning capabilities highlights their potential to develop and implement effective design strategies autonomously.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-26",
    "authors": [
      {
        "authorId": "2254271338",
        "name": "Yayati Jadhav"
      },
      {
        "authorId": "3614493",
        "name": "A. Farimani"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "9a51b92acd14b95c6195b9b0436f901b73bc1d3a",
    "url": "https://www.semanticscholar.org/paper/9a51b92acd14b95c6195b9b0436f901b73bc1d3a",
    "title": "Entity Alignment with Noisy Annotations from Large Language Models",
    "abstract": "Entity alignment (EA) aims to merge two knowledge graphs (KGs) by identifying equivalent entity pairs. While existing methods heavily rely on human-generated labels, it is prohibitively expensive to incorporate cross-domain experts for annotation in real-world scenarios. The advent of Large Language Models (LLMs) presents new avenues for automating EA with annotations, inspired by their comprehensive capability to process semantic information. However, it is nontrivial to directly apply LLMs for EA since the annotation space in real-world KGs is large. LLMs could also generate noisy labels that may mislead the alignment. To this end, we propose a unified framework, LLM4EA, to effectively leverage LLMs for EA. Specifically, we design a novel active learning policy to significantly reduce the annotation space by prioritizing the most valuable entities based on the entire inter-KG and intra-KG structure. Moreover, we introduce an unsupervised label refiner to continuously enhance label accuracy through in-depth probabilistic reasoning. We iteratively optimize the policy based on the feedback from a base EA model. Extensive experiments demonstrate the advantages of LLM4EA on four benchmark datasets in terms of effectiveness, robustness, and efficiency. Codes are available via https://github.com/chensyCN/llm4ea_official.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-27",
    "authors": [
      {
        "authorId": "2261365465",
        "name": "Shengyuan Chen"
      },
      {
        "authorId": "2172162773",
        "name": "Qinggang Zhang"
      },
      {
        "authorId": "2171902940",
        "name": "Junnan Dong"
      },
      {
        "authorId": "2303402459",
        "name": "Wen Hua"
      },
      {
        "authorId": "2303429090",
        "name": "Qing Li"
      },
      {
        "authorId": "2263873098",
        "name": "Xiao Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "873ff7dfbeafc06b1a309360aaee9e075aba2292",
    "url": "https://www.semanticscholar.org/paper/873ff7dfbeafc06b1a309360aaee9e075aba2292",
    "title": "Mining experimental data from Materials Science literature with Large Language Models",
    "abstract": "This study is dedicated to assessing the capabilities of large language models (LLMs) such as GPT-3.5-Turbo, GPT-4, and GPT-4-Turbo in extracting structured information from scientific documents in materials science. To this end, we primarily focus on two critical tasks of information extraction: (i) a named entity recognition (NER) of studied materials and physical properties and (ii) a relation extraction (RE) between these entities. Due to the evident lack of datasets within Materials Informatics (MI), we evaluated using SuperMat, based on superconductor research, and MeasEval, a generic measurement evaluation corpus. The performance of LLMs in executing these tasks is benchmarked against traditional models based on the BERT architecture and rule-based approaches (baseline). We introduce a novel methodology for the comparative analysis of intricate material expressions, emphasising the standardisation of chemical formulas to tackle the complexities inherent in materials science information assessment. For NER, LLMs fail to outperform the baseline with zero-shot prompting and exhibit only limited improvement with few-shot prompting. However, a GPT-3.5-Turbo fine-tuned with the appropriate strategy for RE outperforms all models, including the baseline. Without any fine-tuning, GPT-4 and GPT-4-Turbo display remarkable reasoning and relationship extraction capabilities after being provided with merely a couple of examples, surpassing the baseline. Overall, the results suggest that although LLMs demonstrate relevant reasoning skills in connecting concepts, specialised models are currently a better choice for tasks requiring extracting complex domain-specific entities like materials. These insights provide initial guidance applicable to other materials science sub-domains in future work.",
    "venue": "Science and Technology of Advanced Materials: Methods",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "https://www.tandfonline.com/doi/pdf/10.1080/27660400.2024.2356506?needAccess=true",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-19",
    "authors": [
      {
        "authorId": "72168167",
        "name": "Luca Foppiano"
      },
      {
        "authorId": "2280138716",
        "name": "Guillaume Lambard"
      },
      {
        "authorId": "2243129172",
        "name": "Toshiyuki Amagasa"
      },
      {
        "authorId": "2242961177",
        "name": "Masashi Ishii"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.1415686865115
  },
  {
    "paperId": "2f3af4552fd75ed4f09a1b48d75b1ad5f352f078",
    "url": "https://www.semanticscholar.org/paper/2f3af4552fd75ed4f09a1b48d75b1ad5f352f078",
    "title": "Language Models Encode Numbers Using Digit Representations in Base 10",
    "abstract": "Large language models (LLMs) frequently make errors when handling even simple numerical problems, such as comparing two small numbers. A natural hypothesis is that these errors stem from how LLMs represent numbers, and specifically, whether their representations of numbers capture their numeric values. We tackle this question from the observation that LLM errors on numerical tasks are often distributed across \\textit{the digits} of the answer rather than normally around \\textit{its numeric value}. Through a series of probing experiments and causal interventions, we show that LLMs internally represent numbers with individual circular representations per-digit in base 10. This digit-wise representation, as opposed to a value representation, sheds light on the error patterns of models on tasks involving numerical reasoning and could serve as a basis for future studies on analyzing numerical mechanisms in LLMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-15",
    "authors": [
      {
        "authorId": "2325952877",
        "name": "Amit Arnold Levy"
      },
      {
        "authorId": "22245981",
        "name": "Mor Geva"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "de5d87fe1c35906c3f84d3da5d9f854922d66302",
    "url": "https://www.semanticscholar.org/paper/de5d87fe1c35906c3f84d3da5d9f854922d66302",
    "title": "ConstraintChecker: A Plugin for Large Language Models to Reason on Commonsense Knowledge Bases",
    "abstract": "Reasoning over Commonsense Knowledge Bases (CSKB), i.e. CSKB reasoning, has been explored as a way to acquire new commonsense knowledge based on reference knowledge in the original CSKBs and external prior knowledge.Despite the advancement of Large Language Models (LLM) and prompt engineering techniques in various reasoning tasks, they still struggle to deal with CSKB reasoning.One of the problems is that it is hard for them to acquire explicit relational constraints in CSKBs from only in-context exemplars, due to a lack of symbolic reasoning capabilities (CITATION).To this end, we proposed **ConstraintChecker**, a plugin over prompting techniques to provide and check explicit constraints.When considering a new knowledge instance, ConstraintChecker employs a rule-based module to produce a list of constraints, then it uses a zero-shot learning module to check whether this knowledge instance satisfies all constraints.The acquired constraint-checking result is then aggregated with the output of the main prompting technique to produce the final output.Experimental results on CSKB Reasoning benchmarks demonstrate the effectiveness of our method by bringing consistent improvements over all prompting methods.",
    "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-01-25",
    "authors": [
      {
        "authorId": "2187874252",
        "name": "Quyet V. Do"
      },
      {
        "authorId": "2044202073",
        "name": "Tianqing Fang"
      },
      {
        "authorId": "50826757",
        "name": "Shizhe Diao"
      },
      {
        "authorId": "2187830802",
        "name": "Zhaowei Wang"
      },
      {
        "authorId": "2241325169",
        "name": "Yangqiu Song"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "e9d7b6a0d8e1160a1b557bff9bd86d94d3c38bc0",
    "url": "https://www.semanticscholar.org/paper/e9d7b6a0d8e1160a1b557bff9bd86d94d3c38bc0",
    "title": "A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration",
    "abstract": "With ChatGPT’s release, conversational prompting has become the most popular form of human-LLM interaction. However, its effectiveness is limited for more complex tasks involving reasoning, creativity, and iteration. Through a systematic analysis of HCI papers published since 2021, we identified four key phases in the human-LLM interaction flow—planning, facilitating, iterating, and testing—to precisely understand the dynamics of this process. Additionally, we have developed a taxonomy of four primary interaction modes: Mode 1: Standard Prompting, Mode 2: User Interface, Mode 3: Context-based, and Mode 4: Agent Facilitator. This taxonomy was further enriched using the “5W1H” guideline method, which involved a detailed examination of definitions, participant roles (Who), the phases that happened (When), human objectives and LLM abilities (What), and the mechanics of each interaction mode (How). We anticipate this taxonomy will contribute to the future design and evaluation of human-LLM interaction.",
    "venue": "CHI Extended Abstracts",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3613905.3650786",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2024-03-30",
    "authors": [
      {
        "authorId": "2280287694",
        "name": "Jie Gao"
      },
      {
        "authorId": "2214755899",
        "name": "Simret Araya Gebreegziabher"
      },
      {
        "authorId": "9511005",
        "name": "K. Choo"
      },
      {
        "authorId": "2257852932",
        "name": "T. Li"
      },
      {
        "authorId": "2066290003",
        "name": "S. Perrault"
      },
      {
        "authorId": "2257866472",
        "name": "Thomas W. Malone"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "13a02a5fc06c58b30438dfe003dfea02ac6f7643",
    "url": "https://www.semanticscholar.org/paper/13a02a5fc06c58b30438dfe003dfea02ac6f7643",
    "title": "An Intelligent LLM-Powered Personalized Assistant for Digital Banking Using LangGraph and Chain of Thoughts",
    "abstract": "Text-based applications and chatbots are increasingly popular for delivering banking services and educational tools, offering convenient and efficient solutions for users. Whereas, personalized assistants have transformed user engagement in the digital banking space by utilizing Large Language Models (LLMs) in conjunction with autonomous agents. This study proposes the development of an intelligent personalized assistant for digital banking, utilizing a multi-agent framework based on the LangGraph and Chain of Thoughts (COT) prompting. While COT guarantees context-aware replies, the LangGraph design maps characteristics to nodes to improve user interactions. The objectives of this system are to enhance task efficiency and elevate the capabilities of digital banking assistants. We present a customizable digital banking system powered by LLM-based models, designed to deliver an interactive and personalized banking experience. The system supports a range of services, including adding money, transferring funds, paying bills, accessing telco services like mobile recharge, managing savings interest rates, DPS schemes, fixed deposits, and answering FAQs related to banking information. Therefore, integrating COT for logical reasoning enhances the effectiveness of multi-agent systems, as each single agent benefits from the structured reasoning process. In addition, LangGraph is employed for structured data management, enabling the assistant to support and accelerate various digital banking processes efficiently. The code implementation of this work is available for public access at: https://github.com/srv-sh/digital_agent.",
    "venue": "Symposium on Intelligent Systems and Informatics",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-19",
    "authors": [
      {
        "authorId": "48990966",
        "name": "Md. Easin Arafat"
      },
      {
        "authorId": "2241874112",
        "name": "Sourav Saha"
      },
      {
        "authorId": "2332357167",
        "name": "Tamás Orosz"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "503c85a9df91de5dace92d6c5ade8627701f08ac",
    "url": "https://www.semanticscholar.org/paper/503c85a9df91de5dace92d6c5ade8627701f08ac",
    "title": "Large language model empowered participatory urban planning",
    "abstract": "Participatory urban planning is the mainstream of modern urban planning and involves the active engagement of different stakeholders. However, the traditional participatory paradigm encounters challenges in time and manpower, while the generative planning tools fail to provide adjustable and inclusive solutions. This research introduces an innovative urban planning approach integrating Large Language Models (LLMs) within the participatory process. The framework, based on the crafted LLM agent, consists of role-play, collaborative generation, and feedback iteration, solving a community-level land-use task catering to 1000 distinct interests. Empirical experiments in diverse urban communities exhibit LLM's adaptability and effectiveness across varied planning scenarios. The results were evaluated on four metrics, surpassing human experts in satisfaction and inclusion, and rivaling state-of-the-art reinforcement learning methods in service and ecology. Further analysis shows the advantage of LLM agents in providing adjustable and inclusive solutions with natural language reasoning and strong scalability. While implementing the recent advancements in emulating human behavior for planning, this work envisions both planners and citizens benefiting from low-cost, efficient LLM agents, which is crucial for enhancing participation and realizing participatory urban planning.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-24",
    "authors": [
      {
        "authorId": "2149133619",
        "name": "Zhilun Zhou"
      },
      {
        "authorId": "2276812687",
        "name": "Yuming Lin"
      },
      {
        "authorId": "2279652553",
        "name": "Yong Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "7e05833d34eb3dd9a3a4d8eff8a83dfcef2f1804",
    "url": "https://www.semanticscholar.org/paper/7e05833d34eb3dd9a3a4d8eff8a83dfcef2f1804",
    "title": "Extend Model Merging from Fine-Tuned to Pre-Trained Large Language Models via Weight Disentanglement",
    "abstract": "Merging Large Language Models (LLMs) aims to amalgamate multiple homologous LLMs into one with all the capabilities. Ideally, any LLMs sharing the same backbone should be mergeable, irrespective of whether they are Fine-Tuned (FT) with minor parameter changes or Pre-Trained (PT) with substantial parameter shifts. However, existing methods often manually assign the model importance, rendering them feasible only for LLMs with similar parameter alterations, such as multiple FT LLMs. The diverse parameter changed ranges between FT and PT LLMs pose challenges for current solutions in empirically determining the optimal combination. In this paper, we make a pioneering effort to broaden the applicability of merging techniques from FT to PT LLMs. We initially examine the efficacy of current methods in merging FT and PT LLMs, discovering that they struggle to deal with PT LLMs. Subsequently, we introduce an approach based on WeIght DisENtanglement (WIDEN) to effectively extend the merging scope, which first disentangles model weights into magnitude and direction components, and then performs adaptive fusion by considering their respective contributions. In the experiments, we merge Qwen1.5-Chat (an FT LLM with instruction-following skills) with Sailor (a PT LLM with multilingual abilities) across 7B and 14B model scales. Results reveal that: (1) existing solutions usually fail when merging Sailor, either losing both abilities or only retaining instruction-following skills; (2) WIDEN successfully injects the multilingual abilities of Sailor into Qwen1.5-Chat and make it proficient in Southeast Asian languages, achieving enhancements in the fundamental capabilities. In light of previous research, we also merge multiple 13B FT LLMs and observe that WIDEN achieves a balanced amalgamation of instruction following, mathematical reasoning, and code generation skills.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-06",
    "authors": [
      {
        "authorId": "2265527327",
        "name": "Le Yu"
      },
      {
        "authorId": "2249451832",
        "name": "Bowen Yu"
      },
      {
        "authorId": "2288351000",
        "name": "Haiyang Yu"
      },
      {
        "authorId": "2257407873",
        "name": "Fei Huang"
      },
      {
        "authorId": "2287833084",
        "name": "Yongbin Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "3cbfabf0f5335eb0c58ffdffe3cae0275d6b1ad2",
    "url": "https://www.semanticscholar.org/paper/3cbfabf0f5335eb0c58ffdffe3cae0275d6b1ad2",
    "title": "Get Confused Cautiously: Textual Sequence Memorization Erasure with Selective Entropy Maximization",
    "abstract": "Large Language Models (LLMs) have been found to memorize and recite some of the textual sequences from their training set verbatim, raising broad concerns about privacy and copyright issues when using LLMs. This Textual Sequence Memorization (TSM) phenomenon leads to a high demand to regulate LLM output to prevent it from generating certain memorized text to meet user requirements. However, our empirical study reveals that existing methods for TSM erasure fail to forget massive memorized samples without substantially jeopardizing the model utility. To achieve a better trade-off between the effectiveness of TSM erasure and model utility in LLMs, our paper proposes a new framework based on Entropy Maximization with Selective Optimization (EMSO), where the updated weights are chosen with a novel contrastive gradient metric without any participation of additional model or data. Our analysis shows that training with the entropy maximization loss has a more stable optimization process and better keeps model utility than existing methods. The contrastive gradient metric localizes the most influential weight for TSM erasure by taking both the gradient magnitude and direction into consideration. Extensive experiments across three model scales demonstrate that our method excels in handling large-scale forgetting requests while preserving model ability in language generation and reasoning.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-08-09",
    "authors": [
      {
        "authorId": "2315857890",
        "name": "Zhaohan Zhang"
      },
      {
        "authorId": "2315862064",
        "name": "Ziquan Liu"
      },
      {
        "authorId": "2306252618",
        "name": "Ioannis Patras"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "0759076827798ca3e4a4b3c0002bc5687576a9a9",
    "url": "https://www.semanticscholar.org/paper/0759076827798ca3e4a4b3c0002bc5687576a9a9",
    "title": "MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering",
    "abstract": "In today's fast-paced industry, professionals face the challenge of summarizing a large number of documents and extracting vital information from them on a daily basis. These metrics are frequently hidden away in tables and/or their nested hyperlinks. To address this challenge, the approach of Table Question Answering (QA) has been developed to extract the relevant information. However, traditional Table QA training tasks that provide a table and an answer(s) from a gold cell coordinate(s) for a question may not always ensure extracting the accurate answer(s). Recent advancements in Large Language Models (LLMs) have opened up new possibilities for extracting information from tabular data using prompts. In this paper, we introduce the Multi-hop Few-shot Open Rich Table QA (MFORT-QA) approach, which consists of two major steps. The first step involves Few-Shot Learning (FSL), where relevant tables and associated contexts of hyperlinks are retrieved based on a given question. The retrieved content is then used to construct few-shot prompts as inputs to an LLM, such as ChatGPT. To tackle the challenge of answering complex questions, the second step leverages Chain-of-thought (CoT) prompting to decompose the complex question into a sequential chain of questions and reasoning thoughts in a multi-hop manner. Retrieval-Augmented Generation (RAG) enhances this process by retrieving relevant tables and contexts of hyperlinks that are relevant to the resulting reasoning thoughts and questions. These additional contexts are then used to supplement the prompt used in the first step, resulting in more accurate answers from an LLM. Empirical results from OTT-QA demonstrate that our abstractive QA approach significantly improves the accuracy of extractive Table QA methods.",
    "venue": "International Conference on Computing and Artificial Intelligence",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-28",
    "authors": [
      {
        "authorId": "2293720207",
        "name": "Che Guan"
      },
      {
        "authorId": "2293768415",
        "name": "Mengyu Huang"
      },
      {
        "authorId": "2294328049",
        "name": "Peng Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "b8b3200548e1b0c25ab146fdc6066301ba641867",
    "url": "https://www.semanticscholar.org/paper/b8b3200548e1b0c25ab146fdc6066301ba641867",
    "title": "LaMI: Large Language Models for Multi-Modal Human-Robot Interaction",
    "abstract": "This paper presents an innovative large language model (LLM)-based robotic system for enhancing multi-modal human-robot interaction (HRI). Traditional HRI systems relied on complex designs for intent estimation, reasoning, and behavior generation, which were resource-intensive. In contrast, our system empowers researchers and practitioners to regulate robot behavior through three key aspects: providing high-level linguistic guidance, creating \"atomic actions\" and expressions the robot can use, and offering a set of examples. Implemented on a physical robot, it demonstrates proficiency in adapting to multi-modal inputs and determining the appropriate manner of action to assist humans with its arms, following researchers’ defined guidelines. Simultaneously, it coordinates the robot’s lid, neck, and ear movements with speech output to produce dynamic, multi-modal expressions. This showcases the system’s potential to revolutionize HRI by shifting from conventional, manual state-and-flow design methods to an intuitive, guidance-based, and example-driven approach. Supplementary material can be found at https://hri-eu.github.io/Lami/",
    "venue": "CHI Extended Abstracts",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3613905.3651029",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2024-01-26",
    "authors": [
      {
        "authorId": "2144448837",
        "name": "Chao Wang"
      },
      {
        "authorId": "2257039476",
        "name": "Stephan Hasler"
      },
      {
        "authorId": "4338399",
        "name": "Daniel Tanneberg"
      },
      {
        "authorId": "2257038646",
        "name": "Felix Ocker"
      },
      {
        "authorId": "1759554",
        "name": "F. Joublin"
      },
      {
        "authorId": "2832005",
        "name": "Antonello Ceravola"
      },
      {
        "authorId": "2187928830",
        "name": "Joerg Deigmoeller"
      },
      {
        "authorId": "1713430",
        "name": "M. Gienger"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "aaf5f54941808c654b8f15b3a24bfe9b536ab2f4",
    "url": "https://www.semanticscholar.org/paper/aaf5f54941808c654b8f15b3a24bfe9b536ab2f4",
    "title": "GlitchProber: Advancing Effective Detection and Mitigation of Glitch Tokens in Large Language Models",
    "abstract": "Large language models (LLMs) have achieved unprecedented success in the field of natural language processing. However, the black-box nature of their internal mechanisms has brought many concerns about their trustworthiness and interpretability. Recent research has discovered a class of abnormal tokens in the model’s vocabulary space and named them \"glitch tokens\". Those tokens, once included in the input, may induce the model to produce incorrect, irrelevant, or even harmful results, drastically undermining the reliability and practicality of LLMs.In this work, we aim to enhance the understanding of glitch tokens and propose techniques for their detection and mitigation. We first reveal the characteristic features induced by glitch tokens on LLMs, which are evidenced by significant deviations in the distributions of attention patterns and dynamic information from intermediate model layers. Based on the insights, we develop GlitchProber, a tool for efficient glitch token detection and mitigation. GlitchProber utilizes small-scale sampling, principal component analysis for accelerated feature extraction, and a simple classifier for efficient vocabulary screening. Taking one step further, GlitchProber rectifies abnormal model intermediate layer values to mitigate the destructive effects of glitch tokens. Evaluated on five mainstream open-source LLMs, GlitchProber demonstrates higher efficiency, precision, and recall compared to existing approaches, with an average F1 score of 0.86 and an average repair rate of 50.06%. GlitchProber unveils a novel path to address the challenges posed by glitch tokens and inspires future research to-ward more robust and interpretable LLMs. Our code is available at https://github.com/LLM-Integrity-Guard/GlitchProber.CCS CONCEPTS• Computing methodologies → Knowledge representation and reasoning.",
    "venue": "International Conference on Automated Software Engineering",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-08-09",
    "authors": [
      {
        "authorId": "2271977255",
        "name": "Zhibo Zhang"
      },
      {
        "authorId": "2315808472",
        "name": "Wuxia Bai"
      },
      {
        "authorId": "2268093091",
        "name": "Yuxi Li"
      },
      {
        "authorId": "143654918",
        "name": "M. H. Meng"
      },
      {
        "authorId": "2265727621",
        "name": "Kailong Wang"
      },
      {
        "authorId": "2313867161",
        "name": "Ling Shi"
      },
      {
        "authorId": "2315890847",
        "name": "Li Li"
      },
      {
        "authorId": "2315891108",
        "name": "Jun Wang"
      },
      {
        "authorId": "2269008788",
        "name": "Haoyu Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "e46ea277c822448841fc2ab0fc505d8b74e7ed29",
    "url": "https://www.semanticscholar.org/paper/e46ea277c822448841fc2ab0fc505d8b74e7ed29",
    "title": "Semantic Graphs for Syntactic Simplification: A Revisit from the Age of LLM",
    "abstract": "Symbolic sentence meaning representations, such as AMR (Abstract Meaning Representation) provide expressive and structured semantic graphs that act as intermediates that simplify downstream NLP tasks. However, the instruction-following capability of large language models (LLMs) offers a shortcut to effectively solve NLP tasks, questioning the utility of semantic graphs. Meanwhile, recent work has also shown the difficulty of using meaning representations merely as a helpful auxiliary for LLMs. We revisit the position of semantic graphs in syntactic simplification, the task of simplifying sentence structures while preserving their meaning, which requires semantic understanding, and evaluate it on a new complex and natural dataset. The AMR-based method that we propose, AMRS^3, demonstrates that state-of-the-art meaning representations can lead to easy-to-implement simplification methods with competitive performance and unique advantages in cost, interpretability, and generalization. With AMRS^3 as an anchor, we discover that syntactic simplification is a task where semantic graphs are helpful in LLM prompting. We propose AMRCoC prompting that guides LLMs to emulate graph algorithms for explicit symbolic reasoning on AMR graphs, and show its potential for improving LLM on semantic-centered tasks like syntactic simplification.",
    "venue": "Workshop on Graph-based Methods for Natural Language Processing",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-04",
    "authors": [
      {
        "authorId": "51134923",
        "name": "Peiran Yao"
      },
      {
        "authorId": "2210279255",
        "name": "Kostyantyn Guzhva"
      },
      {
        "authorId": "2303401800",
        "name": "Denilson Barbosa"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "220bff7f521de8d37d95680a4d94343db5aae3e2",
    "url": "https://www.semanticscholar.org/paper/220bff7f521de8d37d95680a4d94343db5aae3e2",
    "title": "Calico: Automated Knowledge Calibration and Diagnosis for Elevating AI Mastery in Code Tasks",
    "abstract": "Recent advancements in large language models (LLMs) have exhibited promising capabilities in addressing various tasks such as defect detection and program repair. Despite their prevalence, LLMs still face limitations in effectively handling these tasks. Common strategies to adapt them and improve their performance for specific tasks involve fine-tuning models based on user data or employing in-context learning with examples of desired inputs and outputs. However, they pose challenges for practical adoption due to the need for extensive computational resources, high-quality data, and continuous maintenance. Furthermore, neither strategy can explain or reason about the deficiencies of LLMs in the given tasks. We propose C alico to address the high cost of fine-tuning, eliminate the necessity for task-specific examples, and provide explanations of LLM deficiency. At the heart of Calico is an evolutionary approach that interleaves knowledge calibration and AI deficiency diagnosis. The key essence of Calico is as follows. First, it focuses on identifying knowledge gaps in LLMs’ program comprehension. Second, it conducts automated code refactoring to integrate the overlooked knowledge into the source code for mitigating those gaps. Third, it employs what-if analysis and counterfactual reasoning to determine a minimum set of overlooked knowledge necessary to improve the performance of LLMs in code tasks. We have extensively evaluated Calico over 8,938 programs on three most commonly seen code tasks. Our experimental results show that vanilla ChatGPT cannot fully understand code structures. With knowledge calibration, Calico improves it by 20% and exhibits comparable proficiency compared to fine-tuned LLMs. Deficiency diagnosis contributes to 8% reduction in program sizes while ensuring performance. These impressive results demonstrate the feasibility of utilizing a vanilla LLM for automated software engineering (SE) tasks, thereby avoiding the high computational costs associated with a fine-tuned model.",
    "venue": "International Symposium on Software Testing and Analysis",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-09-11",
    "authors": [
      {
        "authorId": "2321151080",
        "name": "Yuxin Qiu"
      },
      {
        "authorId": "2249068163",
        "name": "Jie Hu"
      },
      {
        "authorId": "2301257566",
        "name": "Qian Zhang"
      },
      {
        "authorId": "2313468215",
        "name": "Heng Yin"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "1035dfceeebd3dd9ba52a8162eeda670a432c56e",
    "url": "https://www.semanticscholar.org/paper/1035dfceeebd3dd9ba52a8162eeda670a432c56e",
    "title": "Self-Taught Evaluators",
    "abstract": "Model-based evaluation is at the heart of successful model development -- as a reward model for training, and as a replacement for human evaluation. To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve. In this work, we present an approach that aims to im-prove evaluators without human annotations, using synthetic training data only. Starting from unlabeled instructions, our iterative self-improvement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions. Without any labeled preference data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms commonly used LLM judges such as GPT-4 and matches the performance of the top-performing reward models trained with labeled examples.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-05",
    "authors": [
      {
        "authorId": "2261365525",
        "name": "Tianlu Wang"
      },
      {
        "authorId": "2308102420",
        "name": "Ilia Kulikov"
      },
      {
        "authorId": "2290916129",
        "name": "Olga Golovneva"
      },
      {
        "authorId": "2308246369",
        "name": "Ping Yu"
      },
      {
        "authorId": "2281387619",
        "name": "Weizhe Yuan"
      },
      {
        "authorId": "2173509991",
        "name": "Jane Dwivedi-Yu"
      },
      {
        "authorId": "46230016",
        "name": "Richard Yuanzhe Pang"
      },
      {
        "authorId": "1399159921",
        "name": "Maryam Fazel-Zarandi"
      },
      {
        "authorId": "2267341626",
        "name": "Jason Weston"
      },
      {
        "authorId": "2315344189",
        "name": "Xian Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "039f82bdfac8aef61f64c3dfa4dc54ac75e418b2",
    "url": "https://www.semanticscholar.org/paper/039f82bdfac8aef61f64c3dfa4dc54ac75e418b2",
    "title": "ChatGPT as a Factual Inconsistency Evaluator for Text Summarization",
    "abstract": "The performance of text summarization has been greatly boosted by pre-trained language models. A main concern of existing methods is that most generated summaries are not factually inconsistent with their source documents. To alleviate the problem, many efforts have focused on developing effective factuality evaluation metrics based on natural language inference, question answering, and syntactic dependency et al. However, these approaches are limited by either their high computational complexity or the uncertainty introduced by multi-component pipelines, resulting in only partial agreement with human judgement. Most recently, large language models(LLMs) have shown excellent performance in not only text generation but also language comprehension. In this paper, we particularly explore ChatGPT's ability to evaluate factual inconsistency under a zero-shot setting by examining it on both coarse-grained and fine-grained evaluation tasks including binary entailment inference, summary ranking, and consistency rating. Experimental results indicate that ChatGPT generally outperforms previous evaluation metrics across the three tasks, indicating its great potential for factual inconsistency evaluation. However, a closer inspection of ChatGPT's output reveals certain limitations including its preference for more lexically similar candidates, false reasoning, and inadequate understanding of instructions.",
    "venue": "",
    "year": 2023,
    "citationCount": 63,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-03-27",
    "authors": [
      {
        "authorId": "31689330",
        "name": "Zheheng Luo"
      },
      {
        "authorId": "145229872",
        "name": "Qianqian Xie"
      },
      {
        "authorId": "1881965",
        "name": "S. Ananiadou"
      }
    ],
    "source": "semantic_scholar",
    "score": 132.38324625039508
  },
  {
    "paperId": "b2c81c7ca020e46381e85f620b7fe793d43208b4",
    "url": "https://www.semanticscholar.org/paper/b2c81c7ca020e46381e85f620b7fe793d43208b4",
    "title": "Large-language models facilitate discovery of the molecular signatures regulating sleep and activity",
    "abstract": null,
    "venue": "Nature Communications",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "https://www.nature.com/articles/s41467-024-48005-w.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-01",
    "authors": [
      {
        "authorId": "101725137",
        "name": "Di Peng"
      },
      {
        "authorId": "2149969502",
        "name": "Liubin Zheng"
      },
      {
        "authorId": "2169009715",
        "name": "Dan Liu"
      },
      {
        "authorId": "2154407164",
        "name": "Cheng Han"
      },
      {
        "authorId": "2299401309",
        "name": "Xin Wang"
      },
      {
        "authorId": "2299316741",
        "name": "Yan Yang"
      },
      {
        "authorId": "2299504851",
        "name": "Li Song"
      },
      {
        "authorId": "2269768779",
        "name": "Miaoying Zhao"
      },
      {
        "authorId": "2299891714",
        "name": "Yanfeng Wei"
      },
      {
        "authorId": "2299329662",
        "name": "Jiayi Li"
      },
      {
        "authorId": "2300815298",
        "name": "Xiaoxue Ye"
      },
      {
        "authorId": "2299903276",
        "name": "Yuxiang Wei"
      },
      {
        "authorId": "2299748536",
        "name": "Zihao Feng"
      },
      {
        "authorId": "2166330991",
        "name": "Xinhe Huang"
      },
      {
        "authorId": "2150263763",
        "name": "Miaomiao Chen"
      },
      {
        "authorId": "2121538218",
        "name": "Y. Gou"
      },
      {
        "authorId": "2114921104",
        "name": "Yu Xue"
      },
      {
        "authorId": "2282133209",
        "name": "Luoying Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 92.95836866004329
  },
  {
    "paperId": "02033e83ff310f35e4623bd339982c52d926f2d5",
    "url": "https://www.semanticscholar.org/paper/02033e83ff310f35e4623bd339982c52d926f2d5",
    "title": "Give us the Facts: Enhancing Large Language Models With Knowledge Graphs for Fact-Aware Language Modeling",
    "abstract": "Recently, ChatGPT, a representative large language model (LLM), has gained considerable attention. Due to their powerful emergent abilities, recent LLMs are considered as a possible alternative to structured knowledge bases like knowledge graphs (KGs). However, while LLMs are proficient at learning probabilistic language patterns and engaging in conversations with humans, they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents. To overcome these limitations, researchers have proposed enhancing data-driven PLMs with knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus improving their performance in generating texts requiring factual knowledge and providing more informed responses to user queries. This paper reviews the studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced pre-trained language models (KGPLMs) as well as their applications. Inspired by existing studies on KGPLM, this paper proposes enhancing LLMs with KGs by developing knowledge graph-enhanced large language models (KGLLMs). KGLLM provides a solution to enhance LLMs’ factual reasoning ability, opening up new avenues for LLM research.",
    "venue": "IEEE Transactions on Knowledge and Data Engineering",
    "year": 2023,
    "citationCount": 61,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2306.11489",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-06-20",
    "authors": [
      {
        "authorId": "40577530",
        "name": "Lin F. Yang"
      },
      {
        "authorId": "2144212252",
        "name": "Hongyang Chen"
      },
      {
        "authorId": "2193503627",
        "name": "Zhao Li"
      },
      {
        "authorId": "2117434160",
        "name": "Xiao Ding"
      },
      {
        "authorId": "1748808",
        "name": "Xindong Wu"
      }
    ],
    "source": "semantic_scholar",
    "score": 137.90701577567637
  },
  {
    "paperId": "5f66d1a667eec13b5d337c3fc5619bcef95092bd",
    "url": "https://www.semanticscholar.org/paper/5f66d1a667eec13b5d337c3fc5619bcef95092bd",
    "title": "Universal Self-Consistency for Large Language Model Generation",
    "abstract": "Self-consistency with chain-of-thought prompting (CoT) has demonstrated remarkable performance gains on various challenging tasks, by utilizing multiple reasoning paths sampled from large language models (LLMs). However, self-consistency relies on the answer extraction process to aggregate multiple solutions, which is not applicable to free-form answers. In this work, we propose Universal Self-Consistency (USC), which leverages LLMs themselves to select the most consistent answer among multiple candidates. We evaluate USC on a variety of benchmarks, including mathematical reasoning, code generation, long-context summarization, and open-ended question answering. On open-ended generation tasks where the original self-consistency method is not applicable, USC effectively utilizes multiple samples and improves the performance. For mathematical reasoning, USC matches the standard self-consistency performance without requiring the answer formats to be similar. Finally, without access to execution results, USC also matches the execution-based voting performance on code generation.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 43,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-29",
    "authors": [
      {
        "authorId": "2238263119",
        "name": "Xinyun Chen"
      },
      {
        "authorId": "2205544066",
        "name": "Renat Aksitov"
      },
      {
        "authorId": "2268672727",
        "name": "Uri Alon"
      },
      {
        "authorId": "2274932077",
        "name": "Jie Ren"
      },
      {
        "authorId": "2268673324",
        "name": "Kefan Xiao"
      },
      {
        "authorId": "2268673722",
        "name": "Pengcheng Yin"
      },
      {
        "authorId": "2268674039",
        "name": "Sushant Prakash"
      },
      {
        "authorId": "2268672977",
        "name": "Charles Sutton"
      },
      {
        "authorId": "2238394232",
        "name": "Xuezhi Wang"
      },
      {
        "authorId": "2256313467",
        "name": "Denny Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 126.76284450877392
  },
  {
    "paperId": "d7c5e66318dfe6d5094e2cf9fc1ded43117095b0",
    "url": "https://www.semanticscholar.org/paper/d7c5e66318dfe6d5094e2cf9fc1ded43117095b0",
    "title": "EffiQA: Efficient Question-Answering with Strategic Multi-Model Collaboration on Knowledge Graphs",
    "abstract": "While large language models (LLMs) have shown remarkable capabilities in natural language processing, they struggle with complex, multi-step reasoning tasks involving knowledge graphs (KGs). Existing approaches that integrate LLMs and KGs either underutilize the reasoning abilities of LLMs or suffer from prohibitive computational costs due to tight coupling. To address these limitations, we propose a novel collaborative framework named EffiQA that can strike a balance between performance and efficiency via an iterative paradigm. EffiQA consists of three stages: global planning, efficient KG exploration, and self-reflection. Specifically, EffiQA leverages the commonsense capability of LLMs to explore potential reasoning pathways through global planning. Then, it offloads semantic pruning to a small plug-in model for efficient KG exploration. Finally, the exploration results are fed to LLMs for self-reflection to further improve the global planning and efficient KG exploration. Empirical evidence on multiple KBQA benchmarks shows EffiQA's effectiveness, achieving an optimal balance between reasoning accuracy and computational costs. We hope the proposed new framework will pave the way for efficient, knowledge-intensive querying by redefining the integration of LLMs and KGs, fostering future research on knowledge-based question answering.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-03",
    "authors": [
      {
        "authorId": "2304549951",
        "name": "Zixuan Dong"
      },
      {
        "authorId": "2304423465",
        "name": "Baoyun Peng"
      },
      {
        "authorId": "2304517047",
        "name": "Yufei Wang"
      },
      {
        "authorId": "2304899507",
        "name": "Jia Fu"
      },
      {
        "authorId": "2301198061",
        "name": "Xiaodong Wang"
      },
      {
        "authorId": "2301154246",
        "name": "Yongxue Shan"
      },
      {
        "authorId": "2301171187",
        "name": "Xin Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "5f5613630ad62a8db6374ad2ab4a15fb51152a2d",
    "url": "https://www.semanticscholar.org/paper/5f5613630ad62a8db6374ad2ab4a15fb51152a2d",
    "title": "Long-horizon Locomotion and Manipulation on a Quadrupedal Robot with Large Language Models",
    "abstract": "We present a large language model (LLM) based system to empower quadrupedal robots with problem-solving abilities for long-horizon tasks beyond short-term motions. Long-horizon tasks for quadrupeds are challenging since they require both a high-level understanding of the semantics of the problem for task planning and a broad range of locomotion and manipulation skills to interact with the environment. Our system builds a high-level reasoning layer with large language models, which generates hybrid discrete-continuous plans as robot code from task descriptions. It comprises multiple LLM agents: a semantic planner for sketching a plan, a parameter calculator for predicting arguments in the plan, and a code generator to convert the plan into executable robot code. At the low level, we adopt reinforcement learning to train a set of motion planning and control skills to unleash the flexibility of quadrupeds for rich environment interactions. Our system is tested on long-horizon tasks that are infeasible to complete with one single skill. Simulation and real-world experiments show that it successfully figures out multi-step strategies and demonstrates non-trivial behaviors, including building tools or notifying a human for help. Demos are available on our project page: https://sites.google.com/view/long-horizon-robot.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-08",
    "authors": [
      {
        "authorId": "2212795288",
        "name": "Yutao Ouyang"
      },
      {
        "authorId": "2266358296",
        "name": "Jinhan Li"
      },
      {
        "authorId": "2110441798",
        "name": "Yunfei Li"
      },
      {
        "authorId": "1491078398",
        "name": "Zhongyu Li"
      },
      {
        "authorId": "2296946468",
        "name": "Chao Yu"
      },
      {
        "authorId": "144116765",
        "name": "K. Sreenath"
      },
      {
        "authorId": "2293552963",
        "name": "Yi Wu"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "58ba42ffc34dd24d6a77fe58c8973b3533c369cd",
    "url": "https://www.semanticscholar.org/paper/58ba42ffc34dd24d6a77fe58c8973b3533c369cd",
    "title": "Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views",
    "abstract": "Large Language Models (LLMs) have achieved great success in various natural language tasks. It has aroused much interest in evaluating the specific reasoning capability of LLMs, such as multilingual reasoning and mathematical reasoning. However, as one of the key reasoning perspectives, logical reasoning capability has not yet been thoroughly evaluated. In this work, we aim to bridge those gaps and provide comprehensive evaluations. Firstly, to offer systematic evaluations, this paper selects fifteen typical logical reasoning datasets and organizes them into deductive, inductive, abductive and mixed-form reasoning settings. Considering the comprehensiveness of evaluations, we include three representative LLMs (i.e., text-davinci-003, ChatGPT and BARD) and evaluate them on all selected datasets under zero-shot, one-shot and three-shot settings. Secondly, different from previous evaluations relying only on simple metrics (e.g., accuracy), we propose fine-level evaluations from objective and subjective manners, covering both answers and explanations. Also, to uncover the logical flaws of LLMs, bad cases will be attributed to five error types from two dimensions Evidence Selection Process and Reasoning Process . The former one includes evidence selection error and hallucination , while the latter one includes no reasoning , mistakes of reasoning perspectives and mistakes during reasoning process . Thirdly, to avoid the influences of knowledge bias and purely focus on benchmarking the logical reasoning capability of LLMs, we propose a new dataset with neutral content. It contains 3K samples and covers deductive, inductive and abductive reasoning settings. Based on the in-depth evaluations, this paper finally concludes the ability maps of logical reasoning capability from six dimensions (i.e., correct, rigorous, self-aware, active, oriented and no hallucination). It reflects the pros and cons of LLMs and gives guiding directions for future works.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 46,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2306.09841",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2143943979",
        "name": "Fangzhi Xu"
      },
      {
        "authorId": "144562160",
        "name": "Qika Lin"
      },
      {
        "authorId": "2111759643",
        "name": "Jiawei Han"
      },
      {
        "authorId": "2153707350",
        "name": "Tianzhe Zhao"
      },
      {
        "authorId": "9756930",
        "name": "Jun Liu"
      },
      {
        "authorId": "49943757",
        "name": "E. Cambria"
      }
    ],
    "source": "semantic_scholar",
    "score": 127.75221402565089
  },
  {
    "paperId": "6354f2639d07bf8d5b08a4dcaef4c5db5fe19fdb",
    "url": "https://www.semanticscholar.org/paper/6354f2639d07bf8d5b08a4dcaef4c5db5fe19fdb",
    "title": "ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning",
    "abstract": "This paper assesses the accuracy, reliability and bias of the Large Language Model (LLM) ChatGPT-4 on the text analysis task of classifying the political affiliation of a Twitter poster based on the content of a tweet. The LLM is compared to manual annotation by both expert classifiers and crowd workers, generally considered the gold standard for such tasks. We use Twitter messages from United States politicians during the 2020 election, providing a ground truth against which to measure accuracy. The paper finds that ChatGPT-4 has achieves higher accuracy, higher reliability, and equal or lower bias than the human classifiers. The LLM is able to correctly annotate messages that require reasoning on the basis of contextual knowledge, and inferences around the author's intentions - traditionally seen as uniquely human abilities. These findings suggest that LLM will have substantial impact on the use of textual data in the social sciences, by enabling interpretive research at a scale.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 116,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2304.06588",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-04-13",
    "authors": [
      {
        "authorId": "2265518947",
        "name": "Petter Törnberg"
      }
    ],
    "source": "semantic_scholar",
    "score": 141.43260902196636
  },
  {
    "paperId": "83f9027ff30430ff7d1ca15d565601d290c0da7d",
    "url": "https://www.semanticscholar.org/paper/83f9027ff30430ff7d1ca15d565601d290c0da7d",
    "title": "Applying Large Language Models and Chain-of-Thought for Automatic Scoring",
    "abstract": "This study investigates the application of large language models (LLMs), specifically GPT-3.5 and GPT-4, with Chain-of-Though (CoT) in the automatic scoring of student-written responses to science assessments. We focused on overcoming the challenges of accessibility, technical complexity, and lack of explainability that have previously limited the use of artificial intelligence-based automatic scoring tools among researchers and educators. With a testing dataset comprising six assessment tasks (three binomial and three trinomial) with 1,650 student responses, we employed six prompt engineering strategies to automatically score student responses. The six strategies combined zero-shot or few-shot learning with CoT, either alone or alongside item stem and scoring rubrics. Results indicated that few-shot (acc = .67) outperformed zero-shot learning (acc = .60), with 12.6% increase. CoT, when used without item stem and scoring rubrics, did not significantly affect scoring accuracy (acc = .60). However, CoT prompting paired with contextual item stems and rubrics proved to be a significant contributor to scoring accuracy (13.44% increase for zero-shot; 3.7% increase for few-shot). We found a more balanced accuracy across different proficiency categories when CoT was used with a scoring rubric, highlighting the importance of domain-specific reasoning in enhancing the effectiveness of LLMs in scoring tasks. We also found that GPT-4 demonstrated superior performance over GPT -3.5 in various scoring tasks when combined with the single-call greedy sampling or ensemble voting nucleus sampling strategy, showing 8.64% difference. Particularly, the single-call greedy sampling strategy with GPT-4 outperformed other approaches.",
    "venue": "Computers and Education: Artificial Intelligence",
    "year": 2023,
    "citationCount": 55,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-30",
    "authors": [
      {
        "authorId": "115696344",
        "name": "Gyeong-Geon Lee"
      },
      {
        "authorId": "2258714804",
        "name": "Ehsan Latif"
      },
      {
        "authorId": "2145346360",
        "name": "Xuansheng Wu"
      },
      {
        "authorId": "2256183798",
        "name": "Ninghao Liu"
      },
      {
        "authorId": "2262445470",
        "name": "Xiaoming Zhai"
      }
    ],
    "source": "semantic_scholar",
    "score": 130.38027536102726
  },
  {
    "paperId": "80c44fab16852ea9599411da14de7079c4514172",
    "url": "https://www.semanticscholar.org/paper/80c44fab16852ea9599411da14de7079c4514172",
    "title": "Vision-Language Models in Remote Sensing: Current progress and future trends",
    "abstract": "The remarkable achievements of ChatGPT and Generative Pre-trained Transformer 4 (GPT-4) have sparked a wave of interest and research in the field of large language models (LLMs) for artificial general intelligence (AGI). These models provide intelligent solutions that are closer to human thinking, enabling us to use general artificial intelligence (AI) to solve problems in various applications. However, in the field of remote sensing (RS), the scientific literature on the implementation of AGI remains relatively scant. Existing AI-related research in RS focuses primarily on visual-understanding tasks while neglecting the semantic understanding of the objects and their relationships. This is where vision-LMs (VLMs) excel as they enable reasoning about images and their associated textual descriptions, allowing for a deeper understanding of the underlying semantics. VLMs can go beyond visual recognition of RS images and can model semantic relationships as well as generate natural language descriptions of the image. This makes them better suited for tasks that require both visual and textual understanding, such as image captioning and visual question answering (VQA). This article provides a comprehensive review of the research on VLMs in RS, summarizing the latest progress, highlighting current challenges, and identifying potential research opportunities. Specifically, we review the application of VLMs in mainstream RS tasks, including image captioning, text-based image generation, text-based image retrieval (TBIR), VQA, scene classification, semantic segmentation, and object detection. For each task, we analyze representative works and discuss research progress. Finally, we summarize the limitations of existing works and provide possible directions for future development. This review aims to provide a comprehensive overview of the current research progress of VLMs in RS (see Figure 1), and to inspire further research in this exciting and promising field.",
    "venue": "IEEE Geoscience and Remote Sensing Magazine",
    "year": 2023,
    "citationCount": 49,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2305.05726",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-05-09",
    "authors": [
      {
        "authorId": "2004956599",
        "name": "Congcong Wen"
      },
      {
        "authorId": "22579514",
        "name": "Yuan Hu"
      },
      {
        "authorId": "51300801",
        "name": "Xiang Li"
      },
      {
        "authorId": "34589688",
        "name": "Zhenghang Yuan"
      },
      {
        "authorId": "2125159330",
        "name": "Xiao Xiang Zhu"
      }
    ],
    "source": "semantic_scholar",
    "score": 138.6803450814222
  },
  {
    "paperId": "7c8a6552fe0e3b33456afdac79614e7dc04f0b4d",
    "url": "https://www.semanticscholar.org/paper/7c8a6552fe0e3b33456afdac79614e7dc04f0b4d",
    "title": "ChemReasoner: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback",
    "abstract": "The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and reaction energy barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automatically guide the exploration without human input, providing competitive performance against expert-enumerated chemical descriptor-based implementations. By integrating language-guided reasoning with computational chemistry feedback, our work pioneers AI-accelerated, trustworthy catalyst discovery.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Physics"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-15",
    "authors": [
      {
        "authorId": "2190281477",
        "name": "Henry W Sprueill"
      },
      {
        "authorId": "48870109",
        "name": "Carl N. Edwards"
      },
      {
        "authorId": "39073194",
        "name": "Khushbu Agarwal"
      },
      {
        "authorId": "15751638",
        "name": "Mariefel V. Olarte"
      },
      {
        "authorId": "37794737",
        "name": "Udishnu Sanyal"
      },
      {
        "authorId": "2284668995",
        "name": "Conrad Johnston"
      },
      {
        "authorId": "2284730320",
        "name": "Hongbin Liu"
      },
      {
        "authorId": "2264201811",
        "name": "Heng Ji"
      },
      {
        "authorId": "7617146",
        "name": "Sutanay Choudhury"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "c27c5a272d1112c6c0ce64ff7892752f7ea4b139",
    "url": "https://www.semanticscholar.org/paper/c27c5a272d1112c6c0ce64ff7892752f7ea4b139",
    "title": "BAT: Learning to Reason about Spatial Sounds with Large Language Models",
    "abstract": "Spatial sound reasoning is a fundamental human skill, enabling us to navigate and interpret our surroundings based on sound. In this paper we present BAT, which combines the spatial sound perception ability of a binaural acoustic scene analysis model with the natural language reasoning capabilities of a large language model (LLM) to replicate this innate ability. To address the lack of existing datasets of in-the-wild spatial sounds, we synthesized a binaural audio dataset using AudioSet and SoundSpaces 2.0. Next, we developed SpatialSoundQA, a spatial sound-based question-answering dataset, offering a range of QA tasks that train BAT in various aspects of spatial sound perception and reasoning. The acoustic front end encoder of BAT is a novel spatial audio encoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by itself achieves strong performance across sound event detection, spatial localization, and distance estimation. By integrating Spatial-AST with LLaMA-2 7B model, BAT transcends standard Sound Event Localization and Detection (SELD) tasks, enabling the model to reason about the relationships between the sounds in its environment. Our experiments demonstrate BAT's superior performance on both spatial sound perception and reasoning, showcasing the immense potential of LLMs in navigating and interpreting complex spatial audio environments.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-02",
    "authors": [
      {
        "authorId": "6950445",
        "name": "Zhisheng Zheng"
      },
      {
        "authorId": "30598090",
        "name": "Puyuan Peng"
      },
      {
        "authorId": "2116609277",
        "name": "Ziyang Ma"
      },
      {
        "authorId": "2276453166",
        "name": "Xie Chen"
      },
      {
        "authorId": "2282503317",
        "name": "Eunsol Choi"
      },
      {
        "authorId": "2307105347",
        "name": "David Harwath"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "8728a996ceddd79eda7280608e61b11e5f993715",
    "url": "https://www.semanticscholar.org/paper/8728a996ceddd79eda7280608e61b11e5f993715",
    "title": "Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs",
    "abstract": "Vision Language Models (VLMs) demonstrate remarkable proficiency in addressing a wide array of visual questions, which requires strong perception and reasoning faculties. Assessing these two competencies independently is crucial for model refinement, despite the inherent difficulty due to the intertwined nature of seeing and reasoning in existing VLMs. To tackle this issue, we present Prism, an innovative framework designed to disentangle the perception and reasoning processes involved in visual question solving. Prism comprises two distinct stages: a perception stage that utilizes a VLM to extract and articulate visual information in textual form, and a reasoning stage that formulates responses based on the extracted visual information using a Large Language Model (LLM). This modular design enables the systematic comparison and assessment of both proprietary and open-source VLM for their perception and reasoning strengths. Our analytical framework provides several valuable insights, underscoring Prism's potential as a cost-effective solution for vision-language tasks. By combining a streamlined VLM focused on perception with a powerful LLM tailored for reasoning, Prism achieves superior results in general vision-language tasks while substantially cutting down on training and operational expenses. Quantitative evaluations show that Prism, when configured with a vanilla 2B LLaVA and freely accessible GPT-3.5, delivers performance on par with VLMs $10 \\times$ larger on the rigorous multimodal benchmark MMStar. The project is released at: https://github.com/SparksJoe/Prism.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-20",
    "authors": [
      {
        "authorId": "2267513330",
        "name": "Yu Qiao"
      },
      {
        "authorId": "31463937",
        "name": "Haodong Duan"
      },
      {
        "authorId": "2307562496",
        "name": "Xinyu Fang"
      },
      {
        "authorId": "2307494152",
        "name": "Junming Yang"
      },
      {
        "authorId": "2307935043",
        "name": "Lin Chen"
      },
      {
        "authorId": "2266356137",
        "name": "Songyang Zhang"
      },
      {
        "authorId": "2269797637",
        "name": "Jiaqi Wang"
      },
      {
        "authorId": "2261095726",
        "name": "Dahua Lin"
      },
      {
        "authorId": "2307526907",
        "name": "Kai Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "ce15b4255bcf79781906c7830973a9df39e7fe24",
    "url": "https://www.semanticscholar.org/paper/ce15b4255bcf79781906c7830973a9df39e7fe24",
    "title": "PB-LLM: Partially Binarized Large Language Models",
    "abstract": "This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression. Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs. Specifically, our exploration first uncovers the ineffectiveness of naive applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving low-bit quantization. Thus, PB-LLM filters a small ratio of salient weights during binarization, allocating them to higher-bit storage, i.e., partially-binarization. PB-LLM is extended to recover the capacities of quantized LMMs, by analyzing from the perspective of post-training quantization (PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts from GPTQ, we reconstruct the binarized weight matrix guided by the Hessian matrix and successfully recover the reasoning capacity of PB-LLM in low-bit. Under QAT, we freeze the salient weights during training, explore the derivation of optimal scaling factors crucial for minimizing the quantization error, and propose a scaling mechanism based on this derived scaling strategy for residual binarized weights. Those explorations and the developed methodologies significantly contribute to rejuvenating the performance of low-bit quantized LLMs and present substantial advancements in the field of network binarization for LLMs.The code is available at https://github.com/hahnyuan/BinaryLLM.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 32,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.00034",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-29",
    "authors": [
      {
        "authorId": "2125633957",
        "name": "Yuzhang Shang"
      },
      {
        "authorId": "2256166978",
        "name": "Zhihang Yuan"
      },
      {
        "authorId": "2249848412",
        "name": "Qiang Wu"
      },
      {
        "authorId": "143879884",
        "name": "Zhen Dong"
      }
    ],
    "source": "semantic_scholar",
    "score": 122.4476134219972
  },
  {
    "paperId": "a8b995f0da78a79447dfb18c2337972b044f4239",
    "url": "https://www.semanticscholar.org/paper/a8b995f0da78a79447dfb18c2337972b044f4239",
    "title": "LLM-FP4: 4-Bit Floating-Point Quantized Transformers",
    "abstract": "We propose LLM-FP4 for quantizing both weights and activations in large language models (LLMs) down to 4-bit floating-point values, in a post-training manner. Existing post-training quantization (PTQ) solutions are primarily integer-based and struggle with bit widths below 8 bits. Compared to integer quantization, floating-point (FP) quantization is more flexible and can better handle long-tail or bell-shaped distributions, and it has emerged as a default choice in many hardware platforms. One characteristic of FP quantization is that its performance largely depends on the choice of exponent bits and clipping range. In this regard, we construct a strong FP-PTQ baseline by searching for the optimal quantization parameters. Furthermore, we observe a high inter-channel variance and low intra-channel variance pattern in activation distributions, which adds activation quantization difficulty. We recognize this pattern to be consistent across a spectrum of transformer models designed for diverse tasks, such as LLMs, BERT, and Vision Transformer models. To tackle this, we propose per-channel activation quantization and show that these additional scaling factors can be reparameterized as exponential biases of weights, incurring a negligible cost. Our method, for the first time, can quantize both weights and activations in the LLaMA-13B to only 4-bit and achieves an average score of 63.1 on the common sense zero-shot reasoning tasks, which is only 5.8 lower than the full-precision model, significantly outperforming the previous state-of-the-art by 12.7 points. Code is available at: https://github.com/nbasyl/LLM-FP4.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 42,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.emnlp-main.39.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-25",
    "authors": [
      {
        "authorId": "2220637583",
        "name": "Shih-Yang Liu"
      },
      {
        "authorId": "2109370860",
        "name": "Zechun Liu"
      },
      {
        "authorId": "2261688809",
        "name": "Xijie Huang"
      },
      {
        "authorId": "2261493190",
        "name": "Pingcheng Dong"
      },
      {
        "authorId": "2256381600",
        "name": "Kwang-Ting Cheng"
      }
    ],
    "source": "semantic_scholar",
    "score": 126.41800173540344
  },
  {
    "paperId": "69f2ba0f33a54e01de32c616b64e85d5d7194067",
    "url": "https://www.semanticscholar.org/paper/69f2ba0f33a54e01de32c616b64e85d5d7194067",
    "title": "Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations",
    "abstract": "Large language models (LLMs) are trained to imitate humans to explain human decisions. However, do LLMs explain themselves? Can they help humans build mental models of how LLMs process different inputs? To answer these questions, we propose to evaluate $\\textbf{counterfactual simulatability}$ of natural language explanations: whether an explanation can enable humans to precisely infer the model's outputs on diverse counterfactuals of the explained input. For example, if a model answers\"yes\"to the input question\"Can eagles fly?\"with the explanation\"all birds can fly\", then humans would infer from the explanation that it would also answer\"yes\"to the counterfactual input\"Can penguins fly?\". If the explanation is precise, then the model's answer should match humans' expectations. We implemented two metrics based on counterfactual simulatability: precision and generality. We generated diverse counterfactuals automatically using LLMs. We then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) on two tasks: multi-hop factual reasoning and reward modeling. We found that LLM's explanations have low precision and that precision does not correlate with plausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may not be a sufficient solution.",
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "citationCount": 39,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.08678",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-07-17",
    "authors": [
      {
        "authorId": "2109268730",
        "name": "Yanda Chen"
      },
      {
        "authorId": "51011000",
        "name": "Ruiqi Zhong"
      },
      {
        "authorId": "2218040104",
        "name": "Narutatsu Ri"
      },
      {
        "authorId": "145756130",
        "name": "Chen Zhao"
      },
      {
        "authorId": "2140062900",
        "name": "He He"
      },
      {
        "authorId": "5164568",
        "name": "J. Steinhardt"
      },
      {
        "authorId": "144007938",
        "name": "Zhou Yu"
      },
      {
        "authorId": "145590324",
        "name": "K. McKeown"
      }
    ],
    "source": "semantic_scholar",
    "score": 125.33319181170904
  },
  {
    "paperId": "88b9a3e5882e5dc6dc56d3476948d1c5be67d798",
    "url": "https://www.semanticscholar.org/paper/88b9a3e5882e5dc6dc56d3476948d1c5be67d798",
    "title": "Evaluating Language Models for Mathematics through Interactions",
    "abstract": "The standard methodology of evaluating large language models (LLMs) based on static pairs of inputs and outputs is insufficient for developing assistants: this kind of assessments fails to take into account the essential interactive element in their deployment, and therefore limits how we understand language model capabilities. We introduce CheckMate, an adaptable prototype platform for humans to interact with and evaluate LLMs. We conduct a study with CheckMate to evaluate three language models~(InstructGPT, ChatGPT, and GPT-4) as assistants in proving undergraduate-level mathematics, with a mixed cohort of participants from undergraduate students to professors of mathematics. We release the resulting interaction and rating dataset, MathConverse. By analysing MathConverse, we derive a preliminary taxonomy of human behaviours and uncover that despite a generally positive correlation, there are notable instances of divergence between correctness and perceived helpfulness in LLM generations, amongst other findings. Further, we identify useful scenarios and existing issues of GPT-4 in mathematical reasoning through a series of case studies contributed by expert mathematicians. We conclude with actionable takeaways for ML practitioners and mathematicians: models which communicate uncertainty, respond well to user corrections, are more interpretable and concise may constitute better assistants; interactive evaluation is a promising way to continually navigate the capability of these models; humans should be aware of language models' algebraic fallibility, and for that reason discern where they should be used.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 39,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.01694",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-02",
    "authors": [
      {
        "authorId": "2055306799",
        "name": "Katherine M. Collins"
      },
      {
        "authorId": "2063969818",
        "name": "Albert Qiaochu Jiang"
      },
      {
        "authorId": "2127069744",
        "name": "Simon Frieder"
      },
      {
        "authorId": "2147199509",
        "name": "L. Wong"
      },
      {
        "authorId": "4570864",
        "name": "Miri Zilka"
      },
      {
        "authorId": "32326200",
        "name": "Umang Bhatt"
      },
      {
        "authorId": "1690572",
        "name": "Thomas Lukasiewicz"
      },
      {
        "authorId": "3374063",
        "name": "Yuhuai Wu"
      },
      {
        "authorId": "1763295",
        "name": "J. Tenenbaum"
      },
      {
        "authorId": "2218912627",
        "name": "William Hart"
      },
      {
        "authorId": "2510584",
        "name": "T. Gowers"
      },
      {
        "authorId": null,
        "name": "Wenda Li"
      },
      {
        "authorId": "145689461",
        "name": "Adrian Weller"
      },
      {
        "authorId": "1708741",
        "name": "M. Jamnik"
      }
    ],
    "source": "semantic_scholar",
    "score": 125.33319181170904
  },
  {
    "paperId": "3e5e0e1841e23fc41f4e3243c9091bf6e7e7d199",
    "url": "https://www.semanticscholar.org/paper/3e5e0e1841e23fc41f4e3243c9091bf6e7e7d199",
    "title": "Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic",
    "abstract": "Recent language models enable new opportunities for structured reasoning with text, such as the construction of intuitive, proof-like textual entailment trees without relying on brittle formal logic. However, progress in this direction has been hampered by a long-standing lack of a clear protocol for determining what _valid decompositional entailment_ is. This absence causes noisy datasets and limited performance gains by modern neuro-symbolic entailment engines. To address these problems, we formulate a consistent and theoretically grounded approach to annotating decompositional entailment and evaluate its impact on LLM-based textual inference. We find that our new dataset, RDTE (Recognizing Decompositional Textual Entailment), has a substantially higher internal consistency than prior decompositional entailment datasets, suggesting that RDTE is a significant step forward in the long-standing problem of forming a clear protocol for discerning entailment. We also find that training an RDTE-oriented entailment classifier via knowledge distillation and employing it in an entailment tree reasoning engine significantly improves both accuracy and proof quality, illustrating the practical benefit of this advance for textual inference.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-22",
    "authors": [
      {
        "authorId": "38912638",
        "name": "Nathaniel Weir"
      },
      {
        "authorId": "2187060946",
        "name": "Kate Sanders"
      },
      {
        "authorId": "47433471",
        "name": "Orion Weller"
      },
      {
        "authorId": "2285978160",
        "name": "Shreya Sharma"
      },
      {
        "authorId": "2285428192",
        "name": "Dongwei Jiang"
      },
      {
        "authorId": "6600801",
        "name": "Zhengping Jiang"
      },
      {
        "authorId": "40135250",
        "name": "Bhavana Dalvi"
      },
      {
        "authorId": "3385516",
        "name": "Oyvind Tafjord"
      },
      {
        "authorId": "144949918",
        "name": "Peter Alexander Jansen"
      },
      {
        "authorId": "2258709497",
        "name": "Peter Clark"
      },
      {
        "authorId": "7536576",
        "name": "Benjamin Van Durme"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "1a6c4b949fb0842e6c8fa24c1935683ab5d07408",
    "url": "https://www.semanticscholar.org/paper/1a6c4b949fb0842e6c8fa24c1935683ab5d07408",
    "title": "Is a 3D-Tokenized LLM the Key to Reliable Autonomous Driving?",
    "abstract": "Rapid advancements in Autonomous Driving (AD) tasks turned a significant shift toward end-to-end fashion, particularly in the utilization of vision-language models (VLMs) that integrate robust logical reasoning and cognitive abilities to enable comprehensive end-to-end planning. However, these VLM-based approaches tend to integrate 2D vision tokenizers and a large language model (LLM) for ego-car planning, which lack 3D geometric priors as a cornerstone of reliable planning. Naturally, this observation raises a critical concern: Can a 2D-tokenized LLM accurately perceive the 3D environment? Our evaluation of current VLM-based methods across 3D object detection, vectorized map construction, and environmental caption suggests that the answer is, unfortunately, NO. In other words, 2D-tokenized LLM fails to provide reliable autonomous driving. In response, we introduce DETR-style 3D perceptrons as 3D tokenizers, which connect LLM with a one-layer linear projector. This simple yet elegant strategy, termed Atlas, harnesses the inherent priors of the 3D physical world, enabling it to simultaneously process high-resolution multi-view images and employ spatiotemporal modeling. Despite its simplicity, Atlas demonstrates superior performance in both 3D detection and ego planning tasks on nuScenes dataset, proving that 3D-tokenized LLM is the key to reliable autonomous driving. The code and datasets will be released.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-28",
    "authors": [
      {
        "authorId": "2277145349",
        "name": "Yifan Bai"
      },
      {
        "authorId": "2269424517",
        "name": "Dongming Wu"
      },
      {
        "authorId": "2282235642",
        "name": "Yingfei Liu"
      },
      {
        "authorId": "2171539443",
        "name": "Fan Jia"
      },
      {
        "authorId": "2267728447",
        "name": "Weixin Mao"
      },
      {
        "authorId": "2303885890",
        "name": "Ziheng Zhang"
      },
      {
        "authorId": "2267429566",
        "name": "Yucheng Zhao"
      },
      {
        "authorId": "2303710021",
        "name": "Jianbing Shen"
      },
      {
        "authorId": "2303698980",
        "name": "Xing Wei"
      },
      {
        "authorId": "3361759",
        "name": "Tiancai Wang"
      },
      {
        "authorId": "2156004872",
        "name": "Xiangyu Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "1d5ffd4f19355c074da1f9e8b128941ca41d9f11",
    "url": "https://www.semanticscholar.org/paper/1d5ffd4f19355c074da1f9e8b128941ca41d9f11",
    "title": "Make LLM a Testing Expert: Bringing Human-Like Interaction to Mobile GUI Testing via Functionality-Aware Decisions",
    "abstract": "Automated Graphical User Interface (GUI) testing plays a crucial role in ensuring app quality, especially as mobile applications have become an integral part of our daily lives. Despite the growing popularity of learning-based techniques in automated GUI testing due to their ability to generate human-like interactions, they still suffer from several limitations, such as low testing coverage, inadequate generalization capabilities, and heavy reliance on training data. Inspired by the success of Large Language Models (LLMs) like ChatGPT in natural language understanding and question answering, we formulate the mobile GUI testing problem as a Q&A task. We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process. Within this framework, we have also introduced a functionality-aware memory prompting mechanism that equips the LLM with the ability to retain testing knowledge of the whole process and conduct long-term, functionality-based reasoning to guide exploration. We evaluate it on 93 apps from Google Play and demonstrate that it outperforms the best baseline by 32% in activity coverage, and detects 31% more bugs at a faster rate. Moreover, GPTDroid identifies 53 new bugs on Google Play, of which 35 have been confirmed and fixed.",
    "venue": "International Conference on Software Engineering",
    "year": 2023,
    "citationCount": 39,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3597503.3639180",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-24",
    "authors": [
      {
        "authorId": "2116746462",
        "name": "Zhe Liu"
      },
      {
        "authorId": "2257028659",
        "name": "Chunyang Chen"
      },
      {
        "authorId": "2109763128",
        "name": "Junjie Wang"
      },
      {
        "authorId": "2217447529",
        "name": "Mengzhuo Chen"
      },
      {
        "authorId": "2217931120",
        "name": "Boyu Wu"
      },
      {
        "authorId": "2184146378",
        "name": "Xing Che"
      },
      {
        "authorId": "2155680314",
        "name": "Dandan Wang"
      },
      {
        "authorId": "2145464944",
        "name": "Qing Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 125.33319181170904
  },
  {
    "paperId": "1d5ffd4f19355c074da1f9e8b128941ca41d9f11",
    "url": "https://www.semanticscholar.org/paper/1d5ffd4f19355c074da1f9e8b128941ca41d9f11",
    "title": "Make LLM a Testing Expert: Bringing Human-Like Interaction to Mobile GUI Testing via Functionality-Aware Decisions",
    "abstract": "Automated Graphical User Interface (GUI) testing plays a crucial role in ensuring app quality, especially as mobile applications have become an integral part of our daily lives. Despite the growing popularity of learning-based techniques in automated GUI testing due to their ability to generate human-like interactions, they still suffer from several limitations, such as low testing coverage, inadequate generalization capabilities, and heavy reliance on training data. Inspired by the success of Large Language Models (LLMs) like ChatGPT in natural language understanding and question answering, we formulate the mobile GUI testing problem as a Q&A task. We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process. Within this framework, we have also introduced a functionality-aware memory prompting mechanism that equips the LLM with the ability to retain testing knowledge of the whole process and conduct long-term, functionality-based reasoning to guide exploration. We evaluate it on 93 apps from Google Play and demonstrate that it outperforms the best baseline by 32% in activity coverage, and detects 31% more bugs at a faster rate. Moreover, GPTDroid identifies 53 new bugs on Google Play, of which 35 have been confirmed and fixed.",
    "venue": "International Conference on Software Engineering",
    "year": 2023,
    "citationCount": 39,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3597503.3639180",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-24",
    "authors": [
      {
        "authorId": "2116746462",
        "name": "Zhe Liu"
      },
      {
        "authorId": "2257028659",
        "name": "Chunyang Chen"
      },
      {
        "authorId": "2109763128",
        "name": "Junjie Wang"
      },
      {
        "authorId": "2217447529",
        "name": "Mengzhuo Chen"
      },
      {
        "authorId": "2217931120",
        "name": "Boyu Wu"
      },
      {
        "authorId": "2184146378",
        "name": "Xing Che"
      },
      {
        "authorId": "2155680314",
        "name": "Dandan Wang"
      },
      {
        "authorId": "2145464944",
        "name": "Qing Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 125.33319181170904
  },
  {
    "paperId": "14cfe2588311870325e2770c5159d3100d7031ea",
    "url": "https://www.semanticscholar.org/paper/14cfe2588311870325e2770c5159d3100d7031ea",
    "title": "Envisioning Outlier Exposure by Large Language Models for Out-of-Distribution Detection",
    "abstract": "Detecting out-of-distribution (OOD) samples is essential when deploying machine learning models in open-world scenarios. Zero-shot OOD detection, requiring no training on in-distribution (ID) data, has been possible with the advent of vision-language models like CLIP. Existing methods build a text-based classifier with only closed-set labels. However, this largely restricts the inherent capability of CLIP to recognize samples from large and open label space. In this paper, we propose to tackle this constraint by leveraging the expert knowledge and reasoning capability of large language models (LLM) to Envision potential Outlier Exposure, termed EOE, without access to any actual OOD data. Owing to better adaptation to open-world scenarios, EOE can be generalized to different tasks, including far, near, and fine-grained OOD detection. Technically, we design (1) LLM prompts based on visual similarity to generate potential outlier class labels specialized for OOD detection, as well as (2) a new score function based on potential outlier penalty to distinguish hard OOD samples effectively. Empirically, EOE achieves state-of-the-art performance across different OOD tasks and can be effectively scaled to the ImageNet-1K dataset. The code is publicly available at: https://github.com/tmlr-group/EOE.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-02",
    "authors": [
      {
        "authorId": "2304614976",
        "name": "Chentao Cao"
      },
      {
        "authorId": "2304682665",
        "name": "Zhun Zhong"
      },
      {
        "authorId": "1768392566",
        "name": "Zhanke Zhou"
      },
      {
        "authorId": "2260822055",
        "name": "Yang Liu"
      },
      {
        "authorId": "2244770736",
        "name": "Tongliang Liu"
      },
      {
        "authorId": "2257285217",
        "name": "Bo Han"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "d403ba14f6ce662781444d46c1c70e7bb80313cb",
    "url": "https://www.semanticscholar.org/paper/d403ba14f6ce662781444d46c1c70e7bb80313cb",
    "title": "LocLLM: Exploiting Generalizable Human Keypoint Localization via Large Language Model",
    "abstract": "The capacity of existing human keypoint localization models is limited by keypoint priors provided by the training data. To alleviate this restriction and pursue more gen-eral model, this work studies keypoint localization from a different perspective by reasoning locations based on key-piont clues in text descriptions. We propose LocLLM, the first Large-Language Model (LLM) based keypoint local-ization model that takes images and text instructions as in-puts and outputs the desired keypoint coordinates. LocLLM leverages the strong reasoning capability of LLM and clues of keypoint type, location, and relationship in textual de-scriptions for keypoint localization. To effectively tune Lo-cLLM, we construct localization-based instruction conver-sations to connect keypoint description with corresponding coordinates in input image, and fine-tune the whole model in a parameter-efficient training pipeline. LocLLM shows remarkable performance on standard 2D/3D keypoint lo-calization benchmarks. Moreover, incorporating language clues into the localization makes LocLLM show superior flexibility and generalizable capability in cross dataset key-point localization, and even detecting novel type of key-points unseen during training††Project page: https://github.com/kennethwdk/LocLLM.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2406.04659",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-07",
    "authors": [
      {
        "authorId": "2111220105",
        "name": "Dongkai Wang"
      },
      {
        "authorId": "1396932886",
        "name": "Shiyu Xuan"
      },
      {
        "authorId": "2251071319",
        "name": "Shiliang Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "af6d0ba799213cbbcbfceb1fb9b78d2858486308",
    "url": "https://www.semanticscholar.org/paper/af6d0ba799213cbbcbfceb1fb9b78d2858486308",
    "title": "Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition",
    "abstract": "We present a framework for robot skill acquisition, which 1) efficiently scale up data generation of language-labelled robot data and 2) effectively distills this data down into a robust multi-task language-conditioned visuo-motor policy. For (1), we use a large language model (LLM) to guide high-level planning, and sampling-based robot planners (e.g. motion or grasp samplers) for generating diverse and rich manipulation trajectories. To robustify this data-collection process, the LLM also infers a code-snippet for the success condition of each task, simultaneously enabling the data-collection process to detect failure and retry as well as the automatic labeling of trajectories with success/failure. For (2), we extend the diffusion policy single-task behavior-cloning approach to multi-task settings with language conditioning. Finally, we propose a new multi-task benchmark with 18 tasks across five domains to test long-horizon behavior, common-sense reasoning, tool-use, and intuitive physics. We find that our distilled policy successfully learned the robust retrying behavior in its data collection procedure, while improving absolute success rates by 33.2% on average across five domains. Code, data, and additional qualitative results are available on https://www.cs.columbia.edu/~huy/scalingup/.",
    "venue": "Conference on Robot Learning",
    "year": 2023,
    "citationCount": 110,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.14535",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-26",
    "authors": [
      {
        "authorId": "9091886",
        "name": "Huy Ha"
      },
      {
        "authorId": "47686265",
        "name": "Peter R. Florence"
      },
      {
        "authorId": "3340170",
        "name": "Shuran Song"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.64295301968502
  },
  {
    "paperId": "11bca2cafe89e14dc733504f97e2489de697ceab",
    "url": "https://www.semanticscholar.org/paper/11bca2cafe89e14dc733504f97e2489de697ceab",
    "title": "Drive Like a Human: Rethinking Autonomous Driving with Large Language Models",
    "abstract": "In this paper, we explore the potential of using a large language model (LLM) to understand the driving environment in a human-like manner and analyze its ability to reason, interpret, and memorize when facing complex scenarios. We argue that traditional optimization-based and modular autonomous driving (AD) systems face inherent performance limitations when dealing with long-tail corner cases. To address this problem, we propose that an ideal AD system should drive like a human, accumulating experience through continuous driving and using common sense to solve problems. To achieve this goal, we identify three key abilities necessary for an AD system: reasoning, interpretation, and memorization. We demonstrate the feasibility of employing an LLM in driving scenarios by building a closed-loop system to showcase its comprehension and environment-interaction abilities. Our extensive experiments show that the LLM exhibits the impressive ability to reason and solve long-tailed cases, providing valuable insights for the development of human-like autonomous driving. The related code are available at https:/ithub.om/PJLab-ADG/DriveLikeAHuman.",
    "venue": "2024 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW)",
    "year": 2023,
    "citationCount": 118,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.07162",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-07-14",
    "authors": [
      {
        "authorId": "150967272",
        "name": "Daocheng Fu"
      },
      {
        "authorId": "2153897378",
        "name": "Xin Li"
      },
      {
        "authorId": "153109152",
        "name": "Licheng Wen"
      },
      {
        "authorId": "2197075911",
        "name": "Min Dou"
      },
      {
        "authorId": "26978261",
        "name": "Pinlong Cai"
      },
      {
        "authorId": "119700639",
        "name": "Botian Shi"
      },
      {
        "authorId": "145858545",
        "name": "Y. Qiao"
      }
    ],
    "source": "semantic_scholar",
    "score": 147.68685239667295
  },
  {
    "paperId": "867dfc021bc3c94dbb5b4c3cba158a2a96e8ea73",
    "url": "https://www.semanticscholar.org/paper/867dfc021bc3c94dbb5b4c3cba158a2a96e8ea73",
    "title": "Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid Approach",
    "abstract": "Due to the scale and complexity of cloud systems, a system fail-ure would trigger an “alert storm”, i.e., massive correlated alerts. Although these alerts can be traced back to a few root causes, the overwhelming number makes it infeasible for manual handling. Alert aggregation is thus critical to help engineers concentrate on the root cause and facilitate failure resolution. Existing methods typically utilize semantic similarity-based methods or statistical methods to aggregate alerts. However, semantic similarity-based methods overlook the causal rationale of alerts, while statistical methods can hardly handle infrequent alerts. To tackle these limitations, we introduce leveraging external knowledge, i.e., Standard Operation Procedure (SOP) of alerts as a supplement. We propose COLA, a novel hybrid approach based on correlation mining and LLM (Large Language Model) reasoning for online alert aggregation. The correlation mining module effectively captures the temporal and spatial relations between alerts, measuring their correlations in an efficient manner. Subsequently, only uncertain pairs with low confidence are forwarded to the LLM rea-soning module for detailed analysis. This hybrid design harnesses both statistical evidence for frequent alerts and the reasoning ca-pabilities of computationally intensive LLMs, ensuring the overall efficiency of COLA in handling large volumes of alerts in practical scenarios. We evaluate COLA on three datasets collected from the production environment of a large-scale cloud platform. The exper-imental results show COLA achieves F1-scores from 0.901 to 0.930, outperforming state-of-the-art methods and achieving comparable efficiency. We also share our experience in deploying COLA in our real-world cloud system, Cloud x11Due to the company policy, we anonymize the name as Cloud x..",
    "venue": "2024 IEEE/ACM 46th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3639477.3639745",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-11",
    "authors": [
      {
        "authorId": "2290801232",
        "name": "Jinxi Kuang"
      },
      {
        "authorId": "2238920811",
        "name": "Jinyang Liu"
      },
      {
        "authorId": "2254040845",
        "name": "Junjie Huang"
      },
      {
        "authorId": "2222967440",
        "name": "Renyi Zhong"
      },
      {
        "authorId": "2256468893",
        "name": "Jiazhen Gu"
      },
      {
        "authorId": "2290811078",
        "name": "Lan Yu"
      },
      {
        "authorId": "2290812111",
        "name": "Rui Tan"
      },
      {
        "authorId": "10686906",
        "name": "Zengyin Yang"
      },
      {
        "authorId": "2217490665",
        "name": "Michael R. Lyu"
      }
    ],
    "source": "semantic_scholar",
    "score": 100.1415686865115
  },
  {
    "paperId": "aa2fa431ce1d5a8d56d138e3330d3df381d36e3a",
    "url": "https://www.semanticscholar.org/paper/aa2fa431ce1d5a8d56d138e3330d3df381d36e3a",
    "title": "Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification",
    "abstract": "This case study investigates the task of job classification in a real-world setting, where the goal is to determine whether an English-language job posting is appropriate for a graduate or entry-level position. We explore multiple approaches to text classification, including supervised approaches such as traditional models like Support Vector Machines (SVMs) and state-of-the-art deep learning methods such as DeBERTa. We compare them with Large Language Models (LLMs) used in both few-shot and zero-shot classification settings. To accomplish this task, we employ prompt engineering, a technique that involves designing prompts to guide the LLMs towards the desired output. Specifically, we evaluate the performance of two commercially available state-of-the-art GPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also conduct a detailed analysis of the impact of different aspects of prompt engineering on the model's performance. Our results show that, with a well-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all other models, achieving a 6% increase in Precision@95% Recall compared to the best supervised approach. Furthermore, we observe that the wording of the prompt is a critical factor in eliciting the appropriate\"reasoning\"in the model, and that seemingly minor aspects of the prompt significantly affect the model's performance.",
    "venue": "International Conference on Applications of Natural Language to Data Bases",
    "year": 2023,
    "citationCount": 33,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2303.07142",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-03-13",
    "authors": [
      {
        "authorId": "31430199",
        "name": "Benjamin Clavié"
      },
      {
        "authorId": "2211431510",
        "name": "Alexandru Ciceu"
      },
      {
        "authorId": "2211431037",
        "name": "Frederick Naylor"
      },
      {
        "authorId": "2211430897",
        "name": "Guillaume Souli'e"
      },
      {
        "authorId": "2211431326",
        "name": "Thomas Brightwell"
      }
    ],
    "source": "semantic_scholar",
    "score": 122.89540786924243
  },
  {
    "paperId": "a09aec45d4eff67fd244b0f4035895cdd3fe72e9",
    "url": "https://www.semanticscholar.org/paper/a09aec45d4eff67fd244b0f4035895cdd3fe72e9",
    "title": "Vision-by-Language for Training-Free Compositional Image Retrieval",
    "abstract": "Given an image and a target modification (e.g an image of the Eiffel tower and the text\"without people and at night-time\"), Compositional Image Retrieval (CIR) aims to retrieve the relevant target image in a database. While supervised approaches rely on annotating triplets that is costly (i.e. query image, textual modification, and target image), recent research sidesteps this need by using large-scale vision-language models (VLMs), performing Zero-Shot CIR (ZS-CIR). However, state-of-the-art approaches in ZS-CIR still require training task-specific, customized models over large amounts of image-text pairs. In this work, we propose to tackle CIR in a training-free manner via our Compositional Image Retrieval through Vision-by-Language (CIReVL), a simple, yet human-understandable and scalable pipeline that effectively recombines large-scale VLMs with large language models (LLMs). By captioning the reference image using a pre-trained generative VLM and asking a LLM to recompose the caption based on the textual target modification for subsequent retrieval via e.g. CLIP, we achieve modular language reasoning. In four ZS-CIR benchmarks, we find competitive, in-part state-of-the-art performance - improving over supervised methods. Moreover, the modularity of CIReVL offers simple scalability without re-training, allowing us to both investigate scaling laws and bottlenecks for ZS-CIR while easily scaling up to in parts more than double of previously reported results. Finally, we show that CIReVL makes CIR human-understandable by composing image and text in a modular fashion in the language domain, thereby making it intervenable, allowing to post-hoc re-align failure cases. Code will be released upon acceptance.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 30,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-13",
    "authors": [
      {
        "authorId": "1387873091",
        "name": "Shyamgopal Karthik"
      },
      {
        "authorId": "2258553172",
        "name": "Karsten Roth"
      },
      {
        "authorId": "38286801",
        "name": "Massimiliano Mancini"
      },
      {
        "authorId": "1854487018",
        "name": "Zeynep Akata"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.50980806727719
  },
  {
    "paperId": "6a1d85d1a73d2b80f8a61db62e9b64299eb2dd7e",
    "url": "https://www.semanticscholar.org/paper/6a1d85d1a73d2b80f8a61db62e9b64299eb2dd7e",
    "title": "Will GPT-4 Run DOOM?",
    "abstract": "We show that GPT-4's reasoning and planning capabilities extend to the 1993 first-person shooter Doom. This large language model (LLM) is able to run and play the game with only a few instructions, plus a textual description--generated by the model itself from screenshots--about the state of the game being observed. We find that GPT-4 can play the game to a passable degree: it is able to manipulate doors, combat enemies, and perform pathing. More complex prompting strategies involving multiple model calls provide better results. While further work is required to enable the LLM to play the game as well as its classical, reinforcement learning-based counterparts, we note that GPT-4 required no training, leaning instead on its own reasoning and observational capabilities. We hope our work pushes the boundaries on intelligent, LLM-based agents in video games. We conclude by discussing the ethical implications of our work.",
    "venue": "IEEE Transactions on Games",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-08",
    "authors": [
      {
        "authorId": "1388060354",
        "name": "Adrian de Wynter"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.79441541679836
  },
  {
    "paperId": "5d0b03d545c6468c943858ce46a865380811d05b",
    "url": "https://www.semanticscholar.org/paper/5d0b03d545c6468c943858ce46a865380811d05b",
    "title": "Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?",
    "abstract": "Despite the power of Large Language Models (LLMs) like GPT-4, they still struggle with tasks that require generating complex, structured outputs. In this study, we assess the capability of Current LLMs in generating complex structured data and propose a structure-aware fine-tuning approach as a solution to improve this ability. To perform a comprehensive evaluation, we propose S TRUC -B ENCH , include five representative LLMs (i.e., GPT-NeoX-20B, GPT-3.5, GPT-4, and Vicuna) and evaluate them on our carefully constructed datasets spanning raw text, HTML, and LaTeX tables. Based on our analysis of current model performance, we identify specific common formatting errors and areas of potential improvement. To address complex formatting requirements, we utilize a F ORMAT C O T (Chain-of-Thought) to generate format instructions from target outputs. Our experiments show that our structure-aware fine-tuning method, when applied to LLaMA-7B, significantly improves adherence to natural language constraints, outperforming other evaluated LLMs. Based on these results, we present an ability map of model capabilities from six dimensions (i.e., coverage, formatting, reasoning, comprehension, pragmatics, and hallucination). This map highlights the weaknesses of LLMs in handling complex structured outputs and suggests promising directions for future work.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 24,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.08963",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "47274259",
        "name": "Xiangru Tang"
      },
      {
        "authorId": "2241610409",
        "name": "Yiming Zong"
      },
      {
        "authorId": "2241609611",
        "name": "Jason Phang"
      },
      {
        "authorId": "46316984",
        "name": "Yilun Zhao"
      },
      {
        "authorId": "2249891947",
        "name": "Wangchunshu Zhou"
      },
      {
        "authorId": "2527954",
        "name": "Arman Cohan"
      },
      {
        "authorId": "2201323142",
        "name": "Mark B. Gerstein"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.28313737302301
  },
  {
    "paperId": "65d7663b60d95f98e6281ecc4da9c7a975119b91",
    "url": "https://www.semanticscholar.org/paper/65d7663b60d95f98e6281ecc4da9c7a975119b91",
    "title": "GeoGPT: Understanding and Processing Geospatial Tasks through An Autonomous GPT",
    "abstract": "Decision-makers in GIS need to combine a series of spatial algorithms and operations to solve geospatial tasks. For example, in the task of facility siting, the Buffer tool is usually first used to locate areas close or away from some specific entities; then, the Intersect or Erase tool is used to select candidate areas satisfied multiple requirements. Though professionals can easily understand and solve these geospatial tasks by sequentially utilizing relevant tools, it is difficult for non-professionals to handle these problems. Recently, Generative Pre-trained Transformer (e.g., ChatGPT) presents strong performance in semantic understanding and reasoning. Especially, AutoGPT can further extend the capabilities of large language models (LLMs) by automatically reasoning and calling externally defined tools. Inspired by these studies, we attempt to lower the threshold of non-professional users to solve geospatial tasks by integrating the semantic understanding ability inherent in LLMs with mature tools within the GIS community. Specifically, we develop a new framework called GeoGPT that can conduct geospatial data collection, processing, and analysis in an autonomous manner with the instruction of only natural language. In other words, GeoGPT is used to understand the demands of non-professional users merely based on input natural language descriptions, and then think, plan, and execute defined GIS tools to output final effective results. Several cases including geospatial data crawling, spatial query, facility siting, and mapping validate the effectiveness of our framework. Though limited cases are presented in this paper, GeoGPT can be further extended to various tasks by equipping with more GIS tools, and we think the paradigm of\"foundational plus professional\"implied in GeoGPT provides an effective way to develop next-generation GIS in this era of large foundation models.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 20,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.07930",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-16",
    "authors": [
      {
        "authorId": "2145062550",
        "name": "Yifan Zhang"
      },
      {
        "authorId": "2223729388",
        "name": "Cheng Wei"
      },
      {
        "authorId": "2000181768",
        "name": "Shangyou Wu"
      },
      {
        "authorId": "2223819575",
        "name": "Zhengting He"
      },
      {
        "authorId": "70461341",
        "name": "Wenhao Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 115.66783656585135
  },
  {
    "paperId": "1a01c982aa20c1a1ad1ad94866e3197da99a52a2",
    "url": "https://www.semanticscholar.org/paper/1a01c982aa20c1a1ad1ad94866e3197da99a52a2",
    "title": "Extractive Summarization via ChatGPT for Faithful Summary Generation",
    "abstract": "Extractive summarization is a crucial task in natural language processing that aims to condense long documents into shorter versions by directly extracting sentences. The recent introduction of large language models has attracted significant interest in the NLP community due to its remarkable performance on a wide range of downstream tasks. This paper first presents a thorough evaluation of ChatGPT's performance on extractive summarization and compares it with traditional fine-tuning methods on various benchmark datasets. Our experimental analysis reveals that ChatGPT exhibits inferior extractive summarization performance in terms of ROUGE scores compared to existing supervised systems, while achieving higher performance based on LLM-based evaluation metrics. In addition, we explore the effectiveness of in-context learning and chain-of-thought reasoning for enhancing its performance. Furthermore, we find that applying an extract-then-generate pipeline with ChatGPT yields significant performance improvements over abstractive baselines in terms of summary faithfulness. These observations highlight potential directions for enhancing ChatGPT's capabilities in faithful summarization using two-stage approaches.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 70,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2304.04193",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-04-09",
    "authors": [
      {
        "authorId": "2135688409",
        "name": "Haopeng Zhang"
      },
      {
        "authorId": null,
        "name": "Xiao Liu"
      },
      {
        "authorId": "2168548350",
        "name": "Jiawei Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.94019815561973
  },
  {
    "paperId": "967efeb2933635b53e63e52b6beb1fd1f9c8aee7",
    "url": "https://www.semanticscholar.org/paper/967efeb2933635b53e63e52b6beb1fd1f9c8aee7",
    "title": "Empowering Hardware Security with LLM: The Development of a Vulnerable Hardware Database",
    "abstract": "The scarcity of comprehensive databases and bench-marks in hardware design specifically tailored for security tasks is a significant challenge in the community. Such databases are crucial for developing machine learning-based methods and benchmarking, providing a foundation for evaluating and improving hardware security solutions. However, manually creating these extensive datasets is impractical due to the significant time and effort required. Given the proficiency of large language models (LLM) in natural language processing, coding, and advanced reasoning tasks, using LLM as an artificial intelligence (AI) agent presents a viable option to efficiently create such extensive datasets. In this light, this paper introduces Vul-FSM, a database of 10,000 vulnerable finite state machine (FSM) designs incorporating 16 distinct security weaknesses and vulnerabilities generated using the proposed SecRT-Llmframework. The framework combines the in-context learning capability of LLM, the guidance of developed prompting strategies, and the scrutiny of fidelity-check to not only insert but also detect hardware vulnerabilities and weaknesses. To demonstrate the efficacy of SecRT-LLM, we present an exhaustive analysis, highlighting the proficiency of GPT models in vulnerability insertion, detection, and mitigation. Our proposed SecRT-LLM framework, using gpt-3.5-turbo, demonstrates strong effectiveness, achieving macroaverage pass rates of 81.98% and 80.30% on the first attempt and 97.37% and 99.07% within five attempts for vulnerability insertion and detection, respectively.",
    "venue": "IEEE International Symposium on Hardware Oriented Security and Trust",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-06",
    "authors": [
      {
        "authorId": "2256992493",
        "name": "Dipayan Saha"
      },
      {
        "authorId": "2256991081",
        "name": "Katayoon Yahyaei"
      },
      {
        "authorId": "2231854143",
        "name": "S. Saha"
      },
      {
        "authorId": "145954982",
        "name": "M. Tehranipoor"
      },
      {
        "authorId": "1997019",
        "name": "Farimah Farahmandi"
      }
    ],
    "source": "semantic_scholar",
    "score": 92.47918433002164
  },
  {
    "paperId": "7e17ef56273063dfa838de30b7cc0546b2e5ee10",
    "url": "https://www.semanticscholar.org/paper/7e17ef56273063dfa838de30b7cc0546b2e5ee10",
    "title": "Jellyfish: A Large Language Model for Data Preprocessing",
    "abstract": "This paper explores the utilization of LLMs for data preprocessing (DP), a crucial step in the data mining pipeline that transforms raw data into a clean format conducive to easy processing. Whereas the use of LLMs has sparked interest in devising universal solutions to DP, recent initiatives in this domain typically rely on GPT APIs, raising inevitable data breach concerns. Unlike these approaches, we consider instruction-tuning local LLMs (7 -- 13B models) as universal DP task solvers that operate on a local, single, and low-priced GPU, ensuring data security and enabling further customization. We select a collection of datasets across four representative DP tasks and construct instruction tuning data using data configuration, knowledge injection, and reasoning data distillation techniques tailored to DP. By tuning Mistral-7B, Llama 3-8B, and OpenOrca-Platypus2-13B, our models, namely, Jellyfish-7B/8B/13B, deliver competitiveness compared to GPT-3.5/4 models and strong generalizability to unseen tasks while barely compromising the base models' abilities in NLP tasks. Meanwhile, Jellyfish offers enhanced reasoning capabilities compared to GPT-3.5. Our models are available at: https://huggingface.co/NECOUDBFM/Jellyfish . Our instruction dataset is available at: https://huggingface.co/datasets/NECOUDBFM/Jellyfish-Instruct .",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 18,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-04",
    "authors": [
      {
        "authorId": "2269741433",
        "name": "Haochen Zhang"
      },
      {
        "authorId": "2815918",
        "name": "Yuyang Dong"
      },
      {
        "authorId": "2268317424",
        "name": "Chuan Xiao"
      },
      {
        "authorId": "37267314",
        "name": "M. Oyamada"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.1665846874966
  },
  {
    "paperId": "d183cca56404f09653ba14f952597235dc37c387",
    "url": "https://www.semanticscholar.org/paper/d183cca56404f09653ba14f952597235dc37c387",
    "title": "Large Language Model Enhanced Multi-Agent Systems for 6G Communications",
    "abstract": "The rapid development of the large language model (LLM) presents huge opportunities for 6G communications – for example, network optimization and management – by allowing users to input task requirements to LLMs with natural language. However, directly applying native LLMs in 6G encounters various challenges, such as a lack of communication data and knowledge, and limited logical reasoning, evaluation, and refinement abilities. Integrating LLMs with the capabilities of retrieval, planning, memory, evaluation, and reflection in agents can greatly enhance the potential of LLMs for 6G communications. To this end, we propose CommLLM, a multi-agent system with customized communication knowledge and tools for solving communication-related tasks using natural language. This system consists of three components: multi-agent data retrieval (MDR), which employs the condensate and inference agents to refine and summarize communication knowledge from the knowledge base, expanding the knowledge boundaries of LLMs in 6G communications; multi-agent collaborative planning (MCP), which utilizes multiple planning agents to generate feasible solutions for the communication-re-lated task from different perspectives based on the retrieved knowledge; and multi-agent evaluation and reflection (MER), which utilizes the evaluation agent to assess the solutions, and applies the reflection agent and refinement agent to provide improvement suggestions for current solutions. Finally, we validate the effectiveness of the proposed multi-agent system by designing a semantic communication system as a case study of 6G communications.",
    "venue": "IEEE wireless communications",
    "year": 2023,
    "citationCount": 21,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2312.07850",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-13",
    "authors": [
      {
        "authorId": "40540111",
        "name": "Feibo Jiang"
      },
      {
        "authorId": "2152288497",
        "name": "Li Dong"
      },
      {
        "authorId": "2238127953",
        "name": "Yubo Peng"
      },
      {
        "authorId": "2244014700",
        "name": "Kezhi Wang"
      },
      {
        "authorId": "2214014346",
        "name": "Kun Yang"
      },
      {
        "authorId": "2238046378",
        "name": "Cunhua Pan"
      },
      {
        "authorId": "1713586",
        "name": "D. Niyato"
      },
      {
        "authorId": "1739783",
        "name": "O. Dobre"
      }
    ],
    "source": "semantic_scholar",
    "score": 122.36563680037474
  },
  {
    "paperId": "b6b49a667dd268782150d2af60c73324a7d78aac",
    "url": "https://www.semanticscholar.org/paper/b6b49a667dd268782150d2af60c73324a7d78aac",
    "title": "Automated Domain Modeling with Large Language Models: A Comparative Study",
    "abstract": "Domain modeling is an essential part of software engineering and serves as a way to represent and understand the concepts and relationships in a problem domain. Typically, software engineers interpret the problem description written in natural language and manually translate it into a domain model. Domain modeling can be time-consuming and highly depends on the expertise of software engineers. Recently, Large Language Models (LLMs) have exhibited remarkable ability in language understanding, generation, and reasoning. In this paper, we conduct a comprehensive, comparative study of using LLMs for fully automated domain modeling. We assess two powerful LLMs, GPT3.5 and GPT4, employing various prompt engineering techniques on a data set containing ten diverse domain modeling examples with reference solutions created by modeling experts. Our findings reveal that while LLMs demonstrate impressive domain understanding capabilities, they are still impractical for full automation, with the top-performing LLM achieving F1 scores of 0.76 for class generation, 0.61 for attribute generation, and 0.34 for relationship generation. Moreover, the F1 score is characterized by higher precision and lower recall; thus, domain elements retrieved by LLMs are often reliable, but there are many missing elements. Furthermore, modeling best practices are rarely followed in auto-generated domain models. Our data set and evaluation provide a valuable baseline for future research in automated LLM-based domain modeling.",
    "venue": "ACM/IEEE International Conference on Model Driven Engineering Languages and Systems",
    "year": 2023,
    "citationCount": 15,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-01",
    "authors": [
      {
        "authorId": "2274062680",
        "name": "Kua Chen"
      },
      {
        "authorId": "2274094258",
        "name": "Yujing Yang"
      },
      {
        "authorId": "2237865625",
        "name": "Boqi Chen"
      },
      {
        "authorId": "2278216476",
        "name": "José Antonio Hernández López"
      },
      {
        "authorId": "2273986771",
        "name": "Gunter Mussbacher"
      },
      {
        "authorId": "2302859031",
        "name": "Dániel Varró"
      }
    ],
    "source": "semantic_scholar",
    "score": 117.58883083359672
  },
  {
    "paperId": "53f087d7f5ac5a24910c0fa0162079a2c35d8f64",
    "url": "https://www.semanticscholar.org/paper/53f087d7f5ac5a24910c0fa0162079a2c35d8f64",
    "title": "DoGE: Domain Reweighting with Generalization Estimation",
    "abstract": "The coverage and composition of the pretraining data significantly impacts the generalization ability of Large Language Models (LLMs). Despite its importance, recent LLMs still rely on heuristics and trial and error to increase or reduce the influence of data-domains. We propose DOmain reweighting with Generalization Estimation (DoGE), which optimizes the probability of sampling from each domain (domain weights) in a principled way. Our approach is a two-stage process consisting of (i) training a proxy model to obtain domain weights using a bi-level optimization algorithm; (ii) training a larger base model by sampling training domains according to the learned domain weights. In our experiments, we extensively show how DoGE improves the generalization of the base model to any target data mixture. On the SlimPajama dataset, our base model gets better perplexity and few-shot reasoning accuracies across $6$ tasks compared to baseline methods. Moreover, aiming to generalize to out-of-domain target tasks, which is unseen in the pretraining corpus (OOD domain), DoGE can effectively identify inter-domain dependencies, and consistently achieves better test perplexity on the target domain.",
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "citationCount": 15,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-23",
    "authors": [
      {
        "authorId": "2261456109",
        "name": "Simin Fan"
      },
      {
        "authorId": "2435537",
        "name": "Matteo Pagliardini"
      },
      {
        "authorId": "2256984280",
        "name": "Martin Jaggi"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.58883083359672
  },
  {
    "paperId": "00557300321dc60998e0f42853f4bba52d6e53db",
    "url": "https://www.semanticscholar.org/paper/00557300321dc60998e0f42853f4bba52d6e53db",
    "title": "Revealing the structure of language model capabilities",
    "abstract": "Building a theoretical understanding of the capabilities of large language models (LLMs) is vital for our ability to predict and explain the behavior of these systems. Here, we investigate the structure of LLM capabilities by extracting latent capabilities from patterns of individual differences across a varied population of LLMs. Using a combination of Bayesian and frequentist factor analysis, we analyzed data from 29 different LLMs across 27 cognitive tasks. We found evidence that LLM capabilities are not monolithic. Instead, they are better explained by three well-delineated factors that represent reasoning, comprehension and core language modeling. Moreover, we found that these three factors can explain a high proportion of the variance in model performance. These results reveal a consistent structure in the capabilities of different LLMs and demonstrate the multifaceted nature of these capabilities. We also found that the three abilities show different relationships to model properties such as model size and instruction tuning. These patterns help refine our understanding of scaling laws and indicate that changes to a model that improve one ability might simultaneously impair others. Based on these findings, we suggest that benchmarks could be streamlined by focusing on tasks that tap into each broad model ability.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 15,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.10062",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-14",
    "authors": [
      {
        "authorId": "1523759881",
        "name": "Ryan Burnell"
      },
      {
        "authorId": "2888774",
        "name": "Hank Hao"
      },
      {
        "authorId": "37842641",
        "name": "Andrew R. A. Conway"
      },
      {
        "authorId": "66969655",
        "name": "José Hernández Orallo"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.58883083359672
  },
  {
    "paperId": "1e26b42669b060a3850e4766dea0db6e3c85cdec",
    "url": "https://www.semanticscholar.org/paper/1e26b42669b060a3850e4766dea0db6e3c85cdec",
    "title": "Towards Understanding the Capability of Large Language Models on Code Clone Detection: A Survey",
    "abstract": "Code cloning, the duplication of code fragments, is common in software development. While some reuse aids productivity, excessive cloning hurts maintainability and introduces bugs. Hence, automatic code clone detection is vital. Meanwhile, large language models (LLMs) possess diverse code-related knowledge, making them versatile for various software engineering challenges. However, LLMs' performance in code clone detection is unclear and needs more study for accurate assessment. In this paper, we provide the first comprehensive evaluation of LLMs for clone detection, covering different clone types, languages, and prompts. We find advanced LLMs excel in detecting complex semantic clones, surpassing existing methods. Adding intermediate reasoning steps via chain-of-thought prompts noticeably enhances performance. Additionally, representing code as vector embeddings, especially with text encoders, effectively aids clone detection.Lastly, the ability of LLMs to detect code clones differs among various programming languages. Our study suggests that LLMs have potential for clone detection due to their language capabilities, offering insights for developing robust LLM-based methods to enhance software engineering.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 17,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.01191",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-08-02",
    "authors": [
      {
        "authorId": "2042683163",
        "name": "Shihan Dou"
      },
      {
        "authorId": "1486063362",
        "name": "Junjie Shan"
      },
      {
        "authorId": "2223116448",
        "name": "Haoxiang Jia"
      },
      {
        "authorId": "2217948574",
        "name": "Wenhao Deng"
      },
      {
        "authorId": "2190751523",
        "name": "Zhiheng Xi"
      },
      {
        "authorId": "2241408708",
        "name": "Wei He"
      },
      {
        "authorId": "2109036133",
        "name": "Yueming Wu"
      },
      {
        "authorId": "2067331064",
        "name": "Tao Gui"
      },
      {
        "authorId": "46399556",
        "name": "Yang Liu"
      },
      {
        "authorId": "1790227",
        "name": "Xuanjing Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.35557636844247
  },
  {
    "paperId": "e9388e699a7ef4d12ae425e341ff610c67cbf64b",
    "url": "https://www.semanticscholar.org/paper/e9388e699a7ef4d12ae425e341ff610c67cbf64b",
    "title": "How far is Language Model from 100% Few-shot Named Entity Recognition in Medical Domain",
    "abstract": "Recent advancements in language models (LMs) have led to the emergence of powerful models such as Small LMs (e.g., T5) and Large LMs (e.g., GPT-4). These models have demonstrated exceptional capabilities across a wide range of tasks, such as name entity recognition (NER) in the general domain. (We define SLMs as pre-trained models with fewer parameters compared to models like GPT-3/3.5/4, such as T5, BERT, and others.) Nevertheless, their efficacy in the medical section remains uncertain and the performance of medical NER always needs high accuracy because of the particularity of the field. This paper aims to provide a thorough investigation to compare the performance of LMs in medical few-shot NER and answer How far is LMs from 100\\% Few-shot NER in Medical Domain, and moreover to explore an effective entity recognizer to help improve the NER performance. Based on our extensive experiments conducted on 16 NER models spanning from 2018 to 2023, our findings clearly indicate that LLMs outperform SLMs in few-shot medical NER tasks, given the presence of suitable examples and appropriate logical frameworks. Despite the overall superiority of LLMs in few-shot medical NER tasks, it is important to note that they still encounter some challenges, such as misidentification, wrong template prediction, etc. Building on previous findings, we introduce a simple and effective method called \\textsc{RT} (Retrieving and Thinking), which serves as retrievers, finding relevant examples, and as thinkers, employing a step-by-step reasoning process. Experimental results show that our proposed \\textsc{RT} framework significantly outperforms the strong open baselines on the two open medical benchmark datasets",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 22,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2307.00186",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-01",
    "authors": [
      {
        "authorId": "47629178",
        "name": "Mingchen Li"
      },
      {
        "authorId": "2118403769",
        "name": "Rui Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 117.03241323893724
  },
  {
    "paperId": "2e77a35c9948e68edf7233a5c309fe25262c0c70",
    "url": "https://www.semanticscholar.org/paper/2e77a35c9948e68edf7233a5c309fe25262c0c70",
    "title": "Do LLM Agents Exhibit Social Behavior?",
    "abstract": "As LLMs increasingly take on roles in human-AI interactions and autonomous AI systems, understanding their social behavior becomes important for informed use and continuous improvement. However, their behaviors in social interactions with humans and other agents, as well as the mechanisms shaping their responses, remain underexplored. To address this gap, we introduce a novel probabilistic framework, State-Understanding-Value-Action (SUVA), to systematically analyze LLM responses in social contexts based on their textual outputs (i.e., utterances). Using canonical behavioral economics games and social preference concepts relatable to LLM users, SUVA assesses LLMs' social behavior through both their final decisions and the response generation processes leading to those decisions. Our analysis of eight LLMs -- including two GPT, four LLaMA, and two Mistral models -- suggests that most models do not generate decisions aligned solely with self-interest; instead, they often produce responses that reflect social welfare considerations and display patterns consistent with direct and indirect reciprocity. Additionally, higher-capacity models more frequently display group identity effects. The SUVA framework also provides explainable tools -- including tree-based visualizations and probabilistic dependency analysis -- to elucidate how factors in LLMs' utterance-based reasoning influence their decisions. We demonstrate that utterance-based reasoning reliably predicts LLMs' final actions; references to altruism, fairness, and cooperation in the reasoning increase the likelihood of prosocial actions, while mentions of self-interest and competition reduce them. Overall, our framework enables practitioners to assess LLMs for applications involving social interactions, and provides researchers with a structured method to interpret how LLM behavior arises from utterance-based reasoning.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 20,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Economics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-23",
    "authors": [
      {
        "authorId": "2276428095",
        "name": "Yan Leng"
      },
      {
        "authorId": "2276488976",
        "name": "Yuan Yuan"
      }
    ],
    "source": "semantic_scholar",
    "score": 115.66783656585135
  },
  {
    "paperId": "b8d0ded3a9c76ccafa60f20f4ed4de51f2430fbc",
    "url": "https://www.semanticscholar.org/paper/b8d0ded3a9c76ccafa60f20f4ed4de51f2430fbc",
    "title": "Exploring the potential utility of AI large language models for medical ethics: an expert panel evaluation of GPT-4",
    "abstract": "Integrating large language models (LLMs) like GPT-4 into medical ethics is a novel concept, and understanding the effectiveness of these models in aiding ethicists with decision-making can have significant implications for the healthcare sector. Thus, the objective of this study was to evaluate the performance of GPT-4 in responding to complex medical ethical vignettes and to gauge its utility and limitations for aiding medical ethicists. Using a mixed-methods, cross-sectional survey approach, a panel of six ethicists assessed LLM-generated responses to eight ethical vignettes. The main outcomes measured were relevance, reasoning, depth, technical and non-technical clarity, as well as acceptability of GPT-4’s responses. The readability of the responses was also assessed. Of the six metrics evaluating the effectiveness of GPT-4’s responses, the overall mean score was 4.1/5. GPT-4 was rated highest in providing technical (4.7/5) and non-technical clarity (4.4/5), whereas the lowest rated metrics were depth (3.8/5) and acceptability (3.8/5). There was poor-to-moderate inter-rater reliability characterised by an intraclass coefficient of 0.54 (95% CI: 0.30 to 0.71). Based on panellist feedback, GPT-4 was able to identify and articulate key ethical issues but struggled to appreciate the nuanced aspects of ethical dilemmas and misapplied certain moral principles. This study reveals limitations in the ability of GPT-4 to appreciate the depth and nuanced acceptability of real-world ethical dilemmas, particularly those that require a thorough understanding of relational complexities and context-specific values. Ongoing evaluation of LLM capabilities within medical ethics remains paramount, and further refinement is needed before it can be used effectively in clinical settings.",
    "venue": "Journal of Medical Ethics",
    "year": 2023,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-11-09",
    "authors": [
      {
        "authorId": "151105349",
        "name": "M. Balas"
      },
      {
        "authorId": "123564228",
        "name": "Jordan Joseph Wadden"
      },
      {
        "authorId": "2266026348",
        "name": "Philip C Hébert"
      },
      {
        "authorId": "2266016477",
        "name": "Eric Mathison"
      },
      {
        "authorId": "2266009972",
        "name": "Marika D Warren"
      },
      {
        "authorId": "2266016475",
        "name": "Victoria Seavilleklein"
      },
      {
        "authorId": "2266016473",
        "name": "Daniel Wyzynski"
      },
      {
        "authorId": "2266026313",
        "name": "Alison Callahan"
      },
      {
        "authorId": "2266023443",
        "name": "Sean A Crawford"
      },
      {
        "authorId": "5914508",
        "name": "Parnian Arjmand"
      },
      {
        "authorId": "2204225725",
        "name": "Edsel B. Ing"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.49820016084324
  },
  {
    "paperId": "f0db9f97b5b84ab0cfaa472206c1627e65c0d373",
    "url": "https://www.semanticscholar.org/paper/f0db9f97b5b84ab0cfaa472206c1627e65c0d373",
    "title": "LILO: Learning Interpretable Libraries by Compressing and Documenting Code",
    "abstract": "While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synthesis benchmarks for string editing, scene reasoning, and graphics composition. Compared to existing neural and symbolic methods - including the state-of-the-art library learning algorithm DreamCoder - LILO solves more complex tasks and learns richer libraries that are grounded in linguistic knowledge.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-30",
    "authors": [
      {
        "authorId": "35748708",
        "name": "Gabriel Grand"
      },
      {
        "authorId": "2147199509",
        "name": "L. Wong"
      },
      {
        "authorId": "2058735196",
        "name": "Matthew Bowers"
      },
      {
        "authorId": "1689656957",
        "name": "Theo X. Olausson"
      },
      {
        "authorId": "2264511729",
        "name": "Muxin Liu"
      },
      {
        "authorId": "2238317928",
        "name": "J. B. Tenenbaum"
      },
      {
        "authorId": "2264203454",
        "name": "Jacob Andreas"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.49820016084324
  },
  {
    "paperId": "ec592e12f45e20819afe203164bbbd0de8990510",
    "url": "https://www.semanticscholar.org/paper/ec592e12f45e20819afe203164bbbd0de8990510",
    "title": "AmadeusGPT: a natural language interface for interactive animal behavioral analysis",
    "abstract": "The process of quantifying and analyzing animal behavior involves translating the naturally occurring descriptive language of their actions into machine-readable code. Yet, codifying behavior analysis is often challenging without deep understanding of animal behavior and technical machine learning knowledge. To limit this gap, we introduce AmadeusGPT: a natural language interface that turns natural language descriptions of behaviors into machine-executable code. Large-language models (LLMs) such as GPT3.5 and GPT4 allow for interactive language-based queries that are potentially well suited for making interactive behavior analysis. However, the comprehension capability of these LLMs is limited by the context window size, which prevents it from remembering distant conversations. To overcome the context window limitation, we implement a novel dual-memory mechanism to allow communication between short-term and long-term memory using symbols as context pointers for retrieval and saving. Concretely, users directly use language-based definitions of behavior and our augmented GPT develops code based on the core AmadeusGPT API, which contains machine learning, computer vision, spatio-temporal reasoning, and visualization modules. Users then can interactively refine results, and seamlessly add new behavioral modules as needed. We benchmark AmadeusGPT and show we can produce state-of-the-art performance on the MABE 2022 behavior challenge tasks. Note, an end-user would not need to write any code to achieve this. Thus, collectively AmadeusGPT presents a novel way to merge deep biological knowledge, large-language models, and core computer vision modules into a more naturally intelligent system. Code and demos can be found at: https://github.com/AdaptiveMotorControlLab/AmadeusGPT.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 15,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.04858",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Biology"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-10",
    "authors": [
      {
        "authorId": "35539001",
        "name": "Shaokai Ye"
      },
      {
        "authorId": "25065656",
        "name": "Jessy Lauer"
      },
      {
        "authorId": "2112530253",
        "name": "Mu Zhou"
      },
      {
        "authorId": "2068891",
        "name": "Alexander Mathis"
      },
      {
        "authorId": "4058359",
        "name": "Mackenzie W. Mathis"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.58883083359672
  },
  {
    "paperId": "e0b05e314372ed580d9612ef5f0ee672b17ad2e4",
    "url": "https://www.semanticscholar.org/paper/e0b05e314372ed580d9612ef5f0ee672b17ad2e4",
    "title": "LMDrive: Closed-Loop End-to-End Driving with Large Language Models",
    "abstract": "Despite significant recent progress in the field of autonomous driving, modern methods still struggle and can incur serious accidents when encountering long-tail unfore-seen events and challenging urban scenarios. On the one hand, large language models (LLM) have shown impres-sive reasoning capabilities that approach “Artificial Gen-eral Intelligence”. On the other hand, previous autonomous driving methods tend to rely on limited-format inputs (e.g., sensor data and navigation waypoints), restricting the vehi-cle's ability to understand language information and inter-act with humans. To this end, this paper introduces LM-Drive, a novel language-guided, end-to-end, closed-loop autonomous driving framework. LMDrive uniquely processes and integrates multimodal sensor data with naturallanguage instructions, enabling interaction with humans and navigation software in realistic instructional settings. To facilitate research in language-based closed-loop autonomous driving, we also publicly release the corresponding dataset which includes approximately 64K instruction-following data clips, and the LangAuto benchmark that tests the system's ability to handle complex instructions and challenging driving scenarios. Extensive closed-loop experiments are conducted to demonstrate LMDrive's effectiveness. To the best of our knowledge, we're the very first work to leverage LLMs for closed-loop end-to-end autonomous driving. Code is available on our webpage.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2023,
    "citationCount": 64,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2312.07488",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-12",
    "authors": [
      {
        "authorId": "2075457131",
        "name": "Hao Shao"
      },
      {
        "authorId": "2274058059",
        "name": "Yuxuan Hu"
      },
      {
        "authorId": "2273906302",
        "name": "Letian Wang"
      },
      {
        "authorId": "145292735",
        "name": "Steven L. Waslander"
      },
      {
        "authorId": "2261417717",
        "name": "Yu Liu"
      },
      {
        "authorId": "2261394248",
        "name": "Hongsheng Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 132.61580904843456
  },
  {
    "paperId": "5eea245cc12c55905d4df827d0c9776c5ddfa743",
    "url": "https://www.semanticscholar.org/paper/5eea245cc12c55905d4df827d0c9776c5ddfa743",
    "title": "Compositional Chain-of-Thought Prompting for Large Multimodal Models",
    "abstract": "The combination of strong visual backbones and Large Language Model (LLM) reasoning has led to Large Multimodal Models (LMMs) becoming the current standard for a wide range of vision and language (VL) tasks. However, recent research has shown that even the most advanced LMMs still struggle to capture aspects of compositional visual reasoning, such as attributes and relationships between objects. One solution is to utilize scene graphs (SGs)-a formalization of objects and their relations and attributes that has been extensively used as a bridge between the visual and textual domains. Yet, scene graph data requires scene graph annotations, which are expensive to collect and thus not easily scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic forgetting of the pretraining objective. To overcome this, inspired by chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a novel zero-shot Chain-of-Thought prompting method that utilizes SG representations in order to extract compositional knowledge from an LMM. Specifically, we first generate an SG using the LMM, and then use that SG in the prompt to produce a response. Through extensive experiments, we find that the proposed CCoT approach not only improves LMM performance on several vision and language (VL) compositional benchmarks but also improves the performance of several popular LMMs on general multimodal benchmarks, without the need for fine-tuning or annotated ground-truth SGs. Code: https://github.com/chancharikmitra/CCoT.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2023,
    "citationCount": 46,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2311.17076",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-27",
    "authors": [
      {
        "authorId": "2197559591",
        "name": "Chancharik Mitra"
      },
      {
        "authorId": "2268725883",
        "name": "Brandon Huang"
      },
      {
        "authorId": "2257973285",
        "name": "Trevor Darrell"
      },
      {
        "authorId": "46796686",
        "name": "Roei Herzig"
      }
    ],
    "source": "semantic_scholar",
    "score": 127.75221402565089
  },
  {
    "paperId": "7a29f47f6509011fe5b19462abf6607867b68373",
    "url": "https://www.semanticscholar.org/paper/7a29f47f6509011fe5b19462abf6607867b68373",
    "title": "GPT-4V(ision) System Card",
    "abstract": "GPT-4 with vision (GPT-4V) enables users to instruct GPT-4 to analyze image inputs provided by the user, and is the latest capability we are making broadly available. Incorporating additional modalities (such as image inputs) into large language models (LLMs) is viewed by some as a key frontier in artificial intelligence research and development [1, 2, 3]. Multimodal LLMs offer the possibility of expanding the impact of language-only systems with novel interfaces and capabilities, enabling them to solve new tasks and provide novel experiences for their users. In this system card, [4, 5] 1 we analyze the safety properties of GPT-4V. Our work on safety for GPT-4V builds on the work done for GPT-4 [7] and here we dive deeper into the evaluations, preparation, and mitigation work done specifically for image inputs. Similar to GPT-4, training of GPT-4V was completed in 2022 and we began providing early access to the system in March 2023. As GPT-4 is the technology behind the visual capabilities of GPT-4V, its training process was the same. The pre-trained model was first trained to predict the next word in a document, using a large dataset of text and image data from the Internet as well as licensed sources of data. It was then fine-tuned with additional data, using an algorithm called reinforcement learning from human feedback (RLHF),[8, 9] to produce outputs that are preferred by human trainers. Large multimodal models introduce different limitations and expand the risk surface compared to text-based language models. GPT-4V possesses the limitations and capabilities of each modality (text and vision), while at the same time presenting novel capabilities emerging from the intersection of said modalities and from the intelligence and reasoning afforded by large scale models. This system card outlines how OpenAI prepared the vision capabilities of GPT-4 for deployment. It describes the early access period of the model for small scale users and safety learnings OpenAI gained from this period, multimodal evaluations built to study the model’s fitness for deployment, key findings of expert red teamers, and the mitigations OpenAI implemented prior to broad release.",
    "venue": "",
    "year": 2023,
    "citationCount": 207,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [],
    "source": "semantic_scholar",
    "score": 150.06307119551977
  },
  {
    "paperId": "b8dd3a023b6f3e3bb862d172d84c3f29d3f840d1",
    "url": "https://www.semanticscholar.org/paper/b8dd3a023b6f3e3bb862d172d84c3f29d3f840d1",
    "title": "Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond",
    "abstract": "Logical reasoning consistently plays a fundamental and significant role in the domains of knowledge engineering and artificial intelligence. Recently, Large Language Models (LLMs) have emerged as a noteworthy innovation in natural language processing (NLP). However, the question of whether LLMs can effectively address the task of logical reasoning, which requires gradual cognitive inference similar to human intelligence, remains unanswered. To this end, we aim to bridge this gap and provide comprehensive evaluations in this paper. Firstly, to offer systematic evaluations, we select fifteen typical logical reasoning datasets and organize them into deductive, inductive, abductive and mixed-form reasoning settings. Considering the comprehensiveness of evaluations, we include 3 early-era representative LLMs and 4 trending LLMs. Secondly, different from previous evaluations relying only on simple metrics (e.g., \\emph{accuracy}), we propose fine-level evaluations in objective and subjective manners, covering both answers and explanations, including \\emph{answer correctness}, \\emph{explain correctness}, \\emph{explain completeness} and \\emph{explain redundancy}. Additionally, to uncover the logical flaws of LLMs, problematic cases will be attributed to five error types from two dimensions, i.e., \\emph{evidence selection process} and \\emph{reasoning process}. Thirdly, to avoid the influences of knowledge bias and concentrate purely on benchmarking the logical reasoning capability of LLMs, we propose a new dataset with neutral content. Based on the in-depth evaluations, this paper finally forms a general evaluation scheme of logical reasoning capability from six dimensions (i.e., \\emph{Correct}, \\emph{Rigorous}, \\emph{Self-aware}, \\emph{Active}, \\emph{Oriented} and \\emph{No hallucination}). It reflects the pros and cons of LLMs and gives guiding directions for future works.",
    "venue": "",
    "year": 2023,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-06-16",
    "authors": [
      {
        "authorId": "2143943979",
        "name": "Fangzhi Xu"
      },
      {
        "authorId": "144562160",
        "name": "Qika Lin"
      },
      {
        "authorId": "2111759643",
        "name": "Jiawei Han"
      },
      {
        "authorId": "2153707350",
        "name": "Tianzhe Zhao"
      },
      {
        "authorId": "9756930",
        "name": "Jun Liu"
      },
      {
        "authorId": "49943757",
        "name": "E. Cambria"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "1533df43f3a52a27b7f2464fd48aea82cdedaff2",
    "url": "https://www.semanticscholar.org/paper/1533df43f3a52a27b7f2464fd48aea82cdedaff2",
    "title": "GPT-4 Can't Reason",
    "abstract": "GPT-4 was released in March 2023 to wide acclaim, marking a very substantial improvement across the board over GPT-3.5 (OpenAI's previously best model, which had powered the initial release of ChatGPT). However, despite the genuinely impressive improvement, there are good reasons to be highly skeptical of GPT-4's ability to reason. This position paper discusses the nature of reasoning; criticizes the current formulation of reasoning problems in the NLP community, as well as the way in which LLM reasoning performance is currently evaluated; introduces a small collection of 21 diverse reasoning problems; and performs a detailed qualitative evaluation of GPT-4's performance on those problems. Based on this analysis, the paper concludes that, despite its occasional flashes of analytical brilliance, GPT-4 at present is utterly incapable of reasoning.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 26,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.03762",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-21",
    "authors": [
      {
        "authorId": "1757320",
        "name": "Konstantine Arkoudas"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.43755299006494
  },
  {
    "paperId": "1e838bd5fa2f5bca805493d0f672d03514b36869",
    "url": "https://www.semanticscholar.org/paper/1e838bd5fa2f5bca805493d0f672d03514b36869",
    "title": "Vamos: Versatile Action Models for Video Understanding",
    "abstract": "What makes good representations for video understanding, such as anticipating future activities, or answering video-conditioned questions? While earlier approaches focus on end-to-end learning directly from video pixels, we propose to revisit text-based representations, such as general-purpose video captions, which are interpretable and can be directly consumed by large language models (LLMs). Intuitively, different video understanding tasks may require representations that are complementary and at different granularity. To this end, we propose versatile action models (Vamos), a learning framework powered by a large language model as the ``reasoner'', and can flexibly leverage visual embedding and free-form text descriptions as its input. To interpret the important text evidence for question answering, we generalize the concept bottleneck model to work with tokens and nonlinear models, which uses hard attention to select a small subset of tokens from the free-form text as inputs to the LLM reasoner. We evaluate Vamos on five complementary benchmarks, Ego4D, NeXT-QA, IntentQA, Spacewalk-18, and EgoSchema, on its capability to model temporal dynamics, encode visual history, and perform reasoning. Surprisingly, we observe that text-based representations consistently achieve competitive performance on all benchmarks, and that visual embeddings provide marginal or no performance improvement, demonstrating the effectiveness of text-based video representation in the LLM era. We also demonstrate that our token bottleneck model is able to select relevant evidence from free-form text, support test-time intervention, and achieves nearly 5 times inference speedup while keeping a competitive question answering performance. Code and models are publicly released at https://brown-palm.github.io/Vamos/",
    "venue": "European Conference on Computer Vision",
    "year": 2023,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-22",
    "authors": [
      {
        "authorId": "2263549673",
        "name": "Shijie Wang"
      },
      {
        "authorId": "2287458652",
        "name": "Qi Zhao"
      },
      {
        "authorId": "2268317591",
        "name": "Minh Quan Do"
      },
      {
        "authorId": "40930518",
        "name": "Nakul Agarwal"
      },
      {
        "authorId": "2668978",
        "name": "Kwonjoon Lee"
      },
      {
        "authorId": "2264387871",
        "name": "Chen Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "fbc32b683cc0f3851a504d577c75663e991cdb8a",
    "url": "https://www.semanticscholar.org/paper/fbc32b683cc0f3851a504d577c75663e991cdb8a",
    "title": "Adapting Large Language Models for Content Moderation: Pitfalls in Data Engineering and Supervised Fine-tuning",
    "abstract": "Nowadays, billions of people engage in communication and express their opinions on the internet daily. Unfortunately, not all of these expressions are friendly or compliant, making content moderation an indispensable task. A common approach is to use a discriminative model to classify the content, but this method often requires strict data engineering, otherwise it will face unacceptable overfitting. With the successful development of Large Language Models (LLMs) in recent years, LLM-based methods have become a feasible solution for handling tasks in various domains. Thanks to the knowledge of the foundation models, we can develop more robust privately deployed models with limited data via fine-tuning these foundation models. Moreover, as a generative model, it can provide detailed analysis of the review process, enhancing interpretability. In this paper, we introduce how to fine-tune a LLM model that can be privately deployed for content moderation. Specifically, we discuss the differences between discriminative and generative models using content moderation as an example. Additionally, we reveal that incorporating reasoning processes during the fine-tuning of LLMs can effectively alleviate overfitting, even if the model is not allowed to directly output reasoning processes during deployment. We present a complete process, from data collection and construction to model training and overfitting elimination, for fine-tuning LLMs in vertical domain deployments. We report the entire research process and the key findings in this paper, hoping to provide valuable experience for researchers who are fine-tuning privately deployed models in their domain-specific research.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 13,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.03400",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-10-05",
    "authors": [
      {
        "authorId": "2110817620",
        "name": "Huan Ma"
      },
      {
        "authorId": "2256775070",
        "name": "Changqing Zhang"
      },
      {
        "authorId": "2258526402",
        "name": "Huazhu Fu"
      },
      {
        "authorId": "2240144403",
        "name": "Peilin Zhao"
      },
      {
        "authorId": "2152564746",
        "name": "Bing Wu"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "0bfc804e31eecfd77f45e4ee7f4d629fffdcd628",
    "url": "https://www.semanticscholar.org/paper/0bfc804e31eecfd77f45e4ee7f4d629fffdcd628",
    "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
    "abstract": "Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 468,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.16789",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-31",
    "authors": [
      {
        "authorId": "50625437",
        "name": "Yujia Qin"
      },
      {
        "authorId": "2163374235",
        "name": "Shi Liang"
      },
      {
        "authorId": "2114059497",
        "name": "Yining Ye"
      },
      {
        "authorId": "2214586034",
        "name": "Kunlun Zhu"
      },
      {
        "authorId": "2214613855",
        "name": "Lan Yan"
      },
      {
        "authorId": "2191753738",
        "name": "Ya-Ting Lu"
      },
      {
        "authorId": "2149202150",
        "name": "Yankai Lin"
      },
      {
        "authorId": "2214579778",
        "name": "Xin Cong"
      },
      {
        "authorId": "47274259",
        "name": "Xiangru Tang"
      },
      {
        "authorId": "2226120351",
        "name": "Bill Qian"
      },
      {
        "authorId": "2226184989",
        "name": "Sihan Zhao"
      },
      {
        "authorId": "2214603370",
        "name": "Runchu Tian"
      },
      {
        "authorId": "3360722",
        "name": "Ruobing Xie"
      },
      {
        "authorId": "49640256",
        "name": "Jie Zhou"
      },
      {
        "authorId": "152573911",
        "name": "Marc H. Gerstein"
      },
      {
        "authorId": "2144118403",
        "name": "Dahai Li"
      },
      {
        "authorId": "2141313179",
        "name": "Zhiyuan Liu"
      },
      {
        "authorId": "1753344",
        "name": "Maosong Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 162.2590415266942
  },
  {
    "paperId": "1ae9afce62c60fd0bfc9f5b57f8d8a1bbc3641eb",
    "url": "https://www.semanticscholar.org/paper/1ae9afce62c60fd0bfc9f5b57f8d8a1bbc3641eb",
    "title": "Audio-Visual LLM for Video Understanding",
    "abstract": "This paper presents Audio-Visual LLM, a Multimodal Large Language Model that takes both visual and auditory inputs for holistic video understanding. A key design is the modality-augmented training, which involves the integration of modality-specific tokens engineered to activate the appropriate visual and/or auditory encoder selectively. This mechanism is pivotal in enabling end-to-end joint training with video data at different modalities, including visual-only, audio-only, and audio-visual formats. Moreover, we introduce a high-quality video instruction dataset, derived from GPT-4. This dataset allows Audio-Visual LLM to adeptly process a variety of task-oriented video instructions, ranging from multi-turn conversations and audio-visual narratives to complex reasoning tasks. Extensive experiments demonstrate that Audio-Visual LLM impressively achieves strong zero-shot results across a range of video understanding tasks. For example, Audio-Visual LLM achieves an accuracy of 53.7% on MSRVTT-QA, outperforming non-LLM-based InterVideo by 6.6% and LLM-based Valley by 4.4%, respectively. Additionally, our Audio-Visual LLM also achieves competitive performance on audio tasks (e.g., AudioCaps).",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 24,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-11",
    "authors": [
      {
        "authorId": "2273645084",
        "name": "Fangxun Shu"
      },
      {
        "authorId": "2273731629",
        "name": "Lei Zhang"
      },
      {
        "authorId": "2274172453",
        "name": "Hao Jiang"
      },
      {
        "authorId": "2239227247",
        "name": "Cihang Xie"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.28313737302301
  },
  {
    "paperId": "4713dc19179cdd9083e47067fa9504751f8759c6",
    "url": "https://www.semanticscholar.org/paper/4713dc19179cdd9083e47067fa9504751f8759c6",
    "title": "Human-in-the-Loop through Chain-of-Thought",
    "abstract": "While the emergence of powerful language models along with Chain-of-thought prompting has made automation more and more omnipresent, it sometimes demonstrates its weakness in long-term or multi-step logical reasoning. For example, users don't always get desirable answers for complex mathematical problems without human involvement. Against this background, we present the Manual Correction System (MCS) -- a human-in-the-loop system enhanced by Chain-of-Thought prompting, which explores how manual correction of sub-logics in rationales can improve LLM's reasoning performance. Moving one step forward, considering a system with human-in-the-loop involves more than having humans improve performance but also controlling the cost. Therefore, we post a Cost-utility Analysis Model for Human-in-the-Loop systems (CAMLOP) based on classical economics theory to analyze, quantify and balance the utility and the corresponding cost. We conduct experiments of MCS and CAMLOP with twelve datasets. A significant advantage w.r.t cost and utility proves its superiority over strong baselines.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 19,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.07932",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-10",
    "authors": [
      {
        "authorId": "2117632647",
        "name": "Zefan Cai"
      },
      {
        "authorId": "7267809",
        "name": "Baobao Chang"
      },
      {
        "authorId": "144836032",
        "name": "Wenjuan Han"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.93598410330986
  },
  {
    "paperId": "c36b4aec0c26f1ff5b3cf7e86c0b90f51575ebea",
    "url": "https://www.semanticscholar.org/paper/c36b4aec0c26f1ff5b3cf7e86c0b90f51575ebea",
    "title": "Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games",
    "abstract": "In this study, we explore the application of Large Language Models (LLMs) in \\textit{Jubensha}, a Chinese detective role-playing game and a novel area in Artificial Intelligence (AI) driven gaming. We introduce the first dataset specifically for Jubensha, including character scripts and game rules, to foster AI agent development in this complex narrative environment. Our work also presents a unique multi-agent interaction framework using LLMs, allowing AI agents to autonomously engage in this game. To evaluate the gaming performance of these AI agents, we developed novel methods measuring their mastery of case information and reasoning skills. Furthermore, we incorporated the latest advancements in in-context learning to improve the agents' performance in information gathering, murderer identification, and logical reasoning. The experimental results validate the effectiveness of our proposed methods. This work aims to offer a novel perspective on understanding LLM capabilities and establish a new benchmark for evaluating large language model-based agents.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-01",
    "authors": [
      {
        "authorId": "48198605",
        "name": "Dekun Wu"
      },
      {
        "authorId": "2273646161",
        "name": "Haochen Shi"
      },
      {
        "authorId": "2269419329",
        "name": "Zhiyuan Sun"
      },
      {
        "authorId": "2269701539",
        "name": "Bang Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "47979c7e95705c1a609ac81ec9a82d9a3af88431",
    "url": "https://www.semanticscholar.org/paper/47979c7e95705c1a609ac81ec9a82d9a3af88431",
    "title": "Hide and Seek (HaS): A Lightweight Framework for Prompt Privacy Protection",
    "abstract": "Numerous companies have started offering services based on large language models (LLM), such as ChatGPT, which inevitably raises privacy concerns as users' prompts are exposed to the model provider. Previous research on secure reasoning using multi-party computation (MPC) has proven to be impractical for LLM applications due to its time-consuming and communication-intensive nature. While lightweight anonymization techniques can protect private information in prompts through substitution or masking, they fail to recover sensitive data replaced in the LLM-generated results. In this paper, we expand the application scenarios of anonymization techniques by training a small local model to de-anonymize the LLM's returned results with minimal computational overhead. We introduce the HaS framework, where\"H(ide)\"and\"S(eek)\"represent its two core processes: hiding private entities for anonymization and seeking private entities for de-anonymization, respectively. To quantitatively assess HaS's privacy protection performance, we propose both black-box and white-box adversarial models. Furthermore, we conduct experiments to evaluate HaS's usability in translation and classification tasks. The experimental findings demonstrate that the HaS framework achieves an optimal balance between privacy protection and utility.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 14,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.03057",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-06",
    "authors": [
      {
        "authorId": "2238005820",
        "name": "Yu Chen"
      },
      {
        "authorId": "2238129923",
        "name": "Tingxin Li"
      },
      {
        "authorId": "2238210681",
        "name": "Huiming Liu"
      },
      {
        "authorId": "144705629",
        "name": "Yang Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "7164f67023761f0c5962bb88ffb775e725cb94de",
    "url": "https://www.semanticscholar.org/paper/7164f67023761f0c5962bb88ffb775e725cb94de",
    "title": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
    "abstract": "Governments are increasingly considering integrating autonomous AI agents in high-stakes military and foreign-policy decision-making, especially with the emergence of advanced generative AI models like GPT-4. Our work aims to scrutinize the behavior of multiple AI agents in simulated wargames, specifically focusing on their predilection to take escalatory actions that may exacerbate multilateral conflicts. Drawing on political science and international relations literature about escalation dynamics, we design a novel wargame simulation and scoring framework to assess the escalation risks of actions taken by these agents in different scenarios. Contrary to prior studies, our research provides both qualitative and quantitative insights and focuses on large language models (LLMs). We find that all five studied off-the-shelf LLMs show forms of escalation and difficult-to-predict escalation patterns. We observe that models tend to develop arms-race dynamics, leading to greater conflict, and in rare cases, even to the deployment of nuclear weapons. Qualitatively, we also collect the models’ reported reasoning for chosen actions and observe worrying justifications based on deterrence and first-strike tactics. Given the high stakes of military and foreign-policy contexts, we recommend further examination and cautious consideration before deploying autonomous language model agents for strategic military or diplomatic decision-making.",
    "venue": "Conference on Fairness, Accountability and Transparency",
    "year": 2024,
    "citationCount": 27,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3630106.3658942",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2024-01-07",
    "authors": [
      {
        "authorId": "2278429633",
        "name": "Juan-Pablo Rivera"
      },
      {
        "authorId": "2261492655",
        "name": "Gabriel Mukobi"
      },
      {
        "authorId": "2209882963",
        "name": "Anka Reuel"
      },
      {
        "authorId": "2260586760",
        "name": "Max Lamparth"
      },
      {
        "authorId": "2278619384",
        "name": "Chandler Smith"
      },
      {
        "authorId": "98353012",
        "name": "Jacquelyn G. Schneider"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.98306765262805
  },
  {
    "paperId": "fee61c03fd5ad4a8d8bcdec5bcdfacfe25b361d9",
    "url": "https://www.semanticscholar.org/paper/fee61c03fd5ad4a8d8bcdec5bcdfacfe25b361d9",
    "title": "Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges",
    "abstract": "Online education platforms, leveraging the internet to distribute education resources, seek to provide convenient education but often fall short in real-time communication with students. They often struggle to address the diverse obstacles students encounter throughout their learning journey. Solving the problems encountered by students poses a significant challenge for traditional deep learning models, as it requires not only a broad spectrum of subject knowledge but also the ability to understand what constitutes a student's individual difficulties. It's challenging for traditional machine learning models, as they lack the capacity to comprehend students' personalized needs. Recently, the emergence of large language models (LLMs) offers the possibility for resolving this issue by comprehending individual requests. Although LLMs have been successful in various fields, creating an LLM-based education system is still challenging for the wide range of educational skills required. This paper reviews the recently emerged LLM research related to educational capabilities, including mathematics, writing, programming, reasoning, and knowledge-based question answering, with the aim to explore their potential in constructing the next-generation intelligent education system. Specifically, for each capability, we focus on investigating two aspects. Firstly, we examine the current state of LLMs regarding this capability: how advanced they have become, whether they surpass human abilities, and what deficiencies might exist. Secondly, we evaluate whether the development methods for LLMs in this area are generalizable, that is, whether these methods can be applied to construct a comprehensive educational supermodel with strengths across various capabilities, rather than being effective in only a singular aspect.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-12-27",
    "authors": [
      {
        "authorId": "2260837684",
        "name": "Qingyao Li"
      },
      {
        "authorId": "2171109846",
        "name": "Lingyue Fu"
      },
      {
        "authorId": "2237819504",
        "name": "Weiming Zhang"
      },
      {
        "authorId": "150343399",
        "name": "Xianyu Chen"
      },
      {
        "authorId": "2276822936",
        "name": "Jingwei Yu"
      },
      {
        "authorId": "2154454480",
        "name": "Wei Xia"
      },
      {
        "authorId": "2240768092",
        "name": "Weinan Zhang"
      },
      {
        "authorId": "2257180930",
        "name": "Ruiming Tang"
      },
      {
        "authorId": "2237958078",
        "name": "Yong Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "cef2e06efd484520808dfbeeee2029c4d06bd799",
    "url": "https://www.semanticscholar.org/paper/cef2e06efd484520808dfbeeee2029c4d06bd799",
    "title": "Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models",
    "abstract": "Despite the remarkable success of Large Language Models (LLMs), the massive size poses significant deployment challenges, particularly on resource-constrained hardware. While existing LLM compression methods focus on quantization, pruning remains relatively unexplored due to the high cost of training-based approaches and data collection challenges. One-shot pruning methods, although cost-effective and data-free, have become dominant in LLM pruning, but lead to performance decline under the structured pruning setting. In this work, we introduce a new paradigm for structurally pruning LLMs, called Compresso. Our approach, through the collaboration of the proposed resource-efficient pruning algorithm and the LLM itself, learns optimal pruning decisions during the training process. Compresso addresses the challenges of expensive training costs and data collection by incorporating Low-Rank Adaptation (LoRA) into the $L_0$ regularization during the instruction tuning process. Then, we further augment the pruning algorithm by introducing a collaborative prompt that fosters collaboration between the LLM and the pruning algorithm, significantly boosting the overall performance. To this end, Compresso prunes LLaMA-7B to 5.4B, maintaining original performance and even surpassing LLaMA-7B in reading comprehension by 2.62%. Extensive experiments demonstrate that Compresso significantly outperforms one-shot pruning baselines across various sparsity ratios, achieving up to 2.21%, 11.43%, 7.04%, and 4.81% higher scores on the commonsense reasoning, reading comprehension, MMLU, and BBH benchmarks, respectively.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 11,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.05015",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-08",
    "authors": [
      {
        "authorId": "2257132390",
        "name": "Song Guo"
      },
      {
        "authorId": "2257094139",
        "name": "Jiahang Xu"
      },
      {
        "authorId": "48571328",
        "name": "L. Zhang"
      },
      {
        "authorId": "2257128651",
        "name": "Mao Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "11ce7cd954d4691e2b3e1fa6563e1377d0cfb28f",
    "url": "https://www.semanticscholar.org/paper/11ce7cd954d4691e2b3e1fa6563e1377d0cfb28f",
    "title": "Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for Retrieval Augmented Generation",
    "abstract": "Large Language Models (LLMs) are smart but forgetful. Recent studies, (e.g., (Bubeck et al., 2023)) on modern LLMs have shown that they are capable of performing amazing tasks typically necessitating human-level intelligence. However, unlike humans, frozen LLMs do not improve over time; they neither acquire new knowledge nor learn from their successes or failures. Some approaches to improving the intelligence of LLMs include fine-tuning models based on problem-solving performance (Zelikman et al., 2022), and building bigger and more sophisticated models (Bubeck et al., 2023). However, these methods have the drawback of requiring substantial data and computational resources to retrain existing models. In this paper, we explore the use of Retrieval Augmented Generation, also known as RAG (Lewis et al., 2021) to improve problem-solving performance. We propose ARM-RAG (Auxiliary Rationale Memory for Retrieval Augmented Generation), a system that learns from its successes without incurring high training costs. We demonstrate that the storage and subsequent retrieval of reasoning chains have a positive influence on performance in grade-school math problems.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-07",
    "authors": [
      {
        "authorId": "2265581318",
        "name": "Eric Melz"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "fb1661dc5dfaeac4326b5e200b25fcb6ac91721a",
    "url": "https://www.semanticscholar.org/paper/fb1661dc5dfaeac4326b5e200b25fcb6ac91721a",
    "title": "LMExplainer: a Knowledge-Enhanced Explainer for Language Models",
    "abstract": "Large language models (LLMs) such as GPT-4 are very powerful and can process different kinds of natural language processing (NLP) tasks. However, it can be difficult to interpret the results due to the multi-layer nonlinear model structure and millions of parameters. A lack of clarity and understanding of how the language models (LMs) work can make them unreliable, difficult to trust, and potentially dangerous for use in real-world scenarios. Most recent works exploit attention weights to provide explanations for LM predictions. However, pure attention-based explanations are unable to support the growing complexity of LMs, and cannot reason about their decision-making processes. We propose LMExplainer, a knowledge-enhanced explainer for LMs that can provide human-understandable explanations. We use a knowledge graph (KG) and a graph attention neural network to extract the key decision signals of the LM. We further explore whether interpretation can also help the AI understand the task better. Our experimental results show that LMExplainer outperforms existing LM+KG methods on CommonsenseQA and OpenBookQA. We compare the explanation results with generated explanation methods and human-annotated results. The comparison shows our method can provide more comprehensive and clearer explanations. LMExplainer demonstrates the potential to enhance model performance and furnish explanations for the LM reasoning process in natural language.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2303.16537",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "8157332",
        "name": "Zi-Yuan Chen"
      },
      {
        "authorId": "1399890865",
        "name": "Ambuj K. Singh"
      },
      {
        "authorId": "3024298",
        "name": "Misha Sra"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "45abc8e27cbfbd938f78d96a6d61f899ecb94c23",
    "url": "https://www.semanticscholar.org/paper/45abc8e27cbfbd938f78d96a6d61f899ecb94c23",
    "title": "Fighting reviewer fatigue or amplifying bias? Considerations and recommendations for use of ChatGPT and other Large Language Models in scholarly peer review",
    "abstract": "Background: The emergence of systems based on large language models (LLMs) such as OpenAI’s ChatGPT has created a range of discussions in scholarly circles. Since LLMs generate grammatically correct and mostly relevant (yet sometimes outright wrong, irrelevant or biased) outputs in response to provided prompts, using them in various writing tasks including writing peer review reports could result in improved productivity. Given the significance of peer reviews in the existing scholarly publication landscape, exploring challenges and opportunities of using LLMs in peer review seems urgent. After the generation of the first scholarly outputs with LLMs, we anticipate that peer review reports too would be generated with the help of these systems. However, there are currently no guidelines on how these systems should be used in review tasks. Methods: To investigate the potential impact of using LLMs on the peer review process, we used five core themes within discussions about peer review suggested by Tennant and Ross-Hellauer. These include 1) reviewers’ role, 2) editors’ role, 3) functions and quality of peer reviews, 4) reproducibility, and 5) the social and epistemic functions of peer reviews. We provide a small-scale exploration of ChatGPT’s performance regarding identified issues. Results: LLMs have the potential to substantially alter the role of both peer reviewers and editors. Through supporting both actors in efficiently writing constructive reports or decision letters, LLMs can facilitate higher quality review and address issues of review shortage. However, the fundamental opacity of LLMs’ inner workings and development, raise questions and concerns about potential biases and the reliability of review reports. Additionally, as editorial work has a prominent function in defining and shaping epistemic communities, as well as negotiating normative frameworks within such communities, partly outsourcing this work to LLMs might have unforeseen consequences for social and epistemic relations within academia. Regarding performance, we identified major enhancements in only a few weeks (between December 2022 and January 2023) and expect ChatGPT to continue improving. Conclusions: We believe that LLMs are likely to have a profound impact on academia and scholarly communication. While they have the potential to address several current issues within the scholarly communication system, many uncertainties remain and their use is not without risks. In particular, concerns about the amplification of existing biases and inequalities in access to appropriate infrastructure warrant further attention. For the moment, we recommend that if LLMs are used to write scholarly reviews, reviewers should disclose their use and accept full responsibility for their reports’ accuracy, tone, reasoning and originality.",
    "venue": "Research Square",
    "year": 2023,
    "citationCount": 9,
    "openAccessPdf": {
      "url": "https://researchintegrityjournal.biomedcentral.com/counter/pdf/10.1186/s41073-023-00133-5",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": "2023-02-20",
    "authors": [
      {
        "authorId": "153355049",
        "name": "Mohammad Hosseini"
      },
      {
        "authorId": "2138013591",
        "name": "S. Horbach"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "0b0525a0fdc1e18978e9861dcb4544a90c2e70ce",
    "url": "https://www.semanticscholar.org/paper/0b0525a0fdc1e18978e9861dcb4544a90c2e70ce",
    "title": "Agent Lumos: Unified and Modular Training for Open-Source Language Agents",
    "abstract": "Closed-source agents suffer from several issues such as a lack of affordability, transparency, and reproducibility, particularly on complex interactive tasks. This motivates the development of open-source alternatives. We introduce LUMOS, one of the first frameworks for training open-source LLM-based agents. LUMOS features a learnable, unified, and modular architecture with a planning module that learns high-level subgoal generation, and a grounding module trained to translate these into actions using various tools in the execution module. The design allows for modular upgrades and wider applicability to diverse interactive tasks. To foster generalizable agent learning, we collect large-scale, unified, and high-quality training annotations derived from diverse ground-truth reasoning rationales across various complex interactive tasks. On 9 datasets, LUMOS exhibits several key advantages: (1) LUMOS excels multiple larger open-source agents on the held-out datasets (unused for training) for each task type. LUMOS even surpasses GPT agents on QA and web tasks; (2) LUMOS outperforms open-source agents produced by chain-of-thoughts and unmodularized integrated training; and (3) LUMOS effectively generalizes to unseen tasks, outperforming 33B-scale agents and domain-specific agents.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 19,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2311.05657",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-09",
    "authors": [
      {
        "authorId": "144508458",
        "name": "Da Yin"
      },
      {
        "authorId": "2223951216",
        "name": "Faeze Brahman"
      },
      {
        "authorId": "3023068",
        "name": "Abhilasha Ravichander"
      },
      {
        "authorId": "37619618",
        "name": "Khyathi Raghavi Chandu"
      },
      {
        "authorId": "2259015895",
        "name": "Kai-Wei Chang"
      },
      {
        "authorId": "2266363632",
        "name": "Yejin Choi"
      },
      {
        "authorId": "51583409",
        "name": "Bill Yuchen Lin"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.93598410330986
  },
  {
    "paperId": "b2991a4b2ecc9db0fbd9ca738022801b4e5ee001",
    "url": "https://www.semanticscholar.org/paper/b2991a4b2ecc9db0fbd9ca738022801b4e5ee001",
    "title": "CogBench: a large language model walks into a psychology lab",
    "abstract": "Large language models (LLMs) have significantly advanced the field of artificial intelligence. Yet, evaluating them comprehensively remains challenging. We argue that this is partly due to the predominant focus on performance metrics in most benchmarks. This paper introduces CogBench, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments. This novel approach offers a toolkit for phenotyping LLMs' behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse dataset. We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs. Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior. Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs' behavior. Finally, we explore the effects of prompt-engineering techniques. We discover that chain-of-thought prompting improves probabilistic reasoning, while take-a-step-back prompting fosters model-based behaviors.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 21,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-28",
    "authors": [
      {
        "authorId": "2215168951",
        "name": "Julian Coda-Forno"
      },
      {
        "authorId": "32354733",
        "name": "Marcel Binz"
      },
      {
        "authorId": "2288037634",
        "name": "Jane X. Wang"
      },
      {
        "authorId": "2271334207",
        "name": "Eric Schulz"
      }
    ],
    "source": "semantic_scholar",
    "score": 116.36563680037474
  },
  {
    "paperId": "76a3f4a79ae9a00db2f2b5f6877021d8deb96ada",
    "url": "https://www.semanticscholar.org/paper/76a3f4a79ae9a00db2f2b5f6877021d8deb96ada",
    "title": "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models",
    "abstract": "We present SPHINX, a versatile multi-modal large language model (MLLM) with a joint mixing of model weights, tuning tasks, and visual embeddings. First, for stronger vision-language alignment, we unfreeze the large language model (LLM) during pre-training, and introduce a weight mix strategy between LLMs trained by real-world and synthetic data. By directly integrating the weights from two domains, the mixed LLM can efficiently incorporate diverse semantics with favorable robustness. Then, to enable multi-purpose capabilities, we mix a variety of tasks for joint visual instruction tuning, and design task-specific instructions to avoid inter-task conflict. In addition to the basic visual question answering, we include more challenging tasks such as region-level understanding, caption grounding, document layout detection, and human pose estimation, contributing to mutual enhancement over different scenarios. Additionally, we propose to extract comprehensive visual embeddings from various network architectures, pre-training paradigms, and information granularity, providing language models with more robust image representations. Based on our proposed joint mixing, SPHINX exhibits superior multi-modal understanding capabilities on a wide range of applications. On top of this, we further propose an efficient strategy aiming to better capture fine-grained appearances of high-resolution images. With a mixing of different scales and high-resolution sub-images, SPHINX attains exceptional visual parsing and reasoning performance on existing evaluation benchmarks. We hope our work may cast a light on the exploration of joint mixing in future MLLM research. Code is released at https://github.com/Alpha-VLLM/LLaMA2-Accessory.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 162,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-13",
    "authors": [
      {
        "authorId": "2112305433",
        "name": "Ziyi Lin"
      },
      {
        "authorId": "2238219059",
        "name": "Chris Liu"
      },
      {
        "authorId": "2115713503",
        "name": "Renrui Zhang"
      },
      {
        "authorId": "144740494",
        "name": "Peng Gao"
      },
      {
        "authorId": "2150467916",
        "name": "Longtian Qiu"
      },
      {
        "authorId": "2238398546",
        "name": "Han Xiao"
      },
      {
        "authorId": "49660254",
        "name": "Han Qiu"
      },
      {
        "authorId": "2266583076",
        "name": "Chen Lin"
      },
      {
        "authorId": "1485702259",
        "name": "Wenqi Shao"
      },
      {
        "authorId": "2266796045",
        "name": "Keqin Chen"
      },
      {
        "authorId": "150147382",
        "name": "Jiaming Han"
      },
      {
        "authorId": "2243292807",
        "name": "Siyuan Huang"
      },
      {
        "authorId": "2266421755",
        "name": "Yichi Zhang"
      },
      {
        "authorId": "2266420500",
        "name": "Xuming He"
      },
      {
        "authorId": "2266421952",
        "name": "Hongsheng Li"
      },
      {
        "authorId": "2059129841",
        "name": "Y. Qiao"
      }
    ],
    "source": "semantic_scholar",
    "score": 146.40625301210144
  },
  {
    "paperId": "24fc9ad715372358bd0108eeb7c944b915963293",
    "url": "https://www.semanticscholar.org/paper/24fc9ad715372358bd0108eeb7c944b915963293",
    "title": "ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation",
    "abstract": "Graphical User Interface (GUI) automation holds significant promise for assisting users with complex tasks, thereby boosting human productivity. Existing works leveraging Large Language Model (LLM) or LLM-based AI agents have shown capabilities in automating tasks on Android and Web platforms. However, these tasks are primarily aimed at simple device usage and entertainment operations. This paper presents a novel benchmark, AssistGUI, to evaluate whether models are capable of manipulating the mouse and keyboard on the Windows platform in response to user-requested tasks. We carefully collected a set of 100 tasks from nine widely-used software applications, such as, After Effects and MS Word, each accompanied by the necessary project files for better evaluation. Moreover, we propose an advanced Actor-Critic Embodied Agent framework, which incorporates a sophisticated GUI parser driven by an LLM-agent and an enhanced reasoning mechanism adept at handling lengthy procedural tasks. Our experimental results reveal that our GUI Parser and Reasoning mechanism outshine existing methods in performance. Nevertheless, the potential remains substantial, with the best model attaining only a 46% success rate on our benchmark. We conclude with a thorough analysis of the current methods' limitations, setting the stage for future breakthroughs in this domain.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-20",
    "authors": [
      {
        "authorId": "2420608",
        "name": "Difei Gao"
      },
      {
        "authorId": "2275213036",
        "name": "Lei Ji"
      },
      {
        "authorId": "2237427303",
        "name": "Zechen Bai"
      },
      {
        "authorId": "2229453614",
        "name": "Mingyu Ouyang"
      },
      {
        "authorId": "2275181286",
        "name": "Peiran Li"
      },
      {
        "authorId": "2142842134",
        "name": "Dongxing Mao"
      },
      {
        "authorId": "2275165529",
        "name": "Qinchen Wu"
      },
      {
        "authorId": "2275294567",
        "name": "Weichen Zhang"
      },
      {
        "authorId": "2276216812",
        "name": "Peiyi Wang"
      },
      {
        "authorId": "2275640124",
        "name": "Xiangwu Guo"
      },
      {
        "authorId": "2275449157",
        "name": "Hengxu Wang"
      },
      {
        "authorId": "2275297086",
        "name": "Luowei Zhou"
      },
      {
        "authorId": "2258957578",
        "name": "Mike Zheng Shou"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.49820016084324
  },
  {
    "paperId": "5aea65d0d712360bb3357cbc23d43707ec27c461",
    "url": "https://www.semanticscholar.org/paper/5aea65d0d712360bb3357cbc23d43707ec27c461",
    "title": "MM-Narrator: Narrating Long-form Videos with Multimodal In-Context Learning",
    "abstract": "We present MM-Narrator, a novel system leveraging GPT-4 with multimodal in-context learning for the gener-ation of audio descriptions (AD). Unlike previous methods that primarily focused on downstream fine-tuning with short video clips, MM-Narrator excels in generating precise audio descriptions for videos of extensive lengths, even be-yond hours, in an autoregressive manner. This capability is made possible by the proposed memory-augmented generation process, which effectively utilizes both the short-term textual context and long-term visual memory through an efficient register-and-recall mechanism. These contextual memories compile pertinent past information, including storylines and character identities, ensuring an accurate tracking and depicting of story-coherent and character-centric audio descriptions. Maintaining the training-free design of MM-Narrator, we further propose a complexity-based demonstration selection strategy to largely enhance its multi-step reasoning capability via few-shot multimodal in-context learning (MM-ICL). Experimental results on MAD-eval dataset demonstrate that MM-Narrator consistently outperforms both the existing fine-tuning-based approaches and LLM-based approaches in most scenarios, as measured by standard evaluation metrics. Additionally, we introduce the first segment-based evaluator for recurrent text generation. Empowered by GPT-4, this evaluator comprehensively reasons and marks AD generation performance in various extendable dimensions.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2023,
    "citationCount": 15,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2311.17435",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-29",
    "authors": [
      {
        "authorId": "2268807637",
        "name": "Chaoyi Zhang"
      },
      {
        "authorId": "2249717753",
        "name": "K. Lin"
      },
      {
        "authorId": "2149231840",
        "name": "Zhengyuan Yang"
      },
      {
        "authorId": "2124948371",
        "name": "Jianfeng Wang"
      },
      {
        "authorId": "50703697",
        "name": "Linjie Li"
      },
      {
        "authorId": "2249709105",
        "name": "Chung-Ching Lin"
      },
      {
        "authorId": "2251726216",
        "name": "Zicheng Liu"
      },
      {
        "authorId": "29957038",
        "name": "Lijuan Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.58883083359672
  },
  {
    "paperId": "0f3bc7bd909d7f8b0d9e1ffc8237dd51eb19abe8",
    "url": "https://www.semanticscholar.org/paper/0f3bc7bd909d7f8b0d9e1ffc8237dd51eb19abe8",
    "title": "A Generative Pretrained Transformer (GPT)–Powered Chatbot as a Simulated Patient to Practice History Taking: Prospective, Mixed Methods Study",
    "abstract": "Background Communication is a core competency of medical professionals and of utmost importance for patient safety. Although medical curricula emphasize communication training, traditional formats, such as real or simulated patient interactions, can present psychological stress and are limited in repetition. The recent emergence of large language models (LLMs), such as generative pretrained transformer (GPT), offers an opportunity to overcome these restrictions Objective The aim of this study was to explore the feasibility of a GPT-driven chatbot to practice history taking, one of the core competencies of communication. Methods We developed an interactive chatbot interface using GPT-3.5 and a specific prompt including a chatbot-optimized illness script and a behavioral component. Following a mixed methods approach, we invited medical students to voluntarily practice history taking. To determine whether GPT provides suitable answers as a simulated patient, the conversations were recorded and analyzed using quantitative and qualitative approaches. We analyzed the extent to which the questions and answers aligned with the provided script, as well as the medical plausibility of the answers. Finally, the students filled out the Chatbot Usability Questionnaire (CUQ). Results A total of 28 students practiced with our chatbot (mean age 23.4, SD 2.9 years). We recorded a total of 826 question-answer pairs (QAPs), with a median of 27.5 QAPs per conversation and 94.7% (n=782) pertaining to history taking. When questions were explicitly covered by the script (n=502, 60.3%), the GPT-provided answers were mostly based on explicit script information (n=471, 94.4%). For questions not covered by the script (n=195, 23.4%), the GPT answers used 56.4% (n=110) fictitious information. Regarding plausibility, 842 (97.9%) of 860 QAPs were rated as plausible. Of the 14 (2.1%) implausible answers, GPT provided answers rated as socially desirable, leaving role identity, ignoring script information, illogical reasoning, and calculation error. Despite these results, the CUQ revealed an overall positive user experience (77/100 points). Conclusions Our data showed that LLMs, such as GPT, can provide a simulated patient experience and yield a good user experience and a majority of plausible answers. Our analysis revealed that GPT-provided answers use either explicit script information or are based on available information, which can be understood as abductive reasoning. Although rare, the GPT-based chatbot provides implausible information in some instances, with the major tendency being socially desirable instead of medically plausible information.",
    "venue": "JMIR Medical Education",
    "year": 2024,
    "citationCount": 14,
    "openAccessPdf": {
      "url": "https://jmir.org/api/download?alt_name=mededu_v10i1e53961_app1.pdf&filename=afea0cccea2a67adc54d26ef18fd7da4.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-16",
    "authors": [
      {
        "authorId": "4741695",
        "name": "F. Holderried"
      },
      {
        "authorId": "2279527424",
        "name": "Christian Stegemann-Philipps"
      },
      {
        "authorId": "2232754818",
        "name": "Lea Herschbach"
      },
      {
        "authorId": "2192137041",
        "name": "Julia-Astrid Moldt"
      },
      {
        "authorId": "2279523456",
        "name": "Andrew Nevins"
      },
      {
        "authorId": "2121839",
        "name": "J. Griewatz"
      },
      {
        "authorId": "2254887944",
        "name": "Martin Holderried"
      },
      {
        "authorId": "1400978602",
        "name": "Anne Herrmann-Werner"
      },
      {
        "authorId": "2182533623",
        "name": "T. Festl-Wietek"
      },
      {
        "authorId": "40267619",
        "name": "Moritz Mahling"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "c7a3f9cc61cfafdc307f8ae24430b6b1121f9b2c",
    "url": "https://www.semanticscholar.org/paper/c7a3f9cc61cfafdc307f8ae24430b6b1121f9b2c",
    "title": "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings",
    "abstract": "Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, $\\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our approach represents each $\\underline{tool}$ as a to$\\underline{ken}$ ($\\textit{toolken}$) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the flexibility to plug in an arbitrary number of tools by expanding the set of toolkens on the fly. In addition, it improves tool use by allowing extensive demonstration data for learning the toolken embeddings. In diverse domains, including numerical reasoning, knowledge-based question answering, and embodied plan generation, our approach effectively augments LLMs with tools and substantially outperforms various latest baselines. ToolkenGPT demonstrates the promising ability to use relevant tools from a large tool set in complex scenarios.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 143,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.11554",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-19",
    "authors": [
      {
        "authorId": "2128965713",
        "name": "Shibo Hao"
      },
      {
        "authorId": "2115347044",
        "name": "Tianyang Liu"
      },
      {
        "authorId": "47197370",
        "name": "Zhen Wang"
      },
      {
        "authorId": "2749311",
        "name": "Zhiting Hu"
      }
    ],
    "source": "semantic_scholar",
    "score": 151.54719949364
  },
  {
    "paperId": "6355483dee7c419f5294124bb35a8e5de4b406de",
    "url": "https://www.semanticscholar.org/paper/6355483dee7c419f5294124bb35a8e5de4b406de",
    "title": "Asking Before Acting: Gather Information in Embodied Decision Making with Language Models",
    "abstract": "With strong capabilities of reasoning and a broad understanding of the world, Large Language Models (LLMs) have demonstrated immense potential in building versatile embodied decision-making agents capable of executing a wide array of tasks. Nevertheless, when deployed in unfamiliar environments, we show that LLM agents encounter challenges in efficiently gathering essential information, leading to suboptimal performance. Conversely, human individuals often seek additional information from their peers prior to taking action, harnessing external knowledge to avoid unnecessary trial and error. Drawing inspiration from this behavior, we propose \\textit{Asking Before Acting} (ABA), a method that empowers the agent to proactively inquire with external sources for pertinent information using natural language during their interactions within the environment. In this way, the agent is able to enhance its efficiency and performance by circumventing potentially laborious steps and combating the difficulties associated with exploration in unfamiliar environments and vagueness of the instructions. We conduct extensive experiments involving a spectrum of environments including text-based household everyday tasks, robot arm manipulation tasks, and real world open domain image based embodied tasks. The experiments involve various models from Vicuna to GPT-4. The results demonstrate that, even with modest prompts modifications, ABA exhibits substantial advantages on both performance and efficiency over baseline LLM agents. Further finetuning ABA with reformulated metadata (ABA-FT) faciliates learning the rationale for asking and allows for additional enhancements especially in tasks that baselines struggle to solve.",
    "venue": "",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-05-25",
    "authors": [
      {
        "authorId": "2218623338",
        "name": "Xiaoyu Chen"
      },
      {
        "authorId": "2145522248",
        "name": "Shenao Zhang"
      },
      {
        "authorId": "1570021289",
        "name": "Pushi Zhang"
      },
      {
        "authorId": "2218154011",
        "name": "Li Zhao"
      },
      {
        "authorId": "1391201846",
        "name": "Jianyu Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "415c1278fc3a9491bc543f2ca9791939e2dd067a",
    "url": "https://www.semanticscholar.org/paper/415c1278fc3a9491bc543f2ca9791939e2dd067a",
    "title": "MedDM: LLM-executable clinical guidance tree for clinical decision-making",
    "abstract": "It is becoming increasingly emphasis on the importance of LLM participating in clinical diagnosis decision-making. However, the low specialization refers to that current medical LLMs can not provide specific medical advice, which are more like a medical Q\\&A. And there is no suitable clinical guidance tree data set that can be used directly with LLM. To address this issue, we first propose LLM-executavle clinical guidance tree(CGT), which can be directly used by large language models, and construct medical diagnostic decision-making dataset (MedDM), from flowcharts in clinical practice guidelines. We propose an approach to screen flowcharts from medical literature, followed by their identification and conversion into standardized diagnostic decision trees. Constructed a knowledge base with 1202 decision trees, which came from 5000 medical literature and covered 12 hospital departments, including internal medicine, surgery, psychiatry, and over 500 diseases.Moreover, we propose a method for reasoning on LLM-executable CGT and a Patient-LLM multi-turn dialogue framework.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-05",
    "authors": [
      {
        "authorId": "2269847554",
        "name": "Binbin Li"
      },
      {
        "authorId": "2269733964",
        "name": "Tianxin Meng"
      },
      {
        "authorId": "2261087633",
        "name": "Xiaoming Shi"
      },
      {
        "authorId": "2269733634",
        "name": "Jie Zhai"
      },
      {
        "authorId": "2230589989",
        "name": "Tong Ruan"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "cb7fa7ee3df826628c113ba0c6db1205751d89a3",
    "url": "https://www.semanticscholar.org/paper/cb7fa7ee3df826628c113ba0c6db1205751d89a3",
    "title": "HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
    "abstract": "Large language models (LLMs), such as Chat-GPT, are prone to generate hallucinations, i.e., content that conﬂicts with the source or cannot be veriﬁed by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the H allucination E valuation for Large L anguage M odels ( HELMA ) benchmark, a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing and alleviating hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-ﬁltering . Speciﬁcally, we ﬁrst adopt two different sampling methods to generate hallucinated samples based on instructions, and then use an example-enhanced ﬁltering method to select the best one. Furthermore, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT has some probabilities to generate hallucinations and existing LLMs face great challenges in recognizing the hallucinations in text. In addition, the performance can be improved by providing external knowledge or adding reasoning steps. Our benchmark can be accessed at https://github.com/ RUCAIBox/HELMA .",
    "venue": "",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2018027",
        "name": "Junyi Li"
      },
      {
        "authorId": "2149479237",
        "name": "Xiaoxue Cheng"
      },
      {
        "authorId": "2542603",
        "name": "Wayne Xin Zhao"
      },
      {
        "authorId": "50204644",
        "name": "J. Nie"
      },
      {
        "authorId": "153693432",
        "name": "Ji-rong Wen"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "7f0d1740e74ce36424d64d608270077b64dfe7c0",
    "url": "https://www.semanticscholar.org/paper/7f0d1740e74ce36424d64d608270077b64dfe7c0",
    "title": "LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models",
    "abstract": "The emergent reasoning and Theory of Mind (ToM) abilities demonstrated by Large Language Models (LLMs) make them promising candidates for developing coordination agents. In this study, we introduce a new LLM-Coordination Benchmark aimed at a detailed analysis of LLMs within the context of Pure Coordination Games, where participating agents need to cooperate for the most gain. This benchmark evaluates LLMs through two distinct tasks: (1) \\emph{Agentic Coordination}, where LLMs act as proactive participants for cooperation in 4 pure coordination games; (2) \\emph{Coordination Question Answering (QA)}, where LLMs are prompted to answer 198 multiple-choice questions from the 4 games for evaluation of three key reasoning abilities: Environment Comprehension, ToM Reasoning, and Joint Planning. Furthermore, to enable LLMs for multi-agent coordination, we introduce a Cognitive Architecture for Coordination (CAC) framework that can easily integrate different LLMs as plug-and-play modules for pure coordination games. Our findings indicate that LLM agents equipped with GPT-4-turbo achieve comparable performance to state-of-the-art reinforcement learning methods in games that require commonsense actions based on the environment. Besides, zero-shot coordination experiments reveal that, unlike RL methods, LLM agents are robust to new unseen partners. However, results on Coordination QA show a large room for improvement in the Theory of Mind reasoning and joint planning abilities of LLMs. The analysis also sheds light on how the ability of LLMs to understand their environment and their partner's beliefs and intentions plays a part in their ability to plan for coordination. Our code is available at \\url{https://github.com/eric-ai-lab/llm_coordination}.",
    "venue": "",
    "year": 2023,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-10-05",
    "authors": [
      {
        "authorId": "2081814989",
        "name": "Saaket Agashe"
      },
      {
        "authorId": "2257161100",
        "name": "Yue Fan"
      },
      {
        "authorId": "2294719522",
        "name": "Anthony Reyna"
      },
      {
        "authorId": "2256599634",
        "name": "Xin Eric Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "9f3d192663818c64331be6729f723c3266344a3a",
    "url": "https://www.semanticscholar.org/paper/9f3d192663818c64331be6729f723c3266344a3a",
    "title": "Chain-of-Thought Embeddings for Stance Detection on Social Media",
    "abstract": "Stance detection on social media is challenging for Large Language Models (LLMs), as emerging slang and colloquial language in online conversations often contain deeply implicit stance labels. Chain-of-Thought (COT) prompting has recently been shown to improve performance on stance detection tasks -- alleviating some of these issues. However, COT prompting still struggles with implicit stance identification. This challenge arises because many samples are initially challenging to comprehend before a model becomes familiar with the slang and evolving knowledge related to different topics, all of which need to be acquired through the training data. In this study, we address this problem by introducing COT Embeddings which improve COT performance on stance detection tasks by embedding COT reasonings and integrating them into a traditional RoBERTa-based stance detection pipeline. Our analysis demonstrates that 1) text encoders can leverage COT reasonings with minor errors or hallucinations that would otherwise distort the COT output label. 2) Text encoders can overlook misleading COT reasoning when a sample's prediction heavily depends on domain-specific patterns. Our model achieves SOTA performance on multiple stance detection datasets collected from social media.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-30",
    "authors": [
      {
        "authorId": "2337095",
        "name": "Joseph Gatto"
      },
      {
        "authorId": "2264340968",
        "name": "Omar Sharif"
      },
      {
        "authorId": "2243193868",
        "name": "S. Preum"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "5ef821267fa68d3231ed8135ff8ec09f25bb1398",
    "url": "https://www.semanticscholar.org/paper/5ef821267fa68d3231ed8135ff8ec09f25bb1398",
    "title": "ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models",
    "abstract": "Large language models (LLMs) have recently demonstrated their potential in clinical applications, providing valuable medical knowledge and advice. For example, a large dialog LLM like ChatGPT has successfully passed part of the US medical licensing exam. However, LLMs currently have difficulty processing images, making it challenging to interpret information from medical images, which are rich in information that supports clinical decisions. On the other hand, computer-aided diagnosis (CAD) networks for medical images have seen significant success in the medical field by using advanced deep-learning algorithms to support clinical decision-making. This paper presents a method for integrating LLMs into medical-image CAD networks. The proposed framework uses LLMs to enhance the output of multiple CAD networks, such as diagnosis networks, lesion segmentation networks, and report generation networks, by summarizing and reorganizing the information presented in natural language text format. The goal is to merge the strengths of LLMs' medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable system for patients compared to conventional CAD systems. In the future, LLM's medical knowledge can be also used to improve the performance of vision-based medical-image CAD models.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 127,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2302.07257",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-02-14",
    "authors": [
      {
        "authorId": "2151487856",
        "name": "Sheng Wang"
      },
      {
        "authorId": "15594254",
        "name": "Zihao Zhao"
      },
      {
        "authorId": "2056468197",
        "name": "Xi Ouyang"
      },
      {
        "authorId": "2215061296",
        "name": "Qian Wang"
      },
      {
        "authorId": "2150038187",
        "name": "Dinggang Shen"
      }
    ],
    "source": "semantic_scholar",
    "score": 142.78045395879425
  },
  {
    "paperId": "c9a6aae4bedf6fd7a85b359e76137848265d4d1e",
    "url": "https://www.semanticscholar.org/paper/c9a6aae4bedf6fd7a85b359e76137848265d4d1e",
    "title": "Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving",
    "abstract": "Large Language Models (LLMs) have achieved tremendous progress, yet they still often struggle with challenging reasoning problems. Current approaches address this challenge by sampling or searching detailed and low-level reasoning chains. However, these methods are still limited in their exploration capabilities, making it challenging for correct solutions to stand out in the huge solution space. In this work, we unleash LLMs' creative potential for exploring multiple diverse problem solving strategies by framing an LLM as a hierarchical policy via in-context learning. This policy comprises of a visionary leader that proposes multiple diverse high-level problem-solving tactics as hints, accompanied by a follower that executes detailed problem-solving processes following each of the high-level instruction. The follower uses each of the leader's directives as a guide and samples multiple reasoning chains to tackle the problem, generating a solution group for each leader proposal. Additionally, we propose an effective and efficient tournament-based approach to select among these explored solution groups to reach the final answer. Our approach produces meaningful and inspiring hints, enhances problem-solving strategy exploration, and improves the final answer accuracy on challenging problems in the MATH dataset. Code will be released at https://github.com/lz1oceani/LLM-As-Hierarchical-Policy.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-01",
    "authors": [
      {
        "authorId": "49706114",
        "name": "Z. Ling"
      },
      {
        "authorId": "2219045025",
        "name": "Yunhao Fang"
      },
      {
        "authorId": "2108263986",
        "name": "Xuanlin Li"
      },
      {
        "authorId": "3431352",
        "name": "Tongzhou Mu"
      },
      {
        "authorId": "2108721816",
        "name": "Mingu Lee"
      },
      {
        "authorId": "2264525112",
        "name": "Reza Pourreza"
      },
      {
        "authorId": "2264497937",
        "name": "Roland Memisevic"
      },
      {
        "authorId": "2264229045",
        "name": "Hao Su"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "9a73effed8775962c86587feb0f9ef841fa2ff4c",
    "url": "https://www.semanticscholar.org/paper/9a73effed8775962c86587feb0f9ef841fa2ff4c",
    "title": "LgTS: Dynamic Task Sampling using LLM-generated sub-goals for Reinforcement Learning Agents",
    "abstract": "Recent advancements in reasoning abilities of Large Language Models (LLM) has promoted their usage in problems that require high-level planning for robots and artificial agents. However, current techniques that utilize LLMs for such planning tasks make certain key assumptions such as, access to datasets that permit finetuning, meticulously engineered prompts that only provide relevant and essential information to the LLM, and most importantly, a deterministic approach to allow execution of the LLM responses either in the form of existing policies or plan operators. In this work, we propose LgTS (LLM-guided Teacher-Student learning), a novel approach that explores the planning abilities of LLMs to provide a graphical representation of the sub-goals to a reinforcement learning (RL) agent that does not have access to the transition dynamics of the environment. The RL agent uses Teacher-Student learning algorithm to learn a set of successful policies for reaching the goal state from the start state while simultaneously minimizing the number of environmental interactions. Unlike previous methods that utilize LLMs, our approach does not assume access to a propreitary or a fine-tuned LLM, nor does it require pre-trained policies that achieve the sub-goals proposed by the LLM. Through experiments on a gridworld based DoorKey domain and a search-and-rescue inspired domain, we show that generating a graphical structure of sub-goals helps in learning policies for the LLM proposed sub-goals and the Teacher-Student learning algorithm minimizes the number of environment interactions when the transition dynamics are unknown.",
    "venue": "Adaptive Agents and Multi-Agent Systems",
    "year": 2023,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-14",
    "authors": [
      {
        "authorId": "2258550477",
        "name": "Yash Shukla"
      },
      {
        "authorId": "2258751660",
        "name": "Wenchang Gao"
      },
      {
        "authorId": "3379438",
        "name": "Vasanth Sarathy"
      },
      {
        "authorId": "2258715054",
        "name": "Alvaro Velasquez"
      },
      {
        "authorId": "2258551993",
        "name": "Robert Wright"
      },
      {
        "authorId": "1715858",
        "name": "Jivko Sinapov"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "74b0976a3a7b7013fd468a043a940dcf401e66f1",
    "url": "https://www.semanticscholar.org/paper/74b0976a3a7b7013fd468a043a940dcf401e66f1",
    "title": "User Modeling in the Era of Large Language Models: Current Research and Future Directions",
    "abstract": "User modeling (UM) aims to discover patterns or learn representations from user data about the characteristics of a specific user, such as profile, preference, and personality. The user models enable personalization and suspiciousness detection in many online applications such as recommendation, education, and healthcare. Two common types of user data are text and graph, as the data usually contain a large amount of user-generated content (UGC) and online interactions. The research of text and graph mining is developing rapidly, contributing many notable solutions in the past two decades. Recently, large language models (LLMs) have shown superior performance on generating, understanding, and even reasoning over text data. The approaches of user modeling have been equipped with LLMs and soon become outstanding. This article summarizes existing research about how and why LLMs are great tools of modeling and understanding UGC. Then it reviews a few categories of large language models for user modeling (LLM-UM) approaches that integrate the LLMs with text and graph-based methods in different ways. Then it introduces specific LLM-UM techniques for a variety of UM applications. Finally, it presents remaining challenges and future directions in the LLM-UM research. We maintain the reading list at: https://github.com/TamSiuhin/LLM-UM-Reading",
    "venue": "IEEE Data Engineering Bulletin",
    "year": 2023,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-12-11",
    "authors": [
      {
        "authorId": "2093186816",
        "name": "Zhaoxuan Tan"
      },
      {
        "authorId": "2275403324",
        "name": "Meng Jiang"
      }
    ],
    "source": "semantic_scholar",
    "score": 100.1415686865115
  },
  {
    "paperId": "8a6a72afbcc1080212d105f8de02239c8718ef35",
    "url": "https://www.semanticscholar.org/paper/8a6a72afbcc1080212d105f8de02239c8718ef35",
    "title": "Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in code generation. The integration of Chain of Thought (CoT) reasoning can further boost their performance. However, current CoT methods often require manual writing or LLMs with over 100 billion parameters to generate, impeding their applicability in resource-constrained scenarios. In this study, we investigate lightweight Language Models (<inline-formula><tex-math notation=\"LaTeX\">$\\ell$</tex-math><alternatives><mml:math><mml:mi>ℓ</mml:mi></mml:math><inline-graphic xlink:href=\"zhou-ieq1-3440503.gif\"/></alternatives></inline-formula>LMs), which are defined to have fewer than 10 billion parameters. Empirically, we find that most <inline-formula><tex-math notation=\"LaTeX\">$\\ell$</tex-math><alternatives><mml:math><mml:mi>ℓ</mml:mi></mml:math><inline-graphic xlink:href=\"zhou-ieq2-3440503.gif\"/></alternatives></inline-formula>LMs cannot generate high-quality CoTs when prompted by the few-shot method, but can take advantage of high-quality CoTs generated elsewhere to improve their performance in code generation. Based on these findings, we design a novel approach <monospace>COTTON</monospace> which can leverage <inline-formula><tex-math notation=\"LaTeX\">$\\ell$</tex-math><alternatives><mml:math><mml:mi>ℓ</mml:mi></mml:math><inline-graphic xlink:href=\"zhou-ieq3-3440503.gif\"/></alternatives></inline-formula>LMs to automatically generate CoTs for code generation. We synthesize new datasets and conduct extensive experiments on various benchmarks. The results show that the CoTs generated by <monospace>COTTON</monospace> outperform the baselines in terms of automated and human evaluation metrics. In particular, the CoTs generated by <monospace>COTTON</monospace> boost various <inline-formula><tex-math notation=\"LaTeX\">$\\ell$</tex-math><alternatives><mml:math><mml:mi>ℓ</mml:mi></mml:math><inline-graphic xlink:href=\"zhou-ieq4-3440503.gif\"/></alternatives></inline-formula>LMs to achieve higher performance gains than those generated by LLMs such as ChatGLM (130B), and are competitive with those generated by Gemini and gpt-3.5-turbo. The results also reveal that <monospace>COTTON</monospace> not only improves the performance of <inline-formula><tex-math notation=\"LaTeX\">$\\ell$</tex-math><alternatives><mml:math><mml:mi>ℓ</mml:mi></mml:math><inline-graphic xlink:href=\"zhou-ieq5-3440503.gif\"/></alternatives></inline-formula>LMs, but also enhances the performance of LLMs. Our study showcases the potential of <inline-formula><tex-math notation=\"LaTeX\">$\\ell$</tex-math><alternatives><mml:math><mml:mi>ℓ</mml:mi></mml:math><inline-graphic xlink:href=\"zhou-ieq6-3440503.gif\"/></alternatives></inline-formula>LMs in software engineering applications.",
    "venue": "IEEE Transactions on Software Engineering",
    "year": 2023,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2312.05562",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-09",
    "authors": [
      {
        "authorId": "2273904837",
        "name": "Guang Yang"
      },
      {
        "authorId": "144452434",
        "name": "Yu Zhou"
      },
      {
        "authorId": "2143738759",
        "name": "Xiang Chen"
      },
      {
        "authorId": "2262845618",
        "name": "Xiangyu Zhang"
      },
      {
        "authorId": "2080123731",
        "name": "Terry Yue Zhuo"
      },
      {
        "authorId": "2273533801",
        "name": "Taolue Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.79441541679836
  },
  {
    "paperId": "7a8918cd3491fa2350bf6a3813d5d2ce3b0db66a",
    "url": "https://www.semanticscholar.org/paper/7a8918cd3491fa2350bf6a3813d5d2ce3b0db66a",
    "title": "Semantic Mechanical Search with Large Vision and Language Models",
    "abstract": "Moving objects to find a fully-occluded target object, known as mechanical search, is a challenging problem in robotics. As objects are often organized semantically, we conjecture that semantic information about object relationships can facilitate mechanical search and reduce search time. Large pretrained vision and language models (VLMs and LLMs) have shown promise in generalizing to uncommon objects and previously unseen real-world environments. In this work, we propose a novel framework called Semantic Mechanical Search (SMS). SMS conducts scene understanding and generates a semantic occupancy distribution explicitly using LLMs. Compared to methods that rely on visual similarities offered by CLIP embeddings, SMS leverages the deep reasoning capabilities of LLMs. Unlike prior work that uses VLMs and LLMs as end-to-end planners, which may not integrate well with specialized geometric planners, SMS can serve as a plug-in semantic module for downstream manipulation or navigation policies. For mechanical search in closed-world settings such as shelves, we compare with a geometric-based planner and show that SMS improves mechanical search performance by 24% across the pharmacy, kitchen, and office domains in simulation and 47.1% in physical experiments. For open-world real environments, SMS can produce better semantic distributions compared to CLIP-based methods, with the potential to be integrated with downstream navigation policies to improve object navigation tasks. Code, data, videos, and the appendix are available: https://sites.google.com/view/semantic-mechanical-search",
    "venue": "Conference on Robot Learning",
    "year": 2023,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-02-24",
    "authors": [
      {
        "authorId": "2116532955",
        "name": "Satvik Sharma"
      },
      {
        "authorId": "144387882",
        "name": "K. Shivakumar"
      },
      {
        "authorId": "2146053008",
        "name": "Huang Huang"
      },
      {
        "authorId": "1387872831",
        "name": "Ryan Hoque"
      },
      {
        "authorId": "2003766365",
        "name": "A. Imran"
      },
      {
        "authorId": "2237986491",
        "name": "Brian Ichter"
      },
      {
        "authorId": "144344283",
        "name": "Ken Goldberg"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "9059dfa3406af7bb1d1a94bd778eef42f0449dff",
    "url": "https://www.semanticscholar.org/paper/9059dfa3406af7bb1d1a94bd778eef42f0449dff",
    "title": "Ladder-of-Thought: Using Knowledge as Steps to Elevate Stance Detection",
    "abstract": "Stance detection aims to determine the attitude or viewpoint expressed in a document regarding a specific target. Recent advancements in Large Language Models (LLMs), such as Chain-of-Thought (CoT) prompting, have improved the reasoning capabilities of these models by integrating intermediate rationales. However, the efficacy of CoT can be limited by the model’s internal knowledge, resulting in inaccurate rationales that compromise the subsequent stance prediction. This limitation could further lead to hallucinations, where LLMs produce unfaithful responses and erroneous reasoning, affecting the output’s reliability and precision. Moreover, CoT can be challenging to implement on smaller language models with constrained knowledge and reasoning depth, which raises concerns about efficiency. In response to these issues, we propose the Ladder-of-Thought (LoT), a novel framework using knowledge as steps to elevate stance detection. LoT implements a triple-phase Progressive Optimization Framework: 1) External Knowledge Injection, which aims to enrich the model’s intrinsic knowledge base; 2) Intermediate Knowledge Generation, allowing the model to generate more accurate and dependable intermediate knowledge to enhance the downstream prediction; and 3) Downstream Fine-tuning & Prediction, which aims to improve the model’s prediction accuracy. This sequential approach symbolizes ascending a ladder, with each phase representing a progressive step towards achieving optimal reasoning and prediction performance. Our empirical results have demonstrated that LoT achieves state-of-the-art results in zero-shot/few-shot and in-target stance detection, marking a 16% improvement over ChatGPT and a 10% enhancement compared to ChatGPT with CoT on stance detection task.",
    "venue": "IEEE International Joint Conference on Neural Network",
    "year": 2023,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.16763",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-08-31",
    "authors": [
      {
        "authorId": "2236645169",
        "name": "Kairui Hu"
      },
      {
        "authorId": "2047087220",
        "name": "Ming Yan"
      },
      {
        "authorId": "10638646",
        "name": "Joey Tianyi Zhou"
      },
      {
        "authorId": "1807998",
        "name": "I. Tsang"
      },
      {
        "authorId": "2223253",
        "name": "Wen-Haw Chong"
      },
      {
        "authorId": "1742123174",
        "name": "Yong Keong Yap"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.79441541679836
  },
  {
    "paperId": "60e6e3767c36bf9e16b58b7221c5712b4d3d5293",
    "url": "https://www.semanticscholar.org/paper/60e6e3767c36bf9e16b58b7221c5712b4d3d5293",
    "title": "Large Language Models Are Semi-Parametric Reinforcement Learning Agents",
    "abstract": "Inspired by the insights in cognitive science with respect to human memory and reasoning mechanism, a novel evolvable LLM-based (Large Language Model) agent framework is proposed as REMEMBERER. By equipping the LLM with a long-term experience memory, REMEMBERER is capable of exploiting the experiences from the past episodes even for different task goals, which excels an LLM-based agent with fixed exemplars or equipped with a transient working memory. We further introduce Reinforcement Learning with Experience Memory (RLEM) to update the memory. Thus, the whole system can learn from the experiences of both success and failure, and evolve its capability without fine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER constitutes a semi-parametric RL agent. Extensive experiments are conducted on two RL task sets to evaluate the proposed framework. The average results with different initialization and training sets exceed the prior SOTA by 4% and 2% for the success rate on two task sets and demonstrate the superiority and robustness of REMEMBERER.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-09",
    "authors": [
      {
        "authorId": "2118333",
        "name": "Danyang Zhang"
      },
      {
        "authorId": "1390833791",
        "name": "Lu Chen"
      },
      {
        "authorId": "2108339988",
        "name": "Situo Zhang"
      },
      {
        "authorId": "2155908631",
        "name": "Hongshen Xu"
      },
      {
        "authorId": "1806179720",
        "name": "Zihan Zhao"
      },
      {
        "authorId": "1736727",
        "name": "Kai Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 116.58585994422887
  },
  {
    "paperId": "19b43ff57e5d8f8a99da4110fbc30b4ecc39a527",
    "url": "https://www.semanticscholar.org/paper/19b43ff57e5d8f8a99da4110fbc30b4ecc39a527",
    "title": "Spoken Language Intelligence of Large Language Models for Language Learning",
    "abstract": "People have long hoped for a conversational system that can assist in real-life situations, and recent progress on large language models (LLMs) is bringing this idea closer to reality. While LLMs are often impressive in performance, their efficacy in real-world scenarios that demand expert knowledge remains unclear. LLMs are believed to hold the most potential and value in education, especially in the development of Artificial intelligence (AI) based virtual teachers capable of facilitating language learning. Our focus is centered on evaluating the efficacy of LLMs in the realm of education, specifically in the areas of spoken language learning which encompass phonetics, phonology, and second language acquisition. We introduce a new multiple-choice question dataset to evaluate the effectiveness of LLMs in the aforementioned scenarios, including understanding and application of spoken language knowledge. In addition, we investigate the influence of various prompting techniques such as zero- and few-shot method (prepending the question with question-answer exemplars), chain-of-thought (CoT, think step-by-step), in-domain exampler and external tools (Google, Wikipedia). We conducted large-scale evaluation on popular LLMs (20 distinct models) using these methods. We achieved significant performance improvements compared to the zero-shot baseline in the practical questions reasoning (GPT-3.5, 49.1% ->63.1%; LLaMA2-70B-Chat, 42.2% ->48.6%). We found that models of different sizes have good understanding of concepts in phonetics, phonology, and second language acquisition, but show limitations in reasoning for real-world problems. Additionally, we also explore preliminary findings on conversational communication.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.14536",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-08-28",
    "authors": [
      {
        "authorId": null,
        "name": "Linkai Peng"
      },
      {
        "authorId": "2234373508",
        "name": "Baorian Nuchged"
      },
      {
        "authorId": "2118543175",
        "name": "Yingming Gao"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "ab77185746975f9acf02a80387f8f0df5767a5af",
    "url": "https://www.semanticscholar.org/paper/ab77185746975f9acf02a80387f8f0df5767a5af",
    "title": "TokenPacker: Efficient Visual Projector for Multimodal LLM",
    "abstract": "The visual projector serves as an essential bridge between the visual encoder and the Large Language Model (LLM) in a Multimodal LLM (MLLM). Typically, MLLMs adopt a simple MLP to preserve all visual contexts via one-to-one transformation. However, the visual tokens are redundant and can be considerably increased when dealing with high-resolution images, impairing the efficiency of MLLMs significantly. Some recent works have introduced resampler or abstractor to reduce the number of resulting visual tokens. Unfortunately, they fail to capture finer details and undermine the visual reasoning capabilities of MLLMs. In this work, we propose a novel visual projector, which adopts a coarse-to-fine scheme to inject the enriched characteristics to generate the condensed visual tokens. In specific, we first interpolate the visual features as a low-resolution point query, providing the overall visual representation as the foundation. Then, we introduce a region-to-point injection module that utilizes high-resolution, multi-level region-based cues as fine-grained reference keys and values, allowing them to be fully absorbed within the corresponding local context region. This step effectively updates the coarse point query, transforming it into an enriched one for the subsequent LLM reasoning. Extensive experiments demonstrate that our approach compresses the visual tokens by 75%~89%, while achieves comparable or even better performance across diverse benchmarks with significantly higher efficiency. The source codes can be found at https://github.com/CircleRadon/TokenPacker.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 19,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-02",
    "authors": [
      {
        "authorId": "2258782665",
        "name": "Wentong Li"
      },
      {
        "authorId": "2258794044",
        "name": "Yuqian Yuan"
      },
      {
        "authorId": "2258797383",
        "name": "Jian Liu"
      },
      {
        "authorId": "2258768941",
        "name": "Dongqi Tang"
      },
      {
        "authorId": "2258801185",
        "name": "Song Wang"
      },
      {
        "authorId": "2258980400",
        "name": "Jianke Zhu"
      },
      {
        "authorId": "2256831252",
        "name": "Lei Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.93598410330986
  },
  {
    "paperId": "46299fee72ca833337b3882ae1d8316f44b32b3c",
    "url": "https://www.semanticscholar.org/paper/46299fee72ca833337b3882ae1d8316f44b32b3c",
    "title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
    "abstract": "Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective heuristic that enables the agent to pinpoint hallucination instances, avoid repetition in action sequences, and, in some environments, construct an internal memory map of the given environment. To assess our approach, we evaluate the agent’s ability to complete decision-making tasks in AlfWorld environments and knowledge-intensive, search-based question-and-answer tasks in HotPotQA environments. We observe success rates of 97% and 51%, respectively, and provide a discussion on the emergent property of self-reflection.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 243,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2303.11366",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2212367248",
        "name": "Noah Shinn"
      },
      {
        "authorId": "2212367414",
        "name": "Beck Labash"
      },
      {
        "authorId": "2162047785",
        "name": "A. Gopinath"
      }
    ],
    "source": "semantic_scholar",
    "score": 152.45752337939803
  },
  {
    "paperId": "303b4ae0e53cca8d007076778e5c17e28116ff7e",
    "url": "https://www.semanticscholar.org/paper/303b4ae0e53cca8d007076778e5c17e28116ff7e",
    "title": "Learning to Decode Collaboratively with Multiple Language Models",
    "abstract": "We propose a method to teach multiple large language models (LLM) to collaborate by interleaving their generations at the token level. We model the decision of which LLM generates the next token as a latent variable. By optimizing the marginal likelihood of a training set under our latent variable model, the base LLM automatically learns when to generate itself and when to call on one of the ``assistant'' language models to generate, all without direct supervision. Token-level collaboration during decoding allows for a fusion of each model's expertise in a manner tailored to the specific task at hand. Our collaborative decoding is especially useful in cross-domain settings where a generalist base LLM learns to invoke domain expert models. On instruction-following, domain-specific QA, and reasoning tasks, we show that the performance of the joint system exceeds that of the individual models. Through qualitative analysis of the learned latent decisions, we show models trained with our method exhibit several interesting collaboration patterns, e.g., template-filling. Our code is available at https://github.com/clinicalml/co-llm.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 17,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-06",
    "authors": [
      {
        "authorId": "101568984",
        "name": "Zejiang Shen"
      },
      {
        "authorId": "30155522",
        "name": "Hunter Lang"
      },
      {
        "authorId": "2257409822",
        "name": "Bailin Wang"
      },
      {
        "authorId": "2257354404",
        "name": "Yoon Kim"
      },
      {
        "authorId": "2266751797",
        "name": "David Sontag"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.35557636844247
  },
  {
    "paperId": "0ea6b7371017721d87f9c5b32b084bd1ca762532",
    "url": "https://www.semanticscholar.org/paper/0ea6b7371017721d87f9c5b32b084bd1ca762532",
    "title": "S3HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering",
    "abstract": "Answering multi-hop questions over hybrid factual knowledge from the given text and table (TextTableQA) is a challenging task. Existing models mainly adopt a retriever-reader framework, which have several deficiencies, such as noisy labeling in training retriever, insufficient utilization of heterogeneous information over text and table, and deficient ability for different reasoning operations. In this paper, we propose a three-stage TextTableQA framework S3HQA, which comprises of retriever, selector, and reasoner. We use a retriever with refinement training to solve the noisy labeling problem. Then, a hybrid selector considers the linked relationships between heterogeneous data to select the most relevant factual knowledge. For the final stage, instead of adapting a reading comprehension module like in previous methods, we employ a generation-based reasoner to obtain answers. This includes two approaches: a row-wise generator and an LLM prompting generator (first time used in this task). The experimental results demonstrate that our method achieves competitive results in the few-shot setting. When trained on the full dataset, our approach outperforms all baseline methods, ranking first on the HybridQA leaderboard.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 11,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.11725",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-19",
    "authors": [
      {
        "authorId": "2257004176",
        "name": "Fangyu Lei"
      },
      {
        "authorId": "1737850",
        "name": "Xiang Lorraine Li"
      },
      {
        "authorId": "2216484616",
        "name": "Yifan Wei"
      },
      {
        "authorId": "1954845",
        "name": "Shizhu He"
      },
      {
        "authorId": "2218096160",
        "name": "Yiming Huang"
      },
      {
        "authorId": "11447228",
        "name": "Jun Zhao"
      },
      {
        "authorId": "2200096",
        "name": "Kang Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "6bca998065fd7644b4b5fca4b717529c10c32941",
    "url": "https://www.semanticscholar.org/paper/6bca998065fd7644b4b5fca4b717529c10c32941",
    "title": "Large Language Model Is Semi-Parametric Reinforcement Learning Agent",
    "abstract": "Inspired by the insights in cognitive science with respect to human memory and reasoning mechanism, a novel evolvable LLM-based (Large Language Model) agent framework is proposed as R EMEMBERER . By equipping the LLM with a long-term experience memory, R EMEMBERER is capable of exploiting the experiences from the past episodes even for different task goals, which excels an LLM-based agent with fixed exemplars or equipped with a transient working memory. We further introduce R einforcement L earning with E xperience M emory ( RLEM ) to update the memory. Thus, the whole system can learn from the experiences of both success and failure, and evolve its capability without fine-tuning the parameters of the LLM. In this way, the proposed R EMEMBERER constitutes a semi-parametric RL agent. Extensive experiments are conducted on two RL task sets to evaluate the proposed framework. The average results with different initialization and training sets exceed the prior SOTA by 4% and 2% for the success rate on two task sets and demonstrate the superiority and robustness of R EMEMBERER .",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 11,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.07929",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2118333",
        "name": "Danyang Zhang"
      },
      {
        "authorId": "2281715543",
        "name": "Lu Chen"
      },
      {
        "authorId": "2283452280",
        "name": "Situo Zhang"
      },
      {
        "authorId": "2155908631",
        "name": "Hongshen Xu"
      },
      {
        "authorId": "1806179720",
        "name": "Zihan Zhao"
      },
      {
        "authorId": "2281719276",
        "name": "Kai Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "edfff0e15449f438a13a7341290c008bf6486afc",
    "url": "https://www.semanticscholar.org/paper/edfff0e15449f438a13a7341290c008bf6486afc",
    "title": "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language Models Fine-tuning",
    "abstract": "Low-rank adaptation (LoRA) and its mixture-of-experts (MOE) variants are highly effective parameter-efficient fine-tuning (PEFT) methods. However, they introduce significant latency in multi-tenant settings due to the LoRA modules and MOE routers added to multiple linear modules in the Transformer layer. To address this issue, we propose Mixture of Low-Rank Adaptation (MiLoRA), a novel and efficient LoRA variant. MiLoRA differs from previous MOE-style LoRA methods by considering each LoRA module as an expert and employing a prompt-aware routing mechanism. This mechanism calculates expert routing results once before generating the first new token and reuses these results for subsequent tokens, reducing latency. Extensive experiments and analysis on commonsense reasoning tasks, math reasoning tasks, and widely used LLM evaluation benchmarks demonstrate that MiLoRA consistently outperforms strong PEFT baselines with comparable tunable parameter budgets. Additionally, MiLoRA significantly reduces latency in multi-tenant settings compared to previous LoRA-based methods.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-10-23",
    "authors": [
      {
        "authorId": "2327293591",
        "name": "Jingfan Zhang"
      },
      {
        "authorId": "2322997112",
        "name": "Yi Zhao"
      },
      {
        "authorId": "2327694040",
        "name": "Dan Chen"
      },
      {
        "authorId": "2293390999",
        "name": "Xing Tian"
      },
      {
        "authorId": "2179528564",
        "name": "Huanran Zheng"
      },
      {
        "authorId": "2322888603",
        "name": "Wei Zhu"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "f887ace024e9eed2dabdcf54921c2b6597f93398",
    "url": "https://www.semanticscholar.org/paper/f887ace024e9eed2dabdcf54921c2b6597f93398",
    "title": "Grounded physical language understanding with probabilistic programs and simulated worlds",
    "abstract": "Human language richly invokes our intuitive physical knowledge. We talk about physical objects, scenes, properties, and events; and we can ask questions and answer them with predictions and inferences about physical worlds described entirely in language. How does language construct meanings that connect to our general physical reasoning? In this paper, we pro-pose PiLoT , a computational model that maps language into a probabilistic language of thought —meanings are constructed as probabilistic programs, which provide a formal basis for probabilistic and physical reasoning. Our model uses a large language model (LLM) to map from language to meanings and a probabilistic physics engine to support inferences over scenes described in language. We conduct a linguistic reasoning experiment based on prior psychophysics studies that requires reasoning about physical outcomes based on linguistic descriptions. We show that PiLoT well predicts human judgments across this experiment and outperforms baseline models which use the LLM to directly perform the same task.",
    "venue": "Annual Meeting of the Cognitive Science Society",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "152592091",
        "name": "Cedegao Zhang"
      },
      {
        "authorId": "2147199509",
        "name": "L. Wong"
      },
      {
        "authorId": "35748708",
        "name": "Gabriel Grand"
      },
      {
        "authorId": "2287922814",
        "name": "Josh Tenenbaum"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.95836866004329
  },
  {
    "paperId": "e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0",
    "url": "https://www.semanticscholar.org/paper/e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0",
    "title": "LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models",
    "abstract": "Recent advancements in text-to-image diffusion models have yielded impressive results in generating realistic and diverse images. However, these models still struggle with complex prompts, such as those that involve numeracy and spatial reasoning. This work proposes to enhance prompt understanding capabilities in diffusion models. Our method leverages a pretrained large language model (LLM) for grounded generation in a novel two-stage process. In the first stage, the LLM generates a scene layout that comprises captioned bounding boxes from a given prompt describing the desired image. In the second stage, a novel controller guides an off-the-shelf diffusion model for layout-grounded image generation. Both stages utilize existing pretrained models without additional model parameter optimization. Our method significantly outperforms the base diffusion model and several strong baselines in accurately generating images according to prompts that require various capabilities, doubling the generation accuracy across four tasks on average. Furthermore, our method enables instruction-based multi-round scene specification and can handle prompts in languages not supported by the underlying diffusion model. We anticipate that our method will unleash users' creativity by accurately following more complex prompts. Our code, demo, and benchmark are available at: https://llm-grounded-diffusion.github.io",
    "venue": "Trans. Mach. Learn. Res.",
    "year": 2023,
    "citationCount": 114,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2305.13655",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-23",
    "authors": [
      {
        "authorId": "2054733874",
        "name": "Long Lian"
      },
      {
        "authorId": "2132473300",
        "name": "Boyi Li"
      },
      {
        "authorId": "3383328",
        "name": "Adam Yala"
      },
      {
        "authorId": "1753210",
        "name": "Trevor Darrell"
      }
    ],
    "source": "semantic_scholar",
    "score": 141.17398192544874
  },
  {
    "paperId": "e4bbd1cfa8e53e529b5e8310838a2e455985137d",
    "url": "https://www.semanticscholar.org/paper/e4bbd1cfa8e53e529b5e8310838a2e455985137d",
    "title": "Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long Videos",
    "abstract": "Temporal Sentence Grounding (TSG), which aims to localize moments from videos based on the given natural language queries, has attracted widespread attention. Existing works are mainly designed for short videos, failing to handle TSG in long videos, which poses two challenges: i) complicated contexts in long videos require temporal reasoning over longer moment sequences, and ii) multiple modalities including textual speech with rich information require special designs for content understanding in long videos. To tackle these challenges, in this work we propose a Grounding-Prompter method, which is capable of conducting TSG in long videos through prompting LLM with multimodal information. In detail, we first transform the TSG task and its multimodal inputs including speech and visual, into compressed task textualization. Furthermore, to enhance temporal reasoning under complicated contexts, a Boundary-Perceptive Prompting strategy is proposed, which contains three folds: i) we design a novel Multiscale Denoising Chain-of-Thought (CoT) to combine global and local semantics with noise filtering step by step, ii) we set up validity principles capable of constraining LLM to generate reasonable predictions following specific formats, and iii) we introduce one-shot In-Context-Learning (ICL) to boost reasoning through imitation, enhancing LLM in TSG task understanding. Experiments demonstrate the state-of-the-art performance of our Grounding-Prompter method, revealing the benefits of prompting LLM with multimodal information for TSG in long videos.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-28",
    "authors": [
      {
        "authorId": "2261888448",
        "name": "Houlun Chen"
      },
      {
        "authorId": "2256599610",
        "name": "Xin Wang"
      },
      {
        "authorId": "2191043236",
        "name": "Hong Chen"
      },
      {
        "authorId": "2261934586",
        "name": "Zihan Song"
      },
      {
        "authorId": "2276961914",
        "name": "Jia Jia"
      },
      {
        "authorId": "2156154955",
        "name": "Wenwu Zhu"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "e3b0bef1b90132477330dc9b81eb57838eb79996",
    "url": "https://www.semanticscholar.org/paper/e3b0bef1b90132477330dc9b81eb57838eb79996",
    "title": "Performance of ChatGPT on the Plastic Surgery Inservice Training Examination.",
    "abstract": "BACKGROUND\nDeveloped originally as a tool for resident self-evaluation, the Plastic Surgery Inservice Training Examination (PSITE) has become a standardized tool adopted by plastic surgery residency programs. The introduction of large language models (LLMs), such as ChatGPT (OpenAI, San Francisco, CA), has demonstrated the potential to help propel the field of plastic surgery.\n\n\nOBJECTIVES\nThe authors of this study wanted to assess whether or not ChatGPT could be utilized as a tool in resident education by assessing its accuracy on the PSITE.\n\n\nMETHODS\nQuestions were obtained from the 2022 PSITE, which was present on the American Council of Academic Plastic Surgeons (ACAPS) website. Questions containing images or tables were carefully inspected and flagged before being inputted into ChatGPT. All responses by ChatGPT were qualified utilizing the properties of natural coherence. Responses that were found to be incorrect were divided into the following categories: logical, informational, or explicit fallacy.\n\n\nRESULTS\nChatGPT answered a total of 242 questions with an accuracy of 54.96%. The software incorporated logical reasoning in 88.8% of questions, internal information in 95.5% of questions, and external information in 92.1% of questions. When stratified by correct and incorrect responses, we determined that there was a statistically significant difference in ChatGPT's use of external information (p <0.05).\n\n\nCONCLUSIONS\nChatGPT is a versatile tool that has the potential to impact resident education by providing general knowledge, clarifying information, providing case-based learning, and promoting evidence-based medicine. With advancements in LLM and artificial intelligence (AI), it is possible that ChatGPT may be an impactful tool for resident education within plastic surgery.",
    "venue": "Aesthetic surgery journal",
    "year": 2023,
    "citationCount": 52,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-02",
    "authors": [
      {
        "authorId": "11169859",
        "name": "Rohun R. Gupta"
      },
      {
        "authorId": "2191377475",
        "name": "Isabel Herzog"
      },
      {
        "authorId": "2191381283",
        "name": "John B. Park"
      },
      {
        "authorId": "51139875",
        "name": "J. Weisberger"
      },
      {
        "authorId": "51127395",
        "name": "Peter K. Firouzbakht"
      },
      {
        "authorId": "2215965763",
        "name": "Vanessa A Ocon"
      },
      {
        "authorId": "144095050",
        "name": "J. Chao"
      },
      {
        "authorId": "19451585",
        "name": "Edward S. Lee"
      },
      {
        "authorId": "5874624",
        "name": "B. Mailey"
      }
    ],
    "source": "semantic_scholar",
    "score": 129.55437870328183
  },
  {
    "paperId": "3e712cddb347c464ac810dbc7d61a8e1a04978b0",
    "url": "https://www.semanticscholar.org/paper/3e712cddb347c464ac810dbc7d61a8e1a04978b0",
    "title": "FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering",
    "abstract": "Knowledge base question answering (KBQA) is a critical yet challenging task due to the vast number of entities within knowledge bases and the diversity of natural language questions posed by users. Unfortunately, the performance of most KBQA models tends to decline significantly in real-world scenarios where high-quality annotated data is insufficient. To mitigate the burden associated with manual annotation, we introduce FlexKBQA by utilizing Large Language Models (LLMs) as program translators for addressing the challenges inherent in the few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms to sample diverse programs, such as SPARQL queries, from the knowledge base, which are subsequently converted into natural language questions via LLMs. This synthetic dataset facilitates training a specialized lightweight model for the KB. Additionally, to reduce the barriers of distribution shift between synthetic data and real user questions, FlexKBQA introduces an executionguided self-training method to iterative leverage unlabeled user questions. Furthermore, we explore harnessing the inherent reasoning capability of LLMs to enhance the entire framework. Consequently, FlexKBQA delivers substantial flexibility, encompassing data annotation, deployment, and being domain agnostic. Through extensive experiments on GrailQA, WebQSP, and KQA Pro, we observe that under the few-shot even the more challenging zero-shot scenarios, FlexKBQA achieves impressive results with a few annotations, surpassing all previous baselines and even approaching the performance of supervised models, achieving a remarkable 93% performance relative to the fully-supervised models. We posit that FlexKBQA represents a significant advancement towards exploring better integration of large and lightweight models. Code is available at https://github.com/leezythu/FlexKBQA.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2023,
    "citationCount": 48,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.12060",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-08-23",
    "authors": [
      {
        "authorId": "2155354022",
        "name": "Zhenyu Li"
      },
      {
        "authorId": "2272781120",
        "name": "Sunqi Fan"
      },
      {
        "authorId": "2022231256",
        "name": "Yu Gu"
      },
      {
        "authorId": "2116521868",
        "name": "Xiuxing Li"
      },
      {
        "authorId": "2150461809",
        "name": "Zhichao Duan"
      },
      {
        "authorId": "2057587396",
        "name": "Bo Dong"
      },
      {
        "authorId": "2152354910",
        "name": "Ning Liu"
      },
      {
        "authorId": "2115642141",
        "name": "Jianyong Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 128.3773044716594
  },
  {
    "paperId": "6ba549097ef898a7548c48c0b718b323eec5dbc3",
    "url": "https://www.semanticscholar.org/paper/6ba549097ef898a7548c48c0b718b323eec5dbc3",
    "title": "Performance of Multimodal GPT-4V on USMLE with Image: Potential for Imaging Diagnostic Support with Explanations",
    "abstract": "Importance Using artificial intelligence (AI) to help clinical diagnoses has been an active research topic for more than six decades. Few research however has the scale and accuracy that can be turned into clinical practice. The tide may be turned today with the power of large language models (LLMs). In this application, we evaluated the accuracy of medical license exam using the newly released Generative Pre-trained Transformer 4 with vision (GPT-4V), a large multimodal model trained to analyze image inputs with the text instructions from the user. This study is the first to evaluate GPTs for interpreting medical images. Objective This study aimed to evaluate the performance of GPT-4V on medical licensing examination questions with images, as well as to analyze interpretability. Design, Setting, and Participants We used 3 sets of multiple-choice questions with images to evaluate GPT-4V performance. The first set was the United States Medical Licensing Examination (USMLE) from the National Board of Medical Examiners (NBME) sample questions in step1, step2CK, and step3. The second set was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the user base. The third set was the Diagnostic Radiology Qualifying Core Exam (DRQCE) from the American Board of Radiology. The study (including data analysis) was conducted from September to October 2023. Main Outcomes and Measures The choice accuracy of GPT-4V was compared to two other large language models, GPT-4 and ChatGPT. The GPT-4V explanation was evaluated across 4 qualitative metrics: image misunderstanding, text hallucination, reasoning error, and non-medical error. Results Of the 3 exams with images, NBME, AMBOSS, and DRQCE, GPT-4V achieved accuracies of 86.2%, 62.0%, and 73.1%, respectively. GPT-4V outperformed ChatGPT and GPT-4 by 131.8% and 64.5% on average across various data sets. The model demonstrated a decreasing trend in performance as question difficulty increased in the AMBOSS dataset. GPT-4V achieves an accuracy of 90.7% in the full USMLE exam, outperforming the passing threshold of about 60% accuracy. Among the incorrect answers, 75.9% of responses included misinterpretation of the image. However, 39.0% of them could be easily solved with a short hint. Conclusion In this cross-sectional study, GPT-4V achieved a high accuracy of USMLE that was in the 70th - 80th percentile with AMBOSS users preparing for the exam. The results suggest the potential of GPT-4V for clinical decision support. However, GPT-4V generated explanation revealed several issues. It needs to improve explanation quality for potential use in clinical decision support.",
    "venue": "medRxiv",
    "year": 2023,
    "citationCount": 34,
    "openAccessPdf": {
      "url": "https://www.medrxiv.org/content/medrxiv/early/2023/11/15/2023.10.26.23297629.full.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-10-26",
    "authors": [
      {
        "authorId": "2261745452",
        "name": "MSc Zhichao Yang"
      },
      {
        "authorId": "1576489304",
        "name": "Zonghai Yao"
      },
      {
        "authorId": "2261745143",
        "name": "BSc Mahbuba Tasmin"
      },
      {
        "authorId": "2261746471",
        "name": "BSc Parth Vashisht"
      },
      {
        "authorId": "2261746469",
        "name": "Won"
      },
      {
        "authorId": "2261745082",
        "name": "RN MSc Seok Jang"
      },
      {
        "authorId": "2261745076",
        "name": "BSc Beining Wang"
      },
      {
        "authorId": "2261745458",
        "name": "MD Dan Berlowitz"
      },
      {
        "authorId": "2261745099",
        "name": "PhD Hong Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 123.3302209223412
  },
  {
    "paperId": "7bcd5c0b17560ee560aec903ea42487a1a54e5d9",
    "url": "https://www.semanticscholar.org/paper/7bcd5c0b17560ee560aec903ea42487a1a54e5d9",
    "title": "BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models",
    "abstract": "Vision language models (VLMs) perceive the world through a combination of a visual encoder and a large language model (LLM). The visual encoder, pre-trained on large-scale vision-text datasets, provides zero-shot generalization to visual data, and the LLM endows its high reasoning ability to VLMs. It leads VLMs to achieve high performance on wide benchmarks without fine-tuning, exhibiting zero or few-shot capability. However, recent studies show that VLMs are vulnerable to hallucination. This undesirable behavior degrades reliability and credibility, thereby making users unable to fully trust the output from VLMs. To enhance trustworthiness and better tackle the hallucination of VLMs, we curate a new evaluation dataset, called the BEfore-AFter hallucination dataset (BEAF), and introduce new metrics: True Understanding (TU), IGnorance (IG), StuBbornness (SB), and InDecision (ID). Unlike prior works that focus only on constructing questions and answers, the key idea of our benchmark is to manipulate visual scene information by image editing models and to design the metrics based on scene changes. This allows us to clearly assess whether VLMs correctly understand a given scene by observing the ability to perceive changes. We also visualize image-wise object relationship by virtue of our two-axis view: vision and text. Upon evaluating VLMs with our dataset, we observed that our metrics reveal different aspects of VLM hallucination that have not been reported before. Project page: \\url{https://beafbench.github.io/}",
    "venue": "European Conference on Computer Vision",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-18",
    "authors": [
      {
        "authorId": "2123211349",
        "name": "Moon Ye-Bin"
      },
      {
        "authorId": "2123210120",
        "name": "Nam Hyeon-Woo"
      },
      {
        "authorId": "2239028163",
        "name": "Wonseok Choi"
      },
      {
        "authorId": "2238955420",
        "name": "Tae-Hyun Oh"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "aeeadd43a95ac7af93f6fbf7b165655157a30055",
    "url": "https://www.semanticscholar.org/paper/aeeadd43a95ac7af93f6fbf7b165655157a30055",
    "title": "Large Language Models: The Next Frontier for Variable Discovery within Metamorphic Testing?",
    "abstract": "Metamorphic testing involves reasoning on necessary properties that a program under test should exhibit regarding multiple input and output variables. A general approach consists of extracting metamorphic relations from auxiliary artifacts such as user manuals or documentation, a strategy particularly fitting to testing scientific software. However, such software typically has large input-output spaces, and the fundamental prerequisite – extracting variables of interest – is an arduous and non-scalable process when performed manually. To this end, we devise a workflow around an autoregressive transformer-based Large Language Model (LLM) towards the extraction of variables from user manuals of scientific software. Our end-to-end approach, besides a prompt specification consisting of few-shot examples by a human user, is fully automated, in contrast to current practice requiring human intervention. We showcase our LLM workflow over a real case, and compare variables extracted to ground truth manually labelled by experts. Our preliminary results show that our LLM-based workflow achieves an accuracy of 0.87, while successfully deriving 61.8% of variables as partial matches and 34.7% as exact matches.",
    "venue": "IEEE International Conference on Software Analysis, Evolution, and Reengineering",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-03-01",
    "authors": [
      {
        "authorId": "3050189",
        "name": "Christos Tsigkanos"
      },
      {
        "authorId": "2044478278",
        "name": "Pooja Rani"
      },
      {
        "authorId": "2112731093",
        "name": "S. Müller"
      },
      {
        "authorId": "1795801",
        "name": "Timo Kehrer"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.95836866004329
  },
  {
    "paperId": "651d7d131414c72b118b7b3061df05784a203301",
    "url": "https://www.semanticscholar.org/paper/651d7d131414c72b118b7b3061df05784a203301",
    "title": "I3: Intent-Introspective Retrieval Conditioned on Instructions",
    "abstract": "Recent studies indicate that dense retrieval models struggle to perform well on a wide variety of retrieval tasks that lack dedicated training data, as different retrieval tasks often entail distinct search intents. To address this challenge, in this work we leverage instructions to flexibly describe retrieval intents and introduce I3, a unified retrieval system that performs Intent-Introspective retrieval across various tasks, conditioned on Instructions without any task-specific training. I3 innovatively incorporates a pluggable introspector in a parameter-isolated manner to comprehend specific retrieval intents by jointly reasoning over the input query and instruction, and seamlessly integrates the introspected intent into the original retrieval model for intent-aware retrieval. Furthermore, we propose progressively-pruned intent learning. It utilizes extensive LLM-generated data to train I3 phase-by-phase, embodying two key designs: progressive structure pruning and drawback extrapolation-based data refinement. Extensive experiments show that in the BEIR benchmark, I3 significantly outperforms baseline methods designed with task-specific retrievers, achieving state-of-the-art zero-shot performance without any task-specific tuning.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2023,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-08-19",
    "authors": [
      {
        "authorId": "2212175601",
        "name": "Kaihang Pan"
      },
      {
        "authorId": "2108998895",
        "name": "Juncheng Li"
      },
      {
        "authorId": "2298418808",
        "name": "Wenjie Wang"
      },
      {
        "authorId": "46959445",
        "name": "Hao Fei"
      },
      {
        "authorId": "2122383983",
        "name": "Hongye Song"
      },
      {
        "authorId": "2150078659",
        "name": "Wei Ji"
      },
      {
        "authorId": "2110808728",
        "name": "Jun Lin"
      },
      {
        "authorId": "1713802",
        "name": "Xiaozhong Liu"
      },
      {
        "authorId": "2279753672",
        "name": "Tat-Seng Chua"
      },
      {
        "authorId": "2118071462",
        "name": "Siliang Tang"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.87639203842082
  },
  {
    "paperId": "41cb6257f262822ff517e2ff5226c283b674caa4",
    "url": "https://www.semanticscholar.org/paper/41cb6257f262822ff517e2ff5226c283b674caa4",
    "title": "Performance of ChatGPT on the MCAT: The Road to Personalized and Equitable Premedical Learning",
    "abstract": "Despite an increasingly diverse population, an unmet demand for undergraduates from underrepresented racial and ethnic minority (URM) backgrounds exists in the field of medicine as a result of financial hurdles and insufficient educational support faced by URM students in the premedical journey. With the capacity to provide highly individualized and accessible no- or low-cost dynamic instruction, large language models (LLMs) and their chatbot derivatives are posed to change this dynamic and subsequently help shape a more diverse future physician workforce. While studies have established the passing performance and insightful explanations of one of the most accurate LLM-powered chatbots to date, Chat Generative Pre-trained Transformer (ChatGPT), on standardized exams such as medical licensing exams, the role of ChatGPT in premedical education remains unknown. We evaluated the performance of ChatGPT on the Medical College Admission Test (MCAT), a standardized 230-question multiple choice exam that assesses a broad range of competencies in the natural, physical, social, and behavioral sciences as well as critical analysis and reasoning. Depending on its visual item response strategy, ChatGPT performed at or above the median performance of 276,779 student test takers on the MCAT. Additionally, ChatGPT-generated answers demonstrated both a high level of agreement with the official answer key as well as insight into its explanations. Based on these promising results, we anticipate two primary applications of ChatGPT and future LLM iterations in premedical education: firstly, such models could provide free or low-cost access to personalized and insightful explanations of MCAT competency-related questions to help students from all socioeconomic and URM backgrounds. Secondly, they could be used to generate additional test questions by test-makers or for targeted preparation by pre-medical students. These applications of ChatGPT in premedical education could be an invaluable, innovative path forward to increase diversity and improve equity among premedical students.",
    "venue": "medRxiv",
    "year": 2023,
    "citationCount": 21,
    "openAccessPdf": {
      "url": "https://www.medrxiv.org/content/medrxiv/early/2023/06/06/2023.03.05.23286533.full.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-03-07",
    "authors": [
      {
        "authorId": "2141395897",
        "name": "Vikas L. Bommineni"
      },
      {
        "authorId": "90147947",
        "name": "Sanaea Bhagwagar"
      },
      {
        "authorId": "2272704415",
        "name": "Daniel Balcarcel"
      },
      {
        "authorId": "2272955941",
        "name": "Vishal M Bommineni"
      },
      {
        "authorId": "2272704447",
        "name": "Christos"
      },
      {
        "authorId": "2272703429",
        "name": "Davazitkos"
      },
      {
        "authorId": "2272703715",
        "name": "Donald Boyer"
      }
    ],
    "source": "semantic_scholar",
    "score": 116.36563680037474
  },
  {
    "paperId": "5215df03f74314b4effcc76b2b26fb7b525be53a",
    "url": "https://www.semanticscholar.org/paper/5215df03f74314b4effcc76b2b26fb7b525be53a",
    "title": "ChatPose: Chatting about 3D Human Pose",
    "abstract": "We introduce ChatPose, a framework employing Large Language Models (LLMs) to understand and reason about 3D human poses from images or textual descriptions. Our work is motivated by the human ability to intuitively under-stand postures from a single image or a brief description, a process that intertwines image interpretation, world knowl-edge, and an understanding of body language. Traditional human pose estimation and generation methods often op-erate in isolation, lacking semantic understanding and rea-soning abilities. ChatPose addresses these limitations by embedding SMPL poses as distinct signal tokens within a multimodal LLM, enabling the direct generation of 3D body poses from both textual and visual inputs. Leveraging the powerful capabilities of multimodal LLMs, ChatPose uni-fies classical 3D human pose and generation tasks while offering user interactions. Additionally, ChatPose empow-ers LLMs to apply their extensive world knowledge in rea-soning about human poses, leading to two advanced tasks: speculative pose generation and reasoning about pose esti-mation. These tasks involve reasoning about humans to generate 3D poses from subtle text queries, possibly ac-companied by images. We establish benchmarks for these tasks, moving beyond traditional 3D pose generation and estimation methods. Our results show that ChatPose out-performs existing multimodal LLMs and task-specific meth-ods on these newly proposed tasks. Furthermore, Chat-Pose's ability to understand and generate 3D human poses based on complex reasoning opens new directions in human pose analysis. Code and data are available for research at https://yfeng95.github.io/ChatPose.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2023,
    "citationCount": 18,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2311.18836",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-30",
    "authors": [
      {
        "authorId": "2258812939",
        "name": "Yao Feng"
      },
      {
        "authorId": "2268777779",
        "name": "Jing Lin"
      },
      {
        "authorId": "102609418",
        "name": "Sai Kumar Dwivedi"
      },
      {
        "authorId": "2268801370",
        "name": "Yu Sun"
      },
      {
        "authorId": "2268809728",
        "name": "Priyanka Patel"
      },
      {
        "authorId": "2258720016",
        "name": "Michael J. Black"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.1665846874966
  },
  {
    "paperId": "8db921900955a447d389582143912eee3046fd3e",
    "url": "https://www.semanticscholar.org/paper/8db921900955a447d389582143912eee3046fd3e",
    "title": "BioinspiredLLM: Conversational Large Language Model for the Mechanics of Biological and Bio‐Inspired Materials",
    "abstract": "The study of biological materials and bio‐inspired materials science is well established; however, surprisingly little knowledge is systematically translated to engineering solutions. To accelerate discovery and guide insights, an open‐source autoregressive transformer large language model (LLM), BioinspiredLLM, is reported. The model is finetuned with a corpus of over a thousand peer‐reviewed articles in the field of structural biological and bio‐inspired materials and can be prompted to recall information, assist with research tasks, and function as an engine for creativity. The model has proven that it is able to accurately recall information about biological materials and is further strengthened with enhanced reasoning ability, as well as with Retrieval‐Augmented Generation (RAG) to incorporate new data during generation that can also help to traceback sources, update the knowledge base, and connect knowledge domains. BioinspiredLLM also has shown to develop sound hypotheses regarding biological materials design and remarkably so for materials that have never been explicitly studied before. Lastly, the model shows impressive promise in collaborating with other generative artificial intelligence models in a workflow that can reshape the traditional materials design process. This collaborative generative artificial intelligence method can stimulate and enhance bio‐inspired materials design workflows. Biological materials are at a critical intersection of multiple scientific fields and models like BioinspiredLLM help to connect knowledge domains.",
    "venue": "Advancement of science",
    "year": 2023,
    "citationCount": 38,
    "openAccessPdf": {
      "url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/advs.202306724",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Medicine",
      "Computer Science",
      "Physics"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-09-15",
    "authors": [
      {
        "authorId": "2182256086",
        "name": "Rachel K. Luu"
      },
      {
        "authorId": "2273480",
        "name": "M. Buehler"
      }
    ],
    "source": "semantic_scholar",
    "score": 134.9534246919447
  },
  {
    "paperId": "4f4a80148cb8f328aeaee68b34f9797cfb5ea150",
    "url": "https://www.semanticscholar.org/paper/4f4a80148cb8f328aeaee68b34f9797cfb5ea150",
    "title": "Unpacking Large Language Models with Conceptual Consistency",
    "abstract": "If a Large Language Model (LLM) answers\"yes\"to the question\"Are mountains tall?\"then does it know what a mountain is? Can you rely on it responding correctly or incorrectly to other questions about mountains? The success of Large Language Models (LLMs) indicates they are increasingly able to answer queries like these accurately, but that ability does not necessarily imply a general understanding of concepts relevant to the anchor query. We propose conceptual consistency to measure a LLM's understanding of relevant concepts. This novel metric measures how well a model can be characterized by finding out how consistent its responses to queries about conceptually relevant background knowledge are. To compute it we extract background knowledge by traversing paths between concepts in a knowledge base and then try to predict the model's response to the anchor query from the background knowledge. We investigate the performance of current LLMs in a commonsense reasoning setting using the CSQA dataset and the ConceptNet knowledge base. While conceptual consistency, like other metrics, does increase with the scale of the LLM used, we find that popular models do not necessarily have high conceptual consistency. Our analysis also shows significant variation in conceptual consistency across different kinds of relations, concepts, and prompts. This serves as a step toward building models that humans can apply a theory of mind to, and thus interact with intuitively.",
    "venue": "arXiv.org",
    "year": 2022,
    "citationCount": 14,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2209.15093",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-09-29",
    "authors": [
      {
        "authorId": "4643090",
        "name": "Pritish Sahu"
      },
      {
        "authorId": "144354133",
        "name": "Michael Cogswell"
      },
      {
        "authorId": "3303727",
        "name": "Yunye Gong"
      },
      {
        "authorId": "47977519",
        "name": "Ajay Divakaran"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "5aea5c8b536380c5ad1d42108c2c6767622318ee",
    "url": "https://www.semanticscholar.org/paper/5aea5c8b536380c5ad1d42108c2c6767622318ee",
    "title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
    "abstract": "Hypothetical induction is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain those observations. Past research on hypothetical induction is under a constrained setting: (1) the observation annotations in the dataset are carefully manually handpicked sentences (resulting in a close-domain setting); and (2) the ground truth hypotheses are mostly commonsense knowledge, making the task less challenging. In this work, we tackle these problems by proposing the first dataset for social science academic hypotheses discovery, with the final goal to create systems that automatically generate valid, novel, and helpful scientific hypotheses, given only a pile of raw web corpus. Unlike previous settings, the new dataset requires (1) using open-domain data (raw web corpus) as observations; and (2) proposing hypotheses even new to humanity. A multi-module framework is developed for the task, including three different feedback mechanisms to boost performance, which exhibits superior performance in terms of both GPT-4 based and expert-based evaluation. To the best of our knowledge, this is the first work showing that LLMs are able to generate novel (''not existing in literature'') and valid (''reflecting reality'') scientific hypotheses.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 24,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.02726",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-09-06",
    "authors": [
      {
        "authorId": "2124477940",
        "name": "Zonglin Yang"
      },
      {
        "authorId": "13728923",
        "name": "Xinya Du"
      },
      {
        "authorId": "2238138967",
        "name": "Junxian Li"
      },
      {
        "authorId": "2237998834",
        "name": "Jie Zheng"
      },
      {
        "authorId": "1746416",
        "name": "Soujanya Poria"
      },
      {
        "authorId": "49943757",
        "name": "E. Cambria"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.28313737302301
  },
  {
    "paperId": "d47b9407cf0938e9f15ce019031d7cd63b308a01",
    "url": "https://www.semanticscholar.org/paper/d47b9407cf0938e9f15ce019031d7cd63b308a01",
    "title": "Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks",
    "abstract": "Making the contents generated by Large Language Model (LLM), accurate, credible and traceable is crucial, especially in complex knowledge-intensive tasks that require multi-step reasoning and each step needs knowledge to solve. Retrieval-augmented generation is good potential to solve this problem. However, where and how to introduce Information Retrieval (IR) to LLM is a big challenge. Previous work has the problems that wrong knowledge retrieved by IR misleads the LLM and interaction between IR and LLM breaks the reasoning chain of LLM. This paper proposes a novel framework named Search-in-the-Chain (SearChain) for the interaction between LLM and IR to solve the challenges. First, LLM generates the reasoning chain named Chain-of-Query (CoQ) where each node consists of an IR-oriented query-answer pair. Second, IR verifies the answer of each node of CoQ. It corrects the answer that is not consistent with the retrieved information when IR gives high confidence, which improves the credibility. Third, LLM can indicate its missing knowledge in CoQ and rely on IR to provide this knowledge to LLM. These operations improve the accuracy in terms of reasoning and knowledge. Finally, SearChain generates the reasoning process and marks references to supporting documents for each reasoning step, which improves traceability. Interaction with IR in SearChain forms a novel reasoning path based on a tree, which enables LLM to dynamically modify the direction of reasoning. Experiments show that SearChain outperforms state-of-the-art baselines on complex knowledge-intensive tasks including multi-hop Q&A, slot filling, fact checking, and long-form Q&A.",
    "venue": "The Web Conference",
    "year": 2023,
    "citationCount": 22,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2304.14732",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-04-28",
    "authors": [
      {
        "authorId": "2202745",
        "name": "Shicheng Xu"
      },
      {
        "authorId": "2111815778",
        "name": "Liang Pang"
      },
      {
        "authorId": "2476503",
        "name": "Huawei Shen"
      },
      {
        "authorId": "2110251463",
        "name": "Xueqi Cheng"
      },
      {
        "authorId": "2281747744",
        "name": "Tat-Seng Chua"
      }
    ],
    "source": "semantic_scholar",
    "score": 117.03241323893724
  },
  {
    "paperId": "b50465f4c93714ce5df1b6e56dd1952080d019a8",
    "url": "https://www.semanticscholar.org/paper/b50465f4c93714ce5df1b6e56dd1952080d019a8",
    "title": "Durably reducing conspiracy beliefs through dialogues with AI",
    "abstract": "Conspiracy theory beliefs are notoriously persistent. Influential hypotheses propose that they fulfill important psychological needs, thus resisting counterevidence. Yet previous failures in correcting conspiracy beliefs may be due to counterevidence being insufficiently compelling and tailored. To evaluate this possibility, we leveraged developments in generative artificial intelligence and engaged 2190 conspiracy believers in personalized evidence-based dialogues with GPT-4 Turbo. The intervention reduced conspiracy belief by ~20%. The effect remained 2 months later, generalized across a wide range of conspiracy theories, and occurred even among participants with deeply entrenched beliefs. Although the dialogues focused on a single conspiracy, they nonetheless diminished belief in unrelated conspiracies and shifted conspiracy-related behavioral intentions. These findings suggest that many conspiracy theory believers can revise their views if presented with sufficiently compelling evidence. Editor’s summary Beliefs in conspiracies that a US election was stolen incited an attempted insurrection on 6 January 2021. Another conspiracy alleging that Germany’s COVID-19 restrictions were motivated by nefarious intentions sparked violent protests at Berlin’s Reichstag parliament building in August 2020. Amid growing threats to democracy, Costello et al. investigated whether dialogs with a generative artificial intelligence (AI) interface could convince people to abandon their conspiratorial beliefs (see the Perspective by Bago and Bonnefon). Human participants described a conspiracy theory that they subscribed to, and the AI then engaged in persuasive arguments with them that refuted their beliefs with evidence. The AI chatbot’s ability to sustain tailored counterarguments and personalized in-depth conversations reduced their beliefs in conspiracies for months, challenging research suggesting that such beliefs are impervious to change. This intervention illustrates how deploying AI may mitigate conflicts and serve society. —Ekeoma Uzogara INTRODUCTION Widespread belief in unsubstantiated conspiracy theories is a major source of public concern and a focus of scholarly research. Despite often being quite implausible, many such conspiracies are widely believed. Prominent psychological theories propose that many people want to adopt conspiracy theories (to satisfy underlying psychic “needs” or motivations), and thus, believers cannot be convinced to abandon these unfounded and implausible beliefs using facts and counterevidence. Here, we question this conventional wisdom and ask whether it may be possible to talk people out of the conspiratorial “rabbit hole” with sufficiently compelling evidence. RATIONALE We hypothesized that interventions based on factual, corrective information may seem ineffective simply because they lack sufficient depth and personalization. To test this hypothesis, we leveraged advancements in large language models (LLMs), a form of artificial intelligence (AI) that has access to vast amounts of information and the ability to generate bespoke arguments. LLMs can thereby directly refute particular evidence each individual cites as supporting their conspiratorial beliefs. To do so, we developed a pipeline for conducting behavioral science research using real-time, personalized interactions between research subjects and AI. Across two experiments, 2190 Americans articulated—in their own words—a conspiracy theory in which they believe, along with the evidence they think supports this theory. They then engaged in a three-round conversation with the LLM GPT-4 Turbo, which we prompted to respond to this specific evidence while trying to reduce participants’ belief in the conspiracy theory (or, as a control condition, to converse with the AI about an unrelated topic). RESULTS The treatment reduced participants’ belief in their chosen conspiracy theory by 20% on average. This effect persisted undiminished for at least 2 months; was consistently observed across a wide range of conspiracy theories, from classic conspiracies involving the assassination of John F. Kennedy, aliens, and the illuminati, to those pertaining to topical events such as COVID-19 and the 2020 US presidential election; and occurred even for participants whose conspiracy beliefs were deeply entrenched and important to their identities. Notably, the AI did not reduce belief in true conspiracies. Furthermore, when a professional fact-checker evaluated a sample of 128 claims made by the AI, 99.2% were true, 0.8% were misleading, and none were false. The debunking also spilled over to reduce beliefs in unrelated conspiracies, indicating a general decrease in conspiratorial worldview, and increased intentions to rebut other conspiracy believers. CONCLUSION Many people who strongly believe in seemingly fact-resistant conspiratorial beliefs can change their minds when presented with compelling evidence. From a theoretical perspective, this paints a surprisingly optimistic picture of human reasoning: Conspiratorial rabbit holes may indeed have an exit. Psychological needs and motivations do not inherently blind conspiracists to evidence—it simply takes the right evidence to reach them. Practically, by demonstrating the persuasive power of LLMs, our findings emphasize both the potential positive impacts of generative AI when deployed responsibly and the pressing importance of minimizing opportunities for this technology to be used irresponsibly. Dialogues with AI durably reduce conspiracy beliefs even among strong believers. (Left) Average belief in participant’s chosen conspiracy theory by condition (treatment, in which the AI attempted to refute the conspiracy theory, in red; control, in which the AI discussed an irrelevant topic, in blue) and time point for study 1. (Right) Change in belief in chosen conspiracy from before to after AI conversation, by condition and participant’s pretreatment belief in the conspiracy.",
    "venue": "Science",
    "year": 2024,
    "citationCount": 34,
    "openAccessPdf": {
      "url": "https://osf.io/xcwdn/download",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-13",
    "authors": [
      {
        "authorId": "49539771",
        "name": "Thomas H. Costello"
      },
      {
        "authorId": "2264181428",
        "name": "Gordon Pennycook"
      },
      {
        "authorId": "2237502266",
        "name": "David G. Rand"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.3302209223412
  },
  {
    "paperId": "094883e42bb9a41f602c0715c1059bc431e33fb2",
    "url": "https://www.semanticscholar.org/paper/094883e42bb9a41f602c0715c1059bc431e33fb2",
    "title": "GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest",
    "abstract": "Visual instruction tuning large language model(LLM) on image-text pairs has achieved general-purpose vision-language abilities. However, the lack of region-text pairs limits their advancements to fine-grained multimodal understanding. In this paper, we propose spatial instruction tuning, which introduces the reference to the region-of-interest(RoI) in the instruction. Before sending to LLM, the reference is replaced by RoI features and interleaved with language embeddings as a sequence. Our model GPT4RoI, trained on 7 region-text pair datasets, brings an unprecedented interactive and conversational experience compared to previous image-level models. (1) Interaction beyond language: Users can interact with our model by both language and drawing bounding boxes to flexibly adjust the referring granularity. (2) Versatile multimodal abilities: A variety of attribute information within each RoI can be mined by GPT4RoI, e.g., color, shape, material, action, etc. Furthermore, it can reason about multiple RoIs based on common sense. On the Visual Commonsense Reasoning(VCR) dataset, GPT4RoI achieves a remarkable accuracy of 81.6%, surpassing all existing models by a significant margin (the second place is 75.6%) and almost reaching human-level performance of 85.0%. The code, dataset, and demo can be found at https://github.com/jshilong/GPT4RoI.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 194,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.03601",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-07",
    "authors": [
      {
        "authorId": "47179740",
        "name": "Shilong Zhang"
      },
      {
        "authorId": "2075416446",
        "name": "Pei Sun"
      },
      {
        "authorId": "2107977968",
        "name": "Shoufa Chen"
      },
      {
        "authorId": "2099595096",
        "name": "Min Xiao"
      },
      {
        "authorId": "1485702259",
        "name": "Wenqi Shao"
      },
      {
        "authorId": "2108037927",
        "name": "Wenwei Zhang"
      },
      {
        "authorId": "152568027",
        "name": "Kai Chen"
      },
      {
        "authorId": "2143481782",
        "name": "Ping Luo"
      }
    ],
    "source": "semantic_scholar",
    "score": 149.0949933784562
  },
  {
    "paperId": "a9d5d97733ccb15002ff3cfb95b0a7d8ba5236e3",
    "url": "https://www.semanticscholar.org/paper/a9d5d97733ccb15002ff3cfb95b0a7d8ba5236e3",
    "title": "LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding",
    "abstract": "Instruction tuning unlocks the superior capability of Large Language Models (LLM) to interact with humans. Furthermore, recent instruction-following datasets include images as visual inputs, collecting responses for image-based instructions. However, visual instruction-tuned models cannot comprehend textual details within images well. This work enhances the current visual instruction tuning pipeline with text-rich images (e.g., movie posters, book covers, etc.). Specifically, we first use publicly available OCR tools to collect results on 422K text-rich images from the LAION dataset. Moreover, we prompt text-only GPT-4 with recognized texts and image captions to generate 16K conversations, each containing question-answer pairs for text-rich images. By combining our collected data with previous multi-modal instruction-following data, our model, LLaVAR, substantially improves the LLaVA model's capability on text-based VQA datasets (up to 20% accuracy improvement) while achieving an accuracy of 91.42% on ScienceQA. The GPT-4-based instruction-following evaluation also demonstrates the improvement of our model on both natural images and text-rich images. Through qualitative analysis, LLaVAR shows promising interaction (e.g., reasoning, writing, and elaboration) skills with humans based on the latest real-world online content that combines text and images. We make our code/data/models publicly available at https://llavar.github.io/.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 183,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.17107",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-29",
    "authors": [
      {
        "authorId": "2121290295",
        "name": "Yanzhe Zhang"
      },
      {
        "authorId": "2261751659",
        "name": "Ruiyi Zhang"
      },
      {
        "authorId": "2174964",
        "name": "Jiuxiang Gu"
      },
      {
        "authorId": "1652060942",
        "name": "Yufan Zhou"
      },
      {
        "authorId": "1793409",
        "name": "Nedim Lipka"
      },
      {
        "authorId": "2143919864",
        "name": "Diyi Yang"
      },
      {
        "authorId": "2190051546",
        "name": "Tongfei Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 148.2240363641348
  },
  {
    "paperId": "01d088de0c8e3c96a2924d4db2aac32b3d42a877",
    "url": "https://www.semanticscholar.org/paper/01d088de0c8e3c96a2924d4db2aac32b3d42a877",
    "title": "An Investigation of Large Language Models for Real-World Hate Speech Detection",
    "abstract": "Hate speech has emerged as a major problem plaguing our social spaces today. While there have been significant efforts to address this problem, existing methods are still significantly limited in effectively detecting hate speech online. A major limitation of existing methods is that hate speech detection is a highly contextual problem, and these methods cannot fully capture the context of hate speech to make accurate predictions. Recently, large language models (LLMs) have demonstrated state-of-the-art performance in several natural language tasks. LLMs have undergone extensive training using vast amounts of natural language data, enabling them to grasp intricate contextual details. Hence, they could be used as knowledge bases for context-aware hate speech detection. However, a fundamental problem with using LLMs to detect hate speech is that there are no studies on effectively prompting LLMs for context-aware hate speech detection. In this study, we conduct a large-scale study of hate speech detection, employing five established hate speech datasets. We discover that LLMs not only match but often surpass the performance of current benchmark machine learning models in identifying hate speech. By proposing four diverse prompting strategies that optimize the use of LLMs in detecting hate speech. Our study reveals that a meticulously crafted reasoning prompt can effectively capture the context of hate speech by fully utilizing the knowledge base in LLMs, significantly outperforming existing techniques. Furthermore, although LLMs can provide a rich knowledge base for the contextual detection of hate speech, suitable prompting strategies play a crucial role in effectively leveraging this knowledge base for efficient detection.",
    "venue": "International Conference on Machine Learning and Applications",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-15",
    "authors": [
      {
        "authorId": "2165583728",
        "name": "Keyan Guo"
      },
      {
        "authorId": "2278428950",
        "name": "Alexander Hu"
      },
      {
        "authorId": "2278429083",
        "name": "Jaden Mu"
      },
      {
        "authorId": "2278480663",
        "name": "Ziheng Shi"
      },
      {
        "authorId": "2276488043",
        "name": "Ziming Zhao"
      },
      {
        "authorId": "8842079",
        "name": "Nishant Vishwamitra"
      },
      {
        "authorId": "2241790138",
        "name": "Hongxin Hu"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "f72be31de9f9a09d4410fd38bc717efe43444827",
    "url": "https://www.semanticscholar.org/paper/f72be31de9f9a09d4410fd38bc717efe43444827",
    "title": "SALMONN: Towards Generic Hearing Abilities for Large Language Models",
    "abstract": "Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as automatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning etc. SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning etc. The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities. To our knowledge, SALMONN is the first model of its type and can be regarded as a step towards AI with generic hearing abilities. The source code, model checkpoints and data are available at https://github.com/bytedance/SALMONN.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 157,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-20",
    "authors": [
      {
        "authorId": "2247237695",
        "name": "Changli Tang"
      },
      {
        "authorId": "2257283478",
        "name": "Wenyi Yu"
      },
      {
        "authorId": "2107310187",
        "name": "Guangzhi Sun"
      },
      {
        "authorId": "2135092817",
        "name": "Xianzhao Chen"
      },
      {
        "authorId": "2257003951",
        "name": "Tian Tan"
      },
      {
        "authorId": "2256598801",
        "name": "Wei Li"
      },
      {
        "authorId": "2257383962",
        "name": "Lu Lu"
      },
      {
        "authorId": "2257135061",
        "name": "Zejun Ma"
      },
      {
        "authorId": "2256775692",
        "name": "Chao Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 145.9389254954045
  },
  {
    "paperId": "aca3b10b76269d914cb1ec92130a060273df29a4",
    "url": "https://www.semanticscholar.org/paper/aca3b10b76269d914cb1ec92130a060273df29a4",
    "title": "Learning and Leveraging Verifiers to Improve Planning Capabilities of Pre-trained Language Models",
    "abstract": "There have been wide spread claims in the literature about the emergent reasoning capabilities of Pretrained Large Language Models. However, recent studies, have found that their ability to plan remains questionable. Through our experiments using GPT-2, we empirically demonstrate that the performance of a finetuned baseline remains poor because it violates pre-conditions of actions in the plans that it generates. To improve the planning capabilities of a finetuned LLM, we train a verifier, which can classify actions as being valid or invalid in a particular state. By randomly sampling actions from the same dataset, we generate examples of invalid actions which are then used to train a verifier which can check for action applicability. In the presence of diverse sampling from a generator and a verifier which can prune invalid trajectories, we show significant gains in the success rate on the Blocksworld domain. Additionally, we show that finetuning the GPT-2 generator itself to create the verifier generalizes better than finetuning the base GPT-2. Lastly, we investigate the role of the sampling temperature which can be used to control the exploration-exploitation tradeoff.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.17077",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-26",
    "authors": [
      {
        "authorId": "2138481119",
        "name": "Daman Arora"
      },
      {
        "authorId": "2047340230",
        "name": "Subbarao Kambhampati"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "8efc20988021ce3b4b05dd44b13e27260ee9b99b",
    "url": "https://www.semanticscholar.org/paper/8efc20988021ce3b4b05dd44b13e27260ee9b99b",
    "title": "Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering",
    "abstract": "In this paper, we explore effective prompting techniques to enhance zero- and few-shot Visual Question Answering (VQA) performance in contemporary Vision-Language Models (VLMs). Central to our investigation is the role of question templates in guiding VLMs to generate accurate answers. We identify that specific templates significantly influence VQA outcomes, underscoring the need for strategic template selection. Another pivotal aspect of our study is augmenting VLMs with image captions, providing them with additional visual cues alongside direct image features in VQA tasks. Surprisingly, this augmentation significantly improves the VLMs' performance in many cases, even though VLMs\"see\"the image directly! We explore chain-of-thought (CoT) reasoning and find that while standard CoT reasoning causes drops in performance, advanced methods like self-consistency can help recover it. Furthermore, we find that text-only few-shot examples enhance VLMs' alignment with the task format, particularly benefiting models prone to verbose zero-shot answers. Lastly, to mitigate the challenges associated with evaluating free-form open-ended VQA responses using string-matching based VQA metrics, we introduce a straightforward LLM-guided pre-processing technique to adapt the model responses to the expected ground-truth answer distribution. In summary, our research sheds light on the intricacies of prompting strategies in VLMs for VQA, emphasizing the synergistic use of captions, templates, and pre-processing to enhance model efficacy.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 9,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.09996",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-16",
    "authors": [
      {
        "authorId": "2019829",
        "name": "Rabiul Awal"
      },
      {
        "authorId": "2108005316",
        "name": "Le Zhang"
      },
      {
        "authorId": "2801949",
        "name": "Aishwarya Agrawal"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "aa6541419efe4dfde4b8a4f806554cceabf4c21b",
    "url": "https://www.semanticscholar.org/paper/aa6541419efe4dfde4b8a4f806554cceabf4c21b",
    "title": "APIGen: Generative API Method Recommendation",
    "abstract": "Automatic API method recommendation is an essential task of code intelligence, which aims to suggest suitable APIs for programming queries. Existing approaches can be categorized into two primary groups: retrieval-based and learning-based approaches. Although these approaches have achieved remarkable success, they still come with notable limitations. The retrieval-based approaches rely on the text representation capabilities of embedding models, while the learning-based approaches require extensive task-specific labeled data for training. To mitigate the limitations, we propose APIGen, a generative API recommendation approach through enhanced in-context learning (ICL). APIGen has a powerful representation capability and can make effective recommendations with only a few examples via I CL. To overcome the limitations of standard ICL in capturing task-specific knowledge, APIGen involves two main components: (1) Diverse Examples Selection. APIGen searches for similar posts to the programming queries from the lexical, syntactical, and semantic perspectives, providing more informative examples for ICL. (2) Guided API Recommendation. APIGen enables large language models (LLMs) to perform reasoning before generating API recommendations, where the reasoning involves fine-grained matching between the task intent behind the queries and the factual knowledge of the APIs. With the reasoning process, APIGen makes recommended APIs better meet the programming requirement of queries and also enhances the interpretability of results. We compare APIGen with four existing approaches on two publicly available benchmarks. Experiments show that APIGen outperforms the best baseline CLEAR by 105.8% in method-level API recommendation and 54.3 % in class-level API recommendation in terms of SuccessRate@l. Besides, APIGen achieves an average 49.87 % increase compared to the zero-shot performance of popular LLMs such as GPT-4 in method-level API recommendation regardina the SuccessRate@ 3 metric.",
    "venue": "IEEE International Conference on Software Analysis, Evolution, and Reengineering",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2401.15843",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-01-29",
    "authors": [
      {
        "authorId": "2280918261",
        "name": "Yujia Chen"
      },
      {
        "authorId": "2278987362",
        "name": "Cuiyun Gao"
      },
      {
        "authorId": "2281930651",
        "name": "Muyijie Zhu"
      },
      {
        "authorId": "2278674692",
        "name": "Qing Liao"
      },
      {
        "authorId": "2281785339",
        "name": "Yong Wang"
      },
      {
        "authorId": "2281788011",
        "name": "Guoai Xu"
      }
    ],
    "source": "semantic_scholar",
    "score": 92.47918433002164
  },
  {
    "paperId": "5c002fd415b0284fe188dbd5d57a0acbed06de55",
    "url": "https://www.semanticscholar.org/paper/5c002fd415b0284fe188dbd5d57a0acbed06de55",
    "title": "A Dynamic LLM-Powered Agent Network for Task-Oriented Agent Collaboration",
    "abstract": "Recent studies show that collaborating multiple large language model (LLM) powered agents is a promising way for task solving. However, current approaches are constrained by using a fixed number of agents and static communication structures. In this work, we propose automatically selecting a team of agents from candidates to collaborate in a dynamic communication structure toward different tasks and domains. Specifically, we build a framework named Dynamic LLM-Powered Agent Network ($\\textbf{DyLAN}$) for LLM-powered agent collaboration, operating a two-stage paradigm: (1) Team Optimization and (2) Task Solving. During the first stage, we utilize an $\\textit{agent selection}$ algorithm, based on an unsupervised metric called $\\textit{Agent Importance Score}$, enabling the selection of best agents according to their contributions in a preliminary trial, oriented to the given task. Then, in the second stage, the selected agents collaborate dynamically according to the query. Empirically, we demonstrate that DyLAN outperforms strong baselines in code generation, decision-making, general reasoning, and arithmetic reasoning tasks with moderate computational cost. On specific subjects in MMLU, selecting a team of agents in the team optimization stage improves accuracy by up to 25.0% in DyLAN.",
    "venue": "",
    "year": 2023,
    "citationCount": 90,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-10-03",
    "authors": [
      {
        "authorId": "2117942065",
        "name": "Zijun Liu"
      },
      {
        "authorId": "2121290295",
        "name": "Yanzhe Zhang"
      },
      {
        "authorId": "144326610",
        "name": "Peng Li"
      },
      {
        "authorId": "2254850259",
        "name": "Yang Liu"
      },
      {
        "authorId": "2254124342",
        "name": "Diyi Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 137.66289259775274
  },
  {
    "paperId": "f6e893b3e2ee7a62c2fe8a3b0e33920c3e596969",
    "url": "https://www.semanticscholar.org/paper/f6e893b3e2ee7a62c2fe8a3b0e33920c3e596969",
    "title": "SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents",
    "abstract": "Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 74,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-18",
    "authors": [
      {
        "authorId": "144101734",
        "name": "Xuhui Zhou"
      },
      {
        "authorId": "2260859845",
        "name": "Hao Zhu"
      },
      {
        "authorId": "2259929826",
        "name": "Leena Mathur"
      },
      {
        "authorId": "46752970",
        "name": "Ruohong Zhang"
      },
      {
        "authorId": "2260283233",
        "name": "Haofei Yu"
      },
      {
        "authorId": "2266241076",
        "name": "Zhengyang Qi"
      },
      {
        "authorId": "49933077",
        "name": "Louis-Philippe Morency"
      },
      {
        "authorId": "3312309",
        "name": "Yonatan Bisk"
      },
      {
        "authorId": "2259931814",
        "name": "Daniel Fried"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      },
      {
        "authorId": "2729164",
        "name": "Maarten Sap"
      }
    ],
    "source": "semantic_scholar",
    "score": 134.76232170304465
  },
  {
    "paperId": "b4531fbd97743cb099cc3cafd21b6da06fc3af67",
    "url": "https://www.semanticscholar.org/paper/b4531fbd97743cb099cc3cafd21b6da06fc3af67",
    "title": "Assessment of Resident and AI Chatbot Performance on the University of Toronto Family Medicine Residency Progress Test: Comparative Study",
    "abstract": "Background Large language model (LLM)–based chatbots are evolving at an unprecedented pace with the release of ChatGPT, specifically GPT-3.5, and its successor, GPT-4. Their capabilities in general-purpose tasks and language generation have advanced to the point of performing excellently on various educational examination benchmarks, including medical knowledge tests. Comparing the performance of these 2 LLM models to that of Family Medicine residents on a multiple-choice medical knowledge test can provide insights into their potential as medical education tools. Objective This study aimed to quantitatively and qualitatively compare the performance of GPT-3.5, GPT-4, and Family Medicine residents in a multiple-choice medical knowledge test appropriate for the level of a Family Medicine resident. Methods An official University of Toronto Department of Family and Community Medicine Progress Test consisting of multiple-choice questions was inputted into GPT-3.5 and GPT-4. The artificial intelligence chatbot’s responses were manually reviewed to determine the selected answer, response length, response time, provision of a rationale for the outputted response, and the root cause of all incorrect responses (classified into arithmetic, logical, and information errors). The performance of the artificial intelligence chatbots were compared against a cohort of Family Medicine residents who concurrently attempted the test. Results GPT-4 performed significantly better compared to GPT-3.5 (difference 25.0%, 95% CI 16.3%-32.8%; McNemar test: P<.001); it correctly answered 89/108 (82.4%) questions, while GPT-3.5 answered 62/108 (57.4%) questions correctly. Further, GPT-4 scored higher across all 11 categories of Family Medicine knowledge. In 86.1% (n=93) of the responses, GPT-4 provided a rationale for why other multiple-choice options were not chosen compared to the 16.7% (n=18) achieved by GPT-3.5. Qualitatively, for both GPT-3.5 and GPT-4 responses, logical errors were the most common, while arithmetic errors were the least common. The average performance of Family Medicine residents was 56.9% (95% CI 56.2%-57.6%). The performance of GPT-3.5 was similar to that of the average Family Medicine resident (P=.16), while the performance of GPT-4 exceeded that of the top-performing Family Medicine resident (P<.001). Conclusions GPT-4 significantly outperforms both GPT-3.5 and Family Medicine residents on a multiple-choice medical knowledge test designed for Family Medicine residents. GPT-4 provides a logical rationale for its response choice, ruling out other answer choices efficiently and with concise justification. Its high degree of accuracy and advanced reasoning capabilities facilitate its potential applications in medical education, including the creation of exam questions and scenarios as well as serving as a resource for medical knowledge or information on community services.",
    "venue": "JMIR Medical Education",
    "year": 2023,
    "citationCount": 30,
    "openAccessPdf": {
      "url": "https://mededu.jmir.org/2023/1/e50514/PDF",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-07-03",
    "authors": [
      {
        "authorId": "2241158119",
        "name": "R. S. Huang"
      },
      {
        "authorId": "2240524533",
        "name": "K. Lu"
      },
      {
        "authorId": "2240525435",
        "name": "C. Meaney"
      },
      {
        "authorId": "2238124565",
        "name": "Joel Kemppainen"
      },
      {
        "authorId": "2238126400",
        "name": "A. Punnett"
      },
      {
        "authorId": "2240512131",
        "name": "F.-H. Leung"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.50980806727719
  },
  {
    "paperId": "33cc222f97ba85678f7d0cd4bf6521122b09c80d",
    "url": "https://www.semanticscholar.org/paper/33cc222f97ba85678f7d0cd4bf6521122b09c80d",
    "title": "Large language models (LLM) and ChatGPT: a medical student perspective",
    "abstract": null,
    "venue": "European Journal of Nuclear Medicine and Molecular Imaging",
    "year": 2023,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle",
      "LettersAndComments"
    ],
    "publicationDate": "2023-04-13",
    "authors": [
      {
        "authorId": "2058440509",
        "name": "Arosh S. Perera Molligoda Arachchige"
      }
    ],
    "source": "semantic_scholar",
    "score": 92.49820016084324
  },
  {
    "paperId": "51601d554d04c89ae71efe1e66c1d67ef44bec71",
    "url": "https://www.semanticscholar.org/paper/51601d554d04c89ae71efe1e66c1d67ef44bec71",
    "title": "A Closer Look at Reward Decomposition for High-Level Robotic Explanations",
    "abstract": "Explaining the behaviour of intelligent agents learned by reinforcement learning (RL) to humans is challenging yet crucial due to their incomprehensible proprioceptive states, variational intermediate goals, and resultant unpredictability. Moreover, one-step explanations for RL agents can be ambiguous as they fail to account for the agent's future behaviour at each transition, adding to the complexity of explaining robot actions. By leveraging abstracted actions that map to task-specific primitives, we avoid explanations on the movement level. To further improve the transparency and explainability of robotic systems, we propose an explainable Q-Map learning framework that combines reward decomposition (RD) with abstracted action spaces, allowing for non-ambiguous and high-level explanations based on object properties in the task. We demonstrate the effectiveness of our framework through quantitative and qualitative analysis of two robotic scenarios, showcasing visual and textual explanations, from output artefacts of RD explanations, that are easy for humans to comprehend. Additionally, we demonstrate the versatility of integrating these artefacts with large language models (LLMs) for reasoning and interactive querying.",
    "venue": "International Conference on Development and Learning",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2304.12958",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-04-25",
    "authors": [
      {
        "authorId": "1999393286",
        "name": "Wenhao Lu"
      },
      {
        "authorId": "2632932",
        "name": "S. Magg"
      },
      {
        "authorId": "2145744284",
        "name": "Xufeng Zhao"
      },
      {
        "authorId": "51118123",
        "name": "M. Gromniak"
      },
      {
        "authorId": "1736513",
        "name": "Stefan Wermter"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "48cfb2e5d32dcfedd9acb8eb38f499d2b7202369",
    "url": "https://www.semanticscholar.org/paper/48cfb2e5d32dcfedd9acb8eb38f499d2b7202369",
    "title": "Prompt Optimization via Adversarial In-Context Learning",
    "abstract": "We propose a new method, Adversarial In-Context Learning (adv-ICL), to optimize prompt for in-context learning (ICL) by employing one LLM as a generator, another as a discriminator, and a third as a prompt modifier. As in traditional adversarial learning, adv-ICL is implemented as a two-player game between the generator and discriminator, where the generator tries to generate realistic enough output to fool the discriminator. In each round, given an input prefixed by task instructions and several exemplars, the generator produces an output. The discriminator is then tasked with classifying the generator input-output pair as model-generated or real data. Based on the discriminator loss, the prompt modifier proposes possible edits to the generator and discriminator prompts, and the edits that most improve the adversarial loss are selected. We show that adv-ICL results in significant improvements over state-of-the-art prompt optimization techniques for both open and closed-source models on 11 generation and classification tasks including summarization, arithmetic reasoning, machine translation, data-to-text generation, and the MMLU and big-bench hard benchmarks. In addition, because our method uses pre-trained models and updates only prompts rather than model parameters, it is computationally efficient, easy to extend to any LLM and task, and effective in low-resource settings.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-05",
    "authors": [
      {
        "authorId": "2060491855",
        "name": "Do Xuan Long"
      },
      {
        "authorId": "2249827256",
        "name": "Yiran Zhao"
      },
      {
        "authorId": "2269735924",
        "name": "Hannah Brown"
      },
      {
        "authorId": "2263604479",
        "name": "Yuxi Xie"
      },
      {
        "authorId": "2269785652",
        "name": "James Xu Zhao"
      },
      {
        "authorId": "2266471203",
        "name": "Nancy F. Chen"
      },
      {
        "authorId": "2266466003",
        "name": "Kenji Kawaguchi"
      },
      {
        "authorId": "2269849261",
        "name": "Michael Qizhe Xie"
      },
      {
        "authorId": "2264312133",
        "name": "Junxian He"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "38a7e831b0246762552499492a746628c85a4011",
    "url": "https://www.semanticscholar.org/paper/38a7e831b0246762552499492a746628c85a4011",
    "title": "Enabling Vision-and-Language Navigation for Intelligent Connected Vehicles Using Large Pre-Trained Models",
    "abstract": "In the field of autonomous driving, Visual-and-Language Navigation (VLN) is a typical multimodal task. In the VLN task, an intelligent vehicle needs to find the target location based on user-provided navigation instructions. However, conventional VLN models generally face the problem of limited generalization ability when dealing with a large number of real-world environmental objects and language instructions. This paper proposed a novel VLN system based on large-scale pre-trained models and applied it to intelligent vehicles. The method consists of an Instruction Extraction System, a Vision-Language Association System, and a Navigational Decisions System. Specifically, a pre-trained Large Language Model (LLM) is first used to extract a series of landmark names from the user’s natural language instructions. Then, the landmark name list is inputted into a pre-trained Visual-Language Model (VLM) to infer the joint probability with environmental objects. Therefore, the image nodes that match the landmarks have been selected. Additionally, the selected image nodes are inputted into another VLM to obtain descriptions of the image nodes. Finally, LLM is used to reason navigation actions for the intelligent vehicle. With the reasoning ability of LLM, the intelligent vehicle takes navigation knowledge, visual environment descriptions, and navigation history as inputs to output navigation actions. The simulation results demonstrate that compared to other fully supervised learning methods, this approach exhibits better generalization ability in unknown environments. Based on Google Map’s Street View data, it achieves a 14.6% higher task success rate compared to the baseline model VLN Transformer.",
    "venue": "2023 IEEE International Conferences on Internet of Things (iThings) and IEEE Green Computing & Communications (GreenCom) and IEEE Cyber, Physical & Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics)",
    "year": 2023,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-17",
    "authors": [
      {
        "authorId": "2299341805",
        "name": "Yaqi Hu"
      },
      {
        "authorId": "2299187084",
        "name": "Dongyuan Ou"
      },
      {
        "authorId": "2211070915",
        "name": "Xiaoxu Wang"
      },
      {
        "authorId": "2300142659",
        "name": "Rong Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 92.47918433002164
  },
  {
    "paperId": "5af084d9d7f37b9f715b980f95c986fc275b8e8f",
    "url": "https://www.semanticscholar.org/paper/5af084d9d7f37b9f715b980f95c986fc275b8e8f",
    "title": "Is ChatGPT’s Knowledge and Interpretative Ability Comparable to First Professional MBBS (Bachelor of Medicine, Bachelor of Surgery) Students of India in Taking a Medical Biochemistry Examination?",
    "abstract": "Introduction ChatGPT is a large language model (LLM)-based chatbot that uses natural language processing to create humanlike conversational dialogue. It has created a significant impact on the entire global landscape, especially in sectors like finance and banking, e-commerce, education, legal, human resources (HR), and recruitment since its inception. There have been multiple ongoing controversies regarding the seamless integration of ChatGPT with the healthcare system because of its factual accuracy, lack of experience, lack of clarity, expertise, and above all, lack of empathy. Our study seeks to compare ChatGPT’s knowledge and interpretative abilities with those of first-year medical students in India in the subject of medical biochemistry. Materials and methods A total of 79 questions (40 multiple choice questions and 39 subjective questions) of medical biochemistry were set for Phase 1, block II term examination. Chat GPT was enrolled as the 101st student in the class. The questions were entered into ChatGPT’s interface and responses were noted. The response time for the multiple-choice questions (MCQs) asked was also noted. The answers given by ChatGPT and 100 students of the class were checked by two subject experts, and marks were given according to the quality of answers. Marks obtained by the AI chatbot were compared with the marks obtained by the students. Results ChatGPT scored 140 marks out of 200 and outperformed almost all the students and ranked fifth in the class. It scored very well in information-based MCQs (92%) and descriptive logical reasoning (80%), whereas performed poorly in descriptive clinical scenario-based questions (52%). In terms of time taken to respond to the MCQs, it took significantly more time to answer logical reasoning MCQs than simple information-based MCQs (3.10±0.882 sec vs. 2.02±0.477 sec, p<0.005). Conclusions ChatGPT was able to outperform almost all the students in the subject of medical biochemistry. If the ethical issues are dealt with efficiently, these LLMs have a huge potential to be used in teaching and learning methods of modern medicine by students successfully.",
    "venue": "Cureus",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://assets.cureus.com/uploads/original_article/pdf/196662/20231019-26838-1wj46lk.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-01",
    "authors": [
      {
        "authorId": "2260726660",
        "name": "Abhra Ghosh"
      },
      {
        "authorId": "2260775209",
        "name": "Nandita Maini Jindal"
      },
      {
        "authorId": "2260730536",
        "name": "V. K. Gupta"
      },
      {
        "authorId": "2260769746",
        "name": "Ekta Bansal"
      },
      {
        "authorId": "2260769458",
        "name": "Navjot Kaur Bajwa"
      },
      {
        "authorId": "2260767774",
        "name": "Abhishek Sett"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  }
]