{
    title: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,
    abstract: Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.,
    publicationDate: 2020-05-22,
    authors: ['Patrick Lewis', 'Ethan Perez', 'Aleksandara Piktus', 'F. Petroni', 'Vladimir Karpukhin', 'Naman Goyal', 'Heinrich Kuttler', 'M. Lewis', 'Wen-tau Yih', 'Tim Rocktäschel', 'Sebastian Riedel', 'Douwe Kiela'],
    score: 202.19251391546203
},
{
    title: REALM: Retrieval-Augmented Language Model Pre-Training,
    abstract: Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. 
To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. 
We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.,
    publicationDate: 2020-02-10,
    authors: ['Kelvin Guu', 'Kenton Lee', 'Zora Tung', 'Panupong Pasupat', 'Ming-Wei Chang'],
    score: 182.13008854850105
},
{
    title: What Makes Good In-Context Examples for GPT-3?,
    abstract: GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting in-context examples (relative to random sampling) that better leverage GPT-3’s in-context learning capabilities.Inspired by the recent success of leveraging a retrieval module to augment neural networks, we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt. Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3’s power of text generation. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline. Moreover, it is observed that the sentence encoders fine-tuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-to-text generation (44.3% on the ToTTo dataset) and open-domain question answering (45.5% on the NQ dataset).,
    publicationDate: 2021-01-17,
    authors: ['Jiachang Liu', 'Dinghan Shen', 'Yizhe Zhang', 'Bill Dolan', 'L. Carin', 'Weizhu Chen'],
    score: 176.18776591914502
},
{
    title: Retrieval-Augmented Generation for Large Language Models: A Survey,
    abstract: Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.,
    publicationDate: 2023-12-18,
    authors: ['Yunfan Gao', 'Yun Xiong', 'Xinyu Gao', 'Kangxiang Jia', 'Jinliu Pan', 'Yuxi Bi', 'Yi Dai', 'Jiawei Sun', 'Qianyu Guo', 'Meng Wang', 'Haofen Wang'],
    score: 172.2510382089245
},
{
    title: Few-shot Learning with Retrieval Augmented Language Models,
    abstract: Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.,
    publicationDate: 2022-08-05,
    authors: ['Gautier Izacard', 'Patrick Lewis', 'M. Lomeli', 'Lucas Hosseini', 'F. Petroni', 'Timo Schick', 'Jane A. Yu', 'Armand Joulin', 'Sebastian Riedel', 'Edouard Grave'],
    score: 166.15262322949252
},
{
    title: A comparison of SIFT, PCA-SIFT and SURF,
    abstract: This paper compares three robust feature detection methods, they are, Scale Invariant Feature Transform (SIFT), Principal Component Analysis (PCA) -SIFT and Speeded Up Robust Features (SURF). Lowe presented SIFT [1], which was successfully used in recognition, stitching and many other applications because of its robustness. Yan Ke [2] gave a change of SIFT by using PCA to normalize the gradient patch instead of histogram. H. Bay [3] presented a faster method for SURF, which used Fast-Hessian detector. The performance of the three methods is compared for scale changes, rotation , blur, illumination changes and affine transformations, all of which uses repeatability as an evaluation measurement. Additionally, RANSAC is used to reject the inconsistent matches [4]. SIFT presents its stability in most situation except rotation and illumination changes. SURF is the fastest one with good performance as the same as SIFT, PCASIFT shows its advantages in rotation, blur and illumination changes. CITED BY (442) 1 Han, C. H. (2010). Reduced Dimensional SURF Based Hand Gesture Recognition. 2 Su, C. R. (2014). Unsupervised Image Segmentation by Dual Morphological Operations and Peer-toPeer Content-Based Image Retrieval Applications (Doctoral dissertation). 3 Hu, H. H. (2013). Smartphone Positioning with Street View Image Database. 4 Fl�rez Revuelta, F., & Chaaraoui, A. A. (2012). Interfaz de control dom�tico basada en un sistema de detecci�n de postura. 5 Idrobo Pizo, G. A. (2014). Projeto de um descritor para o alinhamento de imagens de profundidade de superf�cies com aplica��o em vis�o rob�tica. 6 Einrichtung, B., & f�r Lehrerbildung, Z. OPUS-Passau. 7 Rhee, S. B. (2013). Efficient Image Stitching Using Fast Feature Descriptor Extraction and Matching. KIPS Transactions on Software and Data Engineering, 2(1), 65-70. 8 de Souza Tarallo, A., da Silva, F. A., Hiraga, A. K., de Paiva, M. S. V., & de Castro Jorge, L. A. Mosaicos de Imagens A�reas Sequenciais Constru�dos Automaticamente. 9 Valenzuela, R. E., Schwartz, W. R., & Pedrini, H. Redu��o de Dimensionalidade Aplicada � Descri��o de Caracter�sticas Visuais. 10 COMIT�, Y. A. P. E. S. Eduardo Quintana Contreras. 11 Augereau, O. (2013). Reconnaissance et classification d'images de documents (Doctoral dissertation, Universit� Sciences et Technologies-Bordeaux I). 12 de Souza Tarallo, A., Hiraga, A. K., Martinez, G. A. G., de Paiva, M. S. V., de Castro Jorge, L. A., & Senger, H. Uso de mosaico de imagens a�reas como ferramenta de aux�lio ao diagn�stico de diversas culturas*. 13 GALLA, H. Z. (2015). PENGELOMPOKAN CITRA RAMBU LALU LINTAS DENGAN HIERARCHICAL AGGLOMERATIVE CLUSTERING BERBASIS SCALE INVARIANT FEATURE TRANSFORM. Skripsi, Fakultas Ilmu Komputer. MANUSCRIPT AUTHORS Miss Luo Juan Korea South qiuhehappy@hotmail.com Dr. Oubong Gwun Computer Graphics Lab, Chonbuk National University, Jeonju 561-756, South Korea South Korea A comparison of sift, pca-sift and surf 14 Renkens, I. M. (2015). Prometheus: from 2D to 3D. A reconstruction based on photographs (Doctoral dissertation, TU Delft, Delft University of Technology). 15 CIOU, J. J. (2014). Embedded Omni-Directional Wheeled Mobile Robot Visual SLAM based on Depth Image Database. 16 Prieto S�nchez, J. (2011). Reconocimiento de objetos por visi�n artificial en entornos controlados. 17 Campos, M. F. M. 2� Lista de Exerc�cios 1 Exerc�cios Te�ricos. 18 Liedtke, A. Realisierung eines robusten Verfahrens zur Identifikation von Positionsmarken unter Verwendung von Bildverarbeitung in Videobildern. 19 Quintana Rosales, M. A. (2015). Registro de una secuencia temporal de nubes de puntos utilizando tecnolog�a Kinect para la reconstrucci�n tridimensional de material arqueol�gico. 20 Mauricio, C. J. L., Edgar, F. S., & Samuel, R. H. E. (2012). Sistema M�vil de Virtualizaci�n, Edici�n y Visualizaci�n de Objetos 3D (VIAR). 21 de Souza Tarallo, A. (2013). Escola de Engenharia de S�o Carlos (Doctoral dissertation, Universidade de S�o Paulo). 22 BOUACHIR, W., & DOCTOR, O. D. D. D. P. (2014). SUIVI D�OBJETS PAR CARACT�RISTIQUES LOCALES ENCODANT LA STRUCTURE. 23 Hung, C. Y. (2014). Reasearch and Validation of Application of Adaptive Template Building Technology to Lithography Pattern of Light Emitting Diode Chip. 24 Jatmiko, S. Analisis Dan Implementasi Penggunaan Scale Invariant Feature Transform (SIFT) Pada Sistem Verifikasi Tanda Tangan. 25 Pizarro, C. A. (2014). On the possibility to find coordinates by random features (Doctoral dissertation, Autonomous University of Madrid). 26 Creve, M. Navigatiehulp voor mensen met een visuele beperking. 27 Tarallo, A. D. S., Hiraga, A. K., Martinez, G. A. G., Paiva, M. S. V. D., Jorge, L. A. D. C., & Senger, H. (2013). Parallel Processing Applied to Image Mosaic Generation. In IX Workshop de Vis�o Computacional (WVC 2013). Universidade Federal Fluminense (UFF). 28 Naftalianto, Y. (2012). RancangBangun Sensor Jarak Dengan Korespondensi Citra Dengan Ekstraksi Fitur SURF Dan Konsep Stereo Vision. Jurnal Sarjana Teknik Elektro, 1(1). 29 Meisel, A. Andreas Liedtke Realisierung eines robusten Verfahrens zur Identifikation von Positionsmarken unter Verwendung von Bildverarbeitung in Videobildern. 30 Kim, J. Y., Jeong, S. W., Jeong, M. B., Han, H. J., Kim, J. S., Park, H. M., & Chung, B. H. (2002). Application of total ear canal ablation and Ferreira, A. L. S., Santos, S. R. D., & de Miranda, L. C. (2012, May). TrueSight A Pedestrian Navigation System Based in Automatic Landmark Detection and Extraction on Android Smartphone. In Virtual and Augmente 31 Augereau, O., Journet, N., & Domenger, J. P. (2013). Reconnaissance et extraction de documents. Document num�rique, 16(2), 91-118. 32 Laura, T. L. (2013). Sistema de supervis�o a�rea baseado em navega��o visual para detec��o de anomalias em instala��es de petr�leo e g�s. 33 Wang, P. H. (2011). Implementation of fast SIFT feature extraction and matching using GPU. 34 Mota, I. F. V. (2014). Olh�-passarinho: uma extens�o do TweeProfiles para fotografias (Doctoral dissertation, Master�s thesis, Faculdade de Engenharia da Universidade do Porto). 35 Salamon, N. Z. (2015). Re-identifica��o de pessoas em imagens atrav�s de caracter�sticas descritivas de cores e grupos. 36 Couto, L., & Os�rio, F. Auto-Localiza��o Aut�noma de Rob�s M�veis por Vis�o Computacional Baseada em Pontos de Refer�ncia. 37 Shukla, A. P., & Saini, M. (2015). �Moving Object Tracking of Vehicle Detection�: A Concise Review. International Journal of Signal Processing, Image Processing and Pattern Recognition, 8(3), 169-176. 38 Aguilar, W. G., & Angulo, C. (2015). Real-Time Model-Based Video Stabilization for Microaerial Vehicles. Neural Processing Letters, 1-19. 39 Solehah, S., Yaakob, S. N., Kadim, Z., & Woon, H. H. (2012, December). Moving object extraction in PTZ camera using the integration of background subtraction and local histogram processing. In Computer Applications and Industrial Electronics (ISCAIE), 2012 IEEE Symposium on (pp. 167-172). IEEE. 40 Xie, Z., Chen, J., Yao, T., & Sun, Y. (2015). Geometric structure-constraint tracking with confident parts. Signal Processing: Image Communication, 36, 43-52. 41 Jayachandran, G., Ekin, A., & De Haan, I. G. (2012). Landmark detection in MR brain images using SURF. 42 Lin, S. C. F., Wong, C. Y., Jiang, G., Rahman, M. A., & Kwok, N. M. (2014). Radial Fourier Analysis (RFA) Descriptor with Fourier-based Keypoint Orientation. International Journal of Image Processing (IJIP), 8(6), 397. 43 Xiong, X., & Choi, B. J. (2013, January). Estimation of Relative Self-Localization Based On Natural Landmark and an Improved SURF. In Proceedings of World Academy of Science, Engineering and Technology (No. 73, p. 900). World Academy of Science, Engineering and Technology (WASET). 44 Xiong, P., Liu, X., Gao, C., Zhou, Z., Gao, C., & Liu, Q. (2013, March). A Real-time Stitching Algorithm for UAV Aerial Images. In Proceedings of the 2nd International Conference on Computer Science and Electronics Engineering. Atlantis Press. 45 Averkin, A. N. (2010). Training image-correlation systems by optimizing their attribute representations. Journal of Optical Technology, 77(11), 712-720. 46 Koel, F., Maurer, R. K., & Prieler, Z. R. A Survey on Action Recognition Using Kinematic Feature Extraction Techniques. 47 Xiong, X., & Choi, B. J. (2013). Estimation of Relative Self-Localization for Indoor Mobile Robot and Its Error Analysis. International Journal of Smart Home, 7(4), 69-76. 48 Lin, C. H., & Chen, A. Y. Trip Characteristics Study through Social Media Data. 49 Wang, Z., & Qureshi, F. Z. (2013, May). Topic models for image localization. In Computer and Robot Vision (CRV), 2013 International Conference on (pp. 136-143). IEEE. 50 Li, X., Aouf, N., & Nemra, A. (2012, September). Estimation analysis in VSLAM for UAV application. In Multisensor Fusion and Integration for Intelligent Systems (MFI), 2012 IEEE Conference on (pp. 365370). IEEE. 51 Jain, S., & Kanwal, N. (2014, November). Overview on image registration. In Medical Imaging, mHealth and Emerging Communication Systems (MedCom), 2014 International Conference on (pp. 376-381). IEEE. 52 Potgieter, M., & Van Niekerk, J. (2013). MULTI-AGENT AUGMENTED COMPUTER VISION TECHNOLOGIES TO SUPPORT HUMAN MONITORING OF SECURE COMPUTING FACILITIES. SAIEE Africa Research Journal, 80. 53 Jurgensen, S. M. (2014). The rotated speeded-up robust features algorithm (R-SURF). NAVAL POSTGRADUATE SCHOOL MONTEREY CA. 54 Singh, K., & Chander, S. (2014). CONTENT BASED IMAGE RETRIEVAL WITH SURF, SVM AND COLOR HISTOGRAM. 55 Issac, A., & Velayutham, C. S. (2012). SaddleSURF: A Saddle Based Interest Point Detector. In Mathematical Modelling and Scientific Computation (pp. 413-420). Springer Berlin Heidelberg. 56 Sharma, B., & Sharma, A. (2015). A REVIEW: CONTENT BASED IMAGE RETRIEVAL WITH SURF AND COLOUR HISTOGRAM USING CROSS VALIDATION AND GRAPH MATCHING ON MEDICAL IMAGES. International Journal, 1(4). 57 Walsh, R., & Hornsby, A. (2011, January). Toward,
    publicationDate: None,
    authors: ['Luo Juan', 'O. Gwun'],
    score: 160.5294054900381
},
{
    title: In-Context Retrieval-Augmented Language Models,
    abstract: Abstract Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access.1,
    publicationDate: 2023-01-31,
    authors: ['Ori Ram', 'Yoav Levine', 'Itay Dalmedigos', 'Dor Muhlgay', 'A. Shashua', 'Kevin Leyton-Brown', 'Y. Shoham'],
    score: 160.2788982174435
},
{
    title: Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection,
    abstract: Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.,
    publicationDate: 2023-10-17,
    authors: ['Akari Asai', 'Zeqiu Wu', 'Yizhong Wang', 'Avirup Sil', 'Hannaneh Hajishirzi'],
    score: 160.2788982174435
},
{
    title: Memory-Based Model Editing at Scale,
    abstract: Even the largest neural networks make errors, and once-correct predictions can become invalid as the world changes. Model editors make local updates to the behavior of base (pre-trained) models to inject updated knowledge or correct undesirable behaviors. Existing model editors have shown promise, but also suffer from insufficient expressiveness: they struggle to accurately model an edit's intended scope (examples affected by the edit), leading to inaccurate predictions for test inputs loosely related to the edit, and they often fail altogether after many edits. As a higher-capacity alternative, we propose Semi-Parametric Editing with a Retrieval-Augmented Counterfactual Model (SERAC), which stores edits in an explicit memory and learns to reason over them to modulate the base model's predictions as needed. To enable more rigorous evaluation of model editors, we introduce three challenging language model editing problems based on question answering, fact-checking, and dialogue generation. We find that only SERAC achieves high performance on all three problems, consistently outperforming existing approaches to model editing by a significant margin. Code, data, and additional project information will be made available at https://sites.google.com/view/serac-editing.,
    publicationDate: 2022-06-13,
    authors: ['E. Mitchell', 'Charles Lin', 'Antoine Bosselut', 'Christopher D. Manning', 'Chelsea Finn'],
    score: 153.86480470766284
},
{
    title: Internet-Augmented Dialogue Generation,
    abstract: The largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giving access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training. In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information. We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledgedriven discussions in order to ground their responses. We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or FAISS-based retrieval (Lewis et al., 2020b).,
    publicationDate: 2021-07-15,
    authors: ['M. Komeili', 'Kurt Shuster', 'J. Weston'],
    score: 153.58231048266646
},
{
    title: End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering,
    abstract: We present an end-to-end differentiable training method for retrieval-augmented open-domain question answering systems that combine information from multiple retrieved documents when generating answers. We model retrieval decisions as latent variables over sets of relevant documents. Since marginalizing over sets of retrieved documents is computationally hard, we approximate this using an expectation-maximization algorithm. We iteratively estimate the value of our latent variable (the set of relevant documents for a given question) and then use this estimate to update the retriever and reader parameters. We hypothesize that such end-to-end training allows training signals to flow to the reader and then to the retriever better than staged-wise training. This results in a retriever that is able to select more relevant documents for a question and a reader that is trained on more accurate documents to generate an answer. Experiments on three benchmark datasets demonstrate that our proposed method outperforms all existing approaches of comparable size by 2-3% absolute exact match points, achieving new state-of-the-art results. Our results also demonstrate the feasibility of learning to retrieve to improve answer generation without explicit supervision of retrieval decisions.,
    publicationDate: 2021-06-09,
    authors: ['Devendra Singh Sachan', 'Siva Reddy', 'William Hamilton', 'Chris Dyer', 'Dani Yogatama'],
    score: 152.6513767537887
},
{
    title: Active Retrieval Augmented Generation,
    abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.,
    publicationDate: 2023-05-11,
    authors: ['Zhengbao Jiang', 'Frank F. Xu', 'Luyu Gao', 'Zhiqing Sun', 'Qian Liu', 'Jane Dwivedi-Yu', 'Yiming Yang', 'Jamie Callan', 'Graham Neubig'],
    score: 147.64224598860744
},
{
    title: Benchmarking Large Language Models in Retrieval-Augmented Generation,
    abstract: Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.,
    publicationDate: 2023-09-04,
    authors: ['Jiawei Chen', 'Hongyu Lin', 'Xianpei Han', 'Le Sun'],
    score: 147.55725992557228
},
{
    title: Comparing noun phrasing techniques for use with medical digital library tools,
    abstract: In an effort to assist medical researchers and professionals in accessing information necessary for their work, the A1 Lab at the University of Arizona is investigating the use of a natural language processing (NLP) technique called noun phrasing. The goal of this research is to determine whether noun phrasing could be a viable technique to include in medical information retrieval applications. Four noun phrase generation tools were evaluated as to their ability to isolate noun phrases from medical journal abstracts. Tests were conducted using the National Cancer Institute's CANCERLIT database. The NLP tools evaluated were Massachusetts Institute of Technology's (MIT's) Chopper, The University of Arizona's Automatic Indexer, Lingsoft's NPtool, and The University of Arizona's AZ Noun Phraser. In addition, the National Library of Medicine's SPECIALIST Lexicon was incorporated into two versions of the AZ Noun Phraser to be evaluated against the other tools as well as a nonaugmented version of the AZ Noun Phraser. Using the metrics relative subject recall and precision, our results show that, with the exception of Chopper, the phrasing tools were fairly comparable in recall and precision. It was also shown that augmenting the AZ Noun Phraser by including the SPECIALIST Lexicon from the National Library of Medicine resulted in improved recall and precision.,
    publicationDate: 2000-03-01,
    authors: ['K. Tolle', 'Hsinchun Chen'],
    score: 147.1249533475399
},
{
    title: ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model,
    abstract: 3D human motion generation is crucial for creative industry. Recent advances rely on generative models with domain knowledge for text-driven motion generation, leading to substantial progress in capturing common motions. However, the performance on more diverse motions remains unsatisfactory. In this work, we propose ReMoDiffuse, a diffusion-model-based motion generation framework that integrates a retrieval mechanism to refine the denoising process. ReMoDiffuse enhances the generalizability and diversity of text-driven motion generation with three key designs: 1) Hybrid Retrieval finds appropriate references from the database in terms of both semantic and kinematic similarities. 2) Semantic-Modulated Transformer selectively absorbs retrieval knowledge, adapting to the difference between retrieved samples and the target motion sequence. 3) Condition Mixture better utilizes the retrieval database during inference, overcoming the scale sensitivity in classifier-free guidance. Extensive experiments demonstrate that ReMoDiffuse outperforms state-of-the-art methods by balancing both text-motion consistency and motion quality, especially for more diverse motion generation. Project page: https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html,
    publicationDate: 2023-04-03,
    authors: ['Mingyuan Zhang', 'Xinying Guo', 'Liang Pan', 'Zhongang Cai', 'Fangzhou Hong', 'Huirong Li', 'Lei Yang', 'Ziwei Liu'],
    score: 146.7774830694264
},
{
    title: Managing Semantic Content for the Web,
    abstract: By associating meaning with content, the Semantic Web will facilitate search, interoperability, and the composition of complex applications. The paper discusses the Semantic Content Organization and Retrieval Engine (SCORE, see vvww.voquette.com), which is based on research transferred from the University of Georgia's Large Scale Distributed Information Systems. SCORE belongs to a new generation of technologies for the emerging Semantic Web. It provides facilities to define ontological components that software agents can maintain. These agents use regular expression based rules in conjunction with various semantic techniques to extract ontology-driven metadata from structured and semistructured content. Automatic classification and information-extraction techniques augment these results and also let the system deal with unstructured text.,
    publicationDate: 2002-07-01,
    authors: ['A. Sheth', 'Clemens Bertram', 'David Avant', 'B. Hammond', 'K. Kochut', 'Yashodhan S. Warke'],
    score: 146.48964022532778
},
{
    title: Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training,
    abstract: Prior work on Data-To-Text Generation, the task of converting knowledge graph (KG) triples into natural text, focused on domain-specific benchmark datasets. In this paper, however, we verbalize the entire English Wikidata KG, and discuss the unique challenges associated with a broad, open-domain, large-scale verbalization. We further show that verbalizing a comprehensive, encyclopedic KG like Wikidata can be used to integrate structured KGs and natural language corpora. In contrast to the many architectures that have been developed to integrate these two sources, our approach converts the KG into natural text, allowing it to be seamlessly integrated into existing language models. It carries the further advantages of improved factual accuracy and reduced toxicity in the resulting language model. We evaluate this approach by augmenting the retrieval corpus in a retrieval language model and showing significant improvements on the knowledge intensive tasks of open domain QA and the LAMA knowledge probe.,
    publicationDate: 2021-06-01,
    authors: ['Oshin Agarwal', 'Heming Ge', 'Siamak Shakeri', 'Rami Al-Rfou'],
    score: 145.8436870802246
},
{
    title: RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation,
    abstract: The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark are publicly available: https://github.com/microsoft/CodeT/tree/main/RepoCoder,
    publicationDate: 2023-03-22,
    authors: ['Fengji Zhang', 'B. Chen', 'Yue Zhang', 'Jin Liu', 'Daoguang Zan', 'Yi Mao', 'Jian-Guang Lou', 'Weizhu Chen'],
    score: 145.74784010874305
},
{
    title: Partial-match retrieval via the method of superimposed codes,
    abstract: This paper presents and analyzes an effective and practical method of accomplishing partial-match retrieval on a computer file containing a large number of information records. In partial-match retrieval a subset of the records in the file is selected and retrieved by specifying a query set consisting of a small number of key values; the records selected and retrieved are those having a match to all the key values in the query set. Partial-match retrieval can be a powerful capability when used in information retrieval systems, and a stored file augmented with such a capability is equivalent to an associative or content-addressable store. The method presented in this paper is based upon the use of superimposed codes, a technique used previously with some success in notched-edge card filing systems in which card selection was accomplished mechanically with long metal needles. A new algorithm is presented for generating superimposed codes without the use of a stored code dictionary. The new algorithm executes rapidly on a digital computer and employs a hash function and a pseudo-random number generator; it allows the generation of binary codes having any desired width and weight. It is pointed out that the use of variableweight binary codes gives an advantage on files in which the key values occur with substantially different frequencies. It is shown that organizing the bits of the superimposed code words properly in storage leads to the property that only a small fraction of these bits need be retrieved and processed on each query; this substantially reduces the time required to execute a query. Also, because of the simplicity of the bit operations required in the processing of superimposed code words, a very fast query processor could be designed and built with currently available digital hardware devices and techniques. With the query processor implemented in such digital hardware, partial-match query rates as high as 100/s or even higher appear to be feasible. In order to demonstrate experimentally the power of the method of superimposed codes, the method was implemented in software on a file consisting of the Suffolk County, NY, telephone-directory listings.,
    publicationDate: 1979-12-01,
    authors: ['Ievrwiav ICNlaWl', 'Ivd IWlsLIp'],
    score: 145.69808968562683
},
{
    title: From Local to Global: A Graph RAG Approach to Query-Focused Summarization,
    abstract: The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as"What are the main themes in the dataset?", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose a Graph RAG approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed. Our approach uses an LLM to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that Graph RAG leads to substantial improvements over a na\"ive RAG baseline for both the comprehensiveness and diversity of generated answers. An open-source, Python-based implementation of both global and local Graph RAG approaches is forthcoming at https://aka.ms/graphrag.,
    publicationDate: 2024-04-24,
    authors: ['Darren Edge', 'Ha Trinh', 'Newman Cheng', 'Joshua Bradley', 'Alex Chao', 'Apurva Mody', 'Steven Truitt', 'Jonathan Larson'],
    score: 145.05919458918189
},
{
    title: Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy,
    abstract: Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.,
    publicationDate: 2023-05-24,
    authors: ['Zhihong Shao', 'Yeyun Gong', 'Yelong Shen', 'Minlie Huang', 'Nan Duan', 'Weizhu Chen'],
    score: 144.6510061363086
},
{
    title: Retrieval Augmented Code Generation and Summarization,
    abstract: Software developers write a lot of source code and documentation during software development. Intrinsically, developers often recall parts of source code or code summaries that they had written in the past while implementing software or documenting them. To mimic developers' code or summary generation behavior, we propose a retrieval augmented framework, REDCODER, that retrieves relevant code or summaries from a retrieval database and provides them as a supplement to code generation or summarization models. REDCODER has a couple of uniqueness. First, it extends the state-of-the-art dense retrieval technique to search for relevant code or summaries. Second, it can work with retrieval databases that include unimodal (only code or natural language description) or bimodal instances (code-description pairs). We conduct experiments and extensive analysis on two benchmark datasets of code generation and summarization in Java and Python, and the promising results endorse the effectiveness of our proposed retrieval augmented framework.,
    publicationDate: 2021-08-26,
    authors: ['Md. Rizwan Parvez', 'W. Ahmad', 'Saikat Chakraborty', 'Baishakhi Ray', 'Kai-Wei Chang'],
    score: 144.54719949364
},
{
    title: Reinforcement-Learning-Guided Source Code Summarization Using Hierarchical Attention,
    abstract: Code summarization (aka comment generation) provides a high-level natural language description of the function performed by code, which can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, the state-of-the-art approaches follow an encoder-decoder framework which encodes source code into a hidden space and later decodes it into a natural language space. Such approaches suffer from the following drawbacks: (a) they are mainly input by representing code as a sequence of tokens while ignoring code hierarchy; (b) most of the encoders only input simple features (e.g., tokens) while ignoring the features that can help capture the correlations between comments and code; (c) the decoders are typically trained to predict subsequent words by maximizing the likelihood of subsequent ground truth words, while in real world, they are excepted to generate the entire word sequence from scratch. As a result, such drawbacks lead to inferior and inconsistent comment generation accuracy. To address the above limitations, this paper presents a new code summarization approach using hierarchical attention network by incorporating multiple code features, including type-augmented abstract syntax trees and program control flows. Such features, along with plain code sequences, are injected into a deep reinforcement learning (DRL) framework (e.g., actor-critic network) for comment generation. Our approach assigns weights (pays “attention”) to tokens and statements when constructing the code representation to reflect the hierarchical code structure under different contexts regarding code features (e.g., control flows and abstract syntax trees). Our reinforcement learning mechanism further strengthens the prediction results through the actor network and the critic network, where the actor network provides the confidence of predicting subsequent words based on the current state, and the critic network computes the reward values of all the possible extensions of the current state to provide global guidance for explorations. Eventually, we employ an advantage reward to train both networks and conduct a set of experiments on a real-world dataset. The experimental results demonstrate that our approach outperforms the baselines by around 22 to 45 percent in BLEU-1 and outperforms the state-of-the-art approaches by around 5 to 60 percent in terms of S-BLEU and C-BLEU.,
    publicationDate: 2022-01-01,
    authors: ['Wenhua Wang', 'Yuqun Zhang', 'Yulei Sui', 'Yao Wan', 'Zhou Zhao', 'Jian Wu', 'Philip S. Yu', 'Guandong Xu'],
    score: 144.46522287201753
},
{
    title: KAT: A Knowledge Augmented Transformer for Vision-and-Language,
    abstract: The primary focus of recent work with large-scale transformers has been on optimizing the amount of information packed into the model’s parameters. In this work, we ask a complementary question: Can multimodal transformers leverage explicit knowledge in their reasoning? Existing, primarily unimodal, methods have explored approaches under the paradigm of knowledge retrieval followed by answer prediction, but leave open questions about the quality and relevance of the retrieved knowledge used, and how the reasoning processes over implicit and explicit knowledge should be integrated. To address these challenges, we propose a - Knowledge Augmented Transformer (KAT) - which achieves a strong state-of-the-art result (+6% absolute) on the open-domain multimodal task of OK-VQA. Our approach integrates implicit and explicit knowledge in an encoder-decoder architecture, while still jointly reasoning over both knowledge sources during answer generation. Additionally, explicit knowledge integration improves interpretability of model predictions in our analysis.,
    publicationDate: 2021-12-16,
    authors: ['Liangke Gui', 'Borui Wang', 'Qiuyuan Huang', 'A. Hauptmann', 'Yonatan Bisk', 'Jianfeng Gao'],
    score: 143.6898232860408
},
{
    title: A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models,
    abstract: As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc. This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types. This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs. Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.,
    publicationDate: 2024-01-02,
    authors: ['S. Tonmoy', 'S. M. M. Zaman', 'Vinija Jain', 'Anku Rani', 'Vipula Rawte', 'Aman Chadha', 'Amitava Das'],
    score: 143.24202883879556
},
{
    title: Learned hardware-in-the-loop phase retrieval for holographic near-eye displays,
    abstract: Holography is arguably the most promising technology to provide wide field-of-view compact eyeglasses-style near-eye displays for augmented and virtual reality. However, the image quality of existing holographic displays is far from that of current generation conventional displays, effectively making today's holographic display systems impractical. This gap stems predominantly from the severe deviations in the idealized approximations of the "unknown" light transport model in a real holographic display, used for computing holograms. In this work, we depart from such approximate "ideal" coherent light transport models for computing holograms. Instead, we learn the deviations of the real display from the ideal light transport from the images measured using a display-camera hardware system. After this unknown light propagation is learned, we use it to compensate for severe aberrations in real holographic imagery. The proposed hardware-in-the-loop approach is robust to spatial, temporal and hardware deviations, and improves the image quality of existing methods qualitatively and quantitatively in SNR and perceptual quality. We validate our approach on a holographic display prototype and show that the method can fully compensate unknown aberrations and erroneous and non-linear SLM phase delays, without explicitly modeling them. As a result, the proposed method significantly outperforms existing state-of-the-art methods in simulation and experimentation - just by observing captured holographic images.,
    publicationDate: 2020-11-26,
    authors: ['Praneeth Chakravarthula', 'Ethan Tseng', 'Tarun Srivastava', 'H. Fuchs', 'Felix Heide'],
    score: 142.98862177981874
},
{
    title: Retrieval-Augmented Generation for AI-Generated Content: A Survey,
    abstract: Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Github: https://github.com/PKU-DAIR/RAG-Survey.,
    publicationDate: 2024-02-29,
    authors: ['Penghao Zhao', 'Hailin Zhang', 'Qinhan Yu', 'Zhengren Wang', 'Yunteng Geng', 'Fangcheng Fu', 'Ling Yang', 'Wentao Zhang', 'Bin Cui'],
    score: 141.93685818395113
},
{
    title: Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning,
    abstract: We present CM3Leon (pronounced"Chameleon"), a retrieval-augmented, token-based, decoder-only multi-modal language model capable of generating and infilling both text and images. CM3Leon uses the CM3 multi-modal architecture but additionally shows the extreme benefits of scaling up and tuning on more diverse instruction-style data. It is the first multi-modal model trained with a recipe adapted from text-only language models, including a large-scale retrieval-augmented pre-training stage and a second multi-task supervised fine-tuning (SFT) stage. It is also a general-purpose model that can do both text-to-image and image-to-text generation, allowing us to introduce self-contained contrastive decoding methods that produce high-quality outputs. Extensive experiments demonstrate that this recipe is highly effective for multi-modal models. CM3Leon achieves state-of-the-art performance in text-to-image generation with 5x less training compute than comparable methods (zero-shot MS-COCO FID of 4.88). After SFT, CM3Leon can also demonstrate unprecedented levels of controllability in tasks ranging from language-guided image editing to image-controlled generation and segmentation.,
    publicationDate: 2023-09-05,
    authors: ['L. Yu', 'Bowen Shi', 'Ramakanth Pasunuru', 'Benjamin Muller', 'O. Yu. Golovneva', 'Tianlu Wang', 'Arun Babu', 'Binh Tang', 'Brian Karrer', 'Shelly Sheynin', 'Candace Ross', 'Adam Polyak', 'Russell Howes', 'Vasu Sharma', 'Puxin Xu', 'Hovhannes Tamoyan', 'Oron Ashual', 'Uriel Singer', 'Shang-Wen Li', 'Susan Zhang', 'Rich James', 'Gargi Ghosh', 'Yaniv Taigman', 'Maryam Fazel-Zarandi', 'Asli Celikyilmaz', 'Luke Zettlemoyer', 'Armen Aghajanyan'],
    score: 141.68685239667295
},
{
    title: RAGAs: Automated Evaluation of Retrieval Augmented Generation,
    abstract: We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAGAs is available at [https://github.com/explodinggradients/ragas]. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.,
    publicationDate: 2023-09-26,
    authors: ['ES Shahul', 'Jithin James', 'Luis Espinosa Anke', 'S. Schockaert'],
    score: 140.9108172806851
},
{
    title: A Survey on Retrieval-Augmented Text Generation,
    abstract: Recently, retrieval-augmented text generation attracted increasing attention of the computational linguistics community. Compared with conventional generation models, retrieval-augmented text generation has remarkable advantages and particularly has achieved state-of-the-art performance in many NLP tasks. This paper aims to conduct a survey about retrieval-augmented text generation. It firstly highlights the generic paradigm of retrieval-augmented generation, and then it reviews notable approaches according to different tasks including dialogue response generation, machine translation, and other generation tasks. Finally, it points out some important directions on top of recent methods to facilitate future research.,
    publicationDate: 2022-02-02,
    authors: ['Huayang Li', 'Yixuan Su', 'Deng Cai', 'Yan Wang', 'Lemao Liu'],
    score: 140.55428903620444
},
{
    title: Almanac: Retrieval-Augmented Language Models for Clinical Medicine,
    abstract: Large-language models have recently demonstrated impressive zero-shot capabilities in a variety of natural language tasks such as summarization, dialogue generation, and question-answering. Despite many promising applications in clinical medicine, adoption of these models in real-world settings has been largely limited by their tendency to generate incorrect and sometimes even toxic statements. In this study, we develop Almanac, a large language model framework augmented with retrieval capabilities for medical guideline and treatment recommendations. Performance on a novel dataset of clinical scenarios (n= 130) evaluated by a panel of 5 board-certified and resident physicians demonstrates significant increases in factuality (mean of 18% at p-value < 0.05) across all specialties, with improvements in completeness and safety. Our results demonstrate the potential for large language models to be effective tools in the clinical decision-making process, while also emphasizing the importance of careful testing and deployment to mitigate their shortcomings.,
    publicationDate: 2023-03-01,
    authors: ['W. Hiesinger', 'Cyril Zakka', 'Akash Chaurasia', 'R. Shad', 'Alex R. Dalal', 'Jennifer L. Kim', 'Michael Moor', 'Kevin Alexander', 'Euan A. Ashley', 'Jack Boyd', 'Kathleen Boyd', 'Karen Hirsch', 'C. Langlotz', 'Joanna Nelson'],
    score: 140.09243251692857
},
{
    title: Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory,
    abstract: With direct access to human-written reference as memory, retrieval-augmented generation has achieved much progress in a wide range of text generation tasks. Since better memory would typically prompt better generation~(we define this as primal problem). The traditional approach for memory retrieval involves selecting memory that exhibits the highest similarity to the input. However, this method is constrained by the quality of the fixed corpus from which memory is retrieved. In this paper, by exploring the duality of the primal problem: better generation also prompts better memory, we propose a novel framework, selfmem, which addresses this limitation by iteratively employing a retrieval-augmented generator to create an unbounded memory pool and using a memory selector to choose one output as memory for the subsequent generation round. This enables the model to leverage its own output, referred to as self-memory, for improved generation. We evaluate the effectiveness of selfmem on three distinct text generation tasks: neural machine translation, abstractive text summarization, and dialogue generation, under two generation paradigms: fine-tuned small model and few-shot LLM. Our approach achieves state-of-the-art results in four directions in JRC-Acquis, XSum (50.3 ROUGE-1), and BigPatent (62.9 ROUGE-1), demonstrating the potential of self-memory in enhancing retrieval-augmented generation models. Furthermore, we conduct thorough analyses of each component in the selfmem framework to identify bottlenecks and provide insights for future research.,
    publicationDate: 2023-05-03,
    authors: ['Xin Cheng', 'Di Luo', 'Xiuying Chen', 'Lemao Liu', 'Dongyan Zhao', 'Rui Yan'],
    score: 140.07038929086448
},
{
    title: FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation,
    abstract: Retrieval-augmented generation models offer many benefits over standalone language models: besides a textual answer to a given query they provide provenance items retrieved from an updateable knowledge base. However, they are also more complex systems and need to handle long inputs. In this work, we introduce FiD-Light to strongly increase the efficiency of the state-of-the-art retrieval-augmented FiD model, while maintaining the same level of effectiveness. Our FiD-Light model constrains the information flow from the encoder (which encodes passages separately) to the decoder (using concatenated encoded representations). Furthermore, we adapt FiD-Light with re-ranking capabilities through textual source pointers, to improve the top-ranked provenance precision. Our experiments on a diverse set of seven knowledge intensive tasks (KILT) show FiD-Light consistently improves the Pareto frontier between query latency and effectiveness. FiD-Light with source pointing sets substantial new state-of-the-art results on six KILT tasks for combined text generation and provenance retrieval evaluation, while maintaining high efficiency.,
    publicationDate: 2022-09-28,
    authors: ['Sebastian Hofstätter', 'Jiecao Chen', 'K. Raman', 'Hamed Zamani'],
    score: 138.84482113039638
},
{
    title: Benchmarking Retrieval-Augmented Generation for Medicine,
    abstract: While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different LLMs by up to 18% over chain-of-thought prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our results show that the combination of various medical corpora and retrievers achieves the best performance. In addition, we discovered a log-linear scaling property and the"lost-in-the-middle"effects in medical RAG. We believe our comprehensive evaluations can serve as practical guidelines for implementing RAG systems for medicine.,
    publicationDate: 2024-02-20,
    authors: ['Guangzhi Xiong', 'Qiao Jin', 'Zhiyong Lu', 'Aidong Zhang'],
    score: 138.46522287201753
},
{
    title: Watson: Anticipating and Contextualizing Information Needs,
    abstract: In this paper, we introduce a class of systems called Information Management Assistants (IMAs). IMAs automatically discover related material on behalf of the user by serving as an intermediary between the user and information retrieval systems. IMAs observe users interact with everyday applications and then anticipate their information needs using a model of the task at hand. IMAs then automatically fulfill these needs using the text of the document the user is manipulating and a knowledge of how to form queries to traditional information retrieval systems (e.g., Internet search engines, abstract databases, etc.). IMAs automatically query information systems on behalf of users as well as provide an interface by which the user can pose queries explicitly. Because IMAs are aware of the user’s task, they can augment their explicit query with terms representative of the context of this task. In this way, IMAs provide a framework for bringing implicit task context to bear on servicing explicit information requests, significantly reducing ambiguity. IMAs embody a just-in-time information infrastructure in which information is brought to users as they need it, without requiring explicit requests. In this paper, we present our work on an architecture for this class of system, and our progress implementing Watson, a prototype of such a system. Watson observes users in word processing and Web browsing applications and uses a simple model of the user’s tasks, knowledge of term importance, and an understanding of query generation to find relevant documents and service explicit queries. We close by discussing our experimental evaluations of the system.,
    publicationDate: 1999-12-01,
    authors: ['Jay Budzik', 'K. Hammond'],
    score: 136.76990718625132
},
{
    title: Response Generation by Context-aware Prototype Editing,
    abstract: Open domain response generation has achieved remarkable progress in recent years, but sometimes yields short and uninformative responses. We propose a new paradigm, prototypethen-edit for response generation, that first retrieves a prototype response from a pre-defined index and then edits the prototype response according to the differences between the prototype context and current context. Our motivation is that the retrieved prototype provides a good start-point for generation because it is grammatical and informative, and the post-editing process further improves the relevance and coherence of the prototype. In practice, we design a contextaware editing model that is built upon an encoder-decoder framework augmented with an editing vector. We first generate an edit vector by considering lexical differences between a prototype context and current context. After that, the edit vector and the prototype response representation are fed to a decoder to generate a new response. Experiment results on a large scale dataset demonstrate that our new paradigm significantly increases the relevance, diversity and originality of generation results, compared to traditional generative models. Furthermore, our model outperforms retrieval-based methods in terms of relevance and originality.,
    publicationDate: 2018-06-19,
    authors: ['Yu Wu', 'Furu Wei', 'Shaohan Huang', 'Zhoujun Li', 'Ming Zhou'],
    score: 136.56026936698498
},
{
    title: Retrieval-Augmented Multimodal Language Modeling,
    abstract: Recent multimodal models such as DALL-E and
CM3 have achieved remarkable progress in textto-image and image-to-text generation. However, these models store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the
model parameters, requiring increasingly larger models and training data to capture more knowledge. To integrate knowledge in a more scalable and modular way, we propose a retrievalaugmented multimodal model, which enables a base multimodal model (generator) to refer to relevant knowledge fetched by a retriever from external memory (e.g., multimodal documents on the web). Specifically, we implement a retriever using the pretrained CLIP model and a generator using the CM3 Transformer architecture, and
train this model using the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can retrieve and generate mixtures of text and images.
We show that RA-CM3 significantly outperforms baseline multimodal models such as DALL-E and CM3 on both image and caption generation tasks (12 FID and 17 CIDEr improvements on MSCOCO), while requiring much less compute for training (<30% of DALL-E). Moreover, we show that RA-CM3 exhibits novel capabilities such as knowledge-intensive image generation and multimodal in-context learning,
    publicationDate: None,
    authors: ['Michihiro Yasunaga', 'Armen Aghajanyan', 'Weijia Shi', 'Rich James', 'J. Leskovec', 'Percy Liang', 'M. Lewis', 'Luke Zettlemoyer', 'Wen-tau Yih'],
    score: 136.46225198264972
},
{
    title: Ptychography retrieval of fully polarized holograms from geometric-phase metasurfaces,
    abstract: None,
    publicationDate: 2020-05-27,
    authors: ['Q. Song', 'Arthur Baroni', 'Rajath Sawant', 'P. Ni', 'V. Brandli', 'S. Chenot', 'S. Vézian', 'B. Damilano', 'P. de Mierry', 'S. Khadir', 'P. Ferrand', 'P. Genevet'],
    score: 135.9389254954045
},
{
    title: Seven Failure Points When Engineering a Retrieval Augmented Generation System,
    abstract: Software engineers are increasingly adding semantic search capabilities to applications using a strategy known as Retrieval Augmented Generation (RAG). A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) such as ChatGPT to extract the right answer using an LLM. RAG systems aim to: a) reduce the problem of hallucinated responses from LLMs, b) link sources/references to generated responses, and c) remove the need for annotating documents with meta-data. However, RAG systems suffer from limitations inherent to information retrieval systems and from reliance on LLMs. In this paper, we present an experience report on the failure points of RAG systems from three case studies from separate domains: research, education, and biomedical. We share the lessons learned and present 7 failure points to consider when designing a RAG system. The two key takeaways arising from our work are: 1) validation of a RAG system is only feasible during operation, and 2) the robustness of a RAG system evolves rather than designed in at the start. We conclude with a list of potential research directions on RAG systems for the software engineering community.CCS CONCEPTS• Software and its engineering → Empirical software validation.,
    publicationDate: 2024-01-11,
    authors: ['Scott Barnett', 'Stefanus Kurniawan', 'Srikanth Thudumu', 'Zach Brannelly', 'Mohamed Abdelrazek'],
    score: 135.83476069846412
},
{
    title: Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models,
    abstract: Retrieval-augmented language model (RALM) represents a significant advancement in mitigating factual hallucination by leveraging external knowledge sources. However, the reliability of the retrieved information is not always guaranteed, and the retrieval of irrelevant data can mislead the response generation. Moreover, standard RALMs frequently neglect their intrinsic knowledge due to the interference from retrieved information. In instances where the retrieved information is irrelevant, RALMs should ideally utilize their intrinsic knowledge or, in the absence of both intrinsic and retrieved knowledge, opt to respond with “unknown” to avoid hallucination. In this paper, we introduces Chain-of-Note (CoN), a novel approach to improve robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios. The core idea of CoN is to generate sequential reading notes for each retrieved document, enabling a thorough evaluation of their relevance to the given question and integrating this information to formulate the final answer. Our experimental results show that GPT-4, when equipped with CoN, outperforms the Chain-of-Thought approach. Besides, we utilized GPT-4 to create 10K CoN data, subsequently trained on smaller models like OPT and LLaMa-2. Our experiments across four open-domain QA benchmarks show that fine-tuned RALMs equipped with CoN significantly outperform standard fine-tuned RALMs.,
    publicationDate: 2023-11-15,
    authors: ['W. Yu', 'Hongming Zhang', 'Xiaoman Pan', 'Kaixin Ma', 'Hongwei Wang', 'Dong Yu'],
    score: 135.73039952010822
},
{
    title: ORCAS: 18 Million Clicked Query-Document Pairs for Analyzing Search,
    abstract: Users of Web search engines reveal their information needs through queries and clicks, making click logs a useful asset for information retrieval. However, click logs have not been publicly released for academic use, because they can be too revealing of personally or commercially sensitive information. This paper describes a click data release related to the TREC Deep Learning Track document corpus. After aggregation and filtering, including a k -anonymity requirement, we find 1.4 million of the TREC DL URLs have 18 million connections to 10 million distinct queries. Our dataset of these queries and connections to TREC documents is of similar size to proprietary datasets used in previous papers on query mining and ranking. We perform some preliminary experiments using the click data to augment the TREC DL training data, offering by comparison: 28x more queries, with 49x more connections to 4.4x more URLs in the corpus. We present a description of the dataset's generation process, characteristics, use in ranking and other potential uses.,
    publicationDate: 2020-06-09,
    authors: ['Nick Craswell', 'Daniel Fernando Campos', 'Bhaskar Mitra', 'Emine Yilmaz', 'B. Billerbeck'],
    score: 135.73039952010822
},
{
    title: BioinspiredLLM: Conversational Large Language Model for the Mechanics of Biological and Bio‐Inspired Materials,
    abstract: The study of biological materials and bio‐inspired materials science is well established; however, surprisingly little knowledge is systematically translated to engineering solutions. To accelerate discovery and guide insights, an open‐source autoregressive transformer large language model (LLM), BioinspiredLLM, is reported. The model is finetuned with a corpus of over a thousand peer‐reviewed articles in the field of structural biological and bio‐inspired materials and can be prompted to recall information, assist with research tasks, and function as an engine for creativity. The model has proven that it is able to accurately recall information about biological materials and is further strengthened with enhanced reasoning ability, as well as with Retrieval‐Augmented Generation (RAG) to incorporate new data during generation that can also help to traceback sources, update the knowledge base, and connect knowledge domains. BioinspiredLLM also has shown to develop sound hypotheses regarding biological materials design and remarkably so for materials that have never been explicitly studied before. Lastly, the model shows impressive promise in collaborating with other generative artificial intelligence models in a workflow that can reshape the traditional materials design process. This collaborative generative artificial intelligence method can stimulate and enhance bio‐inspired materials design workflows. Biological materials are at a critical intersection of multiple scientific fields and models like BioinspiredLLM help to connect knowledge domains.,
    publicationDate: 2023-09-15,
    authors: ['Rachel K. Luu', 'M. Buehler'],
    score: 134.5637923958958
},
{
    title: A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models,
    abstract: As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations, such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the generation quality of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: architectures, training strategies, and applications. As the preliminary knowledge, we briefly introduce the foundations and recent advances of LLMs. Then, to illustrate the practical significance of RAG for LLMs, we systematically review mainstream relevant work by their architectures, training strategies, and application areas, detailing specifically the challenges of each and the corresponding capabilities of RA-LLMs. Finally, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/,
    publicationDate: 2024-05-10,
    authors: ['Wenqi Fan', 'Yujuan Ding', 'Liang-bo Ning', 'Shijie Wang', 'Hengyun Li', 'Dawei Yin', 'Tat-Seng Chua', 'Qing Li'],
    score: 134.14999178524084
},
{
    title: Recent Advances in Retrieval-Augmented Text Generation,
    abstract: Recently retrieval-augmented text generation has achieved state-of-the-art performance in many NLP tasks and has attracted increasing attention of the NLP and IR community, this tutorial thereby aims to present recent advances in retrieval-augmented text generation comprehensively and comparatively. It firstly highlights the generic paradigm of retrieval-augmented text generation, then reviews notable works for different text generation tasks including dialogue generation, machine translation, and other generation tasks, and finally points out some limitations and shortcomings to facilitate future research.,
    publicationDate: 2022-07-06,
    authors: ['Deng Cai', 'Yan Wang', 'Lemao Liu', 'Shuming Shi'],
    score: 134.07038929086448
},
{
    title: Continual Learning for Large Language Models: A Survey,
    abstract: Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.,
    publicationDate: 2024-02-02,
    authors: ['Tongtong Wu', 'Linhao Luo', 'Yuan-Fang Li', 'Shirui Pan', 'Thuy-Trang Vu', 'Gholamreza Haffari'],
    score: 133.7274286307404
},
{
    title: Generating Benchmarks for Factuality Evaluation of Language Models,
    abstract: Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain. Existing methods for factuality evaluation of LLM generation focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent domain specific or rare facts. We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality. FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM’s propensity to generate true facts from the corpus vs. similar but incorrect statements. We use our framework to create three benchmarks: Wiki-FACTOR, News-FACTOR and Expert-FACTOR. We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score and perplexity do not always agree on model ranking; (iii) when perplexity and benchmark score disagree, the latter better reflects factuality in open-ended generation, as measured by human annotators.,
    publicationDate: 2023-07-13,
    authors: ['Dor Muhlgay', 'Ori Ram', 'Inbal Magar', 'Yoav Levine', 'Nir Ratner', 'Yonatan Belinkov', 'Omri Abend', 'Kevin Leyton-Brown', 'A. Shashua', 'Y. Shoham'],
    score: 133.5115975689589
},
{
    title: Knowledge Graph Prompting for Multi-Document Question Answering,
    abstract: The `pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or document structural relations. For graph traversal, we design an LLM-based graph traversal agent that navigates across nodes and gathers supporting passages assisting LLMs in MD-QA. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the graph traversal agent acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality. Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the potential of leveraging graphs in enhancing the prompt design and retrieval augmented generation for LLMs. Our code: https://github.com/YuWVandy/KG-LLM-MDQA.,
    publicationDate: 2023-08-22,
    authors: ['Yu Wang', 'Nedim Lipka', 'Ryan A. Rossi', 'Alexa F. Siu', 'Ruiyi Zhang', 'Tyler Derr'],
    score: 133.5115975689589
},
{
    title: Generative Representational Instruction Tuning,
    abstract: All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by>60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available at https://github.com/ContextualAI/gritlm.,
    publicationDate: 2024-02-15,
    authors: ['Niklas Muennighoff', 'Hongjin Su', 'Liang Wang', 'Nan Yang', 'Furu Wei', 'Tao Yu', 'Amanpreet Singh', 'Douwe Kiela'],
    score: 133.07038929086448
},
{
    title: ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems,
    abstract: Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. We make our code and datasets publicly available on Github.,
    publicationDate: 2023-11-16,
    authors: ['Jon Saad-Falcon', 'O. Khattab', 'Christopher Potts', 'Matei Zaharia'],
    score: 132.84482113039638
},
{
    title: MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter,
    abstract: Language Models (LMs) have demonstrated impressive molecule understanding ability on various 1D text-related tasks. However, they inherently lack 2D graph perception - a critical ability of human professionals in comprehending molecules' topological structures. To bridge this gap, we propose MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter. MolCA enables an LM (e.g., Galactica) to understand both text- and graph-based molecular contents via the cross-modal projector. Specifically, the cross-modal projector is implemented as a Q-Former to connect a graph encoder's representation space and an LM's text space. Further, MolCA employs a uni-modal adapter (i.e., LoRA) for the LM's efficient adaptation to downstream tasks. Unlike previous studies that couple an LM with a graph encoder via cross-modal contrastive learning, MolCA retains the LM's ability of open-ended text generation and augments it with 2D graph information. To showcase its effectiveness, we extensively benchmark MolCA on tasks of molecule captioning, IUPAC name prediction, and molecule-text retrieval, on which MolCA significantly outperforms the baselines. Our codes and checkpoints can be found at https://github.com/acharkq/MolCA.,
    publicationDate: 2023-10-19,
    authors: ['Zhiyuan Liu', 'Sihang Li', 'Yancheng Luo', 'Hao Fei', 'Yixin Cao', 'Kenji Kawaguchi', 'Xiang Wang', 'Tat-Seng Chua'],
    score: 131.90701577567637
},
{
    title: TauREx 3: A Fast, Dynamic, and Extendable Framework for Retrievals,
    abstract: TauREx 3 is the next generation of the TauREx exoplanet atmospheric retrieval framework for Windows, Mac, and Linux. It is a complete rewrite with a full Python stack that makes it easy-to-use, high-performance, dynamic, and flexible. The new main TauREx program is built with modularity in mind, allowing the user to augment its functionalities with custom code and efficiently perform retrievals on custom parameters. We achieve this result by dynamic determination of fitting parameters, whereby TauREx 3 can detect new parameters for retrieval from user code through a simple interface. TauREx 3 can act as a library with a simple import taurex command, providing a rich set of classes and functions related to atmospheric modeling. A 10× speedup in forward model computations is achieved as compared to the previous version with a sixfold reduction in retrieval times while maintaining robust results. TauREx 3 is intended as a standalone, all-in-one package for retrievals while the TauREx 3 Python library can build or augment a user’s custom data pipeline easily.,
    publicationDate: 2019-12-16,
    authors: ['A. Al-Refaie', 'Q. Changeat', 'I. Waldmann', 'G. Tinetti'],
    score: 131.81520944380262
},
{
    title: Texture Memory-Augmented Deep Patch-Based Image Inpainting,
    abstract: Patch-based methods and deep networks have been employed to tackle image inpainting problem, with their own strengths and weaknesses. Patch-based methods are capable of restoring a missing region with high-quality texture through searching nearest neighbor patches from the unmasked regions. However, these methods bring problematic contents when recovering large missing regions. Deep networks, on the other hand, show promising results in completing large regions. Nonetheless, the results often lack faithful and sharp details that resemble the surrounding area. By bringing together the best of both paradigms, we propose a new deep inpainting framework where texture generation is guided by a texture memory of patch samples extracted from unmasked regions. The framework has a novel design that allows texture memory retrieval to be trained end-to-end with the deep inpainting network. In addition, we introduce a patch distribution loss to encourage high-quality patch synthesis. The proposed method shows superior performance both qualitatively and quantitatively on three challenging image benchmarks, i.e., Places, CelebA-HQ, and Paris Street-View datasets (Code will be made publicly available in https://github.com/open-mmlab/mmediting).,
    publicationDate: 2020-09-28,
    authors: ['Rui Xu', 'Minghao Guo', 'Jiaqi Wang', 'Xiaoxiao Li', 'Bolei Zhou', 'Chen Change Loy'],
    score: 131.33319181170904
},
{
    title: Augmented maintenance of powerplants: a prototyping case study of a mobile AR system,
    abstract: Augmented reality (AR) research has progressed in great strides over the past few years. Most current demonstrations focus on providing robust tracking solutions since this is the most critical issue when demonstrating AR systems. An issue that is typically neglected concerns the online access, analysis and visualization of information. The information required by AR demonstration systems is kept to a minimum, is prepared ahead of time, and is stored locally in the form of three-dimensional geometric descriptions. In complex mobile settings, these simplifying assumptions do not work. The authors report on recent efforts at the TU Munich to analyze the information generation, retrieval, transmission, and visualization process in the context of maintenance procedures that are performed in nuclear power plants. The use of AR to present such information online has significant implications for the way information must be acquired, stored, and transmitted. The paper focuses on pointing out open questions, discussing options for addressing them, and evaluating them in prototypical implementations.,
    publicationDate: 2001-10-29,
    authors: ['G. Klinker', 'Oliver Creighton', 'A.H. Dutoit', 'R. Kobylinski', 'Christoph Vilsmeier', 'B. Brügge'],
    score: 130.96100010429495
},
{
    title: Recitation-Augmented Language Models,
    abstract: We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrieval-augmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE first recites one or several relevant passages from LLMs' own memory via sampling, and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of \method~on four pre-trained models (PaLM, UL2, OPT, and Codex) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA). Our code is available at"https://github.com/Edward-Sun/RECITE".,
    publicationDate: 2022-10-04,
    authors: ['Zhiqing Sun', 'Xuezhi Wang', 'Yi Tay', 'Yiming Yang', 'Denny Zhou'],
    score: 130.64576901751826
},
{
    title: RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture,
    abstract: There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47% to 72%. Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains.,
    publicationDate: 2024-01-16,
    authors: ['M. A. D. L. Balaguer', 'Vinamra Benara', 'Renato Luiz de Freitas Cunha', 'Roberto de M. Estevao Filho', 'Todd Hendry', 'Daniel Holstein', 'Jennifer Marsman', 'Nick Mecklenburg', 'S. Malvar', 'Leonardo Nunes', 'Rafael Padilha', 'Morris Sharp', 'B. Silva', 'Swati Sharma', 'Vijay Aski', 'Ranveer Chandra'],
    score: 130.38027536102726
},
{
    title: Embedding Expansion: Augmentation in Embedding Space for Deep Metric Learning,
    abstract: Learning the distance metric between pairs of samples has been studied for image retrieval and clustering. With the remarkable success of pair-based metric learning losses, recent works have proposed the use of generated synthetic points on metric learning losses for augmentation and generalization. However, these methods require additional generative networks along with the main network, which can lead to a larger model size, slower training speed, and harder optimization. Meanwhile, post-processing techniques, such as query expansion and database augmentation, have proposed the combination of feature points to obtain additional semantic information. In this paper, inspired by query expansion and database augmentation, we propose an augmentation method in an embedding space for pair-based metric learning losses, called embedding expansion. The proposed method generates synthetic points containing augmented information by a combination of feature points and performs hard negative pair mining to learn with the most informative feature representations. Because of its simplicity and flexibility, it can be used for existing metric learning losses without affecting model size, training speed, or optimization difficulty. Finally, the combination of embedding expansion and representative metric learning losses outperforms the state-of-the-art losses and previous sample generation methods in both image retrieval and clustering tasks. The implementation is publicly available.,
    publicationDate: 2020-03-05,
    authors: ['ByungSoo Ko', 'Geonmo Gu'],
    score: 129.83476069846412
},
{
    title: Evaluating Retrieval Quality in Retrieval-Augmented Generation,
    abstract: Evaluating retrieval-augmented generation (RAG) presents challenges, particularly for retrieval models within these systems. Traditional end-to-end evaluation methods are computationally expensive. Furthermore, evaluation of the retrieval model's performance based on query-document relevance labels shows a small correlation with the RAG system's downstream performance. We propose a novel evaluation approach, eRAG, where each document in the retrieval list is individually utilized by the large language model within the RAG system. The output generated for each document is then evaluated based on the downstream task ground truth labels. In this manner, the downstream performance for each document serves as its relevance label. We employ various downstream task metrics to obtain document-level annotations and aggregate them using set-based or ranking metrics. Extensive experiments on a wide range of datasets demonstrate that eRAG achieves a higher correlation with downstream RAG performance compared to baseline methods, with improvements in Kendall's $\tau$ correlation ranging from 0.168 to 0.494. Additionally, eRAG offers significant computational advantages, improving runtime and consuming up to 50 times less GPU memory than end-to-end evaluation.,
    publicationDate: 2024-04-21,
    authors: ['Alireza Salemi', 'Hamed Zamani'],
    score: 128.89540786924243
},
{
    title: RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit,
    abstract: Although Large Language Models (LLMs) have demonstrated extraordinary capabilities in many domains, they still have a tendency to hallucinate and generate fictitious responses to user requests. This problem can be alleviated by augmenting LLMs with information retrieval (IR) systems (also known as retrieval-augmented LLMs). Applying this strategy, LLMs can generate more factual texts in response to user input according to the relevant content retrieved by IR systems from external corpora as references. In addition, by incorporating external knowledge, retrieval-augmented LLMs can answer in-domain questions that cannot be answered by solely relying on the world knowledge stored in parameters. To support research in this area and facilitate the development of retrieval-augmented LLM systems, we develop RETA-LLM, a {RET}reival-{A}ugmented LLM toolkit. In RETA-LLM, we create a complete pipeline to help researchers and users build their customized in-domain LLM-based systems. Compared with previous retrieval-augmented LLM systems, RETA-LLM provides more plug-and-play modules to support better interaction between IR systems and LLMs, including {request rewriting, document retrieval, passage extraction, answer generation, and fact checking} modules. Our toolkit is publicly available at https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.,
    publicationDate: 2023-06-08,
    authors: ['Jiongnan Liu', 'Jiajie Jin', 'Zihan Wang', 'Jiehan Cheng', 'Zhicheng Dou', 'Ji-rong Wen'],
    score: 128.6803450814222
},
{
    title: PaperQA: Retrieval-Augmented Generative Agent for Scientific Research,
    abstract: Large Language Models (LLMs) generalize well across language tasks, but suffer from hallucinations and uninterpretability, making it difficult to assess their accuracy without ground-truth. Retrieval-Augmented Generation (RAG) models have been proposed to reduce hallucinations and provide provenance for how an answer was generated. Applying such models to the scientific literature may enable large-scale, systematic processing of scientific knowledge. We present PaperQA, a RAG agent for answering questions over the scientific literature. PaperQA is an agent that performs information retrieval across full-text scientific articles, assesses the relevance of sources and passages, and uses RAG to provide answers. Viewing this agent as a question answering model, we find it exceeds performance of existing LLMs and LLM agents on current science QA benchmarks. To push the field closer to how humans perform research on scientific literature, we also introduce LitQA, a more complex benchmark that requires retrieval and synthesis of information from full-text scientific papers across the literature. Finally, we demonstrate PaperQA's matches expert human researchers on LitQA.,
    publicationDate: 2023-12-08,
    authors: ["Jakub L'ala", "Odhran O'Donoghue", 'Aleksandar Shtedritski', 'Sam Cox', 'Samuel G. Rodriques', 'Andrew D. White'],
    score: 128.3773044716594
},
{
    title: MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents,
    abstract: Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of fact-checking are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to a model to check a single response. In this work, we show how to build small fact-checking models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify datasets from recent work on fact-checking and grounding LLM generations into a new benchmark, LLM-AggreFact. Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data synthesis, and models.,
    publicationDate: 2024-04-16,
    authors: ['Liyan Tang', 'Philippe Laban', 'Greg Durrett'],
    score: 128.06801516361838
},
{
    title: Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation,
    abstract: Generating videos for visual storytelling can be a tedious and complex process that typically requires either live-action filming or graphics animation rendering. To bypass these challenges, our key idea is to utilize the abundance of existing video clips and synthesize a coherent storytelling video by customizing their appearances. We achieve this by developing a framework comprised of two functional modules: (i) Motion Structure Retrieval, which provides video candidates with desired scene or motion context described by query texts, and (ii) Structure-Guided Text-to-Video Synthesis, which generates plot-aligned videos under the guidance of motion structure and text prompts. For the first module, we leverage an off-the-shelf video retrieval system and extract video depths as motion structure. For the second module, we propose a controllable video generation model that offers flexible controls over structure and characters. The videos are synthesized by following the structural guidance and appearance instruction. To ensure visual consistency across clips, we propose an effective concept personalization approach, which allows the specification of the desired character identities through text prompts. Extensive experiments demonstrate that our approach exhibits significant advantages over various existing baselines.,
    publicationDate: 2023-07-13,
    authors: ['Yin-Yin He', 'Menghan Xia', 'Haoxin Chen', 'Xiaodong Cun', 'Yuan Gong', 'Jinbo Xing', 'Yong Zhang', 'Xintao Wang', 'Chao-Liang Weng', 'Ying Shan', 'Qifeng Chen'],
    score: 127.75221402565089
},
{
    title: PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models,
    abstract: Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate those limitations. In particular, given a question, RAG retrieves relevant knowledge from a knowledge database to augment the input of the LLM. For instance, the retrieved knowledge could be a set of top-k texts that are most semantically similar to the given question when the knowledge database contains millions of texts collected from Wikipedia. As a result, the LLM could utilize the retrieved knowledge as the context to generate an answer for the given question. Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored. We aim to bridge the gap in this work. Particularly, we propose PoisonedRAG, a set of knowledge poisoning attacks to RAG, where an attacker could inject a few poisoned texts into the knowledge database such that the LLM generates an attacker-chosen target answer for an attacker-chosen target question. We formulate knowledge poisoning attacks as an optimization problem, whose solution is a set of poisoned texts. Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on the RAG, we propose two solutions to solve the optimization problem, respectively. Our results on multiple benchmark datasets and LLMs show our attacks could achieve 90% attack success rates when injecting 5 poisoned texts for each target question into a database with millions of texts. We also evaluate recent defenses and our results show they are insufficient to defend against our attacks, highlighting the need for new defenses. 1,
    publicationDate: None,
    authors: ['Wei Zou', 'Runpeng Geng', 'Binghui Wang', 'Jinyuan Jia'],
    score: 127.42962094733642
},
{
    title: Learning to Filter Context for Retrieval-Augmented Generation,
    abstract: On-the-fly retrieval of relevant knowledge has proven an essential element of reliable systems for tasks such as open-domain question answering and fact verification. However, because retrieval systems are not perfect, generation models are required to generate outputs given partially or entirely irrelevant passages. This can cause over- or under-reliance on context, and result in problems in the generated output such as hallucinations. To alleviate these problems, we propose FILCO, a method that improves the quality of the context provided to the generator by (1) identifying useful context based on lexical and information-theoretic approaches, and (2) training context filtering models that can filter retrieved contexts at test time. We experiment on six knowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our method outperforms existing approaches on extractive question answering (QA), complex multi-hop and long-form QA, fact verification, and dialog generation tasks. FILCO effectively improves the quality of context, whether or not it supports the canonical output.,
    publicationDate: 2023-11-14,
    authors: ['Zhiruo Wang', 'Jun Araki', 'Zhengbao Jiang', 'Md. Rizwan Parvez', 'Graham Neubig'],
    score: 127.42962094733642
},
{
    title: Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks,
    abstract: Retrieval-augmented generation models have shown state-of-the-art performance across many knowledge-intensive NLP tasks such as open-domain question answering and fact verification. These models are trained to generate a final output given retrieved passages that can be irrelevant to an input query, leading to learning spurious cues or memorization. This work introduces a method to incorporate evidentiality of passages—whether a passage contains correct evidence to support the output—into training the generator. We introduce a multi-task learning framework to jointly generate the final output and predict the evidentiality of each passage. Furthermore, we introduce a new task-agnostic method for obtaining high-quality silver evidentiality labels, addressing the issues of gold evidentiality labels being unavailable in most domains. Our experiments on five datasets across three knowledge-intensive tasks show that our new evidentiality-guided generator significantly outperforms its direct counterpart on all of them, and advances the state of the art on three of them. Our analysis shows that multi-task learning and silver evidentiality mining play key roles. Our code is available at https://github.com/AkariAsai/evidentiality_qa,
    publicationDate: 2021-12-16,
    authors: ['Akari Asai', 'Matt Gardner', 'Hannaneh Hajishirzi'],
    score: 127.42962094733642
},
{
    title: Region-Aware Face Swapping,
    abstract: This paper presents a novel Region-Aware Face Swapping (RAFSwap) network to achieve identity-consistent harmonious high-resolution face generation in a local-global manner: 1) Local Facial Region-Aware (FRA) branch augments local identity-relevant features by introducing the Transformer to effectively model misaligned crossscale semantic interaction. 2) Global Source Feature-Adaptive (SFA) branch further complements global identity-relevant cues for generating identity-consistent swapped faces. Besides, we propose a Face Mask Predictor (FMP) module incorporated with StyleGAN2 to predict identity-relevant soft facial masks in an unsupervised manner that is more practical for generating harmonious high-resolution faces. Abundant experiments qualitatively and quantitatively demonstrate the superiority of our method for generating more identity-consistent high-resolution swapped faces over SOTA methods, e.g., obtaining 96.70 ID retrieval that outperforms SOTA MegaFS by $5.87\uparrow$.,
    publicationDate: 2022-03-09,
    authors: ['Chao Xu', 'Jiangning Zhang', 'Miao Hua', 'Qian He', 'Zili Yi', 'Yong Liu'],
    score: 127.42962094733642
},
{
    title: Corrective Retrieval Augmented Generation,
    abstract: Large language models (LLMs) inevitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retrieval Augmented Generation (CRAG) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval actions can be triggered. Since retrieval from static and limited corpora can only return sub-optimal documents, large-scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose-then-recompose algorithm is designed for retrieved documents to selectively focus on key information and filter out irrelevant information in them. CRAG is plug-and-play and can be seamlessly coupled with various RAG-based approaches. Experiments on four datasets covering short- and long-form generation tasks show that CRAG can significantly improve the performance of RAG-based approaches.,
    publicationDate: 2024-01-29,
    authors: ['Shi-Qi Yan', 'Jia-Chen Gu', 'Yun Zhu', 'Zhen-Hua Ling'],
    score: 127.0999373465548
},
{
    title: Retrieval-augmented Recommender System: Enhancing Recommender Systems with Large Language Models,
    abstract: Recommender Systems (RSs) play a pivotal role in delivering personalized recommendations across various domains, from e-commerce to content streaming platforms. Recent advancements in natural language processing have introduced Large Language Models (LLMs) that exhibit remarkable capabilities in understanding and generating human-like text. RS are renowned for their effectiveness and proficiency within clearly defined domains; nevertheless, they are limited in adaptability and incapable of providing recommendations for unexplored data. Conversely, LLMs exhibit contextual awareness and strong adaptability to unseen data. Combining these technologies creates a powerful tool for delivering contextual and relevant recommendations, even in cold scenarios characterized by high data sparsity. The proposal aims to explore the possibilities of integrating LLMs into RS, introducing a novel approach called Retrieval-augmented Recommender Systems, which combines the strengths of retrieval-based and generation-based models to enhance the ability of RSs to provide relevant suggestions.,
    publicationDate: 2023-09-14,
    authors: ['Dario Di Palma'],
    score: 127.01796072493232
},
{
    title: Efficient Retrieval Augmented Generation from Unstructured Knowledge for Task-Oriented Dialog,
    abstract: This paper summarizes our work on the first track of the ninth Dialog System Technology Challenge (DSTC 9),"Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access". The goal of the task is to generate responses to user turns in a task-oriented dialog that require knowledge from unstructured documents. The task is divided into three subtasks: detection, selection and generation. In order to be compute efficient, we formulate the selection problem in terms of hierarchical classification steps. We achieve our best results with this model. Alternatively, we employ siamese sequence embedding models, referred to as Dense Knowledge Retrieval, to retrieve relevant documents. This method further reduces the computation time by a factor of more than 100x at the cost of degradation in R@1 of 5-6% compared to the first model. Then for either approach, we use Retrieval Augmented Generation to generate responses based on multiple selected snippets and we show how the method can be used to fine-tune trained embeddings.,
    publicationDate: 2021-02-09,
    authors: ['David Thulke', 'Nico Daheim', 'Christian Dugast', 'H. Ney'],
    score: 126.76284450877392
},
{
    title: RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models,
    abstract: Retrieval-augmented generation (RAG) has become a main technique for alleviating hallucinations in large language models (LLMs). Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents. In order to develop effective hallucination prevention strategies under RAG, it is important to create benchmark datasets that can measure the extent of hallucination. This paper presents RAGTruth, a corpus tailored for analyzing word-level hallucinations in various domains and tasks within the standard RAG frameworks for LLM applications. RAGTruth comprises nearly 18,000 naturally generated responses from diverse LLMs using RAG. These responses have undergone meticulous manual annotations at both the individual cases and word levels, incorporating evaluations of hallucination intensity. We not only benchmark hallucination frequencies across different LLMs, but also critically assess the effectiveness of several existing hallucination detection methodologies. Furthermore, we show that using a high-quality dataset such as RAGTruth, it is possible to finetune a relatively small LLM and achieve a competitive level of performance in hallucination detection when compared to the existing prompt-based approaches using state-of-the-art large language models such as GPT-4.,
    publicationDate: 2023-12-31,
    authors: ['Yuanhao Wu', 'Juno Zhu', 'Siliang Xu', 'Kashun Shum', 'Cheng Niu', 'Randy Zhong', 'Juntong Song', 'Tong Zhang'],
    score: 126.76284450877392
},
{
    title: SGLang: Efficient Execution of Structured Language Model Programs,
    abstract: Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs. However, efficient systems are lacking for programming and executing these applications. We introduce SGLang, a system for efficient execution of complex language model programs. SGLang consists of a frontend language and a runtime. The frontend simplifies programming with primitives for generation and parallelism control. The runtime accelerates execution with novel optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding. Experiments show that SGLang achieves up to 6.4x higher throughput compared to state-of-the-art inference systems on various large language and multi-modal models on tasks including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, and multi-turn chat. The code is publicly available at https://github.com/sgl-project/sglang,
    publicationDate: 2023-12-12,
    authors: ['Lianmin Zheng', 'Liangsheng Yin', 'Zhiqiang Xie', 'Chuyue Sun', 'Jeff Huang', 'Cody Hao Yu', 'Shiyi Cao', 'Christos Kozyrakis', 'Ion Stoica', 'Joseph E. Gonzalez', 'Clark W. Barrett', 'Ying Sheng'],
    score: 126.76284450877392
},
{
    title: Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools,
    abstract: Legal practice has witnessed a sharp rise in products incorporating artificial intelligence (AI). Such tools are designed to assist with a wide range of core legal tasks, from search and summarization of caselaw to document drafting. But the large language models used in these tools are prone to"hallucinate,"or make up false information, making their use risky in high-stakes domains. Recently, certain legal research providers have touted methods such as retrieval-augmented generation (RAG) as"eliminating"(Casetext, 2023) or"avoid[ing]"hallucinations (Thomson Reuters, 2023), or guaranteeing"hallucination-free"legal citations (LexisNexis, 2023). Because of the closed nature of these systems, systematically assessing these claims is challenging. In this article, we design and report on the first preregistered empirical evaluation of AI-driven legal research tools. We demonstrate that the providers' claims are overstated. While hallucinations are reduced relative to general-purpose chatbots (GPT-4), we find that the AI research tools made by LexisNexis (Lexis+ AI) and Thomson Reuters (Westlaw AI-Assisted Research and Ask Practical Law AI) each hallucinate between 17% and 33% of the time. We also document substantial differences between systems in responsiveness and accuracy. Our article makes four key contributions. It is the first to assess and report the performance of RAG-based proprietary legal AI tools. Second, it introduces a comprehensive, preregistered dataset for identifying and understanding vulnerabilities in these systems. Third, it proposes a clear typology for differentiating between hallucinations and accurate legal responses. Last, it provides evidence to inform the responsibilities of legal professionals in supervising and verifying AI outputs, which remains a central open question for the responsible integration of AI into law.,
    publicationDate: 2024-05-30,
    authors: ['Varun Magesh', 'Faiz Surani', 'Matthew Dahl', 'Mirac Suzgun', 'Christopher D. Manning', 'Daniel E. Ho'],
    score: 126.41800173540344
},
{
    title: The Multimedia Satellite Task at MediaEval 2017,
    abstract: This paper provides a description of the MediaEval 2017 Multimedia Satellite Task. The primary goal of the task is to extract and fuse content of events which are present in Satellite Imagery and Social Media. Establishing a link from Satellite Imagery to Social Multimedia can yield to a comprehensive event representation which is vital for numerous applications. Focusing on natural disaster events in this year, the main objective of the task is to leverage the combined event representation withing the context of emergency response and environmental monitoring. In particular, our task focuses this year on flooding events and consists of two subtasks. The first Disaster Image Retrieval form Social Media subtask requires participants to retrieve images from Social Media which show a direct evidence of the flooding event. The second task Flood Detection in Satellite Images aims to extract regions in satellite images which are affected by a flooding event. Extracted content from both tasks can be fused by means of the geographic information. The task seeks to go beyond state-of-the-art flooding map generation towards recent approaches in Deep-Learning while augmenting the satellite information at the same time with rich social multimedia.,
    publicationDate: None,
    authors: ['B. Bischke', 'P. Helber', 'Christian Schulze', 'Venkat Srinivasan', 'A. Dengel', 'Damian Borth'],
    score: 126.1630616585858
},
{
    title: RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair,
    abstract: Automatic program repair (APR) is crucial to reduce manual debugging efforts for developers and improve software reliability. While conventional search-based techniques typically rely on heuristic rules or a redundancy assumption to mine fix patterns, recent years have witnessed the surge of deep learning (DL) based approaches to automate the program repair process in a data-driven manner. However, their performance is often limited by a fixed set of parameters to model the highly complex search space of APR. To ease such burden on the parametric models, in this work, we propose a novel Retrieval-Augmented Patch Generation framework (RAP-Gen) by explicitly leveraging relevant fix patterns retrieved from a codebase of previous bug-fix pairs. Specifically, we build a hybrid patch retriever to account for both lexical and semantic matching based on the raw source code in a language-agnostic manner, which does not rely on any code-specific features. In addition, we adapt a code-aware language model CodeT5 as our foundation model to facilitate both patch retrieval and generation tasks in a unified manner. We adopt a stage-wise approach where the patch retriever first retrieves a relevant external bug-fix pair to augment the buggy input for the CodeT5 patch generator, which synthesizes a ranked list of repair patch candidates. Notably, RAP-Gen is a generic APR framework that can flexibly integrate different patch retrievers and generators to repair various types of bugs. We thoroughly evaluate RAP-Gen on three benchmarks in two programming languages, including the TFix benchmark in JavaScript, and Code Refinement and Defects4J benchmarks in Java, where the bug localization information may or may not be provided. Experimental results show that RAP-Gen significantly outperforms previous state-of-the-art (SoTA) approaches on all benchmarks, e.g., boosting the accuracy of T5-large on TFix from 49.70% to 54.15% (repairing 478 more bugs) and repairing 15 more bugs on 818 Defects4J bugs. Further analysis reveals that our patch retriever can search for relevant fix patterns to guide the APR systems.,
    publicationDate: 2023-09-12,
    authors: ['Weishi Wang', 'Yue Wang', 'Shafiq R. Joty', 'Steven C. H. Hoi'],
    score: 126.06504427425052
},
{
    title: Efficiency Implications of Term Weighting for Passage Retrieval,
    abstract: Language model pre-training has spurred a great deal of attention for tasks involving natural language understanding, and has been successfully applied to many downstream tasks with impressive results. Within information retrieval, many of these solutions are too costly to stand on their own, requiring multi-stage ranking architectures. Recent work has begun to consider how to "backport" salient aspects of these computationally expensive models to previous stages of the retrieval pipeline. One such instance is DeepCT, which uses BERT to re-weight term importance in a given context at the passage level. This process, which is computed offline, results in an augmented inverted index with re-weighted term frequency values. In this work, we conduct an investigation of query processing efficiency over DeepCT indexes. Using a number of candidate generation algorithms, we reveal how term re-weighting can impact query processing latency, and explore how DeepCT can be used as a static index pruning technique to accelerate query processing without harming search effectiveness.,
    publicationDate: 2020-07-25,
    authors: ['J. Mackenzie', 'Zhuyun Dai', 'L. Gallagher', 'Jamie Callan'],
    score: 125.98306765262805
},
{
    title: Retrieval-Free Knowledge-Grounded Dialogue Response Generation with Adapters,
    abstract: To diversify and enrich generated dialogue responses, knowledge-grounded dialogue has been investigated in recent years. The existing methods tackle the knowledge grounding challenge by retrieving the relevant sentences over a large corpus and augmenting the dialogues with explicit extra information. Despite their success, however, the existing works have drawbacks on the inference efficiency. This paper proposes KnowExpert, an end-to-end framework to bypass the explicit retrieval process and inject knowledge into the pre-trained language models with lightweight adapters and adapt to the knowledge-grounded dialogue task. To the best of our knowledge, this is the first attempt to tackle this challenge without retrieval in this task under an open-domain chit-chat scenario. The experimental results show that KnowExpert performs comparably with some retrieval-based baselines while being time-efficient in inference, demonstrating the effectiveness of our proposed method.,
    publicationDate: 2021-05-13,
    authors: ['Yan Xu', 'Etsuko Ishii', 'Zihan Liu', 'Genta Indra Winata', 'Dan Su', 'Andrea Madotto', 'Pascale Fung'],
    score: 125.70358100056461
},
{
    title: MechGPT, a language-based strategy for mechanics and materials modeling that connects knowledge across scales, disciplines and modalities,
    abstract: 
 For centuries, researchers have sought out ways to connect disparate areas of knowledge. While early scholars (Galileo, da Vinci, etc.) were experts across fields, specialization has taken hold later. With the advent of Artificial Intelligence, we can now explore relationships across areas (e.g., mechanics-biology) or disparate domains (e.g., failure mechanics-art). To achieve this, we use a fine-tuned Large Language Model (LLM), here for a subset of knowledge in multiscale materials failure. The approach includes the use of a general-purpose LLM to distill question-answer pairs from raw sources followed by LLM fine-tuning. The resulting MechGPT LLM foundation model is used in a series of computational experiments to explore its capacity for knowledge retrieval, various language tasks, hypothesis generation, and connecting knowledge across disparate areas. While the model has some ability to recall knowledge from training, we find that LLMs are particularly useful to extract structural insights through Ontological Knowledge Graphs. These interpretable graph structures provide explanatory insights, frameworks for new research questions, and visual representations of knowledge that also can be used in retrieval-augmented generation. Three versions of MechGPT are discussed, featuring different sizes from 13 billion to 70 billion parameters, and reaching context lengths of more than 10,000 tokens. This provides ample capacity for sophisticated retrieval augmented strategies, as well as agent-based modeling where multiple LLMs interact collaboratively and/or adversarially, the incorporation of new data from the literature or web searches, as well as multimodality.,
    publicationDate: 2023-10-16,
    authors: ['M. Buehler'],
    score: 125.70358100056461
},
{
    title: MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries,
    abstract: Retrieval-augmented generation (RAG) augments large language models (LLM) by retrieving relevant knowledge, showing promising potential in mitigating LLM hallucinations and enhancing response quality, thereby facilitating the great adoption of LLMs in practice. However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries. In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utilizing an English news article dataset as the underlying RAG knowledge base. We demonstrate the benchmarking utility of MultiHop-RAG in two experiments. The first experiment compares different embedding models for retrieving evidence for multi-hop queries. In the second experiment, we examine the capabilities of various state-of-the-art LLMs, including GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop queries given the evidence. Both experiments reveal that existing RAG methods perform unsatisfactorily in retrieving and answering multi-hop queries. We hope MultiHop-RAG will be a valuable resource for the community in developing effective RAG systems, thereby facilitating greater adoption of LLMs in practice. The MultiHop-RAG and implemented RAG system is publicly available at https://github.com/yixuantt/MultiHop-RAG/.,
    publicationDate: 2024-01-27,
    authors: ['Yixuan Tang', 'Yi Yang'],
    score: 124.95342469194469
},
{
    title: Almanac - Retrieval-Augmented Language Models for Clinical Medicine.,
    abstract: BACKGROUND
Large language models (LLMs) have recently shown impressive zero-shot capabilities, whereby they can use auxiliary data, without the availability of task-specific training examples, to complete a variety of natural language tasks, such as summarization, dialogue generation, and question answering. However, despite many promising applications of LLMs in clinical medicine, adoption of these models has been limited by their tendency to generate incorrect and sometimes even harmful statements.


METHODS
We tasked a panel of eight board-certified clinicians and two health care practitioners with evaluating Almanac, an LLM framework augmented with retrieval capabilities from curated medical resources for medical guideline and treatment recommendations. The panel compared responses from Almanac and standard LLMs (ChatGPT-4, Bing, and Bard) versus a novel data set of 314 clinical questions spanning nine medical specialties.


RESULTS
Almanac showed a significant improvement in performance compared with the standard LLMs across axes of factuality, completeness, user preference, and adversarial safety.


CONCLUSIONS
Our results show the potential for LLMs with access to domain-specific corpora to be effective in clinical decision-making. The findings also underscore the importance of carefully testing LLMs before deployment to mitigate their shortcomings. (Funded by the National Institutes of Health, National Heart, Lung, and Blood Institute.).,
    publicationDate: 2024-01-25,
    authors: ['Cyril Zakka', 'R. Shad', 'Akash Chaurasia', 'Alex R. Dalal', 'Jennifer L. Kim', 'M. Moor', 'R. Fong', 'Curran Phillips', 'Kevin Alexander', 'Euan A. Ashley', 'Jack Boyd', 'Kathleen Boyd', 'Karen Hirsch', 'Curtis P. Langlotz', 'Rita Lee', 'Joanna Melia', 'Joanna Nelson', 'Karim Sallam', 'Stacey Tullis', 'M. Vogelsong', 'J. Cunningham', 'W. Hiesinger'],
    score: 124.95342469194469
},
{
    title: Large Scale Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training,
    abstract: Generating natural sentences from Knowledge Graph (KG) triples, known as Data-To-Text Generation, is a task with many datasets for which numerous complex systems have been developed. However, no prior work has attempted to perform this generation at scale by converting an entire KG into natural text. In this paper, we verbalize the entire Wikidata KG, and create a KG-Text aligned corpus in the training process. We discuss the challenges in verbalizing an entire KG versus verbalizing smaller datasets. We further show that verbalizing an entire KG can be used to integrate structured and natural language data. In contrast to the many architectures that have been developed to integrate the structural differences between these two sources, our approach converts the KG into the same format as natural text allowing it to be seamlessly plugged into existing natural language systems. We evaluate this approach by augmenting the retrieval corpus in REALM and showing improvements, both on the LAMA knowledge probe and open domain QA.,
    publicationDate: 2020-10-23,
    authors: ['Oshin Agarwal', 'Heming Ge', 'Siamak Shakeri', 'Rami Al-Rfou'],
    score: 124.95342469194469
},
{
    title: Reason first, then respond: Modular Generation for Knowledge-infused Dialogue,
    abstract: Large language models can produce fluent dialogue but often hallucinate factual inaccuracies. While retrieval-augmented models help alleviate this issue, they still face a difficult challenge of both reasoning to provide correct knowledge and generating conversation simultaneously. In this work, we propose a modular model, Knowledge to Response (K2R), for incorporating knowledge into conversational agents, which breaks down this problem into two easier steps. K2R first generates a knowledge sequence, given a dialogue context, as an intermediate step. After this"reasoning step", the model then attends to its own generated knowledge sequence, as well as the dialogue context, to produce a final response. In detailed experiments, we find that such a model hallucinates less in knowledge-grounded dialogue tasks, and has advantages in terms of interpretability and modularity. In particular, it can be used to fuse QA and dialogue systems together to enable dialogue agents to give knowledgeable answers, or QA models to give conversational responses in a zero-shot setting.,
    publicationDate: 2021-11-09,
    authors: ['Leonard Adolphs', 'Kurt Shuster', 'Jack Urbanek', 'Arthur Szlam', 'J. Weston'],
    score: 124.56379239589579
},
{
    title: Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering,
    abstract: In customer service technical support, swiftly and accurately retrieving relevant past issues is critical for efficiently resolving customer inquiries. The conventional retrieval methods in retrieval-augmented generation (RAG) for large language models (LLMs) treat a large corpus of past issue tracking tickets as plain text, ignoring the crucial intra-issue structure and inter-issue relations, which limits performance. We introduce a novel customer service question-answering method that amalgamates RAG with a knowledge graph (KG). Our method constructs a KG from historical issues for use in retrieval, retaining the intra-issue structure and inter-issue relations. During the question-answering phase, our method parses consumer queries and retrieves related sub-graphs from the KG to generate answers. This integration of a KG not only improves retrieval accuracy by preserving customer service structure information but also enhances answering quality by mitigating the effects of text segmentation. Empirical assessments on our benchmark datasets, utilizing key retrieval (MRR, Recall@K, NDCG@K) and text generation (BLEU, ROUGE, METEOR) metrics, reveal that our method outperforms the baseline by 77.6% in MRR and by 0.32 in BLEU. Our method has been deployed within LinkedIn's customer service team for approximately six months and has reduced the median per-issue resolution time by 28.6%.,
    publicationDate: 2024-04-26,
    authors: ['Zhentao Xu', 'Mark Jerome Cruz', 'Matthew Guevara', 'Tie Wang', 'Manasi Deshpande', 'Xiaofeng Wang', 'Zheng Li'],
    score: 124.28313737302301
},
{
    title: A Large-Scale Comparative Evaluation of IR-Based Tools for Bug Localization,
    abstract: This paper reports on a large-scale comparative evaluation of IR-based tools for automatic bug localization. We have divided the tools in our evaluation into the following three generations: (1) The firstgeneration tools, now over a decade old, that are based purely on the Bag-of-Words (BoW) modeling of software libraries. (2) The somewhat more recent second-generation tools that augment BoW-based modeling with two additional pieces of information: historical data, such as change history, and structured information such as class names, method names, etc. And, finally, (3) The third-generation tools that are currently the focus of much research and that also exploit proximity, order, and semantic relationships between the terms. It is important to realize that the original authors of all these three generations of tools have mostly tested them on relatively small-sized datasets that typically consisted no more than a few thousand bug reports. Additionally, those evaluations only involved Java code libraries. The goal of the present paper is to present a comprehensive large-scale evaluation of all three generations of bug-localization tools with code libraries in multiple languages. Our study involves over 20,000 bug reports drawn from a diverse collection of Java, C/C++, and Python projects. Our results show that the third-generation tools are significantly superior to the older tools. We also show that the word embeddings generated using code files written in one language are effective for retrieval from code libraries in other languages.,
    publicationDate: 2020-05-01,
    authors: ['Shayan A. Akbar', 'A. Kak'],
    score: 124.28313737302301
},
{
    title: G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering,
    abstract: Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever method, introducing the first retrieval-augmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination.~\footnote{Our codes and datasets are available at: \url{https://github.com/XiaoxinHe/G-Retriever}},
    publicationDate: 2024-02-12,
    authors: ['Xiaoxin He', 'Yijun Tian', 'Yifei Sun', 'N. Chawla', 'T. Laurent', 'Yann LeCun', 'Xavier Bresson', 'Bryan Hooi'],
    score: 124.16376868966336
},
{
    title: Evaluation of Retrieval-Augmented Generation: A Survey,
    abstract: Retrieval-Augmented Generation (RAG) has recently gained traction in natural language processing. Numerous studies and real-world applications are leveraging its ability to enhance generative models through external information retrieval. Evaluating these RAG systems, however, poses unique challenges due to their hybrid structure and reliance on dynamic knowledge sources. To better understand these challenges, we conduct A Unified Evaluation Process of RAG (Auepora) and aim to provide a comprehensive overview of the evaluation and benchmarks of RAG systems. Specifically, we examine and compare several quantifiable metrics of the Retrieval and Generation components, such as relevance, accuracy, and faithfulness, within the current RAG benchmarks, encompassing the possible output and ground truth pairs. We then analyze the various datasets and metrics, discuss the limitations of current benchmarks, and suggest potential directions to advance the field of RAG benchmarks.,
    publicationDate: 2024-05-13,
    authors: ['Hao Yu', 'Aoran Gan', 'Kai Zhang', 'Shiwei Tong', 'Qi Liu', 'Zhaofeng Liu'],
    score: 124.16376868966336
},
{
    title: Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning,
    abstract: Paraphrase generation is a fundamental and long-standing task in natural language processing. In this paper, we concentrate on two contributions to the task: (1) we propose Retrieval Augmented Prompt Tuning (RAPT) as a parameter-efficient method to adapt large pre-trained language models for paraphrase generation; (2) we propose Novelty Conditioned RAPT (NC-RAPT) as a simple model-agnostic method of using specialized prompt tokens for controlled paraphrase generation with varying levels of lexical novelty. By conducting extensive experiments on four datasets, we demonstrate the effectiveness of the proposed approaches for retaining the semantic content of the original text while inducing lexical novelty in the generation.,
    publicationDate: 2022-02-01,
    authors: ['Jishnu Ray Chowdhury', 'Yong Zhuang', 'Shuyi Wang'],
    score: 124.16376868966336
},
{
    title: Evaluating Very Long-Term Conversational Memory of LLM Agents,
    abstract: Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.,
    publicationDate: 2024-02-27,
    authors: ['A. Maharana', 'Dong-Ho Lee', 'S. Tulyakov', 'Mohit Bansal', 'Francesco Barbieri', 'Yuwei Fang'],
    score: 124.16376868966336
},
{
    title: RTLFixer: Automatically Fixing RTL Syntax Errors with Large Language Models,
    abstract: This paper presents RTLFixer, a novel framework enabling automatic syntax errors fixing for Verilog code with Large Language Models (LLMs). Despite LLM's promising capabilities, our analysis indicates that approximately 55% of errors in LLM-generated Verilog are syntax-related, leading to compilation failures. To tackle this issue, we introduce a novel debugging framework that employs Retrieval-Augmented Generation (RAG) and ReAct prompting, enabling LLMs to act as autonomous agents in interactively debugging the code with feedback. This framework demonstrates exceptional proficiency in resolving syntax errors, successfully correcting about 98.5% of compilation errors in our debugging dataset, comprising 212 erroneous implementations derived from the VerilogEval benchmark. Our method leads to 32.3% and 10.1% increase in pass@1 success rates in the VerilogEval-Machine and VerilogEval-Human benchmarks, respectively.,
    publicationDate: 2023-11-28,
    authors: ['Yun-Da Tsai', 'Mingjie Liu', 'Haoxing Ren'],
    score: 124.16376868966336
},
{
    title: Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning,
    abstract: Augmenting pretrained language models (LMs) with a vision encoder (e.g., Flamingo) has obtained the state-of-the-art results in image-to-text generation. However, these models store all the knowledge within their parameters, thus often requiring enormous model parameters to model the abundant visual concepts and very rich textual descriptions. Additionally, they are inefficient in incorporating new data, requiring a computational-expensive fine-tuning process. In this work, we introduce a Retrieval-augmented Visual Language Model, Re-ViLM, built upon the Flamingo, that supports retrieving the relevant knowledge from the external database for zero and in-context few-shot image-to-text generations. By storing certain knowledge explicitly in the external database, our approach reduces the number of model parameters and can easily accommodate new data during evaluation by simply updating the database. We also construct an interleaved image and text data that facilitates in-context few-shot learning capabilities. We demonstrate that Re-ViLM significantly boosts performance for image-to-text generation tasks, especially for zero-shot and few-shot generation in out-of-domain settings with 4 times less parameters compared with baseline methods.,
    publicationDate: 2023-02-09,
    authors: ['Zhuolin Yang', 'Wei Ping', 'Zihan Liu', 'V. Korthikanti', 'Weili Nie', 'De-An Huang', 'Linxi (Jim) Fan', 'Zhiding Yu', 'Shiyi Lan', 'Bo Li', 'Mingyan Liu', 'Yuke Zhu', 'Mohammad Shoeybi', 'Bryan Catanzaro', 'Chaowei Xiao', 'Anima Anandkumar'],
    score: 123.75278407684165
},
{
    title: Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking,
    abstract: Standard Full-Data classifiers in NLP demand thousands of labeled examples, which is impractical in data-limited domains. Few-shot methods offer an alternative, utilizing contrastive learning techniques that can be effective with as little as 20 examples per class. Similarly, Large Language Models (LLMs) like GPT-4 can perform effectively with just 1-5 examples per class. However, the performance-cost trade-offs of these methods remain underexplored, a critical concern for budget-limited organizations. Our work addresses this gap by studying the aforementioned approaches over the Banking77 financial intent detection dataset, including the evaluation of cutting-edge LLMs by OpenAI, Cohere, and Anthropic in a comprehensive set of few-shot scenarios. We complete the picture with two additional methods: first, a cost-effective querying method for LLMs based on retrieval-augmented generation (RAG), able to reduce operational costs multiple times compared to classic few-shot approaches, and second, a data augmentation method using GPT-4, able to improve performance in data-limited scenarios. Finally, to inspire future research, we provide a human expert’s curated subset of Banking77, along with extensive error analysis.,
    publicationDate: 2023-11-10,
    authors: ['L. Loukas', 'Ilias Stogiannidis', 'Odysseas Diamantopoulos', 'Prodromos Malakasiotis', 'Stavros Vassos'],
    score: 123.75278407684165
},
{
    title: Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs,
    abstract: Multimodal LLMs are the natural evolution of LLMs, and enlarge their capabilities so as to work beyond the pure textual modality. As research is being carried out to design novel architectures and vision-and-language adapters, in this paper we concentrate on endowing such models with the capability of answering questions that require external knowledge. Our approach, termed Wiki-LLaVA, aims at integrating an external knowledge source of multimodal documents, which is accessed through a hierarchical retrieval pipeline. Relevant passages, using this approach, are retrieved from the external knowledge source and employed as additional context for the LLM, augmenting the effectiveness and precision of generated dialogues. We conduct extensive experiments on datasets tailored for visual question answering with external data and demonstrate the appropriateness of our approach.,
    publicationDate: 2024-04-23,
    authors: ['Davide Caffagni', 'Federico Cocchi', 'Nicholas Moratelli', 'Sara Sarto', 'Marcella Cornia', 'L. Baraldi', 'R. Cucchiara'],
    score: 123.67080745521918
},
{
    title: Retrieval-Generation Synergy Augmented Large Language Models,
    abstract: Large language models augmented with task-relevant documents have demonstrated impressive performance on knowledge-intensive tasks. However, regarding how to obtain effective documents, the existing methods are mainly divided into two categories. One is to retrieve from an external knowledge base, and the other is to utilize large language models to generate documents. We propose an iterative retrieval-generation collaborative framework. It is not only able to leverage both parametric and non-parametric knowledge, but also helps to find the correct reasoning path through retrieval-generation interactions, which is very important for tasks that require multi-step reasoning. We conduct experiments on four question answering datasets, including single-hop QA and multi-hop QA tasks. Empirical results show that our method significantly improves the reasoning ability of large language models and outperforms previous baselines.,
    publicationDate: 2023-10-08,
    authors: ['Zhangyin Feng', 'Xiaocheng Feng', 'Dezhi Zhao', 'Maojin Yang', 'Bing Qin'],
    score: 123.67080745521918
},
{
    title: RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation,
    abstract: Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation (RAG) addresses this by incorporating external, relevant documents into the response generation process, thus leveraging non-parametric knowledge alongside LLMs' in-context learning abilities. However, existing RAG implementations primarily focus on initial input for context retrieval, overlooking the nuances of ambiguous or complex queries that necessitate further clarification or decomposition for accurate responses. To this end, we propose learning to Refine Query for Retrieval Augmented Generation (RQ-RAG) in this paper, endeavoring to enhance the model by equipping it with capabilities for explicit rewriting, decomposition, and disambiguation. Our experimental results indicate that our method, when applied to a 7B Llama2 model, surpasses the previous state-of-the-art (SOTA) by an average of 1.9\% across three single-hop QA datasets, and also demonstrates enhanced performance in handling complex, multi-hop QA datasets. Our code is available at https://github.com/chanchimin/RQ-RAG.,
    publicationDate: 2024-03-31,
    authors: ['Chi-Min Chan', 'Chunpu Xu', 'Ruibin Yuan', 'Hongyin Luo', 'Wei Xue', 'Yi-Ting Guo', 'Jie Fu'],
    score: 123.3302209223412
},
{
    title: FoleyGAN: Visually Guided Generative Adversarial Network-Based Synchronous Sound Generation in Silent Videos,
    abstract: Deep learning based visual-to-sound generation systems have been developed that identify and create audio features from video signals. However, these techniques often fail to consider the time-synchronicity of the visual and audio features. In this paper we introduce a novel method for guiding a class-conditioned GAN to synthesize representative audio with temporally-extracted visual information. We accomplish this visual-to-sound generation task by adapting the synchronicity traits between the audio-visual modalities. Our proposed FoleyGAN model is capable of conditioning action sequences of visual events leading to the generation of visually aligned realistic soundtracks. We expanded our previously proposed Automatic Foley data set. We evaluated FoleyGAN’s synthesized sound output through human surveys that show noteworthy (on average 81%) audio-visual synchronicity performance. Our approach outperforms other baseline models and audio-visual data sets in statistical and ablation experiments achieving improved IS, FID and NDB scores. In ablation analysis we showed the significance of our visual and temporal feature extraction method as well as augmented performance of our generation network. Overall, our FoleyGAN model showed sound retrieval accuracy of 76.08% surpassing existing visual-to-audio synthesis deep neural networks.,
    publicationDate: 2021-07-20,
    authors: ['Sanchita Ghose', 'John J. Prevost'],
    score: 123.03241323893724
},
{
    title: Robust Retrieval Augmented Generation for Zero-shot Slot Filling,
    abstract: Automatically inducing high quality knowledge graphs from a given collection of documents still remains a challenging problem in AI. One way to make headway for this problem is through advancements in a related task known as slot filling. In this task, given an entity query in form of [Entity, Slot, ?], a system is asked to ‘fill’ the slot by generating or extracting the missing value exploiting evidence extracted from relevant passage(s) in the given document collection. The recent works in the field try to solve this task in an end-to-end fashion using retrieval-based language models. In this paper, we present a novel approach to zero-shot slot filling that extends dense passage retrieval with hard negatives and robust training procedures for retrieval augmented generation models. Our model reports large improvements on both T-REx and zsRE slot filling datasets, improving both passage retrieval and slot value generation, and ranking at the top-1 position in the KILT leaderboard. Moreover, we demonstrate the robustness of our system showing its domain adaptation capability on a new variant of the TACRED dataset for slot filling, through a combination of zero/few-shot learning. We release the source code and pre-trained models.,
    publicationDate: 2021-08-31,
    authors: ['Michael R. Glass', 'Gaetano Rossiello', 'Md. Faisal Mahbub Chowdhury', 'A. Gliozzo'],
    score: 122.89540786924243
},
{
    title: Huatuo-26M, a Large-scale Chinese Medical QA Dataset,
    abstract: In this paper, we release a largest ever medical Question Answering (QA) dataset with 26 million QA pairs. We benchmark many existing approaches in our dataset in terms of both retrieval and generation. Experimental results show that the existing models perform far lower than expected and the released dataset is still challenging in the pre-trained language model era. Moreover, we also experimentally show the benefit of the proposed dataset in many aspects: (i) trained models for other QA datasets in a zero-shot fashion; and (ii) as external knowledge for retrieval-augmented generation (RAG); and (iii) improving existing pre-trained language models by using the QA pairs as a pre-training corpus in continued training manner. We believe that this dataset will not only contribute to medical research but also facilitate both the patients and clinical doctors. See \url{https://github.com/FreedomIntelligence/Huatuo-26M}.,
    publicationDate: 2023-05-02,
    authors: ['Jianquan Li', 'Xidong Wang', 'Xiangbo Wu', 'Zhiyi Zhang', 'Xiaolong Xu', 'Jie Fu', 'P. Tiwari', 'Xiang Wan', 'Benyou Wang'],
    score: 122.89540786924243
},
{
    title: Probabilistic Representations for Video Contrastive Learning,
    abstract: This paper presents Probabilistic Video Contrastive Learning, a self-supervised representation learning method that bridges contrastive learning with probabilistic representation. We hypothesize that the clips composing the video have different distributions in short-term duration, but can represent the complicated and sophisticated video distribution through combination in a common embedding space. Thus, the proposed method represents video clips as normal distributions and combines them into a Mixture of Gaussians to model the whole video distribution. By sampling embeddings from the whole video distribution, we can circumvent the careful sampling strategy or transformations to generate augmented views of the clips, unlike previous deterministic methods that have mainly focused on such sample generation strategies for contrastive learning. We further propose a stochastic contrastive loss to learn proper video distributions and handle the inherent uncertainty from the nature of the raw video. Experimental results verify that our probabilistic embedding stands as a state-of-the-art video representation learning for action recognition and video retrieval on the most popular benchmarks, including UCF101 and HMDB51.,
    publicationDate: 2022-04-08,
    authors: ['Jungin Park', 'Jiyoung Lee', 'Ig-Jae Kim', 'K. Sohn'],
    score: 122.89540786924243
},
{
    title: The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG),
    abstract: Retrieval-augmented generation (RAG) is a powerful technique to facilitate language model with proprietary and private data, where data privacy is a pivotal concern. Whereas extensive research has demonstrated the privacy risks of large language models (LLMs), the RAG technique could potentially reshape the inherent behaviors of LLM generation, posing new privacy issues that are currently under-explored. In this work, we conduct extensive empirical studies with novel attack methods, which demonstrate the vulnerability of RAG systems on leaking the private retrieval database. Despite the new risk brought by RAG on the retrieval data, we further reveal that RAG can mitigate the leakage of the LLMs' training data. Overall, we provide new insights in this paper for privacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAG systems builders. Our code is available at https://github.com/phycholosogy/RAG-privacy.,
    publicationDate: 2024-02-23,
    authors: ['Shenglai Zeng', 'Jiankun Zhang', 'Pengfei He', 'Yue Xing', 'Yiding Liu', 'Han Xu', 'Jie Ren', 'Shuaiqiang Wang', 'Dawei Yin', 'Yi Chang', 'Jiliang Tang'],
    score: 122.4476134219972
},
{
    title: RACE: Retrieval-augmented Commit Message Generation,
    abstract: Commit messages are important for software development and maintenance. Many neural network-based approaches have been proposed and shown promising results on automatic commit message generation. However, the generated commit messages could be repetitive or redundant. In this paper, we propose RACE, a new retrieval-augmented neural commit message generation method, which treats the retrieved similar commit as an exemplar and leverages it to generate an accurate commit message. As the retrieved commit message may not always accurately describe the content/intent of the current code diff, we also propose an exemplar guider, which learns the semantic similarity between the retrieved and current code diff and then guides the generation of commit message based on the similarity. We conduct extensive experiments on a large public dataset with five programming languages. Experimental results show that RACE can outperform all baselines. Furthermore, RACE can boost the performance of existing Seq2Seq models in commit message generation.,
    publicationDate: 2022-03-05,
    authors: ['Ensheng Shi', 'Yanlin Wang', 'Wei Tao', 'Lun Du', 'Hongyu Zhang', 'Shi Han', 'Dongmei Zhang', 'Hongbin Sun'],
    score: 122.4476134219972
},
{
    title: Graph Neural Prompting with Large Language Models,
    abstract: Large language models (LLMs) have shown remarkable generalization capability with exceptional performance in various language modeling tasks. However, they still exhibit inherent limitations in precisely capturing and returning grounded knowledge. While existing work has explored utilizing knowledge graphs (KGs) to enhance language modeling via joint training and customized model architectures, applying this to LLMs is problematic owing to their large number of parameters and high computational cost. Therefore, how to enhance pre-trained LLMs using grounded knowledge, e.g., retrieval-augmented generation, remains an open question. In this work, we propose Graph Neural Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs. GNP encompasses various designs, including a standard graph neural network encoder, a cross-modality pooling module, a domain projector, and a self-supervised link prediction objective. Extensive experiments on multiple datasets demonstrate the superiority of GNP on both commonsense and biomedical reasoning tasks across different LLM sizes and settings. Code is available at https://github.com/meettyj/GNP.,
    publicationDate: 2023-09-27,
    authors: ['Yijun Tian', 'Huan Song', 'Zichen Wang', 'Haozhu Wang', 'Ziqing Hu', 'Fang Wang', 'N. Chawla', 'Panpan Xu'],
    score: 122.4476134219972
},
{
    title: Detecting high-emitting methane sources in oil/gas fields using satellite observations,
    abstract: Abstract. Methane emissions from oil/gas fields originate from a large
number of relatively small and densely clustered point sources. A small fraction of
high-mode emitters can make a large contribution to the total methane emission. Here we
conduct observation system simulation experiments (OSSEs) to examine the potential of
recently launched or planned satellites to detect and locate these high-mode emitters
through measurements of atmospheric methane columns. We simulate atmospheric methane over
a generic oil/gas field (20–500 production sites of different size categories in a 50×50 km2 domain) for a 1-week period using the WRF-STILT meteorological model
with 1.3×1.3 km2 horizontal resolution. The
simulations consider many random realizations for the occurrence and distribution of
high-mode emitters in the field by sampling bimodal probability density functions (PDFs)
of emissions from individual sites. The atmospheric methane fields for each realization
are observed virtually with different satellite and surface observing configurations.
Column methane enhancements observed from satellites are small relative to instrument
precision, even for high-mode emitters, so an inverse analysis is necessary. We compare
L1 and L2 regularizations and show that L1 regularization effectively
provides sparse solutions for a bimodally distributed variable and enables the retrieval
of high-mode emitters. We find that the recently launched TROPOMI instrument (low Earth
orbit, 7×7 km2 nadir pixels, daily return time) and the planned GeoCARB
instrument (geostationary orbit, 2.7×3.0 km2 pixels, 2 times or 4 times
per day return times) are successful (> 80 % detection rate,
< 20 % false alarm rate) at locating high-emitting sources for fields of
20–50 emitters within the 50×50 km2 domain as long as skies are clear.
They are unsuccessful for denser fields. GeoCARB does not benefit significantly from more
frequent observations (4 times per day vs. 2 times per day) because of a temporal error
correlation in the inversion, unless under partly cloudy conditions where more frequent
observation increases the probability of clear sky. It becomes marginally successful when
allowing a 5 km error tolerance for localization. A next-generation geostationary
satellite instrument with 1.3×1.3 km2 pixels, hourly return time, and
1 ppb precision can successfully detect and locate the high-mode emitters for a dense
field with up to 500 sites in the 50×50 km2 domain. The capabilities of
TROPOMI and GeoCARB can be usefully augmented with a surface air observation network of
5–20 sites, and in turn the satellite instruments increase the detection capability that
can be achieved from the surface sites alone.
,
    publicationDate: 2018-07-31,
    authors: ['D. Cusworth', 'D. Jacob', 'J. Sheng', 'J. Benmergui', 'A. Turner', 'J. Brandman', 'L. White', 'C. Randles'],
    score: 122.42962094733642
},
{
    title: Interactive AI With Retrieval-Augmented Generation for Next Generation Networking,
    abstract: With the advance of artificial intelligence (AI), the concept of interactive AI (IAI) has been introduced, which can interactively understand and respond not only to human user input but also to dynamic system and network conditions. In this article, we explore an integration and enhancement of IAI in networking. We first review recent developments and future perspectives of AI and then introduce the technology and components of IAI. We then explore the integration of IAI into next-generation networks, focusing on how implicit and explicit interactions can enhance network functionality, improve user experience, and promote efficient network management. Subsequently, we propose an IAI-enabled network management and optimization framework, which consists of environment, perception, action, and brain units. We also design a pluggable large language model (LLM) module and retrieval augmented generation (RAG) module to build the knowledge base and contextual memory for decision-making in the brain unit. We demonstrate through case studies that our IAI framework can effectively perform optimization problem design. Finally, we discuss potential research directions for IAI-based networks.,
    publicationDate: 2024-01-21,
    authors: ['Ruichen Zhang', 'Hongyang Du', 'Yinqiu Liu', 'Dusist Niyato', 'Jiawen Kang', 'Sumei Sun', 'Xuemin Shen', 'H. V. Poor'],
    score: 122.36563680037474
},
{
    title: Attention Sorting Combats Recency Bias In Long Context Language Models,
    abstract: Current language models often fail to incorporate long contexts efficiently during generation. We show that a major contributor to this issue are attention priors that are likely learned during pre-training: relevant information located earlier in context is attended to less on average. Yet even when models fail to use the information from a relevant document in their response, they still pay preferential attention to that document compared to an irrelevant document at the same position. We leverage this fact to introduce ``attention sorting'': perform one step of decoding, sort documents by the attention they receive (highest attention going last), repeat the process, generate the answer with the newly sorted context. We find that attention sorting improves performance of long context models. Our findings highlight some challenges in using off-the-shelf language models for retrieval augmented generation.,
    publicationDate: 2023-09-28,
    authors: ['A. Peysakhovich', 'Adam Lerer'],
    score: 121.9860385419959
},
{
    title: A Study on the Implementation of Generative AI Services Using an Enterprise Data-Based LLM Application Architecture,
    abstract: This study presents a method for implementing generative AI services by utilizing the Large Language Models (LLM) application architecture. With recent advancements in generative AI technology, LLMs have gained prominence across various domains. In this context, the research addresses the challenge of information scarcity and proposes specific remedies by harnessing LLM capabilities. The investigation delves into strategies for mitigating the issue of inadequate data, offering tailored solutions. The study delves into the efficacy of employing fine-tuning techniques and direct document integration to alleviate data insufficiency. A significant contribution of this work is the development of a Retrieval-Augmented Generation (RAG) model, which tackles the aforementioned challenges. The RAG model is carefully designed to enhance information storage and retrieval processes, ensuring improved content generation. The research elucidates the key phases of the information storage and retrieval methodology underpinned by the RAG model. A comprehensive analysis of these steps is undertaken, emphasizing their significance in addressing the scarcity of data. The study highlights the efficacy of the proposed method, showcasing its applicability through illustrative instances. By implementing the RAG model for information storage and retrieval, the research not only contributes to a deeper comprehension of generative AI technology but also facilitates its practical usability within enterprises utilizing LLMs. This work holds substantial value in advancing the field of generative AI, offering insights into enhancing data-driven content generation and fostering active utilization of LLM-based services within corporate settings.,
    publicationDate: 2023-09-03,
    authors: ['CheonSu Jeong'],
    score: 121.9860385419959
},
{
    title: Retrieval-Augmented Text-to-Audio Generation,
    abstract: Despite recent progress in text-to-audio (TTA) generation, we show that the state-of-the-art models, such as AudioLDM, trained on datasets with an imbalanced class distribution, such as AudioCaps, are biased in their generation performance. Specifically, they excel in generating common audio classes while underperforming in the rare ones, thus degrading the overall generation performance. We refer to this problem as long-tailed text-to-audio generation. To address this issue, we propose a simple retrieval-augmented approach for TTA models. Specifically, given an input text prompt, we first leverage a Contrastive Language Audio Pretraining (CLAP) model to retrieve relevant text-audio pairs. The features of the retrieved audio-text data are then used as additional conditions to guide the learning of TTA models. We enhance AudioLDM with our proposed approach and denote the resulting augmented system as Re-AudioLDM. On the AudioCaps dataset, Re-AudioLDM achieves a state-of-the-art Frechet Audio Distance (FAD) of 1.37, outperforming the existing approaches by a large margin. Furthermore, we show that Re-AudioLDM can generate realistic audio for complex scenes, rare audio classes, and even unseen audio types, indicating its potential in TTA tasks.,
    publicationDate: 2023-09-14,
    authors: ['Yiitan Yuan', 'Haohe Liu', 'Xubo Liu', 'Qiushi Huang', 'Mark D. Plumbley', 'Wenwu Wang'],
    score: 121.66783656585135
},
{
    title: BashExplainer: Retrieval-Augmented Bash Code Comment Generation based on Fine-tuned CodeBERT,
    abstract: Developers use shell commands for many tasks, such as file system management, network control, and process management. Bash is one of the most commonly used shells and plays an important role in Linux system development and maintenance. Due to the language flexibility of Bash code, developers who are not familiar with Bash often have difficulty understanding the purpose and functionality of Bash code. In this study, we study Bash code comment generation problem and proposed an automatic method BASHEXPLAINER based on two-stage training strategy. In the first stage, we train a Bash encoder by fine-tuning CodeBERT on our constructed Bash code corpus. In the second stage, we first retrieve the most similar code from the code repository for the target code based on semantic and lexical similarity. Then we use the trained Bash encoder to generate two vector representations. Finally, we fuse these two vector representations via the fusion layer and generate the code comment through the decoder. To show the competitiveness of our proposed method, we construct a high-quality corpus by combining the corpus shared in the previous NL2Bash study and the corpus shared in the NLC2CMD competition. This corpus contains 10,592 Bash codes and corresponding comments. Then we selected ten baselines from previous studies on automatic code comment generation, which cover information retrieval methods, deep learning methods, and hybrid methods. The experimental results show that in terms of the performance measures BLEU-3/4, METEOR, and ROUGR-L, BASHEXPLAINER can outperform all baselines by at least 8.75%, 9.29%, 4.77% and 3.86%. Then we design ablation experiments to show the component setting rationality of BASHEXPLAINER. Later, we conduct a human study to further show the competitiveness of BASHEXPLAINER. Finally, we develop a browser plug-in based on BASHEXPLAINER to facilitate the understanding of the Bash code for developers.,
    publicationDate: 2022-06-27,
    authors: ['Chi Yu', 'Guang Yang', 'Xiang Chen', 'Ke Liu', 'Yanlin Zhou'],
    score: 121.66783656585135
},
{
    title: IMAGE-TO-IMAGE TRANSLATION FOR ENHANCED FEATURE MATCHING, IMAGE RETRIEVAL AND VISUAL LOCALIZATION,
    abstract: Abstract. The performance of machine learning and deep learning algorithms for image analysis depends significantly on the quantity and quality of the training data. The generation of annotated training data is often costly, time-consuming and laborious. Data augmentation is a powerful option to overcome these drawbacks. Therefore, we augment training data by rendering images with arbitrary poses from 3D models to increase the quantity of training images. These training images usually show artifacts and are of limited use for advanced image analysis. Therefore, we propose to use image-to-image translation to transform images from a rendered domain to a captured domain. We show that translated images in the captured domain are of higher quality than the rendered images. Moreover, we demonstrate that image-to-image translation based on rendered 3D models enhances the performance of common computer vision tasks, namely feature matching, image retrieval and visual localization. The experimental results clearly show the enhancement on translated images over rendered images for all investigated tasks. In addition to this, we present the advantages utilizing translated images over exclusively captured images for visual localization.
,
    publicationDate: 2019-09-16,
    authors: ['M. S. Mueller', 'Torsten Sattler', 'M. Pollefeys', 'B. Jutzi'],
    score: 121.36563680037474
},
{
    title: RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation,
    abstract: We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT,
    publicationDate: 2024-03-08,
    authors: ['Zihao Wang', 'Anji Liu', 'Haowei Lin', 'Jiaqi Li', 'Xiaojian Ma', 'Yitao Liang'],
    score: 121.01796072493232
},
{
    title: Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models,
    abstract: 
 Large language models (LLMs) demonstrate impressive capabilities to generate accurate code snippets given natural language intents in a zero-shot manner,
 i.e.
 , without the need for specific fine-tuning. While prior studies have highlighted the advantages of fine-tuning LLMs, this process incurs high computational costs, making it impractical in resource-scarce environments, particularly for models with billions of parameters. To address these challenges, previous research explored in-context learning (ICL) and retrieval-augmented generation (RAG) as strategies to guide the LLM generative process with task-specific prompt examples. However, ICL and RAG introduce inconveniences, such as the need for designing contextually relevant prompts and the absence of learning task-specific parameters, thereby limiting downstream task performance. In this context, we foresee parameter-efficient fine-tuning (PEFT) as a promising approach to efficiently specialize LLMs to task-specific data while maintaining reasonable resource consumption. In this paper, we deliver a comprehensive study of PEFT techniques for LLMs in the context of automated code generation. Our comprehensive investigation of PEFT techniques for LLMs reveals their superiority and potential over ICL and RAG across a diverse set of LLMs and three representative Python code generation datasets: Conala, CodeAlpacaPy, and APPS. Furthermore, our study highlights the potential for tuning larger LLMs and significant reductions in memory usage by combining PEFT with quantization. Therefore, this study opens opportunities for broader applications of PEFT in software engineering scenarios.
,
    publicationDate: 2023-08-21,
    authors: ['M. Weyssow', 'Xin Zhou', 'Kisub Kim', 'David Lo', 'H. Sahraoui'],
    score: 120.93598410330986
},
{
    title: Concept-Aware Video Captioning: Describing Videos With Effective Prior Information,
    abstract: Concepts, a collective term for meaningful words that correspond to objects, actions, and attributes, can act as an intermediary for video captioning. While many efforts have been made to augment video captioning with concepts, most methods suffer from limited precision of concept detection and insufficient utilization of concepts, which could provide caption generation with inaccurate and inadequate prior information. Considering these issues, we propose a Concept-awARE video captioning framework (CARE) to facilitate plausible caption generation. Based on the encoder-decoder structure, CARE detects concepts precisely via multimodal-driven concept detection (MCD) and offers sufficient prior information to caption generation by global-local semantic guidance (G-LSG). Specifically, we implement MCD by leveraging video-to-text retrieval and the multimedia nature of videos. To achieve G-LSG, given the concept probabilities predicted by MCD, we weight and aggregate concepts to mine the video’s latent topic to affect decoding globally and devise a simple yet efficient hybrid attention module to exploit concepts and video content to impact decoding locally. Finally, to develop CARE, we emphasize on the knowledge transfer of a contrastive vision-language pre-trained model (i.e., CLIP) in terms of visual understanding and video-to-text retrieval. With the multi-role CLIP, CARE can outperform CLIP-based strong video captioning baselines with affordable extra parameter and inference latency costs. Extensive experiments on MSVD, MSR-VTT, and VATEX datasets demonstrate the versatility of our approach for different encoder-decoder networks and the superiority of CARE against state-of-the-art methods. Our code is available at https://github.com/yangbang18/CARE.,
    publicationDate: 2023-08-28,
    authors: ['Bang Yang', 'Meng Cao', 'Yuexian Zou'],
    score: 120.93598410330986
},
{
    title: HCI Intelligent multimodal interaction environments,
    abstract: I: Multimodality and Conversational Dialogue.- Preferences and Patterns of Paralinguistic Voice Input to Interactive Media.- "Show and Tell": Using Semantically Processable Prosodic Markers for Spatial Expressions in an HCI System for Consumer Complaints.- Exploiting Speech-Gesture Correlation in Multimodal Interaction.- Pictogram Retrieval Based on Collective Semantics.- Enrich Web Applications with Voice Internet Persona Text-to-Speech for Anyone, Anywhere.- Using Recurrent Fuzzy Neural Networks for Predicting Word Boundaries in a Phoneme Sequence in Persian Language.- Subjective Measurement of Workload Related to a Multimodal Interaction Task: NASA-TLX vs. Workload Profile.- Menu Selection Using Auditory Interface.- Analysis of User Interaction with Service Oriented Chatbot Systems.- Performance Analysis of Perceptual Speech Quality and Modules Design for Management over IP Network.- A Tangible User Interface with Multimodal Feedback.- Minimal Parsing Key Concept Based Question Answering System.- Customized Message Generation and Speech Synthesis in Response to Characteristic Behavioral Patterns of Children.- Multi-word Expression Recognition Integrated with Two-Level Finite State Transducer.- Towards Multimodal User Interfaces Composition Based on UsiXML and MBD Principles.- m-LoCoS UI: A Universal Visible Language for Global Mobile Communication.- Developing a Conversational Agent Using Ontologies.- Conspeakuous: Contextualising Conversational Systems.- Persuasive Effects of Embodied Conversational Agent Teams.- Exploration of Possibility of Multithreaded Conversations Using a Voice Communication System.- A Toolkit for Multimodal Interface Design: An Empirical Investigation.- An Input-Parsing Algorithm Supporting Integration of Deictic Gesture in Natural Language Interface.- Multimodal Interfaces for In-Vehicle Applications.- Character Agents in E-Learning Interface Using Multimodal Real-Time Interaction.- An Empirical Study on Users' Acceptance of Speech Recognition Errors in Text-Messaging.- Flexible Multi-modal Interaction Technologies and User Interface Specially Designed for Chinese Car Infotainment System.- A Spoken Dialogue System Based on Keyword Spotting Technology.- II: Adaptive, Intelligent and Emotional User Interfaces.- Dynamic Association Rules Mining to Improve Intermediation Between User Multi-channel Interactions and Interactive e-Services.- Emotionally Expressive Avatars for Chatting, Learning and Therapeutic Intervention.- Can Virtual Humans Be More Engaging Than Real Ones?.- Automatic Mobile Content Conversion Using Semantic Image Analysis.- History Based User Interest Modeling in WWW Access.- Development of a Generic Design Framework for Intelligent Adaptive Systems.- Three Way Relationship of Human-Robot Interaction.- MEMORIA: Personal Memento Service Using Intelligent Gadgets.- A Location-Adaptive Human-Centered Audio Email Notification Service for Multi-user Environments.- Emotion-Based Textile Indexing Using Neural Networks.- Decision Theoretic Perspective on Optimizing Intelligent Help.- Human-Aided Cleaning Algorithm for Low-Cost Robot Architecture.- The Perception of Artificial Intelligence as "Human" by Computer Users.- Speaker Segmentation for Intelligent Responsive Space.- Emotion and Sense of Telepresence: The Effects of Screen Viewpoint, Self-transcendence Style, and NPC in a 3D Game Environment.- Emotional Interaction Through Physical Movement.- Towards Affective Sensing.- Affective User Modeling for Adaptive Intelligent User Interfaces.- A Multidimensional Classification Model for the Interaction in Reactive Media Rooms.- An Adaptive Web Browsing Method for Various Terminals: A Semantic Over-Viewing Method.- Evaluation of P2P Information Recommendation Based on Collaborative Filtering.- Understanding the Social Relationship Between Humans and Virtual Humans.- EREC-II in Use - Studies on Usability and Suitability of a Sensor System for Affect Detection and Human Performance Monitoring.- Development of an Adaptive Multi-agent Based Content Collection System for Digital Libraries.- Using Content-Based Multimedia Data Retrieval for Multimedia Content Adaptation.- Coping with Complexity Through Adaptive Interface Design.- Region-Based Model of Tour Planning Applied to Interactive Tour Generation.- A Learning Interface Agent for User Behavior Prediction.- Sharing Video Browsing Style by Associating Browsing Behavior with Low-Level Features of Videos.- Adaptation in Intelligent Tutoring Systems: Development of Tutoring and Domain Models.- Confidence Measure Based Incremental Adaptation for Online Language Identification.- Study on Speech Emotion Recognition System in E-Learning.- III: Gesture and Eye Gaze Recognition.- How Do Adults Solve Digital Tangram Problems? Analyzing Cognitive Strategies Through Eye Tracking Approach.- Gesture Interaction for Electronic Music Performance.- A New Method for Multi-finger Detection Using a Regular Diffuser.- Lip Contour Extraction Using Level Set Curve Evolution with Shape Constraint.- Visual Foraging of Highlighted Text: An Eye-Tracking Study.- Effects of a Dual-Task Tracking on Eye Fixation Related Potentials (EFRP).- Effect of Glance Duration on Perceived Complexity and Segmentation of User Interfaces.- Movement-Based Interaction and Event Management in Virtual Environments with Optical Tracking Systems.- Multiple People Gesture Recognition for Human-Robot Interaction.- Position and Pose Computation of a Moving Camera Using Geometric Edge Matching for Visual SLAM.- "Shooting a Bird": Game System Using Facial Feature for the Handicapped People.- Human Pose Estimation Using a Mixture of Gaussians Based Image Modeling.- Human Motion Modeling Using Multivision.- Real-Time Face Tracking System Using Adaptive Face Detector and Kalman Filter.- Kalman Filtering in the Design of Eye-Gaze-Guided Computer Interfaces.- Human Shape Tracking for Gait Recognition Using Active Contours with Mean Shift.- Robust Gaze Tracking Method for Stereoscopic Virtual Reality Systems.- EyeScreen: A Gesture Interface for Manipulating On-Screen Objects.- GART: The Gesture and Activity Recognition Toolkit.- Static and Dynamic Hand-Gesture Recognition for Augmented Reality Applications.- Multiple People Labeling and Tracking Using Stereo for Human Computer Interaction.- A Study of Human Vision Inspection for Mura.- Tracing Users' Behaviors in a Multimodal Instructional Material: An Eye-Tracking Study.- A Study on Interactive Artwork as an Aesthetic Object Using Computer Vision System.- Human-Computer Interaction System Based on Nose Tracking.- Evaluating Eye Tracking with ISO 9241 - Part 9.- Impact of Mental Rotation Strategy on Absolute Direction Judgments: Supplementing Conventional Measures with Eye Movement Data.- IV: Interactive TV and Media.- Beyond Mobile TV: Understanding How Mobile Interactive Systems Enable Users to Become Digital Producers.- Media Convergence, an Introduction.- An Improved H.264 Error Concealment Algorithm with User Feedback Design.- Classification of a Person Picture and Scenery Picture Using Structured Simplicity.- Designing Personalized Media Center with Focus on Ethical Issues of Privacy and Security.- Evaluation of VISTO: A New Vector Image Search TOol.- G-Tunes - Physical Interaction Design of Playing Music.- nan0sphere: Location-Driven Fiction for Groups of Users.- How Panoramic Photography Changed Multimedia Presentations in Tourism.- Frame Segmentation Used MLP-Based X-Y Recursive for Mobile Cartoon Content.- Browsing and Sorting Digital Pictures Using Automatic Image Classification and Quality Analysis.- A Usability Study on Personalized EPG (pEPG) UI of Digital TV.- Recognizing Cultural Diversity in Digital Television User Interface Design.- A Study on User Satisfaction Evaluation About the Recommendation Techniques of a Personalized EPG System on Digital TV.- Usability of Hybridmedia Services - PC and Mobile Applications Compared.- m-YouTube Mobile UI: Video Selection Based on Social Influence.- Can Video Support City-Based Communities?.- Watch, Press, and Catch - Impact of Divided Attention on Requirements of Audiovisual Quality.- Media Service Mediation Supporting Resident's Collaboration in ubiTV.- Implementation of a New H.264 Video Watermarking Algorithm with Usability Test.- Innovative TV: From an Old Standard to a New Concept of Interactive TV - An Italian Job.- Evaluating the Effectiveness of Digital Storytelling with Panoramic Images to Facilitate Experience Sharing.- User-Centered Design and Evaluation of a Concurrent Voice Communication and Media Sharing Application.- Customer-Dependent Storytelling Tool with Authoring and Viewing Functions.- Reliable Partner System Always Providing Users with Companionship Through Video Streaming.- Modeling of Places Based on Feature Distribution.- Knowledge Transfer in Semi-automatic Image Interpretation.,
    publicationDate: None,
    authors: ['J. Jacko'],
    score: 120.64576901751826
},
{
    title: Optimizing Science Question Ranking through Model and Retrieval-Augmented Generation,
    abstract: This paper delves into the challenges of discerning optimal answers from science-based questions generated by large language models (LLM), particularly emphasizing the intricate task of ranking. Employing the MAP@3 evaluation metric and drawing from the OpenBookQA dataset, the study explores modeling strategies and highlights the exceptional performance of the Platypus2-70B model. Equipped with a state-of-the-art text encoder, Platypus2-70B achieves an impressive score of 0.909904, setting a benchmark for excellence in future large language model competitions. The paper goes beyond a mere description of model architectures and experimental results, offering a comprehensive journey that envisions the transformative impact of large-scale language models on the landscape of natural language understanding, especially within the intricate domains of scientific exploration.,
    publicationDate: 2023-12-30,
    authors: ['Ye Zhang', 'Mengran Zhu', 'Yulu Gong', 'Rui Ding'],
    score: 120.62075301653314
},
{
    title: Retrieval-augmented Generation across Heterogeneous Knowledge,
    abstract: Retrieval-augmented generation (RAG) methods have been receiving increasing attention from the NLP community and achieved state-of-the-art performance on many NLP downstream tasks. Compared with conventional pre-trained generation models, RAG methods have remarkable advantages such as easy knowledge acquisition, strong scalability, and low training cost. Although existing RAG models have been applied to various knowledge-intensive NLP tasks, such as open-domain QA and dialogue systems, most of the work has focused on retrieving unstructured text documents from Wikipedia. In this paper, I first elaborate on the current obstacles to retrieving knowledge from a single-source homogeneous corpus. Then, I demonstrate evidence from both existing literature and my experiments, and provide multiple solutions on retrieval-augmented generation methods across heterogeneous knowledge.,
    publicationDate: None,
    authors: ['W. Yu'],
    score: 120.5094374497971
},
{
    title: On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks,
    abstract: There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples--ranging from multiplication to simple planning--there persists a wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion. This belief seemingly rests on the assumption that verification of correctness should be easier than generation--a rather classical argument from computational complexity--which should be irrelevant to LLMs to the extent that what they are doing is approximate retrieval. In this paper, we set out to systematically investigate the effectiveness of iterative prompting in the context of reasoning and planning. We present a principled empirical study of the performance of GPT-4 in three domains: Game of 24, Graph Coloring, and STRIPS planning. We experiment both with the model critiquing its own answers and with an external correct reasoner verifying proposed solutions. In each case, we analyze whether the content of criticisms actually affects bottom line performance, and whether we can ablate elements of the augmented system without losing performance. We observe significant performance collapse with self-critique and significant performance gains with sound external verification. We also note that merely re-prompting with a sound verifier maintains most of the benefits of more involved setups.,
    publicationDate: 2024-02-12,
    authors: ['Kaya Stechly', 'Karthik Valmeekam', 'Subbarao Kambhampati'],
    score: 120.5094374497971
},
{
    title: Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning,
    abstract: Large Language Models~(LLMs) have gained immense popularity and are being increasingly applied in various domains. Consequently, ensuring the security of these models is of paramount importance. Jailbreak attacks, which manipulate LLMs to generate malicious content, are recognized as a significant vulnerability. While existing research has predominantly focused on direct jailbreak attacks on LLMs, there has been limited exploration of indirect methods. The integration of various plugins into LLMs, notably Retrieval Augmented Generation~(RAG), which enables LLMs to incorporate external knowledge bases into their response generation such as GPTs, introduces new avenues for indirect jailbreak attacks. To fill this gap, we investigate indirect jailbreak attacks on LLMs, particularly GPTs, introducing a novel attack vector named Retrieval Augmented Generation Poisoning. This method, Pandora, exploits the synergy between LLMs and RAG through prompt manipulation to generate unexpected responses. Pandora uses maliciously crafted content to influence the RAG process, effectively initiating jailbreak attacks. Our preliminary tests show that Pandora successfully conducts jailbreak attacks in four different scenarios, achieving higher success rates than direct attacks, with 64.3\% for GPT-3.5 and 34.8\% for GPT-4.,
    publicationDate: 2024-02-13,
    authors: ['Gelei Deng', 'Yi Liu', 'Kailong Wang', 'Yuekang Li', 'Tianwei Zhang', 'Yang Liu'],
    score: 119.98306765262805
},
{
    title: Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases,
    abstract: We proposed an end-to-end system design towards utilizing Retrieval Augmented Generation (RAG) to improve the factual accuracy of Large Language Models (LLMs) for domain-specific and time-sensitive queries related to private knowledge-bases. Our system integrates RAG pipeline with upstream datasets processing and downstream performance evaluation. Addressing the challenge of LLM hallucinations, we finetune models with a curated dataset which originates from CMU's extensive resources and annotated with the teacher model. Our experiments demonstrate the system's effectiveness in generating more accurate answers to domain-specific and time-sensitive inquiries. The results also revealed the limitations of fine-tuning LLMs with small-scale and skewed datasets. This research highlights the potential of RAG systems in augmenting LLMs with external datasets for improved performance in knowledge-intensive tasks. Our code and models are available on Github.,
    publicationDate: 2024-03-15,
    authors: ['Jiarui Li', 'Ye Yuan', 'Zehua Zhang'],
    score: 119.98306765262805
},
{
    title: Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification,
    abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content. This issue is common in both non-retrieval-based generation and retrieval-augmented generation approaches, and existing post-hoc rectification methods may not address the accumulated hallucination errors that may be caused by the"snowballing"issue, especially in reasoning tasks. To tackle these challenges, we introduce a novel approach called Real-time Verification and Rectification (Ever). Instead of waiting until the end of the generation process to rectify hallucinations, Ever employs a real-time, step-wise generation and hallucination rectification strategy. The primary objective is to detect and rectify hallucinations as they occur during the text generation process. When compared to both retrieval-based and non-retrieval-based baselines, Ever demonstrates a significant improvement in generating trustworthy and factually accurate text across a diverse range of tasks, including short-form QA, biography generation, and multi-hop reasoning.,
    publicationDate: 2023-11-15,
    authors: ['Haoqiang Kang', 'Juntong Ni', 'Huaxiu Yao'],
    score: 119.98306765262805
},
{
    title: SelfEvolve: A Code Evolution Framework via Large Language Models,
    abstract: Large language models (LLMs) have already revolutionized code generation, after being pretrained on publicly available code data. However, while various methods have been proposed to augment LLMs with retrieved knowledge and enhance the quality of code generation, the performance of these retrieval-based methods is limited by the strength of the retrievers used. In addition, while LLMs show great emergent ability, they still struggle to produce the correct code in one turn. To address these challenges, we propose a novel two-step pipeline, called \autoknow, that leverages LLMs as both knowledge providers and self-reflective programmers. Unlike retrieval-based methods, \autoknow~obtains the knowledge from input prompts and generates intermediate code based on the generated knowledge. After that, \autoknow~asks LLM to act as an expert programmer to perform debugging for the generated code. This is achieved by receiving the error message from the interpreter, without requiring special test cases for correctness verification. We evaluate \autoknow~on three code generation datasets, including DS-1000 for data science code, HumanEval for software engineering code, and TransCoder for C++-to-Python translation. Our empirical experiments show that \autoknow~outperforms strong baselines by a significant margin on all datasets. We also conduct exhaustive analytical experiments to validate the effectiveness of the two stages of \autoknow, and find that both are superior to other prompting-based methods. Further scalability analysis demonstrates that \autoknow~can be adapted to other more advanced models, such as GPT-4, and bring consistent efficacy improvement.,
    publicationDate: 2023-06-05,
    authors: ['Shuyang Jiang', 'Yuhao Wang', 'Yu Wang'],
    score: 119.98306765262805
},
{
    title: Chunk-based Nearest Neighbor Machine Translation,
    abstract: Semi-parametric models, which augment generation with retrieval, have led to impressive results in language modeling and machine translation, due to their ability to retrieve fine-grained information from a datastore of examples. One of the most prominent approaches, kNN-MT, exhibits strong domain adaptation capabilities by retrieving tokens from domain-specific datastores (Khandelwal et al., 2021). However, kNN-MT requires an expensive retrieval operation for every single generated token, leading to a very low decoding speed (around 8 times slower than a parametric model). In this paper, we introduce a chunk-based kNN-MT model which retrieves chunks of tokens from the datastore, instead of a single token. We propose several strategies for incorporating the retrieved chunks into the generation process, and for selecting the steps at which the model needs to search for neighbors in the datastore. Experiments on machine translation in two settings, static and “on-the-fly” domain adaptation, show that the chunk-based kNN-MT model leads to significant speed-ups (up to 4 times) with only a small drop in translation quality.,
    publicationDate: 2022-05-24,
    authors: ['Pedro Henrique Martins', 'Zita Marinho', 'André Martins'],
    score: 119.98306765262805
},
{
    title: Multi-stage Training with Improved Negative Contrast for Neural Passage Retrieval,
    abstract: In the context of neural passage retrieval, we study three promising techniques: synthetic data generation, negative sampling, and fusion. We systematically investigate how these techniques contribute to the performance of the retrieval system and how they complement each other. We propose a multi-stage framework comprising of pre-training with synthetic data, fine-tuning with labeled data, and negative sampling at both stages. We study six negative sampling strategies and apply them to the fine-tuning stage and, as a noteworthy novelty, to the synthetic data that we use for pre-training. Also, we explore fusion methods that combine negatives from different strategies. We evaluate our system using two passage retrieval tasks for open-domain QA and using MS MARCO. Our experiments show that augmenting the negative contrast in both stages is effective to improve passage retrieval accuracy and, importantly, they also show that synthetic data generation and negative sampling have additive benefits. Moreover, using the fusion of different kinds allows us to reach performance that establishes a new state-of-the-art level in two of the tasks we evaluated.,
    publicationDate: None,
    authors: ['Jing-Bin Lu', 'Gustavo Hernández Ábrego', 'Ji Ma', 'Jianmo Ni', 'Yinfei Yang'],
    score: 119.43755299006494
},
{
    title: CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models,
    abstract: 
 Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate “hallucinated” content. However, evaluating RAG systems is a challenge. Most benchmarks focus primarily on question answering applications, neglecting other potential scenarios where RAG could be beneficial. Accordingly, in the experiments, these benchmarks often assess only the LLM components of the RAG pipeline or the retriever in knowledge-intensive scenarios, overlooking the impact of external knowledge base construction and the retrieval component on the entire RAG pipeline in non-knowledge-intensive scenarios. To address these issues, this paper constructs a large-scale and more comprehensive benchmark, and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we refer to the CRUD actions that describe interactions between users and knowledge bases, and also categorize the range of RAG applications into four distinct types–Create, Read, Update, and Delete (CRUD). “Create” refers to scenarios requiring the generation of original, varied content. “Read” involves responding to intricate questions in knowledge-intensive situations. “Update” focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. “Delete” pertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories, we have developed different datasets to evaluate the performance of RAG systems. We also analyze the effects of various components of the RAG system, such as the retriever, context length, knowledge base construction, and LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios
 
 1
 
 .
,
    publicationDate: 2024-01-30,
    authors: ['Yuanjie Lyu', 'Zhiyu Li', 'Simin Niu', 'Feiyu Xiong', 'Bo Tang', 'Wenjin Wang', 'Hao Wu', 'Huan Liu', 'Tong Xu', 'Enhong Chen'],
    score: 119.35557636844247
},
{
    title: RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs,
    abstract: Large language models (LLMs) typically utilize the top-k contexts from a retriever in retrieval-augmented generation (RAG). In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data. For generation, we compare our model with many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG benchmarks. Specifically, our Llama3-RankRAG significantly outperforms Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In addition, it also performs comparably to GPT-4 on five RAG benchmarks in the biomedical domain without instruction fine-tuning on biomedical data, demonstrating its superb capability for generalization to new domains.,
    publicationDate: 2024-07-02,
    authors: ['Yue Yu', 'Wei Ping', 'Zihan Liu', 'Boxin Wang', 'Jiaxuan You', 'Chao Zhang', 'Mohammad Shoeybi', 'Bryan Catanzaro'],
    score: 118.87144807032223
},
{
    title: ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence,
    abstract: Retrieval augmented generation (RAG) is frequently used to mitigate hallucinations and provide up-to-date knowledge for large language models (LLMs). However, given that document retrieval is an imprecise task and sometimes results in erroneous or even harmful content being presented in context, this raises the question of how LLMs handle retrieved information: If the provided content is incorrect, does the model know to ignore it, or does it recapitulate the error? Conversely, when the model's initial response is incorrect, does it always know to use the retrieved information to correct itself, or does it insist on its wrong prior response? To answer this, we curate a dataset of over 1200 questions across six domains (e.g., drug dosages, Olympic records, locations) along with content relevant to answering each question. We further apply precise perturbations to the answers in the content that range from subtle to blatant errors. We benchmark six top-performing LLMs, including GPT-4o, on this dataset and find that LLMs are susceptible to adopting incorrect retrieved content, overriding their own correct prior knowledge over 60% of the time. However, the more unrealistic the retrieved content is (i.e. more deviated from truth), the less likely the model is to adopt it. Also, the less confident a model is in its initial response (via measuring token probabilities), the more likely it is to adopt the information in the retrieved content. We exploit this finding and demonstrate simple methods for improving model accuracy where there is conflicting retrieved content. Our results highlight a difficult task and benchmark for LLMs -- namely, their ability to correctly discern when it is wrong in light of correct retrieved content and to reject cases when the provided content is incorrect.,
    publicationDate: 2024-04-16,
    authors: ['Kevin Wu', 'Eric Wu', 'James Zou'],
    score: 118.87144807032223
},
{
    title: ChatQA: Surpassing GPT-4 on Conversational QA and RAG,
    abstract: In this work, we introduce ChatQA, a suite of models that outperform GPT-4 on retrieval-augmented generation (RAG) and conversational question answering (QA). To enhance generation, we propose a two-stage instruction tuning method that significantly boosts the performance of RAG. For effective retrieval, we introduce a dense retriever optimized for conversational QA, which yields results comparable to the alternative state-of-the-art query rewriting models, while substantially reducing deployment costs. We also present the ChatRAG Bench, which encompasses ten datasets covering comprehensive evaluations on RAG, table-related QA, arithmetic calculations, and scenarios involving unanswerable questions. Our ChatQA-1.0-70B (score: 54.14), built on Llama2, a weaker foundation model than GPT-4, can slightly outperform GPT-4-0613 (score: 53.90) and GPT-4-Turbo-2024-04-09 (score: 54.03) on the ChatRAG Bench, without relying on any synthetic data from OpenAI GPT models. Notably, the Llama3-ChatQA-1.5-70B model surpasses the accuracy of GPT-4-Turbo-2024-04-09, achieving a 4.4% improvement. To advance research in this field, we open-sourced the model weights, instruction tuning data, ChatRAG Bench, and retriever for the community: https://chatqa-project.github.io/.,
    publicationDate: 2024-01-18,
    authors: ['Zihan Liu', 'Wei Ping', 'Rajarshi Roy', 'Peng Xu', 'Chankyu Lee', 'Mohammad Shoeybi', 'Bryan Catanzaro'],
    score: 118.87144807032223
},
{
    title: Engineering Tissue Fabrication With Machine Intelligence: Generating a Blueprint for Regeneration,
    abstract: Regenerating lost or damaged tissue is the primary goal of Tissue Engineering. 3D bioprinting technologies have been widely applied in many research areas of tissue regeneration and disease modeling with unprecedented spatial resolution and tissue-like complexity. However, the extraction of tissue architecture and the generation of high-resolution blueprints are challenging tasks for tissue regeneration. Traditionally, such spatial information is obtained from a collection of microscopic images and then combined together to visualize regions of interest. To fabricate such engineered tissues, rendered microscopic images are transformed to code to inform a 3D bioprinting process. If this process is augmented with data-driven approaches and streamlined with machine intelligence, identification of an optimal blueprint can become an achievable task for functional tissue regeneration. In this review, our perspective is guided by an emerging paradigm to generate a blueprint for regeneration with machine intelligence. First, we reviewed recent articles with respect to our perspective for machine intelligence-driven information retrieval and fabrication. After briefly introducing recent trends in information retrieval methods from publicly available data, our discussion is focused on recent works that use machine intelligence to discover tissue architectures from imaging and spectral data. Then, our focus is on utilizing optimization approaches to increase print fidelity and enhance biomimicry with machine learning (ML) strategies to acquire a blueprint ready for 3D bioprinting.,
    publicationDate: 2020-01-10,
    authors: ['Joohyun Kim', 'Jane A. McKee', 'Jake J. Fontenot', 'Jangwook P. Jung'],
    score: 118.87144807032223
},
{
    title: DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple Experts Fine-tuning,
    abstract: We propose Multiple Experts Fine-tuning Framework to build a financial large language model (LLM), DISC-FinLLM. Our methodology improves general LLMs by endowing them with multi-turn question answering abilities, domain text processing capabilities, mathematical computation skills, and retrieval-enhanced generation capabilities. We build a financial instruction-tuning dataset named DISC-FIN-SFT, including instruction samples of four categories (consulting, NLP tasks, computing and retrieval-augmented generation). Evaluations conducted on multiple benchmarks demonstrate that our model performs better than baseline models in various financial scenarios. Further resources can be found at https://github.com/FudanDISC/DISC-FinLLM.,
    publicationDate: 2023-10-23,
    authors: ['Wei Chen', 'Qiushi Wang', 'Zefei Long', 'Xianyin Zhang', 'Zhongtian Lu', 'Bingxuan Li', 'Siyuan Wang', 'Jiarong Xu', 'Xiang Bai', 'Xuanjing Huang', 'Zhongyu Wei'],
    score: 118.3302209223412
},
{
    title: LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs,
    abstract: In traditional RAG framework, the basic retrieval units are normally short. The common retrievers like DPR normally work with 100-word Wikipedia paragraphs. Such a design forces the retriever to search over a large corpus to find the `needle' unit. In contrast, the readers only need to generate answers from the short retrieved units. The imbalanced `heavy' retriever and `light' reader design can lead to sub-optimal performance. The loss of contextual information in the short, chunked units may increase the likelihood of introducing hard negatives during the retrieval stage. Additionally, the reader might not fully leverage the capabilities of recent advancements in LLMs. In order to alleviate the imbalance, we propose a new framework LongRAG, consisting of a `long retriever' and a `long reader'. In the two Wikipedia-based datasets, NQ and HotpotQA, LongRAG processes the entire Wikipedia corpus into 4K-token units by grouping related documents. By increasing the unit size, we significantly reduce the total number of units. This greatly reduces the burden on the retriever, resulting in strong retrieval performance with only a few (less than 8) top units. Without requiring any training, LongRAG achieves an EM of 62.7% on NQ and 64.3% on HotpotQA, which are on par with the (fully-trained) SoTA model. Furthermore, we test on two non-Wikipedia-based datasets, Qasper and MultiFieldQA-en. LongRAG processes each individual document as a single (long) unit rather than chunking them into smaller units. By doing so, we achieve an F1 score of 25.9% on Qasper and 57.5% on MultiFieldQA-en. Our study offers insights into the future roadmap for combining RAG with long-context LLMs.,
    publicationDate: 2024-06-21,
    authors: ['Ziyan Jiang', 'Xueguang Ma', 'Wenhu Chen'],
    score: 118.28313737302301
},
{
    title: Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference,
    abstract: For middle-school math students, interactive question-answering (QA) with tutors is an effective way to learn. The flexibility and emergent capabilities of generative large language models (LLMs) has led to a surge of interest in automating portions of the tutoring process - including interactive QA to support conceptual discussion of mathematical concepts. However, LLM responses to math questions can be incorrect or mismatched to the educational context - such as being misaligned with a school's curriculum. One potential solution is retrieval-augmented generation (RAG), which involves incorporating a vetted external knowledge source in the LLM prompt to increase response quality. In this paper, we designed prompts that retrieve and use content from a high-quality open-source math textbook to generate responses to real student questions. We evaluate the efficacy of this RAG system for middle-school algebra and geometry QA by administering a multi-condition survey, finding that humans prefer responses generated using RAG, but not when responses are too grounded in the textbook content. We argue that while RAG is able to improve response quality, designers of math QA systems must consider trade-offs between generating responses preferred by students and responses closely matched to specific educational resources.,
    publicationDate: 2023-10-04,
    authors: ['Zachary Levonian', 'Chenglu Li', 'Wangda Zhu', 'Anoushka Gade', 'Owen Henkel', 'Millie-Ellen Postle', 'Wanli Xing'],
    score: 118.28313737302301
},
{
    title: Retrieval-Augmented Code Generation for Universal Information Extraction,
    abstract: Information Extraction (IE) aims to extract structural knowledge (e.g., entities, relations, events) from natural language texts, which brings challenges to existing methods due to task-specific schemas and complex text expressions. Code, as a typical kind of formalized language, is capable of describing structural knowledge under various schemas in a universal way. On the other hand, Large Language Models (LLMs) trained on both codes and texts have demonstrated powerful capabilities of transforming texts into codes, which provides a feasible solution to IE tasks. Therefore, in this paper, we propose a universal retrieval-augmented code generation framework based on LLMs, called Code4UIE, for IE tasks. Specifically, Code4UIE adopts Python classes to define task-specific schemas of various structural knowledge in a universal way. By so doing, extracting knowledge under these schemas can be transformed into generating codes that instantiate the predefined Python classes with the information in texts. To generate these codes more precisely, Code4UIE adopts the in-context learning mechanism to instruct LLMs with examples. In order to obtain appropriate examples for different tasks, Code4UIE explores several example retrieval strategies, which can retrieve examples semantically similar to the given texts. Extensive experiments on five representative IE tasks across nine datasets demonstrate the effectiveness of the Code4UIE framework.,
    publicationDate: 2023-11-06,
    authors: ['Yucan Guo', 'Zixuan Li', 'Xiaolong Jin', 'Yantao Liu', 'Yutao Zeng', 'Wenxuan Liu', 'Xiang Li', 'Pan Yang', 'Long Bai', 'J. Guo', 'Xueqi Cheng'],
    score: 118.28313737302301
},
{
    title: BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack,
    abstract: In recent years, the input context sizes of large language models (LLMs) have increased dramatically. However, existing evaluation methods have not kept pace, failing to comprehensively assess the efficiency of models in handling long contexts. To bridge this gap, we introduce the BABILong benchmark, designed to test language models' ability to reason across facts distributed in extremely long documents. BABILong includes a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets. These tasks are challenging on their own, and even more demanding when the required facts are scattered across long natural text. Our evaluations show that popular LLMs effectively utilize only 10-20\% of the context and their performance declines sharply with increased reasoning complexity. Among alternatives to in-context reasoning, Retrieval-Augmented Generation methods achieve a modest 60\% accuracy on single-fact question answering, independent of context length. Among context extension methods, the highest performance is demonstrated by recurrent memory transformers after fine-tuning, enabling the processing of lengths up to 50 million tokens. The BABILong benchmark is extendable to any length to support the evaluation of new upcoming models with increased capabilities, and we provide splits up to 10 million token lengths.,
    publicationDate: 2024-06-14,
    authors: ['Yuri Kuratov', 'Aydar Bulatov', 'Petr Anokhin', 'Ivan Rodkin', 'Dmitry Sorokin', 'Artyom Sorokin', 'Mikhail Burtsev'],
    score: 118.28313737302301
},
{
    title: Integrating Retrieval-Augmented Generation with Large Language Models in Nephrology: Advancing Practical Applications,
    abstract: The integration of large language models (LLMs) into healthcare, particularly in nephrology, represents a significant advancement in applying advanced technology to patient care, medical research, and education. These advanced models have progressed from simple text processors to tools capable of deep language understanding, offering innovative ways to handle health-related data, thus improving medical practice efficiency and effectiveness. A significant challenge in medical applications of LLMs is their imperfect accuracy and/or tendency to produce hallucinations—outputs that are factually incorrect or irrelevant. This issue is particularly critical in healthcare, where precision is essential, as inaccuracies can undermine the reliability of these models in crucial decision-making processes. To overcome these challenges, various strategies have been developed. One such strategy is prompt engineering, like the chain-of-thought approach, which directs LLMs towards more accurate responses by breaking down the problem into intermediate steps or reasoning sequences. Another one is the retrieval-augmented generation (RAG) strategy, which helps address hallucinations by integrating external data, enhancing output accuracy and relevance. Hence, RAG is favored for tasks requiring up-to-date, comprehensive information, such as in clinical decision making or educational applications. In this article, we showcase the creation of a specialized ChatGPT model integrated with a RAG system, tailored to align with the KDIGO 2023 guidelines for chronic kidney disease. This example demonstrates its potential in providing specialized, accurate medical advice, marking a step towards more reliable and efficient nephrology practices.,
    publicationDate: 2024-03-01,
    authors: ['Jing Miao', 'C. Thongprayoon', 'S. Suppadungsuk', 'Oscar A. Garcia Valencia', 'W. Cheungpasitporn'],
    score: 117.67080745521918
},
{
    title: Pearl: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers,
    abstract: Powerful large language models have facilitated the development of writing assistants that promise to significantly improve the quality and efficiency of composition and communication. However, a barrier to effective assistance is the lack of personalization in LLM outputs to the author’s communication style, specialized knowledge, and values. In this paper, we address this challenge by proposing Pearl, a LLM writing assistant personalized with a retriever that is trained to be generation-calibrated for personalization. Generation calibration ensures that our retriever selects historic user authored documents to augment an LLM prompt such that they are likely to help an LLM generation better adhere to a users’ preferences. We propose two key novelties for training such a retriever: (1) A training data selection method that identifies user requests likely to benefit from personalization and documents that provide that benefit; and (2) A scale-calibrating KL-divergence objective that ensures that our retriever scores remain proportional to the downstream generation quality from using the document for personalized generation. In a series of holistic evaluations, we demonstrate the effectiveness of Pearl in generating long-form texts on multiple social media datasets. Finally, we demonstrate how a generation-calibrated retriever can double as a performance predictor – detecting low quality retrieval, and improving potentially under-performing outputs via revision with LLMs.,
    publicationDate: 2023-11-15,
    authors: ['Sheshera Mysore', 'Zhuoran Lu', 'Mengting Wan', 'Longqi Yang', 'Steve Menezes', 'Tina Baghaee', 'Emmanuel Barajas Gonzalez', 'Jennifer Neville', 'Tara Safavi'],
    score: 117.03241323893724
},
{
    title: Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA,
    abstract: Long-context modeling capabilities of Large Language Models (LLMs) have garnered widespread attention, leading to the emergence of LLMs with ultra-context windows. Meanwhile, benchmarks for evaluating long-context language models are gradually catching up. However, existing benchmarks employ irrelevant noise texts to artificially extend the length of test cases, diverging from the real-world scenarios of long-context applications. To bridge this gap, we propose a novel long-context benchmark, Loong, aligning with realistic scenarios through extended multi-document question answering (QA). Unlike typical document QA, in Loong’s test cases, each document is relevant to the final answer, ignoring any document will lead to the failure of the answer. Furthermore, Loong introduces four types of tasks with a range of context lengths: Spotlight Locating, Comparison, Clustering, and Chain of Reasoning, to facilitate a more realistic and comprehensive evaluation of long-context understanding. Extensive experiments indicate that existing long-context language models still exhibit considerable potential for enhancement. Retrieval augmented generation (RAG) achieves poor performance, demonstrating that Loong can reliably assess the model’s long-context modeling capabilities.,
    publicationDate: 2024-06-25,
    authors: ['Minzheng Wang', 'Longze Chen', 'Cheng Fu', 'Shengyi Liao', 'Xinghua Zhang', 'Bingli Wu', 'Haiyang Yu', 'Nan Xu', 'Lei Zhang', 'Run Luo', 'Yunshui Li', 'Min Yang', 'Fei Huang', 'Yongbin Li'],
    score: 117.03241323893724
},
{
    title: RAG-Fusion: a New Take on Retrieval-Augmented Generation,
    abstract: Infineon has identified a need for engineers, account managers, and customers to rapidly obtain product information. This problem is traditionally addressed with retrieval-augmented generation (RAG) chatbots, but in this study, I evaluated the use of the newly popularized RAG-Fusion method. RAG-Fusion combines RAG and reciprocal rank fusion (RRF) by generating multiple queries, reranking them with reciprocal scores and fusing the documents and scores. Through manually evaluating answers on accuracy, relevance, and comprehensiveness, I found that RAG-Fusion was able to provide accurate and comprehensive answers due to the generated queries contextualizing the original query from various perspectives. However, some answers strayed off topic when the generated queries' relevance to the original query is insufficient. This research marks significant progress in artificial intelligence (AI) and natural language processing (NLP) applications and demonstrates transformations in a global and multiindustry context.,
    publicationDate: 2024-01-31,
    authors: ['Zackary Rackauckas'],
    score: 116.36563680037474
},
{
    title: RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation,
    abstract: Retrieval-Augmented Generation (RAG) has shown significant improvements in various natural language processing tasks by integrating the strengths of large language models (LLMs) and external knowledge databases. However, RAG introduces long sequence generation and leads to high computation and memory costs. We propose RAGCache, a novel multilevel dynamic caching system tailored for RAG. Our analysis benchmarks current RAG systems, pinpointing the performance bottleneck (i.e., long sequence due to knowledge injection) and optimization opportunities (i.e., caching knowledge's intermediate states). Based on these insights, we design RAGCache, which organizes the intermediate states of retrieved knowledge in a knowledge tree and caches them in the GPU and host memory hierarchy. RAGCache proposes a replacement policy that is aware of LLM inference characteristics and RAG retrieval patterns. It also dynamically overlaps the retrieval and inference steps to minimize the end-to-end latency. We implement RAGCache and evaluate it on vLLM, a state-of-the-art LLM inference system and Faiss, a state-of-the-art vector database. The experimental results show that RAGCache reduces the time to first token (TTFT) by up to 4x and improves the throughput by up to 2.1x compared to vLLM integrated with Faiss.,
    publicationDate: 2024-04-18,
    authors: ['Chao Jin', 'Zili Zhang', 'Xuanlin Jiang', 'Fangyue Liu', 'Xin Liu', 'Xuanzhe Liu', 'Xin Jin'],
    score: 116.36563680037474
},
{
    title: A Survey on Retrieval-Augmented Text Generation for Large Language Models,
    abstract: Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but possibly incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.,
    publicationDate: 2024-04-17,
    authors: ['Yizheng Huang', 'Jimmy X. Huang'],
    score: 116.36563680037474
},
{
    title: Generative Retrieval-Augmented Ontologic Graph and Multiagent Strategies for Interpretive Large Language Model-Based Materials Design,
    abstract: Transformer neural networks show promising capabilities, in particular for uses in materials analysis, design, and manufacturing, including their capacity to work effectively with human language, symbols, code, and numerical data. Here, we explore the use of large language models (LLMs) as a tool that can support engineering analysis of materials, applied to retrieving key information about subject areas, developing research hypotheses, discovery of mechanistic relationships across disparate areas of knowledge, and writing and executing simulation codes for active knowledge generation based on physical ground truths. Moreover, when used as sets of AI agents with specific features, capabilities, and instructions, LLMs can provide powerful problem-solution strategies for applications in analysis and design problems. Our experiments focus on using a fine-tuned model, MechGPT, developed based on training data in the mechanics of materials domain. We first affirm how fine-tuning endows LLMs with a reasonable understanding of subject area knowledge. However, when queried outside the context of learned matter, LLMs can have difficulty recalling correct information and may hallucinate. We show how this can be addressed using retrieval-augmented Ontological Knowledge Graph strategies. The graph-based strategy helps us not only to discern how the model understands what concepts are important but also how they are related, which significantly improves generative performance and also naturally allows for injection of new and augmented data sources into generative AI algorithms. We find that the additional feature of relatedness provides advantages over regular retrieval augmentation approaches and not only improves LLM performance but also provides mechanistic insights for exploration of a material design process. Illustrated for a use case of relating distinct areas of knowledge, here, music and proteins, such strategies can also provide an interpretable graph structure with rich information at the node, edge, and subgraph level that provides specific insights into mechanisms and relationships. We discuss other approaches to improve generative qualities, including nonlinear sampling strategies and agent-based modeling that offer enhancements over single-shot generations, whereby LLMs are used to both generate content and assess content against an objective target. Examples provided include complex question answering, code generation, and execution in the context of automated force-field development from actively learned density functional theory (DFT) modeling and data analysis.,
    publicationDate: 2024-01-12,
    authors: ['Markus J. Buehler'],
    score: 116.36563680037474
},
{
    title: Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks,
    abstract: Making the contents generated by Large Language Model (LLM), accurate, credible and traceable is crucial, especially in complex knowledge-intensive tasks that require multi-step reasoning and each step needs knowledge to solve. Retrieval-augmented generation is good potential to solve this problem. However, where and how to introduce Information Retrieval (IR) to LLM is a big challenge. Previous work has the problems that wrong knowledge retrieved by IR misleads the LLM and interaction between IR and LLM breaks the reasoning chain of LLM. This paper proposes a novel framework named Search-in-the-Chain (SearChain) for the interaction between LLM and IR to solve the challenges. First, LLM generates the reasoning chain named Chain-of-Query (CoQ) where each node consists of an IR-oriented query-answer pair. Second, IR verifies the answer of each node of CoQ. It corrects the answer that is not consistent with the retrieved information when IR gives high confidence, which improves the credibility. Third, LLM can indicate its missing knowledge in CoQ and rely on IR to provide this knowledge to LLM. These operations improve the accuracy in terms of reasoning and knowledge. Finally, SearChain generates the reasoning process and marks references to supporting documents for each reasoning step, which improves traceability. Interaction with IR in SearChain forms a novel reasoning path based on a tree, which enables LLM to dynamically modify the direction of reasoning. Experiments show that SearChain outperforms state-of-the-art baselines on complex knowledge-intensive tasks including multi-hop Q&A, slot filling, fact checking, and long-form Q&A.,
    publicationDate: 2023-04-28,
    authors: ['Shicheng Xu', 'Liang Pang', 'Huawei Shen', 'Xueqi Cheng', 'Tat-Seng Chua'],
    score: 116.36563680037474
},
{
    title: Logical Form Generation via Multi-task Learning for Complex Question Answering over Knowledge Bases,
    abstract: Question answering over knowledge bases (KBQA) for complex questions is a challenging task in natural language processing. Recently, generation-based methods that translate natural language questions to executable logical forms have achieved promising performance. These methods use auxiliary information to augment the logical form generation of questions with unseen KB items or novel combinations, but the noise introduced can also leads to more incorrect results. In this work, we propose GMT-KBQA, a Generation-based KBQA method via Multi-Task learning, to better retrieve and utilize auxiliary information. GMT-KBQA first obtains candidate entities and relations through dense retrieval, and then introduces a multi-task model which jointly learns entity disambiguation, relation classification, and logical form generation. Experimental results show that GMT-KBQA achieves state-of-the-art results on both ComplexWebQuestions and WebQuestionsSP datasets. Furthermore, the detailed evaluation demonstrates that GMT-KBQA benefits from the auxiliary tasks and has a strong generalization capability.,
    publicationDate: None,
    authors: ['Xixin Hu', 'X. Wu', 'Yiheng Shu', 'Yuzhong Qu'],
    score: 116.36563680037474
},
{
    title: Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers,
    abstract: Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q&A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the ‘Blended RAG’ method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies. Our study achieves better retrieval results and sets new benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID datasets. We further extend such a ‘Blended Retriever’ to the RAG system to demonstrate far superior results on Generative Q&A datasets like SQUAD, even surpassing fine-tuning performance.,
    publicationDate: 2024-03-22,
    authors: ['Kunal Sawarkar', 'Abhilasha Mangal', 'Shivam Raj Solanki'],
    score: 115.66783656585135
},
{
    title: A Comprehensive Evaluation of GPT-4V on Knowledge-Intensive Visual Question Answering,
    abstract: The emergence of multimodal large models (MLMs) has significantly advanced the field of visual understanding, offering remarkable capabilities in the realm of visual question answering (VQA). Yet, the true challenge lies in the domain of knowledge-intensive VQA tasks, which necessitate not just recognition of visual elements, but also a deep comprehension of the visual information in conjunction with a vast repository of learned knowledge. To uncover such capabilities of MLMs, particularly the newly introduced GPT-4V and Gemini, we provide an in-depth evaluation from three perspectives: 1) Commonsense Knowledge, which assesses how well models can understand visual cues and connect to general knowledge; 2) Fine-grained World Knowledge, which tests the model's skill in reasoning out specific knowledge from images, showcasing their proficiency across various specialized fields; 3) Comprehensive Knowledge with Decision-making Rationales, which examines model's capability to provide logical explanations for its inference, facilitating a deeper analysis from the interpretability perspective. Additionally, we utilize a visual knowledge-enhanced training strategy and multimodal retrieval-augmented generation approach to enhance MLMs, highlighting the future need for advancements in this research direction. Extensive experiments indicate that: a) GPT-4V demonstrates enhanced explanation generation when using composite images as few-shots; b) GPT-4V and other MLMs produce severe hallucinations when dealing with world knowledge; c) Visual knowledge enhanced training and prompting technicals present potential to improve performance. Codes: https://github.com/HITsz-TMG/Cognitive-Visual-Language-Mapper,
    publicationDate: 2023-11-13,
    authors: ['Yunxin Li', 'Longyue Wang', 'Baotian Hu', 'Xinyu Chen', 'Wanqi Zhong', 'Chenyang Lyu', 'Min Zhang'],
    score: 115.66783656585135
},
{
    title: Bounds on Topological Descriptors of the Corona Product of  $F$ -Sum of Connected Graphs,
    abstract: The present-day trend of the numerical coding of chemical structures with topological indices (TIs) has established quite successful in medicinal chemistry and bioinformatics. This strategy provides the annotation, comparison, rapid collection, mining, and retrieval of chemical structures within large databases. Afterward, TIs can be used to look for quantitative structure-activity relationships and quantitative structure-property relationships, which are models that associate chemical structure with biological activity. In these analyses, degree-based TIs have secured a significant place among the different types of descriptors because of the ease of generation and the momentum with which these computations can be executed. In this paper, we compute the lower and upper bounds of the first, second, and third Zagreb, the first and the second multiple Zagreb, the geometric-arithmetic, the general sum connectivity, the general Randi<inline-formula> <tex-math notation="LaTeX">$\acute {c}$ </tex-math></inline-formula>, the atom-bond connectivity, the augmented Zagreb, and the harmonic indices of the corona product of <inline-formula> <tex-math notation="LaTeX">$F$ </tex-math></inline-formula>-sum of connected graphs in the form of their factor graphs by applying combinatorial inequalities.,
    publicationDate: None,
    authors: ['Wei Gao', 'Zahid Iqbal', 'Muhammad Ishaq', 'A. Aslam', 'M. Aamir', 'M. Binyamin'],
    score: 115.1665846874966
},
{
    title: Overview of the Special Issue on Contextual Search and Recommendation,
    abstract: Information systems that leverage contextual knowledge about their users and their search situations – such as histories, demographics, surroundings, constraints or devices – can provide tailored search experiences and higher-quality task outcomes. Within information retrieval, there is a growing focus on how knowledge of user interests, intentions, and context can improve aspects of search and recommendation such as ranking and query suggestion, especially for exploratory and/or complex tasks that can span multiple queries or search sessions. The interactions that occur during these complex tasks provide context that can be leveraged by search systems to support users’ broader information-seeking activities. Next-generation recommender systems face analogous challenges, including integrating signals from user exploration to update recommendations in real time. Within the space of search, much of the work on modeling context and search personalization has focused on constructing topical profiles of the user’s shortand long-term search history [Gauch et al. 2004; Chirita et al. 2005; Speretta and Gauch 2005; Ma et al. 2007; Bennett et al. 2010; White et al. 2010; Xiang et al. 2010; Sontag et al. 2012] or more generally, models of their query and result-click sequences [Cao et al. 2008; Cao et al. 2009; Mihalkova and Mooney 2009]. Related research has also considered a more content-driven representation such as language-model based approaches [Tan et al. 2006] or weighted term vectors derived from long-term desktop search activities [Teevan et al. 2005; Matthijs and Radlinski 2011]. However, a variety of recent investigations to contextualize search include a broader set of factors based on: a user’s location [Bennett et al. 2011], a user’s task-based search activity [Jones and Klinkner 2008; Kanoulas et al. 2011b; 2011a; Kanoulas et al. 2012; Sontag et al. 2012; Melucci 2012; Raman et al. 2014], the long-term vs. short-term interests of the user [Sugiyama et al. 2004; Li et al. 2007; Bennett et al. 2012], the ability of users to consume information at differing levels of complexity [Collins-Thompson et al. 2011], and patterns of re-finding the same search result over time [Teevan et al. 2011; Shokouhi et al. 2013]. The growth in the types of context explored and the information available to search systems derives from the timely convergence of several factors. The rapid growth in the use of different devices – most notably smartphones and tablets, but also including stationary devices such as game consoles, smart televisions, and augmented conference rooms – provides opportunities to obtain both raw and derived contextual signals that could power next-generation search and recommendation systems. The use of such signals in search and recommendation tasks has been recently explored in such venues as the Context-awareness in Retrieval and Recommendation workshops at IUI 20112012 [Luca et al. 2011; Luca et al. 2012], WSDM 2013 [Bohmer et al. 2013], and ECIR 2014 [Said et al. 2014]. Furthermore, a variety of recent work and venues have noted that much information retrieval research on web search has focused on optimizing and evaluating single queries, even though a significant fraction of queries are associated with more complex tasks [Jones and Klinkner 2008; Kanoulas et al. 2011b; 2011a; Belkin et al. 2012a;,
    publicationDate: 2015-02-17,
    authors: ['Paul N. Bennett', 'Kevyn Collins-Thompson', 'D. Kelly', 'Ryen W. White', 'Yi Zhang'],
    score: 115.1665846874966
},
{
    title: C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models,
    abstract: Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk functions under test distribution shifts. We prove that RAG achieves a lower conformal generation risk than that of a single LLM when the quality of the retrieval model and transformer is non-trivial. Our intensive empirical results demonstrate the soundness and tightness of our conformal generation risk guarantees across four widely-used NLP datasets on four state-of-the-art retrieval models.,
    publicationDate: 2024-02-05,
    authors: ['Mintong Kang', 'Nezihe Merve Gurel', 'Ning Yu', 'D. Song', 'Bo Li'],
    score: 114.93598410330986
},
{
    title: Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT models,
    abstract: We propose Retrieval Augmented Generation (RAG) as an approach for automated radiology report writing that leverages multimodally aligned embeddings from a contrastively pretrained vision language model for retrieval of relevant candidate radiology text for an input radiology image and a general domain generative model like OpenAI text-davinci-003, gpt-3.5-turbo and gpt-4 for report generation using the relevant radiology text retrieved. This approach keeps hallucinated generations under check and provides capabilities to generate report content in the format we desire leveraging the instruction following capabilities of these generative models. Our approach achieves better clinical metrics with a BERTScore of 0.2865 ({\Delta}+ 25.88%) and Semb score of 0.4026 ({\Delta}+ 6.31%). Our approach can be broadly relevant for different clinical settings as it allows to augment the automated radiology report generation process with content relevant for that setting while also having the ability to inject user intents and requirements in the prompts as part of the report generation process to modulate the content and format of the generated reports as applicable for that clinical setting.,
    publicationDate: 2023-05-05,
    authors: ['M. Ranjit', 'G. Ganapathy', 'R. Manuel', 'T. Ganu'],
    score: 114.93598410330986
},
{
    title: Memory and Knowledge Augmented Language Models for Inferring Salience in Long-Form Stories,
    abstract: Measuring event salience is essential in the understanding of stories. This paper takes a recent unsupervised method for salience detection derived from Barthes Cardinal Functions and theories of surprise and applies it to longer narrative forms. We improve the standard transformer language model by incorporating an external knowledgebase (derived from Retrieval Augmented Generation) and adding a memory mechanism to enhance performance on longer works. We use a novel approach to derive salience annotation using chapter-aligned summaries from the Shmoop corpus for classic literary works. Our evaluation against this data demonstrates that our salience detection model improves performance over and above a non-knowledgebase and memory augmented language model, both of which are crucial to this improvement.,
    publicationDate: 2021-09-08,
    authors: ['David Wilmot', 'Frank Keller'],
    score: 114.93598410330986
},
{
    title: End-to-End Learning of Flowchart Grounded Task-Oriented Dialogs,
    abstract: We propose a novel problem within end-to-end learning of task oriented dialogs (TOD), in which the dialog system mimics a troubleshooting agent who helps a user by diagnosing their problem (e.g., car not starting). Such dialogs are grounded in domain-specific flowcharts, which the agent is supposed to follow during the conversation. Our task exposes novel technical challenges for neural TOD, such as grounding an utterance to the flowchart without explicit annotation, referring to additional manual pages when user asks a clarification question, and ability to follow unseen flowcharts at test time. We release a dataset (FLODIAL) consisting of 2,738 dialogs grounded on 12 different troubleshooting flowcharts. We also design a neural model, FLONET, which uses a retrieval-augmented generation architecture to train the dialog agent. Our experiments find that FLONET can do zero-shot transfer to unseen flowcharts, and sets a strong baseline for future research.,
    publicationDate: 2021-09-15,
    authors: ['Dinesh Raghu', 'Shantanu Agarwal', 'Sachindra Joshi', 'Mausam'],
    score: 114.93598410330986
},
{
    title: Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence,
    abstract: The rapid advancement of large language models (LLMs) has paved the way for the development of highly capable autonomous agents. However, existing multi-agent frameworks often struggle with integrating diverse capable third-party agents due to reliance on agents defined within their own ecosystems. They also face challenges in simulating distributed environments, as most frameworks are limited to single-device setups. Furthermore, these frameworks often rely on hard-coded communication pipelines, limiting their adaptability to dynamic task requirements. Inspired by the concept of the Internet, we propose the Internet of Agents (IoA), a novel framework that addresses these limitations by providing a flexible and scalable platform for LLM-based multi-agent collaboration. IoA introduces an agent integration protocol, an instant-messaging-like architecture design, and dynamic mechanisms for agent teaming and conversation flow control. Through extensive experiments on general assistant tasks, embodied AI tasks, and retrieval-augmented generation benchmarks, we demonstrate that IoA consistently outperforms state-of-the-art baselines, showcasing its ability to facilitate effective collaboration among heterogeneous agents. IoA represents a step towards linking diverse agents in an Internet-like environment, where agents can seamlessly collaborate to achieve greater intelligence and capabilities. Our codebase has been released at \url{https://github.com/OpenBMB/IoA}.,
    publicationDate: 2024-07-09,
    authors: ['Weize Chen', 'Ziming You', 'Ran Li', 'Yitong Guan', 'Cheng Qian', 'Chenyang Zhao', 'Cheng Yang', 'Ruobing Xie', 'Zhiyuan Liu', 'Maosong Sun'],
    score: 114.93598410330986
},
{
    title: Supporting sociable literacy in the international children's digital library,
    abstract: As each generation of children grows up in a world shaped by the affordances available to them in both physical and digital environments, their expectations of tools to support changing literacy practices make new demands on technologists and designers. To ensure that digital libraries (DLs) for young people support their understandings of libraries and reading (and not just adults' conceptions), an intergenerational design team (IDT) at the University of Baltimore (UB) used contextual inquiry and participatory design to develop concepts for augmenting the International Children's Digital Library (ICDL) to make it more appropriate for 10-14 year olds. Our prototype aims to support "sociable literacy," a set of practices made possible by digital storage, retrieval and use of texts.,
    publicationDate: 2004-06-01,
    authors: ['Nancy Kaplan', 'Yoram Chisik', 'K. Knudtzon', 'R. Kulkarni', 'Stuart Moulthrop', 'K. Summers', 'H. Weeks'],
    score: 114.56379239589579
},
{
    title: Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems,
    abstract: Large language models (LLMs) and foundation models have been recently touted as a game-changer for 6 G systems. However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language models that were designed for natural language processing (NLP) applications. To address this challenge and create wireless-centric foundation models, this paper presents a comprehensive vision on how to design universal foundation models that are tailored towards the unique needs of next-generation wireless systems, thereby paving the way towards the deployment of artificial intelligence (AI)-native networks. Diverging from NLP-based foundation models, the proposed framework promotes the design of large multi-modal models (LMMs) fostered by three key capabilities: 1) processing of multi-modal sensing data, 2) grounding of physical symbol representations in real-world wireless systems using causal reasoning and retrieval-augmented generation (RAG), and 3) enabling instructibility from the wireless environment feedback to facilitate dynamic network adaptation thanks to logical and mathematical reasoning facilitated by neuro-symbolic AI. In essence, these properties enable the proposed LMM framework to build universal capabilities that cater to various cross-layer networking tasks and alignment of intents across different domains. Preliminary results from experimental evaluation demonstrate the efficacy of grounding using RAG in LMMs, and showcase the alignment of LMMs with wireless system designs. Furthermore, the enhanced rationale exhibited in the responses to mathematical questions by LMMs, compared to vanilla LLMs, demonstrates the logical and mathematical reasoning capabilities inherent in LMMs. Building on those results, we present a sequel of open questions and challenges for LMMs. We then conclude with a set of recommendations that ignite the path towards LMM-empowered AI-native systems.,
    publicationDate: 2024-01-30,
    authors: ['Shengzhe Xu', 'Christo Kurisummoottil Thomas', 'Omar Hashash', 'N. Muralidhar', 'Walid Saad', 'Naren Ramakrishnan'],
    score: 114.47424036192305
},
{
    title: READSUM: Retrieval-Augmented Adaptive Transformer for Source Code Summarization,
    abstract: Code summarization is the process of automatically generating brief and informative summaries of source code to aid in software comprehension and maintenance. In this paper, we propose a novel model called READSUM, REtrieval-augmented ADaptive transformer for source code SUMmarization, that combines both abstractive and extractive approaches. Our proposed model generates code summaries in an abstractive manner, taking into account both the structural and sequential information of the input code, while also utilizing an extractive approach that leverages a retrieved summary of similar code to increase the frequency of important keywords. To effectively blend the original code and the retrieved similar code at the embedding layer stage, we obtain the augmented representation of the original code and the retrieved code through multi-head self-attention. In addition, we develop a self-attention network that adaptively learns the structural and sequential information for the representations in the encoder stage. Furthermore, we design a fusion network to capture the relation between the original code and the retrieved summary at the decoder stage. The fusion network effectively guides summary generation based on the retrieved summary. Finally, READSUM extracts important keywords using an extractive approach and generates high-quality summaries using an abstractive approach that considers both the structural and sequential information of the source code. We demonstrate the superiority of READSUM through various experiments and an ablation study. Additionally, we perform a human evaluation to assess the quality of the generated summary.,
    publicationDate: None,
    authors: ['YunSeok Choi', 'CheolWon Na', 'Hyojun Kim', 'Jee-Hyong Lee'],
    score: 114.47424036192305
},
