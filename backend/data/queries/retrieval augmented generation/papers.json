[
  {
    "paperId": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22",
    "url": "https://www.semanticscholar.org/paper/58ed1fbaabe027345f7bb3a6312d41c5aac63e22",
    "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
    "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "citationCount": 4213,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2020-05-22",
    "authors": [
      {
        "authorId": "145222654",
        "name": "Patrick Lewis"
      },
      {
        "authorId": "3439053",
        "name": "Ethan Perez"
      },
      {
        "authorId": "1716179427",
        "name": "Aleksandara Piktus"
      },
      {
        "authorId": "40052301",
        "name": "F. Petroni"
      },
      {
        "authorId": "2067091563",
        "name": "Vladimir Karpukhin"
      },
      {
        "authorId": "39589154",
        "name": "Naman Goyal"
      },
      {
        "authorId": "103131985",
        "name": "Heinrich Kuttler"
      },
      {
        "authorId": "35084211",
        "name": "M. Lewis"
      },
      {
        "authorId": "144105277",
        "name": "Wen-tau Yih"
      },
      {
        "authorId": "2620211",
        "name": "Tim Rockt√§schel"
      },
      {
        "authorId": "48662861",
        "name": "Sebastian Riedel"
      },
      {
        "authorId": "1743722",
        "name": "Douwe Kiela"
      }
    ],
    "source": "semantic_scholar",
    "score": 202.19251391546203
  },
  {
    "paperId": "46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5",
    "url": "https://www.semanticscholar.org/paper/46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5",
    "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
    "abstract": "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 912,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-12-18",
    "authors": [
      {
        "authorId": "2280046531",
        "name": "Yunfan Gao"
      },
      {
        "authorId": "2275320371",
        "name": "Yun Xiong"
      },
      {
        "authorId": "2275341478",
        "name": "Xinyu Gao"
      },
      {
        "authorId": "2275191447",
        "name": "Kangxiang Jia"
      },
      {
        "authorId": "2275530552",
        "name": "Jinliu Pan"
      },
      {
        "authorId": "2275171009",
        "name": "Yuxi Bi"
      },
      {
        "authorId": "2276187454",
        "name": "Yi Dai"
      },
      {
        "authorId": "2275540959",
        "name": "Jiawei Sun"
      },
      {
        "authorId": "2258800561",
        "name": "Qianyu Guo"
      },
      {
        "authorId": "2291409458",
        "name": "Meng Wang"
      },
      {
        "authorId": "2256769434",
        "name": "Haofen Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 172.2510382089245
  },
  {
    "paperId": "b798cf6af813638fab09a8af6ad0f3df6c241485",
    "url": "https://www.semanticscholar.org/paper/b798cf6af813638fab09a8af6ad0f3df6c241485",
    "title": "Benchmarking Retrieval-Augmented Generation for Medicine",
    "abstract": "While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different LLMs by up to 18% over chain-of-thought prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our results show that the combination of various medical corpora and retrievers achieves the best performance. In addition, we discovered a log-linear scaling property and the\"lost-in-the-middle\"effects in medical RAG. We believe our comprehensive evaluations can serve as practical guidelines for implementing RAG systems for medicine.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 95,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-20",
    "authors": [
      {
        "authorId": "2048053804",
        "name": "Guangzhi Xiong"
      },
      {
        "authorId": "2261406713",
        "name": "Qiao Jin"
      },
      {
        "authorId": "2237094367",
        "name": "Zhiyong Lu"
      },
      {
        "authorId": "2265729351",
        "name": "Aidong Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 138.46522287201753
  },
  {
    "paperId": "ab15463babf98fffc6f683fe2026de0725b5e1a9",
    "url": "https://www.semanticscholar.org/paper/ab15463babf98fffc6f683fe2026de0725b5e1a9",
    "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
    "abstract": "Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Github: https://github.com/PKU-DAIR/RAG-Survey.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 120,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-02-29",
    "authors": [
      {
        "authorId": "2268718776",
        "name": "Penghao Zhao"
      },
      {
        "authorId": "2288557803",
        "name": "Hailin Zhang"
      },
      {
        "authorId": "2289597580",
        "name": "Qinhan Yu"
      },
      {
        "authorId": "2288675277",
        "name": "Zhengren Wang"
      },
      {
        "authorId": "2288532368",
        "name": "Yunteng Geng"
      },
      {
        "authorId": "46182701",
        "name": "Fangcheng Fu"
      },
      {
        "authorId": "2249513224",
        "name": "Ling Yang"
      },
      {
        "authorId": "2277807793",
        "name": "Wentao Zhang"
      },
      {
        "authorId": "2277742543",
        "name": "Bin Cui"
      }
    ],
    "source": "semantic_scholar",
    "score": 141.93685818395113
  },
  {
    "paperId": "ba454ba8c594dfb86c25dff2e265c8a2686aa037",
    "url": "https://www.semanticscholar.org/paper/ba454ba8c594dfb86c25dff2e265c8a2686aa037",
    "title": "Seven Failure Points When Engineering a Retrieval Augmented Generation System",
    "abstract": "Software engineers are increasingly adding semantic search capabilities to applications using a strategy known as Retrieval Augmented Generation (RAG). A RAG system involves finding documents that semantically match a query and then passing the documents to a large language model (LLM) such as ChatGPT to extract the right answer using an LLM. RAG systems aim to: a) reduce the problem of hallucinated responses from LLMs, b) link sources/references to generated responses, and c) remove the need for annotating documents with meta-data. However, RAG systems suffer from limitations inherent to information retrieval systems and from reliance on LLMs. In this paper, we present an experience report on the failure points of RAG systems from three case studies from separate domains: research, education, and biomedical. We share the lessons learned and present 7 failure points to consider when designing a RAG system. The two key takeaways arising from our work are: 1) validation of a RAG system is only feasible during operation, and 2) the robustness of a RAG system evolves rather than designed in at the start. We conclude with a list of potential research directions on RAG systems for the software engineering community.CCS CONCEPTS‚Ä¢ Software and its engineering ‚Üí Empirical software validation.",
    "venue": "2024 IEEE/ACM 3rd International Conference on AI Engineering ‚Äì Software Engineering for AI (CAIN)",
    "year": 2024,
    "citationCount": 53,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3644815.3644945",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2024-01-11",
    "authors": [
      {
        "authorId": "2052845461",
        "name": "Scott Barnett"
      },
      {
        "authorId": "2266469333",
        "name": "Stefanus Kurniawan"
      },
      {
        "authorId": "2257020336",
        "name": "Srikanth Thudumu"
      },
      {
        "authorId": "2279020735",
        "name": "Zach Brannelly"
      },
      {
        "authorId": "47505933",
        "name": "Mohamed Abdelrazek"
      }
    ],
    "source": "semantic_scholar",
    "score": 135.83476069846412
  },
  {
    "paperId": "5c204b2421d05b83d3c96a6c515cc03143073935",
    "url": "https://www.semanticscholar.org/paper/5c204b2421d05b83d3c96a6c515cc03143073935",
    "title": "PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models",
    "abstract": "Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate those limitations. In particular, given a question, RAG retrieves relevant knowledge from a knowledge database to augment the input of the LLM. For instance, the retrieved knowledge could be a set of top-k texts that are most semantically similar to the given question when the knowledge database contains millions of texts collected from Wikipedia. As a result, the LLM could utilize the retrieved knowledge as the context to generate an answer for the given question. Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored. We aim to bridge the gap in this work. Particularly, we propose PoisonedRAG, a set of knowledge poisoning attacks to RAG, where an attacker could inject a few poisoned texts into the knowledge database such that the LLM generates an attacker-chosen target answer for an attacker-chosen target question. We formulate knowledge poisoning attacks as an optimization problem, whose solution is a set of poisoned texts. Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on the RAG, we propose two solutions to solve the optimization problem, respectively. Our results on multiple benchmark datasets and LLMs show our attacks could achieve 90% attack success rates when injecting 5 poisoned texts for each target question into a database with millions of texts. We also evaluate recent defenses and our results show they are insufficient to defend against our attacks, highlighting the need for new defenses. 1",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 45,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2281827168",
        "name": "Wei Zou"
      },
      {
        "authorId": "2260340372",
        "name": "Runpeng Geng"
      },
      {
        "authorId": "2284264092",
        "name": "Binghui Wang"
      },
      {
        "authorId": "2282387150",
        "name": "Jinyuan Jia"
      }
    ],
    "source": "semantic_scholar",
    "score": 127.42962094733642
  },
  {
    "paperId": "4e71624e90960cb003e311a0fe3b8be4c2863239",
    "url": "https://www.semanticscholar.org/paper/4e71624e90960cb003e311a0fe3b8be4c2863239",
    "title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries",
    "abstract": "Retrieval-augmented generation (RAG) augments large language models (LLM) by retrieving relevant knowledge, showing promising potential in mitigating LLM hallucinations and enhancing response quality, thereby facilitating the great adoption of LLMs in practice. However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries. In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utilizing an English news article dataset as the underlying RAG knowledge base. We demonstrate the benchmarking utility of MultiHop-RAG in two experiments. The first experiment compares different embedding models for retrieving evidence for multi-hop queries. In the second experiment, we examine the capabilities of various state-of-the-art LLMs, including GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop queries given the evidence. Both experiments reveal that existing RAG methods perform unsatisfactorily in retrieving and answering multi-hop queries. We hope MultiHop-RAG will be a valuable resource for the community in developing effective RAG systems, thereby facilitating greater adoption of LLMs in practice. The MultiHop-RAG and implemented RAG system is publicly available at https://github.com/yixuantt/MultiHop-RAG/.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 38,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-27",
    "authors": [
      {
        "authorId": "2260449655",
        "name": "Yixuan Tang"
      },
      {
        "authorId": "2246043972",
        "name": "Yi Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.95342469194469
  },
  {
    "paperId": "5bbc2b5aa6c63c6a2cfccf095d6020b063ad47ac",
    "url": "https://www.semanticscholar.org/paper/5bbc2b5aa6c63c6a2cfccf095d6020b063ad47ac",
    "title": "Corrective Retrieval Augmented Generation",
    "abstract": "Large language models (LLMs) inevitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retrieval Augmented Generation (CRAG) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval actions can be triggered. Since retrieval from static and limited corpora can only return sub-optimal documents, large-scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose-then-recompose algorithm is designed for retrieved documents to selectively focus on key information and filter out irrelevant information in them. CRAG is plug-and-play and can be seamlessly coupled with various RAG-based approaches. Experiments on four datasets covering short- and long-form generation tasks show that CRAG can significantly improve the performance of RAG-based approaches.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 44,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-29",
    "authors": [
      {
        "authorId": "2281761331",
        "name": "Shi-Qi Yan"
      },
      {
        "authorId": "3028818",
        "name": "Jia-Chen Gu"
      },
      {
        "authorId": "2281839893",
        "name": "Yun Zhu"
      },
      {
        "authorId": "2072392338",
        "name": "Zhen-Hua Ling"
      }
    ],
    "source": "semantic_scholar",
    "score": 127.0999373465548
  },
  {
    "paperId": "a41d4a3b005c8ec4f821e6ee96672d930ca9596c",
    "url": "https://www.semanticscholar.org/paper/a41d4a3b005c8ec4f821e6ee96672d930ca9596c",
    "title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering",
    "abstract": "Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever method, introducing the first retrieval-augmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination.~\\footnote{Our codes and datasets are available at: \\url{https://github.com/XiaoxinHe/G-Retriever}}",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 36,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-12",
    "authors": [
      {
        "authorId": "2283895736",
        "name": "Xiaoxin He"
      },
      {
        "authorId": "46879986",
        "name": "Yijun Tian"
      },
      {
        "authorId": "2283901770",
        "name": "Yifei Sun"
      },
      {
        "authorId": "144539424",
        "name": "N. Chawla"
      },
      {
        "authorId": "81634721",
        "name": "T. Laurent"
      },
      {
        "authorId": "2265899558",
        "name": "Yann LeCun"
      },
      {
        "authorId": "2279831845",
        "name": "Xavier Bresson"
      },
      {
        "authorId": "2283877888",
        "name": "Bryan Hooi"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.16376868966336
  },
  {
    "paperId": "ea89b058ce619ed16d4de633126b02a8179457c8",
    "url": "https://www.semanticscholar.org/paper/ea89b058ce619ed16d4de633126b02a8179457c8",
    "title": "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)",
    "abstract": "Retrieval-augmented generation (RAG) is a powerful technique to facilitate language model with proprietary and private data, where data privacy is a pivotal concern. Whereas extensive research has demonstrated the privacy risks of large language models (LLMs), the RAG technique could potentially reshape the inherent behaviors of LLM generation, posing new privacy issues that are currently under-explored. In this work, we conduct extensive empirical studies with novel attack methods, which demonstrate the vulnerability of RAG systems on leaking the private retrieval database. Despite the new risk brought by RAG on the retrieval data, we further reveal that RAG can mitigate the leakage of the LLMs' training data. Overall, we provide new insights in this paper for privacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAG systems builders. Our code is available at https://github.com/phycholosogy/RAG-privacy.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 32,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-23",
    "authors": [
      {
        "authorId": "2253682835",
        "name": "Shenglai Zeng"
      },
      {
        "authorId": "2282560420",
        "name": "Jiankun Zhang"
      },
      {
        "authorId": "2185740224",
        "name": "Pengfei He"
      },
      {
        "authorId": "2253469617",
        "name": "Yue Xing"
      },
      {
        "authorId": "2249554788",
        "name": "Yiding Liu"
      },
      {
        "authorId": "2253881697",
        "name": "Han Xu"
      },
      {
        "authorId": "2256589810",
        "name": "Jie Ren"
      },
      {
        "authorId": "2237948548",
        "name": "Shuaiqiang Wang"
      },
      {
        "authorId": "2243455567",
        "name": "Dawei Yin"
      },
      {
        "authorId": "2267019992",
        "name": "Yi Chang"
      },
      {
        "authorId": "2115879611",
        "name": "Jiliang Tang"
      }
    ],
    "source": "semantic_scholar",
    "score": 122.4476134219972
  },
  {
    "paperId": "38fcc3667a907d6c94267c674aad114aae68441e",
    "url": "https://www.semanticscholar.org/paper/38fcc3667a907d6c94267c674aad114aae68441e",
    "title": "xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token",
    "abstract": "This paper introduces xRAG, an innovative context compression method tailored for retrieval-augmented generation. xRAG reinterprets document embeddings in dense retrieval--traditionally used solely for retrieval--as features from the retrieval modality. By employing a modality fusion methodology, xRAG seamlessly integrates these embeddings into the language model representation space, effectively eliminating the need for their textual counterparts and achieving an extreme compression rate. In xRAG, the only trainable component is the modality bridge, while both the retriever and the language model remain frozen. This design choice allows for the reuse of offline-constructed document embeddings and preserves the plug-and-play nature of retrieval augmentation. Experimental results demonstrate that xRAG achieves an average improvement of over 10% across six knowledge-intensive tasks, adaptable to various language model backbones, ranging from a dense 7B model to an 8x7B Mixture of Experts configuration. xRAG not only significantly outperforms previous context compression methods but also matches the performance of uncompressed models on several datasets, while reducing overall FLOPs by a factor of 3.53. Our work pioneers new directions in retrieval-augmented generation from the perspective of multimodality fusion, and we hope it lays the foundation for future efficient and scalable retrieval-augmented systems",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 15,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-22",
    "authors": [
      {
        "authorId": "2193630544",
        "name": "Xin Cheng"
      },
      {
        "authorId": "2193104542",
        "name": "Xun Wang"
      },
      {
        "authorId": "2284863493",
        "name": "Xingxing Zhang"
      },
      {
        "authorId": "50251691",
        "name": "Tao Ge"
      },
      {
        "authorId": "2263708536",
        "name": "Si-Qing Chen"
      },
      {
        "authorId": "2290016262",
        "name": "Furu Wei"
      },
      {
        "authorId": "2302812779",
        "name": "Huishuai Zhang"
      },
      {
        "authorId": "2302798554",
        "name": "Dongyan Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.58883083359672
  },
  {
    "paperId": "1027d120189fd6cd9aec1af273cd9a5baaf645d7",
    "url": "https://www.semanticscholar.org/paper/1027d120189fd6cd9aec1af273cd9a5baaf645d7",
    "title": "BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models",
    "abstract": "Large Language Models (LLMs) are constrained by outdated information and a tendency to generate incorrect data, commonly referred to as\"hallucinations.\"Retrieval-Augmented Generation (RAG) addresses these limitations by combining the strengths of retrieval-based methods and generative models. This approach involves retrieving relevant information from a large, up-to-date dataset and using it to enhance the generation process, leading to more accurate and contextually appropriate responses. Despite its benefits, RAG introduces a new attack surface for LLMs, particularly because RAG databases are often sourced from public data, such as the web. In this paper, we propose \\TrojRAG{} to identify the vulnerabilities and attacks on retrieval parts (RAG database) and their indirect attacks on generative parts (LLMs). Specifically, we identify that poisoning several customized content passages could achieve a retrieval backdoor, where the retrieval works well for clean queries but always returns customized poisoned adversarial queries. Triggers and poisoned passages can be highly customized to implement various attacks. For example, a trigger could be a semantic group like\"The Republican Party, Donald Trump, etc.\"Adversarial passages can be tailored to different contents, not only linked to the triggers but also used to indirectly attack generative LLMs without modifying them. These attacks can include denial-of-service attacks on RAG and semantic steering attacks on LLM generations conditioned by the triggers. Our experiments demonstrate that by just poisoning 10 adversarial passages can induce 98.2\\% success rate to retrieve the adversarial passages. Then, these passages can increase the reject ratio of RAG-based GPT-4 from 0.01\\% to 74.6\\% or increase the rate of negative responses from 0.22\\% to 72\\% for targeted queries.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 18,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-03",
    "authors": [
      {
        "authorId": "2185588559",
        "name": "Jiaqi Xue"
      },
      {
        "authorId": "2204787408",
        "name": "Meng Zheng"
      },
      {
        "authorId": "2218546567",
        "name": "Yebowen Hu"
      },
      {
        "authorId": "2278462154",
        "name": "Fei Liu"
      },
      {
        "authorId": "2275119116",
        "name": "Xun Chen"
      },
      {
        "authorId": "2304489773",
        "name": "Qian Lou"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.1665846874966
  },
  {
    "paperId": "b4d1da748ba87f38150ad38e30be68f3584ae462",
    "url": "https://www.semanticscholar.org/paper/b4d1da748ba87f38150ad38e30be68f3584ae462",
    "title": "RAG-Fusion: a New Take on Retrieval-Augmented Generation",
    "abstract": "Infineon has identified a need for engineers, account managers, and customers to rapidly obtain product information. This problem is traditionally addressed with retrieval-augmented generation (RAG) chatbots, but in this study, I evaluated the use of the newly popularized RAG-Fusion method. RAG-Fusion combines RAG and reciprocal rank fusion (RRF) by generating multiple queries, reranking them with reciprocal scores and fusing the documents and scores. Through manually evaluating answers on accuracy, relevance, and comprehensiveness, I found that RAG-Fusion was able to provide accurate and comprehensive answers due to the generated queries contextualizing the original query from various perspectives. However, some answers strayed off topic when the generated queries' relevance to the original query is insufficient. This research marks significant progress in artificial intelligence (AI) and natural language processing (NLP) applications and demonstrates transformations in a global and multiindustry context.",
    "venue": "International Journal on Natural Language Computing",
    "year": 2024,
    "citationCount": 21,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-31",
    "authors": [
      {
        "authorId": "2282961608",
        "name": "Zackary Rackauckas"
      }
    ],
    "source": "semantic_scholar",
    "score": 116.36563680037474
  },
  {
    "paperId": "28e2ecb4183ebc0eec504b12dddc677f8aef8745",
    "url": "https://www.semanticscholar.org/paper/28e2ecb4183ebc0eec504b12dddc677f8aef8745",
    "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
    "abstract": "Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2023,
    "citationCount": 175,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.01431",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-09-04",
    "authors": [
      {
        "authorId": "2115448879",
        "name": "Jiawei Chen"
      },
      {
        "authorId": "2116455765",
        "name": "Hongyu Lin"
      },
      {
        "authorId": "2118233348",
        "name": "Xianpei Han"
      },
      {
        "authorId": "2110832778",
        "name": "Le Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 147.55725992557228
  },
  {
    "paperId": "88884b8806262a4095036041e3567d450dba39f7",
    "url": "https://www.semanticscholar.org/paper/88884b8806262a4095036041e3567d450dba39f7",
    "title": "Active Retrieval Augmented Generation",
    "abstract": "Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 176,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.06983",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-11",
    "authors": [
      {
        "authorId": "2669515",
        "name": "Zhengbao Jiang"
      },
      {
        "authorId": "40027632",
        "name": "Frank F. Xu"
      },
      {
        "authorId": "49715441",
        "name": "Luyu Gao"
      },
      {
        "authorId": "48064856",
        "name": "Zhiqing Sun"
      },
      {
        "authorId": "1409707585",
        "name": "Qian Liu"
      },
      {
        "authorId": "2173509991",
        "name": "Jane Dwivedi-Yu"
      },
      {
        "authorId": "46286308",
        "name": "Yiming Yang"
      },
      {
        "authorId": "144987107",
        "name": "Jamie Callan"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ],
    "source": "semantic_scholar",
    "score": 147.64224598860744
  },
  {
    "paperId": "f5e9e5bbe22f0263be1f1ce88c66978a2b927772",
    "url": "https://www.semanticscholar.org/paper/f5e9e5bbe22f0263be1f1ce88c66978a2b927772",
    "title": "RAGAs: Automated Evaluation of Retrieval Augmented Generation",
    "abstract": "We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAGAs is available at [https://github.com/explodinggradients/ragas]. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",
    "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 112,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.15217",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-09-26",
    "authors": [
      {
        "authorId": "2214583051",
        "name": "ES Shahul"
      },
      {
        "authorId": "2248138289",
        "name": "Jithin James"
      },
      {
        "authorId": "2258950306",
        "name": "Luis Espinosa Anke"
      },
      {
        "authorId": "2265382",
        "name": "S. Schockaert"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.9108172806851
  },
  {
    "paperId": "3c6a6c8de005ef5722a54847747f65922e79d622",
    "url": "https://www.semanticscholar.org/paper/3c6a6c8de005ef5722a54847747f65922e79d622",
    "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
    "abstract": "Retrieval-Augmented Generation (RAG) has recently gained traction in natural language processing. Numerous studies and real-world applications are leveraging its ability to enhance generative models through external information retrieval. Evaluating these RAG systems, however, poses unique challenges due to their hybrid structure and reliance on dynamic knowledge sources. To better understand these challenges, we conduct A Unified Evaluation Process of RAG (Auepora) and aim to provide a comprehensive overview of the evaluation and benchmarks of RAG systems. Specifically, we examine and compare several quantifiable metrics of the Retrieval and Generation components, such as relevance, accuracy, and faithfulness, within the current RAG benchmarks, encompassing the possible output and ground truth pairs. We then analyze the various datasets and metrics, discuss the limitations of current benchmarks, and suggest potential directions to advance the field of RAG benchmarks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 36,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-05-13",
    "authors": [
      {
        "authorId": "2301195826",
        "name": "Hao Yu"
      },
      {
        "authorId": "2301156375",
        "name": "Aoran Gan"
      },
      {
        "authorId": "2263584690",
        "name": "Kai Zhang"
      },
      {
        "authorId": "66187823",
        "name": "Shiwei Tong"
      },
      {
        "authorId": "2301169021",
        "name": "Qi Liu"
      },
      {
        "authorId": "2301158163",
        "name": "Zhaofeng Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.16376868966336
  },
  {
    "paperId": "746b96ee17e329f1085a047116c05e12eaa3925a",
    "url": "https://www.semanticscholar.org/paper/746b96ee17e329f1085a047116c05e12eaa3925a",
    "title": "RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation",
    "abstract": "Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation (RAG) addresses this by incorporating external, relevant documents into the response generation process, thus leveraging non-parametric knowledge alongside LLMs' in-context learning abilities. However, existing RAG implementations primarily focus on initial input for context retrieval, overlooking the nuances of ambiguous or complex queries that necessitate further clarification or decomposition for accurate responses. To this end, we propose learning to Refine Query for Retrieval Augmented Generation (RQ-RAG) in this paper, endeavoring to enhance the model by equipping it with capabilities for explicit rewriting, decomposition, and disambiguation. Our experimental results indicate that our method, when applied to a 7B Llama2 model, surpasses the previous state-of-the-art (SOTA) by an average of 1.9\\% across three single-hop QA datasets, and also demonstrates enhanced performance in handling complex, multi-hop QA datasets. Our code is available at https://github.com/chanchimin/RQ-RAG.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 34,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-31",
    "authors": [
      {
        "authorId": "2151547817",
        "name": "Chi-Min Chan"
      },
      {
        "authorId": "2280285922",
        "name": "Chunpu Xu"
      },
      {
        "authorId": "2032236274",
        "name": "Ruibin Yuan"
      },
      {
        "authorId": "2310574886",
        "name": "Hongyin Luo"
      },
      {
        "authorId": "2239201089",
        "name": "Wei Xue"
      },
      {
        "authorId": "2118270918",
        "name": "Yi-Ting Guo"
      },
      {
        "authorId": "2239395272",
        "name": "Jie Fu"
      }
    ],
    "source": "semantic_scholar",
    "score": 123.3302209223412
  },
  {
    "paperId": "e90435e1ae06fab4efa272f5f46ed74ca0a8cde0",
    "url": "https://www.semanticscholar.org/paper/e90435e1ae06fab4efa272f5f46ed74ca0a8cde0",
    "title": "Evaluating Retrieval Quality in Retrieval-Augmented Generation",
    "abstract": "Evaluating retrieval-augmented generation (RAG) presents challenges, particularly for retrieval models within these systems. Traditional end-to-end evaluation methods are computationally expensive. Furthermore, evaluation of the retrieval model's performance based on query-document relevance labels shows a small correlation with the RAG system's downstream performance. We propose a novel evaluation approach, eRAG, where each document in the retrieval list is individually utilized by the large language model within the RAG system. The output generated for each document is then evaluated based on the downstream task ground truth labels. In this manner, the downstream performance for each document serves as its relevance label. We employ various downstream task metrics to obtain document-level annotations and aggregate them using set-based or ranking metrics. Extensive experiments on a wide range of datasets demonstrate that eRAG achieves a higher correlation with downstream RAG performance compared to baseline methods, with improvements in Kendall's $\\tau$ correlation ranging from 0.168 to 0.494. Additionally, eRAG offers significant computational advantages, improving runtime and consuming up to 50 times less GPU memory than end-to-end evaluation.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2024,
    "citationCount": 33,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-21",
    "authors": [
      {
        "authorId": "2073044451",
        "name": "Alireza Salemi"
      },
      {
        "authorId": "2295731593",
        "name": "Hamed Zamani"
      }
    ],
    "source": "semantic_scholar",
    "score": 128.89540786924243
  },
  {
    "paperId": "965a0969b460f9246158d88fb28e21c5d80d0a8b",
    "url": "https://www.semanticscholar.org/paper/965a0969b460f9246158d88fb28e21c5d80d0a8b",
    "title": "Optimization of hepatological clinical guidelines interpretation by large language models: a retrieval augmented generation-based framework",
    "abstract": null,
    "venue": "npj Digit. Medicine",
    "year": 2024,
    "citationCount": 29,
    "openAccessPdf": {
      "url": "https://www.nature.com/articles/s41746-024-01091-y.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Medicine",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-04-23",
    "authors": [
      {
        "authorId": "2299973097",
        "name": "Simone Kresevic"
      },
      {
        "authorId": "1518680977",
        "name": "M. Giuffr√©"
      },
      {
        "authorId": "2976047",
        "name": "M. Ajƒçeviƒá"
      },
      {
        "authorId": "145196122",
        "name": "A. Accardo"
      },
      {
        "authorId": "38830838",
        "name": "L. Croc√®"
      },
      {
        "authorId": "2256766771",
        "name": "Dennis L. Shung"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.01796072493232
  },
  {
    "paperId": "80478de9c7a81561e2f3dac9b8b1ef3df389ff2d",
    "url": "https://www.semanticscholar.org/paper/80478de9c7a81561e2f3dac9b8b1ef3df389ff2d",
    "title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs",
    "abstract": "Large language models (LLMs) typically utilize the top-k contexts from a retriever in retrieval-augmented generation (RAG). In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data. For generation, we compare our model with many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG benchmarks. Specifically, our Llama3-RankRAG significantly outperforms Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In addition, it also performs comparably to GPT-4 on five RAG benchmarks in the biomedical domain without instruction fine-tuning on biomedical data, demonstrating its superb capability for generalization to new domains.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 25,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-02",
    "authors": [
      {
        "authorId": "2259265562",
        "name": "Yue Yu"
      },
      {
        "authorId": "2253664013",
        "name": "Wei Ping"
      },
      {
        "authorId": "2256582287",
        "name": "Zihan Liu"
      },
      {
        "authorId": "2256656241",
        "name": "Boxin Wang"
      },
      {
        "authorId": "2287859963",
        "name": "Jiaxuan You"
      },
      {
        "authorId": "2256776233",
        "name": "Chao Zhang"
      },
      {
        "authorId": "1911755",
        "name": "Mohammad Shoeybi"
      },
      {
        "authorId": "2264406909",
        "name": "Bryan Catanzaro"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.87144807032223
  },
  {
    "paperId": "daebec92963ab8dea492f0c209bdf57e87bcaa07",
    "url": "https://www.semanticscholar.org/paper/daebec92963ab8dea492f0c209bdf57e87bcaa07",
    "title": "FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research",
    "abstract": "With the advent of Large Language Models (LLMs), the potential of Retrieval Augmented Generation (RAG) techniques have garnered considerable research attention. Numerous novel algorithms and models have been introduced to enhance various aspects of RAG systems. However, the absence of a standardized framework for implementation, coupled with the inherently intricate RAG process, makes it challenging and time-consuming for researchers to compare and evaluate these approaches in a consistent environment. Existing RAG toolkits like LangChain and LlamaIndex, while available, are often heavy and unwieldy, failing to meet the personalized needs of researchers. In response to this challenge, we propose FlashRAG, an efficient and modular open-source toolkit designed to assist researchers in reproducing existing RAG methods and in developing their own RAG algorithms within a unified framework. Our toolkit implements 12 advanced RAG methods and has gathered and organized 32 benchmark datasets. Our toolkit has various features, including customizable modular framework, rich collection of pre-implemented RAG works, comprehensive datasets, efficient auxiliary pre-processing scripts, and extensive and standard evaluation metrics. Our toolkit and resources are available at https://github.com/RUC-NLPIR/FlashRAG.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 15,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-22",
    "authors": [
      {
        "authorId": "4376097",
        "name": "Jiajie Jin"
      },
      {
        "authorId": "1900406",
        "name": "Yutao Zhu"
      },
      {
        "authorId": "2302980850",
        "name": "Xinyu Yang"
      },
      {
        "authorId": "2279440154",
        "name": "Chenghao Zhang"
      },
      {
        "authorId": "2257039188",
        "name": "Zhicheng Dou"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.58883083359672
  },
  {
    "paperId": "e2050c0aa8aa27235c0708b5a5ff741dfd11e2a9",
    "url": "https://www.semanticscholar.org/paper/e2050c0aa8aa27235c0708b5a5ff741dfd11e2a9",
    "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
    "abstract": "\n Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate ‚Äúhallucinated‚Äù content. However, evaluating RAG systems is a challenge. Most benchmarks focus primarily on question answering applications, neglecting other potential scenarios where RAG could be beneficial. Accordingly, in the experiments, these benchmarks often assess only the LLM components of the RAG pipeline or the retriever in knowledge-intensive scenarios, overlooking the impact of external knowledge base construction and the retrieval component on the entire RAG pipeline in non-knowledge-intensive scenarios. To address these issues, this paper constructs a large-scale and more comprehensive benchmark, and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we refer to the CRUD actions that describe interactions between users and knowledge bases, and also categorize the range of RAG applications into four distinct types‚ÄìCreate, Read, Update, and Delete (CRUD). ‚ÄúCreate‚Äù refers to scenarios requiring the generation of original, varied content. ‚ÄúRead‚Äù involves responding to intricate questions in knowledge-intensive situations. ‚ÄúUpdate‚Äù focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. ‚ÄúDelete‚Äù pertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories, we have developed different datasets to evaluate the performance of RAG systems. We also analyze the effects of various components of the RAG system, such as the retriever, context length, knowledge base construction, and LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios\n \n 1\n \n .\n",
    "venue": "ACM Transactions on Information Systems",
    "year": 2024,
    "citationCount": 17,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-30",
    "authors": [
      {
        "authorId": "2187857206",
        "name": "Yuanjie Lyu"
      },
      {
        "authorId": "2268429641",
        "name": "Zhiyu Li"
      },
      {
        "authorId": "2268393907",
        "name": "Simin Niu"
      },
      {
        "authorId": "2268399953",
        "name": "Feiyu Xiong"
      },
      {
        "authorId": "2268400606",
        "name": "Bo Tang"
      },
      {
        "authorId": "2117833477",
        "name": "Wenjin Wang"
      },
      {
        "authorId": "2282083454",
        "name": "Hao Wu"
      },
      {
        "authorId": "2304320758",
        "name": "Huan Liu"
      },
      {
        "authorId": "2277237058",
        "name": "Tong Xu"
      },
      {
        "authorId": "2265580543",
        "name": "Enhong Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.35557636844247
  },
  {
    "paperId": "858cbd99d5a3d2658254d055cd26e06f81050927",
    "url": "https://www.semanticscholar.org/paper/858cbd99d5a3d2658254d055cd26e06f81050927",
    "title": "PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design",
    "abstract": "Retrieval-augmented generation (RAG) can enhance the generation quality of large language models (LLMs) by incorporating external token databases. However, retrievals from large databases can constitute a substantial portion of the overall generation time, particularly when retrievals are periodically performed to align the retrieved content with the latest states of generation. In this paper, we introduce PipeRAG, a novel algorithm-system co-design approach to reduce generation latency and enhance generation quality. PipeRAG integrates (1) pipeline parallelism to enable concurrent retrieval and generation processes, (2) flexible retrieval intervals to maximize the efficiency of pipeline parallelism, and (3) a performance model to automatically balance retrieval quality and latency based on the generation states and underlying hardware. Our evaluation shows that, by combining the three aforementioned methods, PipeRAG achieves up to 2.6$\\times$ speedup in end-to-end generation latency while improving generation quality. These promising results showcase the effectiveness of co-designing algorithms with underlying systems, paving the way for the adoption of PipeRAG in future RAG systems.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 17,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-08",
    "authors": [
      {
        "authorId": "2288227983",
        "name": "Wenqi Jiang"
      },
      {
        "authorId": "1583098764",
        "name": "Shuai Zhang"
      },
      {
        "authorId": "2257285260",
        "name": "Boran Han"
      },
      {
        "authorId": "2290949154",
        "name": "Jie Wang"
      },
      {
        "authorId": "2290866430",
        "name": "Bernie Wang"
      },
      {
        "authorId": "2312328583",
        "name": "Tim Kraska"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.35557636844247
  },
  {
    "paperId": "7326329c09c11aac423ef4910222a16952bb01dc",
    "url": "https://www.semanticscholar.org/paper/7326329c09c11aac423ef4910222a16952bb01dc",
    "title": "RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation",
    "abstract": "Retrieval-Augmented Generation (RAG) has shown significant improvements in various natural language processing tasks by integrating the strengths of large language models (LLMs) and external knowledge databases. However, RAG introduces long sequence generation and leads to high computation and memory costs. We propose RAGCache, a novel multilevel dynamic caching system tailored for RAG. Our analysis benchmarks current RAG systems, pinpointing the performance bottleneck (i.e., long sequence due to knowledge injection) and optimization opportunities (i.e., caching knowledge's intermediate states). Based on these insights, we design RAGCache, which organizes the intermediate states of retrieved knowledge in a knowledge tree and caches them in the GPU and host memory hierarchy. RAGCache proposes a replacement policy that is aware of LLM inference characteristics and RAG retrieval patterns. It also dynamically overlaps the retrieval and inference steps to minimize the end-to-end latency. We implement RAGCache and evaluate it on vLLM, a state-of-the-art LLM inference system and Faiss, a state-of-the-art vector database. The experimental results show that RAGCache reduces the time to first token (TTFT) by up to 4x and improves the throughput by up to 2.1x compared to vLLM integrated with Faiss.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 21,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-18",
    "authors": [
      {
        "authorId": "2170749149",
        "name": "Chao Jin"
      },
      {
        "authorId": "2182609505",
        "name": "Zili Zhang"
      },
      {
        "authorId": "2297730218",
        "name": "Xuanlin Jiang"
      },
      {
        "authorId": "2297499793",
        "name": "Fangyue Liu"
      },
      {
        "authorId": "2305828489",
        "name": "Xin Liu"
      },
      {
        "authorId": "2237080638",
        "name": "Xuanzhe Liu"
      },
      {
        "authorId": "2182349318",
        "name": "Xin Jin"
      }
    ],
    "source": "semantic_scholar",
    "score": 116.36563680037474
  },
  {
    "paperId": "b708e0f49d8e9708bc649debd9a9372748fffa3d",
    "url": "https://www.semanticscholar.org/paper/b708e0f49d8e9708bc649debd9a9372748fffa3d",
    "title": "Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering",
    "abstract": "In customer service technical support, swiftly and accurately retrieving relevant past issues is critical for efficiently resolving customer inquiries. The conventional retrieval methods in retrieval-augmented generation (RAG) for large language models (LLMs) treat a large corpus of past issue tracking tickets as plain text, ignoring the crucial intra-issue structure and inter-issue relations, which limits performance. We introduce a novel customer service question-answering method that amalgamates RAG with a knowledge graph (KG). Our method constructs a KG from historical issues for use in retrieval, retaining the intra-issue structure and inter-issue relations. During the question-answering phase, our method parses consumer queries and retrieves related sub-graphs from the KG to generate answers. This integration of a KG not only improves retrieval accuracy by preserving customer service structure information but also enhances answering quality by mitigating the effects of text segmentation. Empirical assessments on our benchmark datasets, utilizing key retrieval (MRR, Recall@K, NDCG@K) and text generation (BLEU, ROUGE, METEOR) metrics, reveal that our method outperforms the baseline by 77.6% in MRR and by 0.32 in BLEU. Our method has been deployed within LinkedIn's customer service team for approximately six months and has reduced the median per-issue resolution time by 28.6%.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2024,
    "citationCount": 24,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-26",
    "authors": [
      {
        "authorId": "2298963010",
        "name": "Zhentao Xu"
      },
      {
        "authorId": "2298950259",
        "name": "Mark Jerome Cruz"
      },
      {
        "authorId": "2298905096",
        "name": "Matthew Guevara"
      },
      {
        "authorId": "2298945918",
        "name": "Tie Wang"
      },
      {
        "authorId": "2298905834",
        "name": "Manasi Deshpande"
      },
      {
        "authorId": "2298932063",
        "name": "Xiaofeng Wang"
      },
      {
        "authorId": "2299007327",
        "name": "Zheng Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.28313737302301
  },
  {
    "paperId": "a2a4ddbed34916cfa345e957cf060da99685e37b",
    "url": "https://www.semanticscholar.org/paper/a2a4ddbed34916cfa345e957cf060da99685e37b",
    "title": "Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning",
    "abstract": "Large Language Models~(LLMs) have gained immense popularity and are being increasingly applied in various domains. Consequently, ensuring the security of these models is of paramount importance. Jailbreak attacks, which manipulate LLMs to generate malicious content, are recognized as a significant vulnerability. While existing research has predominantly focused on direct jailbreak attacks on LLMs, there has been limited exploration of indirect methods. The integration of various plugins into LLMs, notably Retrieval Augmented Generation~(RAG), which enables LLMs to incorporate external knowledge bases into their response generation such as GPTs, introduces new avenues for indirect jailbreak attacks. To fill this gap, we investigate indirect jailbreak attacks on LLMs, particularly GPTs, introducing a novel attack vector named Retrieval Augmented Generation Poisoning. This method, Pandora, exploits the synergy between LLMs and RAG through prompt manipulation to generate unexpected responses. Pandora uses maliciously crafted content to influence the RAG process, effectively initiating jailbreak attacks. Our preliminary tests show that Pandora successfully conducts jailbreak attacks in four different scenarios, achieving higher success rates than direct attacks, with 64.3\\% for GPT-3.5 and 34.8\\% for GPT-4.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 27,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-13",
    "authors": [
      {
        "authorId": "73776889",
        "name": "Gelei Deng"
      },
      {
        "authorId": "2277563961",
        "name": "Yi Liu"
      },
      {
        "authorId": "2284035726",
        "name": "Kailong Wang"
      },
      {
        "authorId": "22799258",
        "name": "Yuekang Li"
      },
      {
        "authorId": "2277573704",
        "name": "Tianwei Zhang"
      },
      {
        "authorId": "2277257260",
        "name": "Yang Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.98306765262805
  },
  {
    "paperId": "bbf77bd463768a5322a63ffc19322d5c764493e0",
    "url": "https://www.semanticscholar.org/paper/bbf77bd463768a5322a63ffc19322d5c764493e0",
    "title": "Development of a liver disease-Specific large language model chat Interface using retrieval augmented generation.",
    "abstract": "BACKGROUND\nLarge language models (LLMs) have significant capabilities in clinical information processing tasks. Commercially available LLMs, however, are not optimized for clinical uses and are prone to generating hallucinatory information. Retrieval-augmented generation (RAG) is an enterprise architecture that allows embedding of customized data into LLMs. This approach \"specializes\" the LLMs and is thought to reduce hallucinations.\n\n\nMETHODS\nWe developed \"LiVersa,\" a liver disease-specific LLM, by using our institution's protected health information (PHI)-complaint text embedding and LLM platform, \"Versa.\" We conducted RAG on 30 publicly available American Association for the Study of Liver Diseases guidance documents to be incorporated into LiVersa.\n\n\nRESULTS\nWe evaluated LiVersa's performance by conducting two rounds of testing. First, we compared LiVersa's outputs versus those of trainees from a previously published knowledge assessment. LiVersa answered all 10 questions correctly. Second, we asked 15 hepatologists to evaluate the outputs to ten hepatology topic questions generated by LiVersa, OpenAI's ChatGPT 4, and Meta's LLaMA 2. LiVersa's outputs were more accurate but were rated less comprehensive and safe compared to those of ChatGPT 4.\n\n\nDISCUSSION\nIn this demonstration, we built a disease-specific and PHI-compliant LLMs using RAG. While LiVersa demonstrated higher accuracy in answering questions related to hepatology - there were some deficiencies due to limitations set by the number of documents used for RAG. LiVersa will likely require further refinement before potential live deployment. The LiVersa prototype, however, is a proof of concept for utilizing RAG to customize LLMs for clinical use cases.",
    "venue": "Hepatology",
    "year": 2024,
    "citationCount": 15,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-07",
    "authors": [
      {
        "authorId": "47744700",
        "name": "J. Ge"
      },
      {
        "authorId": "2268795342",
        "name": "Steve Sun"
      },
      {
        "authorId": "2268730255",
        "name": "Joseph Owens"
      },
      {
        "authorId": "2268735598",
        "name": "Victor Galvez"
      },
      {
        "authorId": "2278684814",
        "name": "Oksana Gologorskaya"
      },
      {
        "authorId": "2290302786",
        "name": "Jennifer C Lai"
      },
      {
        "authorId": "2215787921",
        "name": "M. Pletcher"
      },
      {
        "authorId": "2268733993",
        "name": "Ki Lai"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.58883083359672
  },
  {
    "paperId": "9ab45aa875b56335303398e84a59a3756cd9d530",
    "url": "https://www.semanticscholar.org/paper/9ab45aa875b56335303398e84a59a3756cd9d530",
    "title": "Graph Retrieval-Augmented Generation: A Survey",
    "abstract": "Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable success in addressing the challenges of Large Language Models (LLMs) without necessitating retraining. By referencing an external knowledge base, RAG refines LLM outputs, effectively mitigating issues such as ``hallucination'', lack of domain-specific knowledge, and outdated information. However, the complex structure of relationships among different entities in databases presents challenges for RAG systems. In response, GraphRAG leverages structural information across entities to enable more precise and comprehensive retrieval, capturing relational knowledge and facilitating more accurate, context-aware responses. Given the novelty and potential of GraphRAG, a systematic review of current technologies is imperative. This paper provides the first comprehensive overview of GraphRAG methodologies. We formalize the GraphRAG workflow, encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced Generation. We then outline the core technologies and training methods at each stage. Additionally, we examine downstream tasks, application domains, evaluation methodologies, and industrial use cases of GraphRAG. Finally, we explore future research directions to inspire further inquiries and advance progress in the field. In order to track recent progress in this field, we set up a repository at \\url{https://github.com/pengboci/GraphRAG-Survey}.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 18,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-08-15",
    "authors": [
      {
        "authorId": "2314827534",
        "name": "Boci Peng"
      },
      {
        "authorId": "2257195454",
        "name": "Yun Zhu"
      },
      {
        "authorId": "2313693489",
        "name": "Yongchao Liu"
      },
      {
        "authorId": "2316431106",
        "name": "Xiaohe Bo"
      },
      {
        "authorId": "2313685962",
        "name": "Haizhou Shi"
      },
      {
        "authorId": "2313754922",
        "name": "Chuntao Hong"
      },
      {
        "authorId": "2316581992",
        "name": "Yan Zhang"
      },
      {
        "authorId": "2257997261",
        "name": "Siliang Tang"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.1665846874966
  },
  {
    "paperId": "2986b2b06173e065c94bae49c7a9a3718dad486c",
    "url": "https://www.semanticscholar.org/paper/2986b2b06173e065c94bae49c7a9a3718dad486c",
    "title": "Reducing hallucination in structured outputs via Retrieval-Augmented Generation",
    "abstract": "A common and fundamental limitation of Generative AI (GenAI) is its propensity to hallucinate. While large language models (LLM) have taken the world by storm, without eliminating or at least reducing hallucinations, real-world GenAI systems may face challenges in user adoption. In the process of deploying an enterprise application that produces workflows based on natural language requirements, we devised a system leveraging Retrieval Augmented Generation (RAG) to greatly improve the quality of the structured output that represents such workflows. Thanks to our implementation of RAG, our proposed system significantly reduces hallucinations in the output and improves the generalization of our LLM in out-of-domain settings. In addition, we show that using a small, well-trained retriever encoder can reduce the size of the accompanying LLM, thereby making deployments of LLM-based systems less resource-intensive.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 17,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-12",
    "authors": [
      {
        "authorId": "2296597690",
        "name": "Patrice B'echard"
      },
      {
        "authorId": "2296597772",
        "name": "Orlando Marquez Ayala"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.35557636844247
  },
  {
    "paperId": "c81c7fb670ac0e27c9d2025d95d68f4752aed99d",
    "url": "https://www.semanticscholar.org/paper/c81c7fb670ac0e27c9d2025d95d68f4752aed99d",
    "title": "Interactive AI With Retrieval-Augmented Generation for Next Generation Networking",
    "abstract": "With the advance of artificial intelligence (AI), the concept of interactive AI (IAI) has been introduced, which can interactively understand and respond not only to human user input but also to dynamic system and network conditions. In this article, we explore an integration and enhancement of IAI in networking. We first review recent developments and future perspectives of AI and then introduce the technology and components of IAI. We then explore the integration of IAI into next-generation networks, focusing on how implicit and explicit interactions can enhance network functionality, improve user experience, and promote efficient network management. Subsequently, we propose an IAI-enabled network management and optimization framework, which consists of environment, perception, action, and brain units. We also design a pluggable large language model (LLM) module and retrieval augmented generation (RAG) module to build the knowledge base and contextual memory for decision-making in the brain unit. We demonstrate through case studies that our IAI framework can effectively perform optimization problem design. Finally, we discuss potential research directions for IAI-based networks.",
    "venue": "IEEE Network",
    "year": 2024,
    "citationCount": 21,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2401.11391",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-01-21",
    "authors": [
      {
        "authorId": "2266463634",
        "name": "Ruichen Zhang"
      },
      {
        "authorId": "2175043468",
        "name": "Hongyang Du"
      },
      {
        "authorId": "2237948862",
        "name": "Yinqiu Liu"
      },
      {
        "authorId": "2266084696",
        "name": "Dusist Niyato"
      },
      {
        "authorId": "2237507634",
        "name": "Jiawen Kang"
      },
      {
        "authorId": "2280290356",
        "name": "Sumei Sun"
      },
      {
        "authorId": "2256129286",
        "name": "Xuemin Shen"
      },
      {
        "authorId": "2265928102",
        "name": "H. V. Poor"
      }
    ],
    "source": "semantic_scholar",
    "score": 122.36563680037474
  },
  {
    "paperId": "d8d0d704446ffaf09b9722360ed76341934ec3a3",
    "url": "https://www.semanticscholar.org/paper/d8d0d704446ffaf09b9722360ed76341934ec3a3",
    "title": "TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models",
    "abstract": "Large language models (LLMs) have raised concerns about potential security threats despite performing significantly in Natural Language Processing (NLP). Backdoor attacks initially verified that LLM is doing substantial harm at all stages, but the cost and robustness have been criticized. Attacking LLMs is inherently risky in security review, while prohibitively expensive. Besides, the continuous iteration of LLMs will degrade the robustness of backdoors. In this paper, we propose TrojanRAG, which employs a joint backdoor attack in the Retrieval-Augmented Generation, thereby manipulating LLMs in universal attack scenarios. Specifically, the adversary constructs elaborate target contexts and trigger sets. Multiple pairs of backdoor shortcuts are orthogonally optimized by contrastive learning, thus constraining the triggering conditions to a parameter subspace to improve the matching. To improve the recall of the RAG for the target contexts, we introduce a knowledge graph to construct structured data to achieve hard matching at a fine-grained level. Moreover, we normalize the backdoor scenarios in LLMs to analyze the real harm caused by backdoors from both attackers' and users' perspectives and further verify whether the context is a favorable tool for jailbreaking models. Extensive experimental results on truthfulness, language understanding, and harmfulness show that TrojanRAG exhibits versatility threats while maintaining retrieval capabilities on normal queries.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 15,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-05-22",
    "authors": [
      {
        "authorId": "1988975660",
        "name": "Pengzhou Cheng"
      },
      {
        "authorId": "2302815380",
        "name": "Yidong Ding"
      },
      {
        "authorId": "2283307652",
        "name": "Tianjie Ju"
      },
      {
        "authorId": "2239160962",
        "name": "Zongru Wu"
      },
      {
        "authorId": "2239100461",
        "name": "Wei Du"
      },
      {
        "authorId": "2302796482",
        "name": "Ping Yi"
      },
      {
        "authorId": "2284695096",
        "name": "Zhuosheng Zhang"
      },
      {
        "authorId": "2267384727",
        "name": "Gongshen Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.58883083359672
  },
  {
    "paperId": "9e015a8524261d4c040c10fa773ddbb02a49ee38",
    "url": "https://www.semanticscholar.org/paper/9e015a8524261d4c040c10fa773ddbb02a49ee38",
    "title": "LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs",
    "abstract": "In traditional RAG framework, the basic retrieval units are normally short. The common retrievers like DPR normally work with 100-word Wikipedia paragraphs. Such a design forces the retriever to search over a large corpus to find the `needle' unit. In contrast, the readers only need to generate answers from the short retrieved units. The imbalanced `heavy' retriever and `light' reader design can lead to sub-optimal performance. The loss of contextual information in the short, chunked units may increase the likelihood of introducing hard negatives during the retrieval stage. Additionally, the reader might not fully leverage the capabilities of recent advancements in LLMs. In order to alleviate the imbalance, we propose a new framework LongRAG, consisting of a `long retriever' and a `long reader'. In the two Wikipedia-based datasets, NQ and HotpotQA, LongRAG processes the entire Wikipedia corpus into 4K-token units by grouping related documents. By increasing the unit size, we significantly reduce the total number of units. This greatly reduces the burden on the retriever, resulting in strong retrieval performance with only a few (less than 8) top units. Without requiring any training, LongRAG achieves an EM of 62.7% on NQ and 64.3% on HotpotQA, which are on par with the (fully-trained) SoTA model. Furthermore, we test on two non-Wikipedia-based datasets, Qasper and MultiFieldQA-en. LongRAG processes each individual document as a single (long) unit rather than chunking them into smaller units. By doing so, we achieve an F1 score of 25.9% on Qasper and 57.5% on MultiFieldQA-en. Our study offers insights into the future roadmap for combining RAG with long-context LLMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 24,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-21",
    "authors": [
      {
        "authorId": "2112347577",
        "name": "Ziyan Jiang"
      },
      {
        "authorId": "2307982793",
        "name": "Xueguang Ma"
      },
      {
        "authorId": "2253811180",
        "name": "Wenhu Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.28313737302301
  },
  {
    "paperId": "ccb5afb760a73f5507e31995397f80960db7842d",
    "url": "https://www.semanticscholar.org/paper/ccb5afb760a73f5507e31995397f80960db7842d",
    "title": "Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach",
    "abstract": "Retrieval Augmented Generation (RAG) has been a powerful tool for Large Language Models (LLMs) to efficiently process overly lengthy contexts. However, recent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to understand long contexts directly. We conduct a comprehensive comparison between RAG and long-context (LC) LLMs, aiming to leverage the strengths of both. We benchmark RAG and LC across various public datasets using three latest LLMs. Results reveal that when resourced sufficiently, LC consistently outperforms RAG in terms of average performance. However, RAG‚Äôs significantly lower cost remains a distinct advantage. Based on this observation, we propose Self-Route, a simple yet effective method that routes queries to RAG or LC based on model self-reflection. Self-Route significantly reduces the computation cost while maintaining a comparable performance to LC. Our findings provide a guideline for long-context applications of LLMs using RAG and LC.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-23",
    "authors": [
      {
        "authorId": "2312752299",
        "name": "Zhuowan Li"
      },
      {
        "authorId": "2249775049",
        "name": "Cheng Li"
      },
      {
        "authorId": "2249838528",
        "name": "Mingyang Zhang"
      },
      {
        "authorId": "2253452633",
        "name": "Qiaozhu Mei"
      },
      {
        "authorId": "2240516450",
        "name": "Michael Bendersky"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "1cc6cc4960f7df59e7813d9a8e11098d0a0d0720",
    "url": "https://www.semanticscholar.org/paper/1cc6cc4960f7df59e7813d9a8e11098d0a0d0720",
    "title": "DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models",
    "abstract": "Dynamic retrieval augmented generation (RAG) paradigm actively decides when and what to retrieve during the text generation process of Large Language Models (LLMs). There are two key elements of this paradigm: identifying the optimal moment to activate the retrieval module (deciding when to retrieve) and crafting the appropriate query once retrieval is triggered (determining what to retrieve). However, current dynamic RAG methods fall short in both aspects. Firstly, the strategies for deciding when to retrieve often rely on static rules. Moreover, the strategies for deciding what to retrieve typically limit themselves to the LLM's most recent sentence or the last few tokens, while the LLM's real-time information needs may span across the entire context. To overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic Retrieval Augmented Generation based on the real-time Information Needs of LLMs. Our framework is specifically designed to make decisions on when and what to retrieve based on the LLM's real-time information needs during the text generation process. We evaluate DRAGIN along with existing methods comprehensively over 4 knowledge-intensive generation datasets. Experimental results show that DRAGIN achieves superior performance on all tasks, demonstrating the effectiveness of our method. We have open-sourced all the code, data, and models in GitHub: https://github.com/oneal2000/DRAGIN/tree/main",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 14,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-15",
    "authors": [
      {
        "authorId": "2147219374",
        "name": "Weihang Su"
      },
      {
        "authorId": "2291993837",
        "name": "Yichen Tang"
      },
      {
        "authorId": "2256982003",
        "name": "Qingyao Ai"
      },
      {
        "authorId": "47039225",
        "name": "Zhijing Wu"
      },
      {
        "authorId": "2260835922",
        "name": "Yiqun Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "50ea1f3387e0e1a5f45722fa9d657982c78c2ce8",
    "url": "https://www.semanticscholar.org/paper/50ea1f3387e0e1a5f45722fa9d657982c78c2ce8",
    "title": "CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs for Legal Question Answering",
    "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Model (LLM) output by providing prior knowledge as context to input. This is beneficial for knowledge-intensive and expert reliant tasks, including legal question-answering, which require evidence to validate generated text outputs. We highlight that Case-Based Reasoning (CBR) presents key opportunities to structure retrieval as part of the RAG process in an LLM. We introduce CBR-RAG, where CBR cycle's initial retrieval stage, its indexing vocabulary, and similarity knowledge containers are used to enhance LLM queries with contextually relevant cases. This integration augments the original LLM query, providing a richer prompt. We present an evaluation of CBR-RAG, and examine different representations (i.e. general and domain-specific embeddings) and methods of comparison (i.e. inter, intra and hybrid similarity) on the task of legal question-answering. Our results indicate that the context provided by CBR's case reuse enforces similarity between relevant components of the questions and the evidence base leading to significant improvements in the quality of generated answers.",
    "venue": "International Conference on Case-Based Reasoning",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-04",
    "authors": [
      {
        "authorId": "1784639",
        "name": "N. Wiratunga"
      },
      {
        "authorId": "66680254",
        "name": "Ramitha Abeyratne"
      },
      {
        "authorId": "2288593555",
        "name": "Lasal Jayawardena"
      },
      {
        "authorId": "143999525",
        "name": "Kyle Martin"
      },
      {
        "authorId": "144004005",
        "name": "Stewart Massie"
      },
      {
        "authorId": "1399085102",
        "name": "Ikechukwu Nkisi-Orji"
      },
      {
        "authorId": "2837941",
        "name": "R. Weerasinghe"
      },
      {
        "authorId": "2633228",
        "name": "A. Liret"
      },
      {
        "authorId": "2217276579",
        "name": "Bruno Fleisch"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "810b3f4475f22f6ca0f1bded3b8523f3cdebee8d",
    "url": "https://www.semanticscholar.org/paper/810b3f4475f22f6ca0f1bded3b8523f3cdebee8d",
    "title": "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems",
    "abstract": "Large Language Models (LLMs) has shown exceptional capabilities in many natual language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system. To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation. We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens. Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their behavior to diverse task requirements. Meanwhile, evaluation tokens gauge the relevance score between the dialogue context and the retrieved evidence. In addition, we carefully design a self-refinement mechanism to iteratively refine the generated response considering 1) the consistency scores between the generated response and retrieved evidence; and 2) the relevance scores. Experiments on two personalized datasets (DuLeMon and KBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge source selection and response generation task with itself as a retriever in a unified manner. Extensive analyses and discussions are provided for shedding some new perspectives for personalized dialogue systems.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-24",
    "authors": [
      {
        "authorId": "22642319",
        "name": "Hongru Wang"
      },
      {
        "authorId": "1896870270",
        "name": "Wenyu Huang"
      },
      {
        "authorId": "2280964158",
        "name": "Yang Deng"
      },
      {
        "authorId": "2248766573",
        "name": "Rui Wang"
      },
      {
        "authorId": "2108726649",
        "name": "Zezhong Wang"
      },
      {
        "authorId": "2268629268",
        "name": "Yufei Wang"
      },
      {
        "authorId": "2248150493",
        "name": "Fei Mi"
      },
      {
        "authorId": "2280059593",
        "name": "Jeff Z. Pan"
      },
      {
        "authorId": "2237563835",
        "name": "Kam-Fai Wong"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "403ff3523a949743290d4ce7b770da28ef3e1953",
    "url": "https://www.semanticscholar.org/paper/403ff3523a949743290d4ce7b770da28ef3e1953",
    "title": "Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering",
    "abstract": "Multi-Hop Question Answering (MHQA) tasks present a significant challenge for large language models (LLMs) due to the intensive knowledge required. Current solutions, like Retrieval-Augmented Generation, typically retrieve potential documents from an external corpus to read an answer. However, the performance of this retrieve-then-read paradigm is constrained by the retriever and the inevitable noise in the retrieved documents. To mitigate these challenges, we introduce a novel generate-then-ground (GenGround) framework, synergizing the parametric knowledge of LLMs and external documents to solve a multi-hop question. GenGround empowers LLMs to alternate two phases until the final answer is derived: (1) formulate a simpler, single-hop question and directly generate the answer; (2) ground the question-answer pair in retrieved documents, amending any wrong predictions in the answer. We also propose an instructional grounding distillation method to generalize our method into smaller models. Extensive experiments conducted on four datasets illustrate the superiority of our method.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-21",
    "authors": [
      {
        "authorId": "2195381022",
        "name": "Zhengliang Shi"
      },
      {
        "authorId": "2307994549",
        "name": "Shuo Zhang"
      },
      {
        "authorId": "2153198380",
        "name": "Weiwei Sun"
      },
      {
        "authorId": "2268683617",
        "name": "Shen Gao"
      },
      {
        "authorId": "1749477",
        "name": "Pengjie Ren"
      },
      {
        "authorId": "1721165",
        "name": "Zhumin Chen"
      },
      {
        "authorId": "2260895127",
        "name": "Zhaochun Ren"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "27f8af480211586d07ff5ee6441ff6724ce85f4e",
    "url": "https://www.semanticscholar.org/paper/27f8af480211586d07ff5ee6441ff6724ce85f4e",
    "title": "PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers",
    "abstract": "In this paper, we conduct a study to utilize LLMs as a solution for decision making that requires complex data analysis. We define **Decision QA** as the task of answering the best decision, d_{best}, for a decision-making question Q, business rules R and a database D. Since there is no benchmark that can examine Decision QA, we propose Decision QA benchmark, **DQA**. It has two scenarios, Locating and Building, constructed from two video games (Europa Universalis IV and Victoria 3) that have almost the same goal as Decision QA. To address Decision QA effectively, we also propose a new RAG technique called the *iterative plan-then-retrieval augmented generation* (**PlanRAG**). Our PlanRAG-based LM generates the plan for decision making as the first step, and the retriever generates the queries for data analysis as the second step. The proposed method outperforms the state-of-the-art iterative RAG method by 15.8% in the Locating scenario and by 7.4% in the Building scenario, respectively. We release our code and benchmark at https://github.com/myeon9h/PlanRAG.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-18",
    "authors": [
      {
        "authorId": "2307268734",
        "name": "Myeonghwa Lee"
      },
      {
        "authorId": "2307439827",
        "name": "Seonho An"
      },
      {
        "authorId": "2306662510",
        "name": "Min-Soo Kim"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "4df2b1e7d54fe5ad81dc2ed6774b93ef7891b3c8",
    "url": "https://www.semanticscholar.org/paper/4df2b1e7d54fe5ad81dc2ed6774b93ef7891b3c8",
    "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems",
    "abstract": "Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. We make our code and datasets publicly available on Github.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 65,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-16",
    "authors": [
      {
        "authorId": "2127522115",
        "name": "Jon Saad-Falcon"
      },
      {
        "authorId": "144112155",
        "name": "O. Khattab"
      },
      {
        "authorId": "2254255092",
        "name": "Christopher Potts"
      },
      {
        "authorId": "2253469012",
        "name": "Matei Zaharia"
      }
    ],
    "source": "semantic_scholar",
    "score": 132.84482113039638
  },
  {
    "paperId": "179a60cc2469859543aac6abc81c97752de50446",
    "url": "https://www.semanticscholar.org/paper/179a60cc2469859543aac6abc81c97752de50446",
    "title": "Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs",
    "abstract": "Multimodal LLMs are the natural evolution of LLMs, and enlarge their capabilities so as to work beyond the pure textual modality. As research is being carried out to design novel architectures and vision-and-language adapters, in this paper we concentrate on endowing such models with the capability of answering questions that require external knowledge. Our approach, termed Wiki-LLaVA, aims at integrating an external knowledge source of multimodal documents, which is accessed through a hierarchical retrieval pipeline. Relevant passages, using this approach, are retrieved from the external knowledge source and employed as additional context for the LLM, augmenting the effectiveness and precision of generated dialogues. We conduct extensive experiments on datasets tailored for visual question answering with external data and demonstrate the appropriateness of our approach.",
    "venue": "2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
    "year": 2024,
    "citationCount": 23,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2404.15406",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-23",
    "authors": [
      {
        "authorId": "2238815102",
        "name": "Davide Caffagni"
      },
      {
        "authorId": "2238814159",
        "name": "Federico Cocchi"
      },
      {
        "authorId": "2202986093",
        "name": "Nicholas Moratelli"
      },
      {
        "authorId": "2179325162",
        "name": "Sara Sarto"
      },
      {
        "authorId": "3468983",
        "name": "Marcella Cornia"
      },
      {
        "authorId": "1843795",
        "name": "L. Baraldi"
      },
      {
        "authorId": "1741922",
        "name": "R. Cucchiara"
      }
    ],
    "source": "semantic_scholar",
    "score": 123.67080745521918
  },
  {
    "paperId": "21dc5aa608721f7e8a09e7dd9de8e48a71d9f0c9",
    "url": "https://www.semanticscholar.org/paper/21dc5aa608721f7e8a09e7dd9de8e48a71d9f0c9",
    "title": "Is My Data in Your Retrieval Database? Membership Inference Attacks Against Retrieval Augmented Generation",
    "abstract": "Retrieval Augmented Generation (RAG) systems have shown great promise in natural language processing. However, their reliance on data stored in a retrieval database, which may contain proprietary or sensitive information, introduces new privacy concerns. Specifically, an attacker may be able to infer whether a certain text passage appears in the retrieval database by observing the outputs of the RAG system, an attack known as a Membership Inference Attack (MIA). Despite the significance of this threat, MIAs against RAG systems have yet remained under-explored. This study addresses this gap by introducing an efficient and easy-to-use method for conducting MIA against RAG systems. We demonstrate the effectiveness of our attack using two benchmark datasets and multiple generative models, showing that the membership of a document in the retrieval database can be efficiently determined through the creation of an appropriate prompt in both black-box and gray-box settings. Moreover, we introduce an initial defense strategy based on adding instructions to the RAG template, which shows high effectiveness for some datasets and models. Our findings highlight the importance of implementing security countermeasures in deployed RAG systems and developing more advanced defenses to protect the privacy and security of retrieval databases.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-30",
    "authors": [
      {
        "authorId": "39868788",
        "name": "Maya Anderson"
      },
      {
        "authorId": "2291066051",
        "name": "Guy Amit"
      },
      {
        "authorId": "2652502",
        "name": "Abigail Goldsteen"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "9a946c503b6e799b3d57375b6edfaf4e24febcea",
    "url": "https://www.semanticscholar.org/paper/9a946c503b6e799b3d57375b6edfaf4e24febcea",
    "title": "Searching for Best Practices in Retrieval-Augmented Generation",
    "abstract": "Retrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a ‚Äúretrieval as generation‚Äù strategy.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-01",
    "authors": [
      {
        "authorId": "2273537815",
        "name": "Xiaohua Wang"
      },
      {
        "authorId": "2308276345",
        "name": "Zhenghua Wang"
      },
      {
        "authorId": "2292070745",
        "name": "Xuan Gao"
      },
      {
        "authorId": "2308226671",
        "name": "Feiran Zhang"
      },
      {
        "authorId": "2308043953",
        "name": "Yixin Wu"
      },
      {
        "authorId": "2308044030",
        "name": "Zhibo Xu"
      },
      {
        "authorId": "2308036711",
        "name": "Tianyuan Shi"
      },
      {
        "authorId": "2309182278",
        "name": "Zhengyuan Wang"
      },
      {
        "authorId": "2309656885",
        "name": "Shizheng Li"
      },
      {
        "authorId": "2309176521",
        "name": "Qi Qian"
      },
      {
        "authorId": "2292032843",
        "name": "Ruicheng Yin"
      },
      {
        "authorId": "2220896023",
        "name": "Changze Lv"
      },
      {
        "authorId": "2257315404",
        "name": "Xiaoqing Zheng"
      },
      {
        "authorId": "2257129987",
        "name": "Xuanjing Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "320b86d8a23da014d018db23a5f43b94368dd749",
    "url": "https://www.semanticscholar.org/paper/320b86d8a23da014d018db23a5f43b94368dd749",
    "title": "Machine Against the RAG: Jamming Retrieval-Augmented Generation with Blocker Documents",
    "abstract": "Retrieval-augmented generation (RAG) systems respond to queries by retrieving relevant documents from a knowledge database and applying an LLM to the retrieved documents. We demonstrate that RAG systems that operate on databases with untrusted content are vulnerable to denial-of-service attacks we call jamming. An adversary can add a single ``blocker'' document to the database that will be retrieved in response to a specific query and result in the RAG system not answering this query - ostensibly because it lacks the relevant information or because the answer is unsafe. We describe and measure the efficacy of several methods for generating blocker documents, including a new method based on black-box optimization. This method (1) does not rely on instruction injection, (2) does not require the adversary to know the embedding or LLM used by the target RAG system, and (3) does not rely on an auxiliary LLM. We evaluate jamming attacks on several LLMs and embeddings and demonstrate that the existing safety metrics for LLMs do not capture their vulnerability to jamming. We then discuss defenses against blocker documents.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-09",
    "authors": [
      {
        "authorId": "1432237131",
        "name": "Avital Shafran"
      },
      {
        "authorId": "39347554",
        "name": "R. Schuster"
      },
      {
        "authorId": "2302179436",
        "name": "Vitaly Shmatikov"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "9af8bccf3e42996cbb198a6ceccafa2a084689f6",
    "url": "https://www.semanticscholar.org/paper/9af8bccf3e42996cbb198a6ceccafa2a084689f6",
    "title": "HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction",
    "abstract": "Extraction and interpretation of intricate information from unstructured text data arising in financial applications, such as earnings call transcripts, present substantial challenges to large language models (LLMs) even using the current best practices to use Retrieval Augmented Generation (RAG) (referred to as VectorRAG techniques which utilize vector databases for information retrieval) due to challenges such as domain specific terminology and complex formats of the documents. We introduce a novel approach based on a combination, called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called GraphRAG) and VectorRAG techniques to enhance question-answer (Q&A) systems for information extraction from financial documents that is shown to be capable of generating accurate and contextually relevant answers. Using experiments on a set of financial earning call transcripts documents which come in the form of Q&A format, and hence provide a natural set of pairs of ground-truth Q&As, we show that HybridRAG which retrieves context from both vector database and KG outperforms both traditional VectorRAG and GraphRAG individually when evaluated at both the retrieval and generation stages in terms of retrieval accuracy and answer generation. The proposed technique has applications beyond the financial domain",
    "venue": "International Conference on AI in Finance",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Economics",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-09",
    "authors": [
      {
        "authorId": "23570698",
        "name": "Bhaskarjit Sarmah"
      },
      {
        "authorId": "2315812262",
        "name": "Benika Hall"
      },
      {
        "authorId": "2315811806",
        "name": "Rohan Rao"
      },
      {
        "authorId": "2315808930",
        "name": "Sunil Patel"
      },
      {
        "authorId": "2258960763",
        "name": "Stefano Pasquali"
      },
      {
        "authorId": "2258962314",
        "name": "Dhagash Mehta"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "c072d217732edb066de2192ab9ad6b02aec9c7a0",
    "url": "https://www.semanticscholar.org/paper/c072d217732edb066de2192ab9ad6b02aec9c7a0",
    "title": "Financial Report Chunking for Effective Retrieval Augmented Generation",
    "abstract": "Chunking information is a key step in Retrieval Augmented Generation (RAG). Current research primarily centers on paragraph-level chunking. This approach treats all texts as equal and neglects the information contained in the structure of documents. We propose an expanded approach to chunk documents by moving beyond mere paragraph-level chunking to chunk primary by structural element components of documents. Dissecting documents into these constituent elements creates a new way to chunk documents that yields the best chunk size without tuning. We introduce a novel framework that evaluates how chunking based on element types annotated by document understanding models contributes to the overall context and accuracy of the information retrieved. We also demonstrate how this approach impacts RAG assisted Question&Answer task performance. Our research includes a comprehensive analysis of various element types, their role in effective information retrieval, and the impact they have on the quality of RAG outputs. Findings support that element type based chunking largely improve RAG results on financial reporting. Through this research, we are also able to answer how to uncover highly accurate RAG.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-05",
    "authors": [
      {
        "authorId": "2284073889",
        "name": "Antonio Jimeno-Yepes"
      },
      {
        "authorId": "2283308522",
        "name": "Yao You"
      },
      {
        "authorId": "2283307145",
        "name": "Jan Milczek"
      },
      {
        "authorId": "2283306503",
        "name": "Sebastian Laverde"
      },
      {
        "authorId": "2288440307",
        "name": "Renyu Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "160924af0791331ec8fa5a3d526ea125355f3b8b",
    "url": "https://www.semanticscholar.org/paper/160924af0791331ec8fa5a3d526ea125355f3b8b",
    "title": "Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting",
    "abstract": "Retrieval augmented generation (RAG) combines the generative abilities of large language models (LLMs) with external knowledge sources to provide more accurate and up-to-date responses. Recent RAG advancements focus on improving retrieval outcomes through iterative LLM refinement or self-critique capabilities acquired through additional instruction tuning of LLMs. In this work, we introduce Speculative RAG - a framework that leverages a larger generalist LM to efficiently verify multiple RAG drafts produced in parallel by a smaller, distilled specialist LM. Each draft is generated from a distinct subset of retrieved documents, offering diverse perspectives on the evidence while reducing input token counts per draft. This approach enhances comprehension of each subset and mitigates potential position bias over long context. Our method accelerates RAG by delegating drafting to the smaller specialist LM, with the larger generalist LM performing a single verification pass over the drafts. Extensive experiments demonstrate that Speculative RAG achieves state-of-the-art performance with reduced latency on TriviaQA, MuSiQue, PubHealth, and ARC-Challenge benchmarks. It notably enhances accuracy by up to 12.97% while reducing latency by 51% compared to conventional RAG systems on PubHealth.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-11",
    "authors": [
      {
        "authorId": "1762478",
        "name": "Zilong Wang"
      },
      {
        "authorId": "2278799290",
        "name": "Zifeng Wang"
      },
      {
        "authorId": "2253355959",
        "name": "Long T. Le"
      },
      {
        "authorId": "2253804927",
        "name": "Huaixiu Steven Zheng"
      },
      {
        "authorId": "2301179512",
        "name": "Swaroop Mishra"
      },
      {
        "authorId": "2066252171",
        "name": "Vincent Perot"
      },
      {
        "authorId": "2284259968",
        "name": "Yuwei Zhang"
      },
      {
        "authorId": "2257348141",
        "name": "Anush Mattapalli"
      },
      {
        "authorId": "2313338667",
        "name": "Ankur Taly"
      },
      {
        "authorId": "2310730051",
        "name": "Jingbo Shang"
      },
      {
        "authorId": "2278969944",
        "name": "Chen-Yu Lee"
      },
      {
        "authorId": "2305619900",
        "name": "Tomas Pfister"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "d7e70b562784659721e9c12e162c3b0b1a023b6d",
    "url": "https://www.semanticscholar.org/paper/d7e70b562784659721e9c12e162c3b0b1a023b6d",
    "title": "Integrating Retrieval-Augmented Generation with Large Language Models in Nephrology: Advancing Practical Applications",
    "abstract": "The integration of large language models (LLMs) into healthcare, particularly in nephrology, represents a significant advancement in applying advanced technology to patient care, medical research, and education. These advanced models have progressed from simple text processors to tools capable of deep language understanding, offering innovative ways to handle health-related data, thus improving medical practice efficiency and effectiveness. A significant challenge in medical applications of LLMs is their imperfect accuracy and/or tendency to produce hallucinations‚Äîoutputs that are factually incorrect or irrelevant. This issue is particularly critical in healthcare, where precision is essential, as inaccuracies can undermine the reliability of these models in crucial decision-making processes. To overcome these challenges, various strategies have been developed. One such strategy is prompt engineering, like the chain-of-thought approach, which directs LLMs towards more accurate responses by breaking down the problem into intermediate steps or reasoning sequences. Another one is the retrieval-augmented generation (RAG) strategy, which helps address hallucinations by integrating external data, enhancing output accuracy and relevance. Hence, RAG is favored for tasks requiring up-to-date, comprehensive information, such as in clinical decision making or educational applications. In this article, we showcase the creation of a specialized ChatGPT model integrated with a RAG system, tailored to align with the KDIGO 2023 guidelines for chronic kidney disease. This example demonstrates its potential in providing specialized, accurate medical advice, marking a step towards more reliable and efficient nephrology practices.",
    "venue": "Medicina",
    "year": 2024,
    "citationCount": 23,
    "openAccessPdf": {
      "url": "https://www.mdpi.com/1648-9144/60/3/445/pdf?version=1709877206",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "Review",
      "JournalArticle"
    ],
    "publicationDate": "2024-03-01",
    "authors": [
      {
        "authorId": "2260930470",
        "name": "Jing Miao"
      },
      {
        "authorId": "3464205",
        "name": "C. Thongprayoon"
      },
      {
        "authorId": "2248691301",
        "name": "S. Suppadungsuk"
      },
      {
        "authorId": "2086832167",
        "name": "Oscar A. Garcia Valencia"
      },
      {
        "authorId": "4112984",
        "name": "W. Cheungpasitporn"
      }
    ],
    "source": "semantic_scholar",
    "score": 117.67080745521918
  },
  {
    "paperId": "996dc8160751faf0175441cf2cca4fc36a2da84f",
    "url": "https://www.semanticscholar.org/paper/996dc8160751faf0175441cf2cca4fc36a2da84f",
    "title": "Stochastic RAG: End-to-End Retrieval-Augmented Generation through Expected Utility Maximization",
    "abstract": "This paper introduces Stochastic RAG--a novel approach for end-to-end optimization of retrieval-augmented generation (RAG) models that relaxes the simplifying assumptions of marginalization and document independence, made in most prior work. Stochastic RAG casts the retrieval process in RAG as a stochastic sampling without replacement process. Through this formulation, we employ straight-through Gumbel-top-k that provides a differentiable approximation for sampling without replacement and enables effective end-to-end optimization for RAG. We conduct extensive experiments on seven diverse datasets on a wide range of tasks, from open-domain question answering to fact verification to slot-filling for relation extraction and to dialogue systems. By applying this optimization method to a recent and effective RAG model, we advance state-of-the-art results on six out of seven datasets.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2024,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-05",
    "authors": [
      {
        "authorId": "2293725953",
        "name": "Hamed Zamani"
      },
      {
        "authorId": "2240516450",
        "name": "Michael Bendersky"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.27359974682
  },
  {
    "paperId": "9111d6632e3ad648e65c57c52fd945641ccbdac2",
    "url": "https://www.semanticscholar.org/paper/9111d6632e3ad648e65c57c52fd945641ccbdac2",
    "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
    "abstract": "Language Models (LMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LMs in handling low-frequency entities on question answering tasks. We conduct extensive experiments on twelve LMs of varying size and type and different fine tuning, data augmentation, and retrieval models. Our findings indicate that while FT boosts the performance across entities of varying popularity, RAG surpasses FT by a large margin particularly for least popular factual knowledge. Additionally, the success of both RAG and FT approaches is amplified by improving retrieval and data augmentation techniques. Fine tuning, while beneficial for small LMs, requires extensive resources. To address this issue, we propose the new Stimulus RAG approach that surpasses the effectiveness of fine tuning based approaches, thereby eliminating the need for the costly data augmentation and fine tuning step for enriching LMs with less popular factual knowledge. The code is available at \\url{https://github.com/informagi/RAGvsFT}.",
    "venue": "SIGIR-AP",
    "year": 2024,
    "citationCount": 11,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3673791.3698415",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-03",
    "authors": [
      {
        "authorId": "2165569122",
        "name": "Heydar Soudani"
      },
      {
        "authorId": "1713134",
        "name": "E. Kanoulas"
      },
      {
        "authorId": "1951737",
        "name": "Faegheh Hasibi"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "a82a1be7639301ea47ebcb346f6119065b68b3d0",
    "url": "https://www.semanticscholar.org/paper/a82a1be7639301ea47ebcb346f6119065b68b3d0",
    "title": "GRAG: Graph Retrieval-Augmented Generation",
    "abstract": "Naive Retrieval-Augmented Generation (RAG) focuses on individual documents during retrieval and, as a result, falls short in handling networked documents which are very popular in many applications such as citation graphs, social media, and knowledge graphs. To overcome this limitation, we introduce Graph Retrieval-Augmented Generation (GRAG), which tackles the fundamental challenges in retrieving textual subgraphs and integrating the joint textual and topological information into Large Language Models (LLMs) to enhance its generation. To enable efficient textual subgraph retrieval, we propose a novel divide-and-conquer strategy that retrieves the optimal subgraph structure in linear time. To achieve graph context-aware generation, incorporate textual graphs into LLMs through two complementary views-the text view and the graph view-enabling LLMs to more effectively comprehend and utilize the graph context. Extensive experiments on graph reasoning benchmarks demonstrate that in scenarios requiring multi-hop reasoning on textual graphs, our GRAG approach significantly outperforms current state-of-the-art RAG methods.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-26",
    "authors": [
      {
        "authorId": "2257349205",
        "name": "Yuntong Hu"
      },
      {
        "authorId": "2303415589",
        "name": "Zhihan Lei"
      },
      {
        "authorId": "2021011947",
        "name": "Zhengwu Zhang"
      },
      {
        "authorId": "2257038621",
        "name": "Bo Pan"
      },
      {
        "authorId": "2284591355",
        "name": "Chen Ling"
      },
      {
        "authorId": "2257314969",
        "name": "Liang Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "d66cde9b86795f265dfc0260317cb70f54afcb90",
    "url": "https://www.semanticscholar.org/paper/d66cde9b86795f265dfc0260317cb70f54afcb90",
    "title": "Advanced Embedding Techniques in Multimodal Retrieval Augmented Generation A Comprehensive Study on Cross Modal AI Applications",
    "abstract": "Our study presents significant advancements in the handling of multimodal data through an extended Retrieval-Augmented Generation (RAG) model. By integrating advanced embedding techniques, efficient retrieval mechanisms, and robust generative capabilities, our model demonstrates notable improvements in retrieval accuracy, real-time efficiency, generative quality, and scalability. The retrieval accuracy of our model reached 85%, showing a 10% improvement over existing benchmarks. Furthermore, the retrieval time was reduced by 40%, enhancing real-time application performance. The model's generative quality was also significantly improved, with BLEU and ROUGE scores increasing by 15% and 12%, respectively. These results validate the effectiveness of our approach and its applicability to various AI applications, including information retrieval, recommendation systems, and content creation. Future research directions include the integration of additional modalities and further optimization of retrieval mechanisms to broaden the applicability of our model.",
    "venue": "Journal of Computing and Electronic Information Management",
    "year": 2024,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-29",
    "authors": [
      {
        "authorId": "2319196234",
        "name": "Ren Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "2bb9a87bdfc8a35bc1813e5a88180f43615785a8",
    "url": "https://www.semanticscholar.org/paper/2bb9a87bdfc8a35bc1813e5a88180f43615785a8",
    "title": "Improving Retrieval-Augmented Generation in Medicine with Iterative Follow-up Questions",
    "abstract": "The emergent abilities of large language models (LLMs) have demonstrated great potential in solving medical questions. They can possess considerable medical knowledge, but may still hallucinate and are inflexible in the knowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed to enhance the medical question-answering capabilities of LLMs with external knowledge bases, it may still fail in complex cases where multiple rounds of information-seeking are required. To address such an issue, we propose iterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up queries based on previous information-seeking attempts. In each iteration of i-MedRAG, the follow-up queries will be answered by a vanilla RAG system and they will be further used to guide the query generation in the next iteration. Our experiments show the improved performance of various LLMs brought by i-MedRAG compared with vanilla RAG on complex questions from clinical vignettes in the United States Medical Licensing Examination (USMLE), as well as various knowledge tests in the Massive Multitask Language Understanding (MMLU) dataset. Notably, our zero-shot i-MedRAG outperforms all existing prompt engineering and fine-tuning methods on GPT-3.5, achieving an accuracy of 69.68% on the MedQA dataset. In addition, we characterize the scaling properties of i-MedRAG with different iterations of follow-up queries and different numbers of queries per iteration. Our case studies show that i-MedRAG can flexibly ask follow-up queries to form reasoning chains, providing an in-depth analysis of medical questions. To the best of our knowledge, this is the first-of-its-kind study on incorporating follow-up queries into medical RAG.",
    "venue": "Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-01",
    "authors": [
      {
        "authorId": "2048053804",
        "name": "Guangzhi Xiong"
      },
      {
        "authorId": "2261406713",
        "name": "Qiao Jin"
      },
      {
        "authorId": "2314146159",
        "name": "Xiao Wang"
      },
      {
        "authorId": "2314147661",
        "name": "Minjia Zhang"
      },
      {
        "authorId": "2237094367",
        "name": "Zhiyong Lu"
      },
      {
        "authorId": "2265729351",
        "name": "Aidong Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "7b00cb1fe1773f964d123dab6d4812c7bc63de06",
    "url": "https://www.semanticscholar.org/paper/7b00cb1fe1773f964d123dab6d4812c7bc63de06",
    "title": "Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems",
    "abstract": "Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs). We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection. The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. We also study multiple effects of RAG setup on the extractability of data, indicating that following unexpected instructions to regurgitate data can be an outcome of failure in effectively utilizing contexts for modern LMs, and further show that such vulnerability can be greatly mitigated by position bias elimination strategies. Extending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,000 words by prompting the GPTs with only 100 queries generated by themselves.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-27",
    "authors": [
      {
        "authorId": "2186056193",
        "name": "Zhenting Qi"
      },
      {
        "authorId": "2119078297",
        "name": "Hanlin Zhang"
      },
      {
        "authorId": "2273055197",
        "name": "Eric P. Xing"
      },
      {
        "authorId": "144695232",
        "name": "S. Kakade"
      },
      {
        "authorId": "2273056782",
        "name": "Hima Lakkaraju"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "a314b4c385e9732794a48d1d34f637b13245c71d",
    "url": "https://www.semanticscholar.org/paper/a314b4c385e9732794a48d1d34f637b13245c71d",
    "title": "RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering",
    "abstract": "Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine the necessity of retrieval for queries instead of retrieving indiscriminately to enhance the efficiency and relevance of the sourced information. However, previous works largely overlook the evaluation of ARAG approaches, leading to their effectiveness being understudied. This work presents a benchmark, RetrievalQA, comprising 1,271 short-form questions covering new world and long-tail knowledge. The knowledge necessary to answer the questions is absent from LLMs; therefore, external information must be retrieved to answer correctly. This makes RetrievalQA a suitable testbed to evaluate existing ARAG methods. We observe that calibration-based methods heavily rely on threshold tuning, while vanilla prompting is inadequate for guiding LLMs to make reliable retrieval decisions. Based on our findings, we propose Time-Aware Adaptive Retrieval (TA-ARE), a simple yet effective method that helps LLMs assess the necessity of retrieval without calibration or additional training. The dataset and code will be available at https://github.com/hyintell/RetrievalQA",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-26",
    "authors": [
      {
        "authorId": "2144370264",
        "name": "Zihan Zhang"
      },
      {
        "authorId": "2257039084",
        "name": "Meng Fang"
      },
      {
        "authorId": "2261455899",
        "name": "Ling Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "7423e5c903fb2befaf471cae64e2530f7c1d0404",
    "url": "https://www.semanticscholar.org/paper/7423e5c903fb2befaf471cae64e2530f7c1d0404",
    "title": "Development and Testing of Retrieval Augmented Generation in Large Language Models - A Case Study Report",
    "abstract": "Purpose: Large Language Models (LLMs) hold significant promise for medical applications. Retrieval Augmented Generation (RAG) emerges as a promising approach for customizing domain knowledge in LLMs. This case study presents the development and evaluation of an LLM-RAG pipeline tailored for healthcare, focusing specifically on preoperative medicine. Methods: We developed an LLM-RAG model using 35 preoperative guidelines and tested it against human-generated responses, with a total of 1260 responses evaluated. The RAG process involved converting clinical documents into text using Python-based frameworks like LangChain and Llamaindex, and processing these texts into chunks for embedding and retrieval. Vector storage techniques and selected embedding models to optimize data retrieval, using Pinecone for vector storage with a dimensionality of 1536 and cosine similarity for loss metrics. Human-generated answers, provided by junior doctors, were used as a comparison. Results: The LLM-RAG model generated answers within an average of 15-20 seconds, significantly faster than the 10 minutes typically required by humans. Among the basic LLMs, GPT4.0 exhibited the best accuracy of 80.1%. This accuracy was further increased to 91.4% when the model was enhanced with RAG. Compared to the human-generated instructions, which had an accuracy of 86.3%, the performance of the GPT4.0 RAG model demonstrated non-inferiority (p=0.610). Conclusions: In this case study, we demonstrated a LLM-RAG model for healthcare implementation. The pipeline shows the advantages of grounded knowledge, upgradability, and scalability as important aspects of healthcare LLM deployment.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-29",
    "authors": [
      {
        "authorId": "2253471617",
        "name": "Yuhe Ke"
      },
      {
        "authorId": "2226927454",
        "name": "Liyuan Jin"
      },
      {
        "authorId": "2223726066",
        "name": "Kabilan Elangovan"
      },
      {
        "authorId": "6218835",
        "name": "H. Abdullah"
      },
      {
        "authorId": "2279502757",
        "name": "Nan Liu"
      },
      {
        "authorId": "2282536576",
        "name": "Alex Tiong Heng Sia"
      },
      {
        "authorId": "2270716657",
        "name": "Chai Rick Soh"
      },
      {
        "authorId": "2282531767",
        "name": "Joshua Yi Min Tung"
      },
      {
        "authorId": "123021420",
        "name": "J. Ong"
      },
      {
        "authorId": "2238695928",
        "name": "D. Ting"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "f4e06256ab07727ff4e0465deea83fcf45012354",
    "url": "https://www.semanticscholar.org/paper/f4e06256ab07727ff4e0465deea83fcf45012354",
    "title": "PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models",
    "abstract": "Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate these limitations. The key idea of RAG is to ground the answer generation of an LLM on external knowledge retrieved from a knowledge database. Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored. We aim to bridge the gap in this work. We find that the knowledge database in a RAG system introduces a new and practical attack surface. Based on this attack surface, we propose PoisonedRAG, the first knowledge corruption attack to RAG, where an attacker could inject a few malicious texts into the knowledge database of a RAG system to induce an LLM to generate an attacker-chosen target answer for an attacker-chosen target question. We formulate knowledge corruption attacks as an optimization problem, whose solution is a set of malicious texts. Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on a RAG system, we propose two solutions to solve the optimization problem, respectively. Our results show PoisonedRAG could achieve a 90% attack success rate when injecting five malicious texts for each target question into a knowledge database with millions of texts. We also evaluate several defenses and our results show they are insufficient to defend against PoisonedRAG, highlighting the need for new defenses.",
    "venue": "",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-02-12",
    "authors": [
      {
        "authorId": "2281827168",
        "name": "Wei Zou"
      },
      {
        "authorId": "2260340372",
        "name": "Runpeng Geng"
      },
      {
        "authorId": "2284264092",
        "name": "Binghui Wang"
      },
      {
        "authorId": "2282387150",
        "name": "Jinyuan Jia"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "4d44384b0f044a0088564b846bbd3cc16b3b7c63",
    "url": "https://www.semanticscholar.org/paper/4d44384b0f044a0088564b846bbd3cc16b3b7c63",
    "title": "VistaRAG: Toward Safe and Trustworthy Autonomous Driving Through Retrieval-Augmented Generation",
    "abstract": "Autonomous driving based on foundation models has recently garnered widespread attention. However, the risk of hallucinations inherent in foundation models could compromise the safety and reliability of autonomous driving systems. This letter, as part of a series of reports from the Distributed/Decentralized Hybrid Workshop on Foundation/Infrastructure Intelligence (DHW-FII), aims to tackle these issues. We introduce VistaRAG, which integrates retrieval-augmented generation (RAG) technologies into autonomous driving systems based on foundation models, to address the inherent reliability challenges in decision-making. VistaRAG employs a dynamic retrieval mechanism to access highly relevant driving experience, real-time road network status, and other contextual information from external databases. This aids foundation models in informed reasoning and decision-making, thereby enhancing the safety and trustworthiness of foundation-model-based autonomous driving systems under complex traffic scenarios.",
    "venue": "IEEE Transactions on Intelligent Vehicles",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-01",
    "authors": [
      {
        "authorId": "22200602",
        "name": "Xingyuan Dai"
      },
      {
        "authorId": "2110227121",
        "name": "Chao Guo"
      },
      {
        "authorId": "2300130446",
        "name": "Yun Tang"
      },
      {
        "authorId": "2299907772",
        "name": "Haichuan Li"
      },
      {
        "authorId": "2108070100",
        "name": "Yutong Wang"
      },
      {
        "authorId": "2274729221",
        "name": "Jun Huang"
      },
      {
        "authorId": "32847052",
        "name": "Yonglin Tian"
      },
      {
        "authorId": "2150058228",
        "name": "Xin Xia"
      },
      {
        "authorId": "2237937820",
        "name": "Yisheng Lv"
      },
      {
        "authorId": "2148956730",
        "name": "Fei-Yue Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.95836866004329
  },
  {
    "paperId": "81f24ac736735a4e1b0cb860aa1ffd34af469b6d",
    "url": "https://www.semanticscholar.org/paper/81f24ac736735a4e1b0cb860aa1ffd34af469b6d",
    "title": "SDG target detection in environmental reports using Retrieval-augmented Generation with LLMs",
    "abstract": "With the consolidation of Large Language Models (LLM) as a dominant component in approaches for multiple linguistic tasks, the interest in these technologies has greatly increased within a variety of areas and domains. A particular scenario of information needs where to exploit these approaches is climate-aware NLP. Paradigmatically, the vast manual labour of inspecting long, heterogeneous documents to find environment-relevant expressions and claims suits well within a recently established Retrieval-augmented Generation (RAG) framework. In this paper, we tackle two dual problems within environment analysis dealing with the common goal of detecting a Sustainable Developmental Goal (SDG) target being addressed in a textual passage of an environmental assessment report.We develop relevant test collections, and propose and evaluate a series of methods within the general RAG pipeline, in order to assess the current capabilities of LLMs for the tasks of SDG target evidence identification and SDG target detection.",
    "venue": "CLIMATENLP",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2315308998",
        "name": "Dar√≠o Garigliotti"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "522c47365931e0ad722fbdac463ae415c97c65e4",
    "url": "https://www.semanticscholar.org/paper/522c47365931e0ad722fbdac463ae415c97c65e4",
    "title": "Revolutionizing Retrieval-Augmented Generation with Enhanced PDF Structure Recognition",
    "abstract": "With the rapid development of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) has become a predominant method in the field of professional knowledge-based question answering. Presently, major foundation model companies have opened up Embedding and Chat API interfaces, and frameworks like LangChain have already integrated the RAG process. It appears that the key models and steps in RAG have been resolved, leading to the question: are professional knowledge QA systems now approaching perfection? This article discovers that current primary methods depend on the premise of accessing high-quality text corpora. However, since professional documents are mainly stored in PDFs, the low accuracy of PDF parsing significantly impacts the effectiveness of professional knowledge-based QA. We conducted an empirical RAG experiment across hundreds of questions from the corresponding real-world professional documents. The results show that, ChatDOC, a RAG system equipped with a panoptic and pinpoint PDF parser, retrieves more accurate and complete segments, and thus better answers. Empirical experiments show that ChatDOC is superior to baseline on nearly 47% of questions, ties for 38% of cases, and falls short on only 15% of cases. It shows that we may revolutionize RAG with enhanced PDF structure recognition.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-23",
    "authors": [
      {
        "authorId": "2280862078",
        "name": "Demiao Lin"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "03182415b7e769a387ae16c4a61c1df908304e7e",
    "url": "https://www.semanticscholar.org/paper/03182415b7e769a387ae16c4a61c1df908304e7e",
    "title": "Retrieval Augmented Generation Enabled Generative Pre-Trained Transformer 4 (GPT-4) Performance for Clinical Trial Screening",
    "abstract": "Background: Subject screening is a key aspect of all clinical trials; however, traditionally, it is a labor-intensive and error-prone task, demanding significant time and resources. With the advent of large language models (LLMs) and related technologies, a paradigm shift in natural language processing capabilities offers a promising avenue for increasing both quality and efficiency of screening efforts. This study aimed to test the Retrieval-Augmented Generation (RAG) process enabled Generative Pretrained Transformer Version 4 (GPT-4) to accurately identify and report on inclusion and exclusion criteria for a clinical trial. Methods: The (Co-Operative Program for Implementation of Optimal Therapy in Heart Failure) COPILOT-HF trial aims to recruit patients with symptomatic heart failure. As part of the screening process, a list of potentially eligible patients is created through an electronic health record (EHR) query. Currently, structured data in the EHR can only be used to determine 5 out of 6 inclusion and 5 out of 17 exclusion criteria. Trained, but non-licensed, study staff complete manual chart review to determine patient eligibility and record their assessment of the inclusion and exclusion criteria. We obtained the structured assessments completed by the study staff and clinical notes for the past two years and developed a workflow of clinical note-based question answering system powered by RAG architecture and GPT-4 that we named RECTIFIER (RAG-Enabled Clinical Trial Infrastructure for Inclusion Exclusion Review). We used notes from 100 patients as a development dataset, 282 patients as a validation dataset, and 1894 patients as a test set. An expert clinician completed a blinded review of patients' charts to answer the eligibility questions and determine the \"gold standard\" answers. We calculated the sensitivity, specificity, accuracy, and Matthews correlation coefficient (MCC) for each question and screening method. We also performed bootstrapping to calculate the confidence intervals for each statistic. Results: Both RECTIFIER and study staff answers closely aligned with the expert clinician answers across criteria with accuracy ranging between 97.9% and 100% (MCC 0.837 and 1) for RECTIFIER and 91.7% and 100% (MCC 0.644 and 1) for study staff. RECTIFIER performed better than study staff to determine the inclusion criteria of \"symptomatic heart failure\" with an accuracy of 97.9% vs 91.7% and an MCC of 0.924 vs 0.721, respectively. Overall, the sensitivity and specificity of determining eligibility for the RECTIFIER was 92.3% (CI) and 93.9% (CI), and study staff was 90.1% (CI) and 83.6% (CI), respectively. Conclusion: GPT-4 based solutions have the potential to improve efficiency and reduce costs in clinical trial screening. When incorporating new tools such as RECTIFIER, it is important to consider the potential hazards of automating the screening process and set up appropriate mitigation strategies such as final clinician review before patient engagement.",
    "venue": "medRxiv",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": "2024-02-08",
    "authors": [
      {
        "authorId": "2258113315",
        "name": "Ozan Unlu"
      },
      {
        "authorId": "2283302172",
        "name": "Jiyeon Shin"
      },
      {
        "authorId": "123271410",
        "name": "Charlotte J. Mailly"
      },
      {
        "authorId": "2283261639",
        "name": "Michael Oates"
      },
      {
        "authorId": "2190273257",
        "name": "Michela R. Tucci"
      },
      {
        "authorId": "3040805",
        "name": "Matthew Varugheese"
      },
      {
        "authorId": "2727465",
        "name": "K. Wagholikar"
      },
      {
        "authorId": "2283311300",
        "name": "Fei Wang"
      },
      {
        "authorId": "2274921165",
        "name": "Benjamin M. Scirica"
      },
      {
        "authorId": "40128337",
        "name": "A. Blood"
      },
      {
        "authorId": "2275613770",
        "name": "Samuel J. Aronson"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "3d51b9cf8d23fd038dac0ae7ade4fe27f1c03e9d",
    "url": "https://www.semanticscholar.org/paper/3d51b9cf8d23fd038dac0ae7ade4fe27f1c03e9d",
    "title": "InstructRAG: Instructing Retrieval-Augmented Generation with Explicit Denoising",
    "abstract": "Retrieval-augmented generation (RAG) has shown promising potential to enhance the accuracy and factuality of language models (LMs). However, imperfect retrievers or noisy corpora can introduce misleading or even erroneous information to the retrieved contents, posing a significant challenge to the generation quality. Existing RAG methods typically address this challenge by directly predicting final answers despite potentially noisy inputs, resulting in an implicit denoising process that is difficult to interpret and verify. On the other hand, the acquisition of explicit denoising supervision is often costly, involving significant human efforts. In this work, we propose I NSTRUCT RAG, where LMs explicitly learn the denoising process through self-synthesized rationales ‚Äî First, we instruct the LM to explain how the ground-truth answer is derived from retrieved documents. Then, these rationales can be used either as demonstrations for in-context learning of explicit denoising or as supervised fine-tuning data to train the model. Compared to standard RAG approaches, I NSTRUCT RAG requires no additional supervision, allows for easier verification of the predicted answers, and effectively improves generation accuracy. Experiments show I NSTRUCT RAG consistently outperforms existing RAG meth-ods in both training-free and trainable scenarios, achieving a relative improvement of 8.3% over the best baseline method on average across five knowledge-intensive benchmarks. Extensive analysis indicates that I NSTRUCT RAG scales well with increased numbers of retrieved documents and consistently exhibits robust denoising ability even in out-of-domain datasets, demonstrating strong generalizability. 1",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "122835755",
        "name": "Zhepei Wei"
      },
      {
        "authorId": "2128184431",
        "name": "Wei-Lin Chen"
      },
      {
        "authorId": "145391513",
        "name": "Yu Meng"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "55e74f1f8d0cc387064594eb5d3a1f55ddfd4c0a",
    "url": "https://www.semanticscholar.org/paper/55e74f1f8d0cc387064594eb5d3a1f55ddfd4c0a",
    "title": "Img2Loc: Revisiting Image Geolocalization using Multi-modality Foundation Models and Image-based Retrieval-Augmented Generation",
    "abstract": "Geolocating precise locations from images presents a challenging problem in computer vision and information retrieval.Traditional methods typically employ either classification, which dividing the Earth surface into grid cells and classifying images accordingly, or retrieval, which identifying locations by matching images with a database of image-location pairs. However, classification-based approaches are limited by the cell size and cannot yield precise predictions, while retrieval-based systems usually suffer from poor search quality and inadequate coverage of the global landscape at varied scale and aggregation levels. To overcome these drawbacks, we present Img2Loc, a novel system that redefines image geolocalization as a text generation task. This is achieved using cutting-edge large multi-modality models like GPT4V or LLaVA with retrieval augmented generation. Img2Loc first employs CLIP-based representations to generate an image-based coordinate query database. It then uniquely combines query results with images itself, forming elaborate prompts customized for LMMs. When tested on benchmark datasets such as Im2GPS3k and YFCC4k, Img2Loc not only surpasses the performance of previous state-of-the-art models but does so without any model training.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2403.19584",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-28",
    "authors": [
      {
        "authorId": "2116589565",
        "name": "Zhongliang Zhou"
      },
      {
        "authorId": "150167509",
        "name": "Jielu Zhang"
      },
      {
        "authorId": "2260652610",
        "name": "Zihan Guan"
      },
      {
        "authorId": "2215174877",
        "name": "Mengxuan Hu"
      },
      {
        "authorId": "2135307417",
        "name": "Ni Lao"
      },
      {
        "authorId": "2215168738",
        "name": "Lan Mu"
      },
      {
        "authorId": "2293760697",
        "name": "Sheng Li"
      },
      {
        "authorId": "2240534117",
        "name": "Gengchen Mai"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.19162312519754
  },
  {
    "paperId": "d4a5c2ab2b459426869e1a3ab1550897b005303e",
    "url": "https://www.semanticscholar.org/paper/d4a5c2ab2b459426869e1a3ab1550897b005303e",
    "title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
    "abstract": "Large language models (LLMs) have demonstrated great success in various fields, benefiting from their huge amount of parameters that store knowledge. However, LLMs still suffer from several key issues, such as hallucination problems, knowledge update issues, and lacking domain-specific expertise. The appearance of retrieval-augmented generation (RAG), which leverages an external knowledge database to augment LLMs, makes up those drawbacks of LLMs. This paper reviews all significant techniques of RAG, especially in the retriever and the retrieval fusions. Besides, tutorial codes are provided for implementing the representative techniques in RAG. This paper further discusses the RAG training, including RAG with/without datastore update. Then, we introduce the application of RAG in representative natural language processing tasks and industrial scenarios. Finally, this paper discusses the future directions and challenges of RAG for promoting its development.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-07-18",
    "authors": [
      {
        "authorId": "2112539433",
        "name": "Shangyu Wu"
      },
      {
        "authorId": "2303313918",
        "name": "Ying Xiong"
      },
      {
        "authorId": "2301404967",
        "name": "Yufei Cui"
      },
      {
        "authorId": "107747459",
        "name": "Haolun Wu"
      },
      {
        "authorId": "2243412535",
        "name": "Can Chen"
      },
      {
        "authorId": "2283264350",
        "name": "Ye Yuan"
      },
      {
        "authorId": "2303518782",
        "name": "Lianming Huang"
      },
      {
        "authorId": "2272581493",
        "name": "Xue Liu"
      },
      {
        "authorId": "2271790635",
        "name": "Tei-Wei Kuo"
      },
      {
        "authorId": "2290008872",
        "name": "Nan Guan"
      },
      {
        "authorId": "2302177675",
        "name": "C. Xue"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "0d2103620d4e72688e2aef6258345418fe6a3a3b",
    "url": "https://www.semanticscholar.org/paper/0d2103620d4e72688e2aef6258345418fe6a3a3b",
    "title": "Web Application for Retrieval-Augmented Generation: Implementation and Testing",
    "abstract": "The purpose of this paper is to explore the implementation of retrieval-augmented generation (RAG) technology with open-source large language models (LLMs). A dedicated web-based application, PaSSER, was developed, integrating RAG with Mistral:7b, Llama2:7b, and Orca2:7b models. Various software instruments were used in the application‚Äôs development. PaSSER employs a set of evaluation metrics, including METEOR, ROUGE, BLEU, perplexity, cosine similarity, Pearson correlation, and F1 score, to assess LLMs‚Äô performance, particularly within the smart agriculture domain. The paper presents the results and analyses of two tests. One test assessed the performance of LLMs across different hardware configurations, while the other determined which model delivered the most accurate and contextually relevant responses within RAG. The paper discusses the integration of blockchain with LLMs to manage and store assessment results within a blockchain environment. The tests revealed that GPUs are essential for fast text generation, even for 7b models. Orca2:7b on Mac M1 was the fastest, and Mistral:7b had superior performance on the 446 question‚Äìanswer dataset. The discussion is on technical and hardware considerations affecting LLMs‚Äô performance. The conclusion outlines future developments in leveraging other LLMs, fine-tuning approaches, and further integration with blockchain and IPFS.",
    "venue": "Electronics",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://www.mdpi.com/2079-9292/13/7/1361/pdf?version=1712217836",
      "status": "GOLD"
    },
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-04",
    "authors": [
      {
        "authorId": "2447573",
        "name": "I. Radeva"
      },
      {
        "authorId": "145992070",
        "name": "I. Popchev"
      },
      {
        "authorId": "2525508",
        "name": "L. Doukovska"
      },
      {
        "authorId": "2272331260",
        "name": "Miroslava Dimitrova"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "8d0df3168870fd17b36ecd5575e406feb5a5a1b5",
    "url": "https://www.semanticscholar.org/paper/8d0df3168870fd17b36ecd5575e406feb5a5a1b5",
    "title": "M-RAG: Reinforcing Large Language Model Performance through Retrieval-Augmented Generation with Multiple Partitions",
    "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by retrieving relevant memories from an external database. However, existing RAG methods typically organize all memories in a whole database, potentially limiting focus on crucial memories and introducing noise. In this paper, we introduce a multiple partition paradigm for RAG (called M-RAG), where each database partition serves as a basic unit for RAG execution. Based on this paradigm, we propose a novel framework that leverages LLMs with Multi-Agent Reinforcement Learning to optimize different language generation tasks explicitly. Through comprehensive experiments conducted on seven datasets, spanning three language generation tasks and involving three distinct language model architectures, we confirm that M-RAG consistently outperforms various baseline methods, achieving improvements of 11%, 8%, and 12% for text summarization, machine translation, and dialogue generation, respectively.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-26",
    "authors": [
      {
        "authorId": "2303565983",
        "name": "Zheng Wang"
      },
      {
        "authorId": "2274319013",
        "name": "Shu Xian Teo"
      },
      {
        "authorId": "2303398119",
        "name": "Jieer Ouyang"
      },
      {
        "authorId": "2303476812",
        "name": "Yongjun Xu"
      },
      {
        "authorId": "2261895678",
        "name": "Wei Shi"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "840ecbd1b61287ac2c9563b9506d98f4e7f117ad",
    "url": "https://www.semanticscholar.org/paper/840ecbd1b61287ac2c9563b9506d98f4e7f117ad",
    "title": "CPR: Retrieval Augmented Generation for Copyright Protection",
    "abstract": "Retrieval Augmented Generation (RAG) is emerging as a flexible and robust technique to adapt models to private users data without training, to handle credit attribution, and to allow efficient machine unlearning at scale. However, RAG techniques for image generation may lead to parts of the retrieved samples being copied in the model's output. To reduce risks of leaking private information contained in the retrieved set, we introduce Copy-Protected generation with Retrieval (CPR), a new method for RAG with strong copyright protection guarantees in a mixed-private setting for diffusion models. CPR allows to condition the output of diffusion models on a set of retrieved images, while also guaranteeing that unique identifiable information about those example is not exposed in the generated outputs. In particular, it does so by sampling from a mixture of public (safe) distribution and private (user) distribution by merging their diffusion scores at inference. We prove that CPR satisfies Near Access Freeness (NAF) which bounds the amount of information an attacker may be able to extract from the generated images. We provide two algorithms for copyright protection, CPR-KL and CPR-Choose. Unlike previously proposed rejection-sampling-based NAF methods, our methods enable efficient copyright-protected sampling with a single run of backward diffusion. We show that our method can be applied to any pre-trained conditional diffusion model, such as Stable Diffusion or unCLIP. In particular, we empirically show that applying CPR on top of unCLIP improves quality and text-to-image alignment of the generated results (81.4 to 83.17 on TIFA benchmark), while enabling credit attribution, copy-right protection, and deterministic, constant time, unlearning.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2403.18920",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-27",
    "authors": [
      {
        "authorId": "35838711",
        "name": "Aditya Golatkar"
      },
      {
        "authorId": "16163297",
        "name": "A. Achille"
      },
      {
        "authorId": "1914913470",
        "name": "L. Zancato"
      },
      {
        "authorId": "2293780337",
        "name": "Yu-Xiang Wang"
      },
      {
        "authorId": "2288200185",
        "name": "Ashwin Swaminathan"
      },
      {
        "authorId": "2075295257",
        "name": "S. Soatto"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "f091acf9dc2cc486c253dcead3a74ba916c64eb7",
    "url": "https://www.semanticscholar.org/paper/f091acf9dc2cc486c253dcead3a74ba916c64eb7",
    "title": "IM-RAG: Multi-Round Retrieval-Augmented Generation Through Learning Inner Monologues",
    "abstract": "Although the Retrieval-Augmented Generation (RAG) paradigms can use external knowledge to enhance and ground the outputs of Large Language Models (LLMs) to mitigate generative hallucinations and static knowledge base problems, they still suffer from limited flexibility in adopting Information Retrieval (IR) systems with varying capabilities, constrained interpretability during the multi-round retrieval process, and a lack of end-to-end optimization. To address these challenges, we propose a novel LLM-centric approach, IM-RAG, that integrates IR systems with LLMs to support multi-round RAG through learning Inner Monologues (IM, i.e., the human inner voice that narrates one's thoughts). During the IM process, the LLM serves as the core reasoning model (i.e., Reasoner) to either propose queries to collect more information via the Retriever or to provide a final answer based on the conversational context. We also introduce a Refiner that improves the outputs from the Retriever, effectively bridging the gap between the Reasoner and IR modules with varying capabilities and fostering multi-round communications. The entire IM process is optimized via Reinforcement Learning (RL) where a Progress Tracker is incorporated to provide mid-step rewards, and the answer prediction is further separately optimized via Supervised Fine-Tuning (SFT). We conduct extensive experiments with the HotPotQA dataset, a popular benchmark for retrieval-based, multi-step question-answering. The results show that our approach achieves state-of-the-art (SOTA) performance while providing high flexibility in integrating IR modules as well as strong interpretability exhibited in the learned inner monologues.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-15",
    "authors": [
      {
        "authorId": "2326258875",
        "name": "Diji Yang"
      },
      {
        "authorId": "2238635148",
        "name": "Jinmeng Rao"
      },
      {
        "authorId": "2238911526",
        "name": "Kezhen Chen"
      },
      {
        "authorId": "2284184369",
        "name": "Xiaoyuan Guo"
      },
      {
        "authorId": "2145040404",
        "name": "Yawen Zhang"
      },
      {
        "authorId": "2239161755",
        "name": "Jie Yang"
      },
      {
        "authorId": "2311353391",
        "name": "Yi Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.1886522358297
  },
  {
    "paperId": "339d2a56f0e5176b691c358a86891e2923045c8c",
    "url": "https://www.semanticscholar.org/paper/339d2a56f0e5176b691c358a86891e2923045c8c",
    "title": "Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely",
    "abstract": "Large language models (LLMs) augmented with external data have demonstrated remarkable capabilities in completing real-world tasks. Techniques for integrating external data into LLMs, such as Retrieval-Augmented Generation (RAG) and fine-tuning, are gaining increasing attention and widespread application. Nonetheless, the effective deployment of data-augmented LLMs across various specialized fields presents substantial challenges. These challenges encompass a wide range of issues, from retrieving relevant data and accurately interpreting user intent to fully harnessing the reasoning capabilities of LLMs for complex tasks. We believe that there is no one-size-fits-all solution for data-augmented LLM applications. In practice, underperformance often arises from a failure to correctly identify the core focus of a task or because the task inherently requires a blend of multiple capabilities that must be disentangled for better resolution. In this survey, we propose a RAG task categorization method, classifying user queries into four levels based on the type of external data required and primary focus of the task: explicit fact queries, implicit fact queries, interpretable rationale queries, and hidden rationale queries. We define these levels of queries, provide relevant datasets, and summarize the key challenges and most effective techniques for addressing these challenges. Finally, we discuss three main forms of integrating external data into LLMs: context, small model, and fine-tuning, highlighting their respective strengths, limitations, and the types of problems they are suited to solve. This work aims to help readers thoroughly understand and decompose the data requirements and key bottlenecks in building LLM applications, offering solutions to the different challenges and serving as a guide to systematically developing such applications.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-09-23",
    "authors": [
      {
        "authorId": "2268432582",
        "name": "Siyun Zhao"
      },
      {
        "authorId": "2125051198",
        "name": "Yuqing Yang"
      },
      {
        "authorId": "2294387070",
        "name": "Zilong Wang"
      },
      {
        "authorId": "2260609693",
        "name": "Zhiyuan He"
      },
      {
        "authorId": "2180993402",
        "name": "Luna K. Qiu"
      },
      {
        "authorId": "2259937079",
        "name": "Lili Qiu"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "9c45b4af25e192733d42a8d384e41002786d0d32",
    "url": "https://www.semanticscholar.org/paper/9c45b4af25e192733d42a8d384e41002786d0d32",
    "title": "Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation",
    "abstract": "Despite the successes of large language models (LLMs), they exhibit significant drawbacks, particularly when processing long contexts. Their inference cost scales quadratically with respect to sequence length, making it expensive for deployment in some real-world text processing applications, such as retrieval-augmented generation (RAG). Additionally, LLMs also exhibit the\"distraction phenomenon\", where irrelevant context in the prompt degrades output quality. To address these drawbacks, we propose a novel RAG prompting methodology, *superposition prompting*, which can be directly applied to pre-trained transformer-based LLMs *without the need for fine-tuning*. At a high level, superposition prompting allows the LLM to process input documents in parallel *prompt paths*, discarding paths once they are deemed irrelevant. We demonstrate the capability of our method to simultaneously enhance time efficiency across a variety of question-answering benchmarks using multiple pre-trained LLMs. Furthermore, our technique significantly improves accuracy when the retrieved context is large relative the context the model was trained on. For example, our approach facilitates a 93x reduction in compute time while *improving* accuracy by 43% on the NaturalQuestions-Open dataset with the MPT-7B instruction-tuned model over naive RAG.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-10",
    "authors": [
      {
        "authorId": "2178316365",
        "name": "Thomas Merth"
      },
      {
        "authorId": "2275124052",
        "name": "Qichen Fu"
      },
      {
        "authorId": "2284683934",
        "name": "Mohammad Rastegari"
      },
      {
        "authorId": "40465379",
        "name": "Mahyar Najibi"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "425fb2f829d06d3a7b4f5936b4ee9dce71bb823f",
    "url": "https://www.semanticscholar.org/paper/425fb2f829d06d3a7b4f5936b4ee9dce71bb823f",
    "title": "Federated Recommendation via Hybrid Retrieval Augmented Generation",
    "abstract": "Federated Recommendation (FR) emerges as a novel paradigm that enables privacy-preserving recommendations. However, traditional FR systems usually represent users/items with discrete identities (IDs), suffering from performance degradation due to data sparsity and heterogeneity in FR. On the other hand, Large Language Models (LLMs) as recommenders have proven effective across various recommendation scenarios. Yet, LLM-based recommenders encounter challenges such as incomplete recommendation and potential hallucination, compromising their performance in real-world scenarios. To this end, we propose GPT-FedRec, a federated recommendation framework leveraging ChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism. GPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval process, mining ID-based user patterns and text-based item features. Next, in the second stage, the results returned by hybrid retrieval are converted into text prompts and fed into GPT for re-ranking. Under GPT-FedRec, the privacy of both local training data and global test data is well protected, as there is no data exchange across any clients or the global server. For test users, GPT-FedRec executes inference only on the global server: given the historical data of a test user, GPT-FedRec performs hybrid retrieval and GPT-based re-ranking, without exposing test data to any other clients. Our proposed hybrid retrieval mechanism and LLM-based re-ranking aim to extract generalized features from data and exploit pretrained knowledge within LLM, overcoming data sparsity and heterogeneity in FR. Finally, the RAG nature of GPT-FedRec also prevents LLM hallucination, improving the recommendation performance for real-world users. Experimental results on diverse benchmark datasets demonstrate the superior performance of GPT-FedRec against state-of-the-art baseline methods. Our code is available at https://github.com/huiminzeng/GPT-FedRec.git.",
    "venue": "BigData Congress [Services Society]",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-07",
    "authors": [
      {
        "authorId": "2113559163",
        "name": "Huimin Zeng"
      },
      {
        "authorId": "2028213158",
        "name": "Zhenrui Yue"
      },
      {
        "authorId": "2290315866",
        "name": "Qian Jiang"
      },
      {
        "authorId": "2254248851",
        "name": "Dong Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "6a16c62fbbc1d08bb8db14715af565252a0c099e",
    "url": "https://www.semanticscholar.org/paper/6a16c62fbbc1d08bb8db14715af565252a0c099e",
    "title": "Advancing TTP Analysis: Harnessing the Power of Encoder-Only and Decoder-Only Language Models with Retrieval Augmented Generation",
    "abstract": "Tactics, Techniques, and Procedures (TTPs) outline the meth-ods attackers use to exploit vulnerabilities. The interpretation of TTPs in the MITRE ATT&CK framework can be challenging for cybersecurity practitioners due to presumed expertise, complex dependencies, and inherent ambiguity. Meanwhile, advancements with Large Language Models (LLMs) have led to recent surge in studies exploring its uses in cybersecurity operations. This leads us to question how well encoder-only (e.g., RoBERTa) and decoder-only (e.g., GPT-3.5) LLMs can comprehend and summarize TTPs to inform analysts of the intended purposes (i.e., tactics) of a cyberattack procedure. The state-of-the-art LLMs have shown to be prone to hallu-cination by providing inaccurate information, which is problematic in critical domains like cybersecurity. Therefore, we propose the use of Retrieval Augmented Generation (RAG) techniques to extract relevant contexts for each cyberattack procedure for decoder-only LLMs (without fine-tuning). We further contrast such approach against supervised fine-tuning (SFT) of encoder-only LLMs. Our results reveal that both the direct-use of decoder-only LLMs (i.e., its pre-trained knowledge) and the SFT of encoder-only LLMs offer inaccurate interpretation of cyberattack procedures. Significant improvements are shown when RAG is used for decoder-only LLMs, particularly when directly relevant context is found. This study further sheds insights on the limitations and capabilities of using RAG for LLMs in interpreting TTPs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2220549557",
        "name": "Reza Fayyazi"
      },
      {
        "authorId": "2277244956",
        "name": "Rozhina Taghdimi"
      },
      {
        "authorId": "37923918",
        "name": "S. Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "64fed9e0be009f064b72cdcb7d1fadeb28bea3b0",
    "url": "https://www.semanticscholar.org/paper/64fed9e0be009f064b72cdcb7d1fadeb28bea3b0",
    "title": "Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation",
    "abstract": "We introduce a novel graph-based Retrieval-Augmented Generation (RAG) framework specifically designed for the medical domain, called \\textbf{MedGraphRAG}, aimed at enhancing Large Language Model (LLM) capabilities for generating evidence-based medical responses, thereby improving safety and reliability when handling private medical data. Graph-based RAG (GraphRAG) leverages LLMs to organize RAG data into graphs, showing strong potential for gaining holistic insights from long-form documents. However, its standard implementation is overly complex for general use and lacks the ability to generate evidence-based responses, limiting its effectiveness in the medical field. To extend the capabilities of GraphRAG to the medical domain, we propose unique Triple Graph Construction and U-Retrieval techniques over it. In our graph construction, we create a triple-linked structure that connects user documents to credible medical sources and controlled vocabularies. In the retrieval process, we propose U-Retrieval which combines Top-down Precise Retrieval with Bottom-up Response Refinement to balance global context awareness with precise indexing. These effort enable both source information retrieval and comprehensive response generation. Our approach is validated on 9 medical Q\\&A benchmarks, 2 health fact-checking benchmarks, and one collected dataset testing long-form generation. The results show that MedGraphRAG consistently outperforms state-of-the-art models across all benchmarks, while also ensuring that responses include credible source documentation and definitions. Our code is released at: https://github.com/MedicineToken/Medical-Graph-RAG.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-08",
    "authors": [
      {
        "authorId": "2290900099",
        "name": "Junde Wu"
      },
      {
        "authorId": "2275784890",
        "name": "Jiayuan Zhu"
      },
      {
        "authorId": "2316017828",
        "name": "Yunli Qi"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "1b6c568f3b8c10f4f887d2f0487c7c0ad46093e4",
    "url": "https://www.semanticscholar.org/paper/1b6c568f3b8c10f4f887d2f0487c7c0ad46093e4",
    "title": "Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems",
    "abstract": "The development of generative large language models (G-LLM) opened up new opportunities for the development of new types of knowledge-based systems similar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented Generation (RAG) are the techniques that can be used to implement domain adaptation for the development of G-LLM-based knowledge systems. In our study, using ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine the performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2 language models. Based on measurements shown on different datasets, we demonstrate that RAG-based constructions are more efficient than models produced with FN. We point out that connecting RAG and FN is not trivial, because connecting FN models with RAG can cause a decrease in performance. Furthermore, we outline a simple RAG-based architecture which, on average, outperforms the FN models by 16% in terms of the ROGUE score, 15% in the case of the BLEU score, and 53% based on the cosine similarity. This shows the significant advantage of RAG over FN in terms of hallucination, which is not offset by the fact that the average 8% better METEOR score of FN models indicates greater creativity compared to RAG.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-12",
    "authors": [
      {
        "authorId": "2284077584",
        "name": "R√≥bert Lakatos"
      },
      {
        "authorId": "2279978930",
        "name": "P. Pollner"
      },
      {
        "authorId": "2260742529",
        "name": "Andr√°s Hajdu"
      },
      {
        "authorId": "2243157601",
        "name": "Tamas Joo"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "3dc1b657bf821b731c5ed0396823b67c10d54ba1",
    "url": "https://www.semanticscholar.org/paper/3dc1b657bf821b731c5ed0396823b67c10d54ba1",
    "title": "Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference",
    "abstract": "For middle-school math students, interactive question-answering (QA) with tutors is an effective way to learn. The flexibility and emergent capabilities of generative large language models (LLMs) has led to a surge of interest in automating portions of the tutoring process - including interactive QA to support conceptual discussion of mathematical concepts. However, LLM responses to math questions can be incorrect or mismatched to the educational context - such as being misaligned with a school's curriculum. One potential solution is retrieval-augmented generation (RAG), which involves incorporating a vetted external knowledge source in the LLM prompt to increase response quality. In this paper, we designed prompts that retrieve and use content from a high-quality open-source math textbook to generate responses to real student questions. We evaluate the efficacy of this RAG system for middle-school algebra and geometry QA by administering a multi-condition survey, finding that humans prefer responses generated using RAG, but not when responses are too grounded in the textbook content. We argue that while RAG is able to improve response quality, designers of math QA systems must consider trade-offs between generating responses preferred by students and responses closely matched to specific educational resources.",
    "venue": "Educational Data Mining",
    "year": 2023,
    "citationCount": 24,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.03184",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-10-04",
    "authors": [
      {
        "authorId": "51915733",
        "name": "Zachary Levonian"
      },
      {
        "authorId": "2255366203",
        "name": "Chenglu Li"
      },
      {
        "authorId": "2238720345",
        "name": "Wangda Zhu"
      },
      {
        "authorId": "2254272960",
        "name": "Anoushka Gade"
      },
      {
        "authorId": "2254260121",
        "name": "Owen Henkel"
      },
      {
        "authorId": "2254269074",
        "name": "Millie-Ellen Postle"
      },
      {
        "authorId": "2257373730",
        "name": "Wanli Xing"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.28313737302301
  },
  {
    "paperId": "7848d4b4e6ba0897a85cebb6467e94eb0b60d583",
    "url": "https://www.semanticscholar.org/paper/7848d4b4e6ba0897a85cebb6467e94eb0b60d583",
    "title": "Learning to Filter Context for Retrieval-Augmented Generation",
    "abstract": "On-the-fly retrieval of relevant knowledge has proven an essential element of reliable systems for tasks such as open-domain question answering and fact verification. However, because retrieval systems are not perfect, generation models are required to generate outputs given partially or entirely irrelevant passages. This can cause over- or under-reliance on context, and result in problems in the generated output such as hallucinations. To alleviate these problems, we propose FILCO, a method that improves the quality of the context provided to the generator by (1) identifying useful context based on lexical and information-theoretic approaches, and (2) training context filtering models that can filter retrieved contexts at test time. We experiment on six knowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our method outperforms existing approaches on extractive question answering (QA), complex multi-hop and long-form QA, fact verification, and dialog generation tasks. FILCO effectively improves the quality of context, whether or not it supports the canonical output.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 45,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-14",
    "authors": [
      {
        "authorId": "1390877035",
        "name": "Zhiruo Wang"
      },
      {
        "authorId": "2266466286",
        "name": "Jun Araki"
      },
      {
        "authorId": "2669515",
        "name": "Zhengbao Jiang"
      },
      {
        "authorId": "3405393",
        "name": "Md. Rizwan Parvez"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ],
    "source": "semantic_scholar",
    "score": 127.42962094733642
  },
  {
    "paperId": "29528d8cb030a65f62a35b1237f1f5483077ad0a",
    "url": "https://www.semanticscholar.org/paper/29528d8cb030a65f62a35b1237f1f5483077ad0a",
    "title": "Inference Scaling for Long-Context Retrieval Augmented Generation",
    "abstract": "The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring strategies beyond simply increasing the quantity of knowledge. We focus on two inference scaling strategies: in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-06",
    "authors": [
      {
        "authorId": "2028213158",
        "name": "Zhenrui Yue"
      },
      {
        "authorId": "39371343",
        "name": "Honglei Zhuang"
      },
      {
        "authorId": "2324782053",
        "name": "Aijun Bai"
      },
      {
        "authorId": "2261281337",
        "name": "Kai Hui"
      },
      {
        "authorId": "1886219",
        "name": "R. Jagerman"
      },
      {
        "authorId": "2324910979",
        "name": "Hansi Zeng"
      },
      {
        "authorId": "2099586642",
        "name": "Zhen Qin"
      },
      {
        "authorId": "2325158655",
        "name": "Dong Wang"
      },
      {
        "authorId": "2261356664",
        "name": "Xuanhui Wang"
      },
      {
        "authorId": "1815447",
        "name": "Michael Bendersky"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "3fb916e2d701680cca142167496aeca7b9ec3dd3",
    "url": "https://www.semanticscholar.org/paper/3fb916e2d701680cca142167496aeca7b9ec3dd3",
    "title": "RichRAG: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation",
    "abstract": "Retrieval-augmented generation (RAG) effectively addresses issues of static knowledge and hallucination in large language models. Existing studies mostly focus on question scenarios with clear user intents and concise answers. However, it is prevalent that users issue broad, open-ended queries with diverse sub-intents, for which they desire rich and long-form answers covering multiple relevant aspects. To tackle this important yet underexplored problem, we propose a novel RAG framework, namely RichRAG. It includes a sub-aspect explorer to identify potential sub-aspects of input questions, a multi-faceted retriever to build a candidate pool of diverse external documents related to these sub-aspects, and a generative list-wise ranker, which is a key module to provide the top-k most valuable documents for the final generator. These ranked documents sufficiently cover various query aspects and are aware of the generator's preferences, hence incentivizing it to produce rich and comprehensive responses for users. The training of our ranker involves a supervised fine-tuning stage to ensure the basic coverage of documents, and a reinforcement learning stage to align downstream LLM's preferences to the ranking of documents. Experimental results on two publicly available datasets prove that our framework effectively and efficiently provides comprehensive and satisfying responses to users.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-18",
    "authors": [
      {
        "authorId": "2109464614",
        "name": "Shuting Wang"
      },
      {
        "authorId": "2307268443",
        "name": "Xin Xu"
      },
      {
        "authorId": "2242171637",
        "name": "Mang Wang"
      },
      {
        "authorId": "2299165327",
        "name": "Weipeng Chen"
      },
      {
        "authorId": "1900406",
        "name": "Yutao Zhu"
      },
      {
        "authorId": "2257039188",
        "name": "Zhicheng Dou"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "1b0aba023d7aa5fb9853f9e942efb5c243dc1201",
    "url": "https://www.semanticscholar.org/paper/1b0aba023d7aa5fb9853f9e942efb5c243dc1201",
    "title": "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems",
    "abstract": "Retrieval-Augmented Generation (RAG) has become a standard architectural pattern for incorporating domain-specific knowledge into user-facing chat applications powered by Large Language Models (LLMs). RAG systems are characterized by (1) a document retriever that queries a domain-specific corpus for context information relevant to an input query, and (2) an LLM that generates a response based on the provided query and context. However, comprehensive evaluation of RAG systems remains a challenge due to the lack of unified evaluation criteria and annotated datasets. In response, we introduce RAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k examples. It covers five unique industry-specific domains and various RAG task types. RAGBench examples are sourced from industry corpora such as user manuals, making it particularly relevant for industry applications. Further, we formalize the TRACe evaluation framework: a set of explainable and actionable RAG evaluation metrics applicable across all RAG domains. We release the labeled dataset at https://huggingface.co/datasets/rungalileo/ragbench. RAGBench explainable labels facilitate holistic evaluation of RAG systems, enabling actionable feedback for continuous improvement of production applications. Thorough extensive benchmarking, we find that LLM-based RAG evaluation methods struggle to compete with a finetuned RoBERTa model on the RAG evaluation task. We identify areas where existing approaches fall short and propose the adoption of RAGBench with TRACe towards advancing the state of RAG evaluation systems.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-25",
    "authors": [
      {
        "authorId": "2262445823",
        "name": "Robert Friel"
      },
      {
        "authorId": "2298299139",
        "name": "Masha Belyi"
      },
      {
        "authorId": "2123005528",
        "name": "Atindriyo Sanyal"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "0f8546b7634af1a70ae8649d8538d32bf6cc4660",
    "url": "https://www.semanticscholar.org/paper/0f8546b7634af1a70ae8649d8538d32bf6cc4660",
    "title": "Hybrid Retrieval-Augmented Generation Approach for LLMs Query Response Enhancement",
    "abstract": "In the domain of Natural Language Processing (NLP), the integration of Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) represents a significant advancement towards enhancing the depth and relevance of model-generated responses. This paper introduces a novel hybrid RAG framework that synergizes the Sentence-Window and Parent-Child methodologies with an innovative re-ranking mechanism, aimed at optimizing the query response capabilities of LLMs. By leveraging external knowledge sources more effectively, the proposed method enriches LLM outputs with greater accuracy, relevance, and information fidelity. We subject our hybrid model to rigorous evaluation against benchmark datasets and metrics, demonstrating its superior performance over existing state-of-the-art RAG techniques. The results highlight our method‚Äôs enhanced ability to generate responses that are not only contextually appropriate but also demonstrate a high degree of faithfulness to the source material, thereby setting a new standard for query response enhancement in LLMs. Our study underscores the potential of hybrid RAG models in refining the interaction between LLMs and external knowledge, paving the way for future research in the field of NLP.",
    "venue": "2024 10th International Conference on Web Research (ICWR)",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2024-04-24",
    "authors": [
      {
        "authorId": "2220062271",
        "name": "Pouria Omrani"
      },
      {
        "authorId": "2290015636",
        "name": "Alireza Hosseini"
      },
      {
        "authorId": "2281975931",
        "name": "Kiana Hooshanfar"
      },
      {
        "authorId": "2178450101",
        "name": "Zahra Ebrahimian"
      },
      {
        "authorId": "81287136",
        "name": "Ramin Toosi"
      },
      {
        "authorId": "2302594291",
        "name": "Mohammad Ali Akhaee"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "273c145ea080f277839b89628c255017fc0e1e7c",
    "url": "https://www.semanticscholar.org/paper/273c145ea080f277839b89628c255017fc0e1e7c",
    "title": "Trustworthiness in Retrieval-Augmented Generation Systems: A Survey",
    "abstract": "Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal paradigm in the development of Large Language Models (LLMs). While much of the current research in this field focuses on performance optimization, particularly in terms of accuracy and efficiency, the trustworthiness of RAG systems remains an area still under exploration. From a positive perspective, RAG systems are promising to enhance LLMs by providing them with useful and up-to-date knowledge from vast external databases, thereby mitigating the long-standing problem of hallucination. While from a negative perspective, RAG systems are at the risk of generating undesirable contents if the retrieved information is either inappropriate or poorly utilized. To address these concerns, we propose a unified framework that assesses the trustworthiness of RAG systems across six key dimensions: factuality, robustness, fairness, transparency, accountability, and privacy. Within this framework, we thoroughly review the existing literature on each dimension. Additionally, we create the evaluation benchmark regarding the six dimensions and conduct comprehensive evaluations for a variety of proprietary and open-source models. Finally, we identify the potential challenges for future research based on our investigation results. Through this work, we aim to lay a structured foundation for future investigations and provide practical insights for enhancing the trustworthiness of RAG systems in real-world applications.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-09-16",
    "authors": [
      {
        "authorId": "2118788278",
        "name": "Yujia Zhou"
      },
      {
        "authorId": "2319219743",
        "name": "Yan Liu"
      },
      {
        "authorId": "2144456832",
        "name": "Xiaoxi Li"
      },
      {
        "authorId": "4376097",
        "name": "Jiajie Jin"
      },
      {
        "authorId": "1972030827",
        "name": "Hongjin Qian"
      },
      {
        "authorId": "2284309569",
        "name": "Zheng Liu"
      },
      {
        "authorId": "2321448388",
        "name": "Chaozhuo Li"
      },
      {
        "authorId": "2257039188",
        "name": "Zhicheng Dou"
      },
      {
        "authorId": "2321412234",
        "name": "Tsung-Yi Ho"
      },
      {
        "authorId": "2255871958",
        "name": "Philip S. Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "22467a50298439854d44a40100bf03c6ce6fa001",
    "url": "https://www.semanticscholar.org/paper/22467a50298439854d44a40100bf03c6ce6fa001",
    "title": "CyberMetric: A Benchmark Dataset based on Retrieval-Augmented Generation for Evaluating LLMs in Cybersecurity Knowledge",
    "abstract": "Large Language Models (LLMs) are increasingly used across various domains, from software development to cyber threat intelligence. Understanding all the different cybersecurity fields, including topics such as cryptography, reverse engineering, and risk assessment, poses a challenge even for human experts. The research community needs a diverse, accurate, and up-to-date dataset to test the general knowledge of LLMs in cybersecurity. To address this gap, we present CyberMetric-80, CyberMetric-500, CyberMetric-2000, and CyberMetric-10000, which are multiple-choice Q&A benchmark datasets comprising 80, 500, 2000, and 10,000 questions, respectively. By utilizing GPT-3.5 and Retrieval-Augmented Generation (RAG), we collected documents, including NIST standards, research papers, publicly accessible books, RFCs, and other publications in the cybersecurity domain, to generate questions, each with four possible answers. The results underwent several rounds of error checking and refinement. Human experts invested over 200 hours validating the questions and solutions to ensure their accuracy and relevance and to filter out any questions unrelated to cybersecurity. We have evaluated and compared 25 state-of-the-art LLM models on the CyberMetric datasets. In addition to our primary goal of evaluating LLMs, we involved 30 human participants to solve CyberMetric-80 in a closed-book scenario. The results can serve as a reference for comparing the general cybersecurity knowledge of humans and LLMs. The findings revealed that GPT-4o, GPT-4-turbo, Mixtral-8x7B-Instruct, Falcon-180B-Chat, and GEMINI-pro 1.0 were the best-performing LLMs. Additionally, the top LLMs were more accurate than humans on CyberMetric-80, although highly experienced human experts still outperformed small models such as Llama-3-8B, Phi-2 or Gemma-7b. The CyberMetric dataset is publicly available for the research community and can be downloaded from the projects' website: https://github.com/CyberMetric.",
    "venue": "Computer Science Symposium in Russia",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2402.07688",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-12",
    "authors": [
      {
        "authorId": "2283303502",
        "name": "Norbert Tihanyi"
      },
      {
        "authorId": "2864573",
        "name": "M. Ferrag"
      },
      {
        "authorId": "2113688343",
        "name": "Ridhi Jain"
      },
      {
        "authorId": "1404353535",
        "name": "Tam√°s Bisztray"
      },
      {
        "authorId": "2065834880",
        "name": "M. Debbah"
      }
    ],
    "source": "semantic_scholar",
    "score": 106.87639203842082
  },
  {
    "paperId": "9a3d1d1a1c00feb6c7cbe0e488eff57c606463c9",
    "url": "https://www.semanticscholar.org/paper/9a3d1d1a1c00feb6c7cbe0e488eff57c606463c9",
    "title": "Retrieval-augmented generation in multilingual settings",
    "abstract": "Retrieval-augmented generation (RAG) has recently emerged as a promising solution for incorporating up-to-date or domain-specific knowledge into large language models (LLMs) and improving LLM factuality, but is predominantly studied in English-only settings. In this work, we consider RAG in the multilingual setting (mRAG), i.e. with user queries and the datastore in 13 languages, and investigate which components and with which adjustments are needed to build a well-performing mRAG pipeline, that can be used as a strong baseline in future works. Our findings highlight that despite the availability of high-quality off-the-shelf multilingual retrievers and generators, task-specific prompt engineering is needed to enable generation in user languages. Moreover, current evaluation metrics need adjustments for multilingual setting, to account for variations in spelling named entities. The main limitations to be addressed in future works include frequent code-switching in non-Latin alphabet languages, occasional fluency errors, wrong reading of the provided documents, or irrelevant retrieval. We release the code for the resulting mRAG baseline pipeline at https://github.com/naver/bergen, Documentation: https://github.com/naver/bergen/blob/main/documentations/multilingual.md.",
    "venue": "KNOWLLM",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-01",
    "authors": [
      {
        "authorId": "2258716783",
        "name": "Nadezhda Chirkova"
      },
      {
        "authorId": "2309172052",
        "name": "David Rau"
      },
      {
        "authorId": "2290801744",
        "name": "Herv'e D'ejean"
      },
      {
        "authorId": "1630412772",
        "name": "Thibault Formal"
      },
      {
        "authorId": "2207074",
        "name": "S. Clinchant"
      },
      {
        "authorId": "2841761",
        "name": "Vassilina Nikoulina"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "a76209fea4627974b5e12d8b4942268eb17bc7df",
    "url": "https://www.semanticscholar.org/paper/a76209fea4627974b5e12d8b4942268eb17bc7df",
    "title": "Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation",
    "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval. However, studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it. The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality. In this paper, we propose a novel perspective that considers the role of LLMs in RAG as ``Information Refiner'', which means that regardless of correctness, completeness, or usefulness of retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts. To this end, we propose an information refinement training method named InFO-RAG that optimizes LLMs for RAG in an unsupervised manner. InFO-RAG is low-cost and general across various tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue, and Code Generation show that InFO-RAG improves the performance of LLaMA2 by an average of 9.39\\% relative points. InFO-RAG also shows advantages in in-context learning and robustness of RAG.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-28",
    "authors": [
      {
        "authorId": "2202745",
        "name": "Shicheng Xu"
      },
      {
        "authorId": "2111815778",
        "name": "Liang Pang"
      },
      {
        "authorId": "2265525656",
        "name": "Mo Yu"
      },
      {
        "authorId": "33427918",
        "name": "Fandong Meng"
      },
      {
        "authorId": "2476503",
        "name": "Huawei Shen"
      },
      {
        "authorId": "2110251463",
        "name": "Xueqi Cheng"
      },
      {
        "authorId": "2265517237",
        "name": "Jie Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "d92a423e09804595c8a2e241f890f5a24d326bb5",
    "url": "https://www.semanticscholar.org/paper/d92a423e09804595c8a2e241f890f5a24d326bb5",
    "title": "Prompt-based Code Completion via Multi-Retrieval Augmented Generation",
    "abstract": "Automated code completion, aiming at generating subsequent tokens from unfinished code, has been significantly benefited from recent progress in pre-trained Large Language Models (LLMs). However, these models often suffer from coherence issues and hallucinations when dealing with complex code logic or extrapolating beyond their training data. Existing Retrieval Augmented Generation (RAG) techniques partially address these issues by retrieving relevant code with a separate encoding model where the retrieved snippet serves as contextual reference for code completion. However, their retrieval scope is subject to a singular perspective defined by the encoding model, which largely overlooks the complexity and diversity inherent in code semantics. To address this limitation, we propose ProCC, a code completion framework leveraging prompt engineering and the contextual multi-armed bandits algorithm to flexibly incorporate and adapt to multiple perspectives of code. ProCC first employs a prompt-based multi-retriever system which crafts prompt templates to elicit LLM knowledge to understand code semantics with multiple retrieval perspectives. Then, it adopts the adaptive retrieval selection algorithm to incorporate code similarity into the decision-making process to determine the most suitable retrieval perspective for the LLM to complete the code. Experimental results demonstrate that ProCC outperforms state-of-the-art code completion technique by 8.6% on our collected open-source benchmark suite and 10.1% on the private-domain benchmark suite collected from a billion-user e-commerce company in terms of Exact Match. ProCC also allows augmenting fine-tuned techniques in a plug-and-play manner, yielding 5.6% improvement over our studied fine-tuned model.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-13",
    "authors": [
      {
        "authorId": "2176503078",
        "name": "Hanzhuo Tan"
      },
      {
        "authorId": "2290488730",
        "name": "Qi Luo"
      },
      {
        "authorId": "2301251011",
        "name": "Ling Jiang"
      },
      {
        "authorId": "2301156520",
        "name": "Zizheng Zhan"
      },
      {
        "authorId": "2174027344",
        "name": "Jing Li"
      },
      {
        "authorId": "1557360433",
        "name": "Haotian Zhang"
      },
      {
        "authorId": "2290557105",
        "name": "Yuqun Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "e1446fc9b11e73e9f1d867df9012fdc3d3df60d0",
    "url": "https://www.semanticscholar.org/paper/e1446fc9b11e73e9f1d867df9012fdc3d3df60d0",
    "title": "RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems",
    "abstract": "Retrieval-augmented generation (RAG) can significantly improve the performance of language models (LMs) by providing additional context for tasks such as document-based question answering (DBQA). However, the effectiveness of RAG is highly dependent on its configuration. To systematically find the optimal configuration, we introduce RAGGED, a framework for analyzing RAG configurations across various DBQA tasks. Using the framework, we discover distinct LM behaviors in response to varying context quantities, context qualities, and retrievers. For instance, while some models are robust to noisy contexts, monotonically performing better with more contexts, others are more noise-sensitive and can effectively use only a few contexts before declining in performance. This framework also provides a deeper analysis of these differences by evaluating the LMs' sensitivity to signal and noise under specific context quality conditions. Using RAGGED, researchers and practitioners can derive actionable insights about how to optimally configure their RAG systems for their specific question-answering tasks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-14",
    "authors": [
      {
        "authorId": "2291134414",
        "name": "Jennifer Hsia"
      },
      {
        "authorId": "2291135503",
        "name": "Afreen Shaikh"
      },
      {
        "authorId": "2261355095",
        "name": "Zhiruo Wang"
      },
      {
        "authorId": "2285194103",
        "name": "Graham Neubig"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "40c017dbb11a8f63f986654f5e8deda89dc28ccf",
    "url": "https://www.semanticscholar.org/paper/40c017dbb11a8f63f986654f5e8deda89dc28ccf",
    "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems",
    "abstract": "Augmenting LLMs with context leads to improved performance across many applications. Despite much research on Retrieval Augmented Generation (RAG) systems, an open question is whether errors arise because LLMs fail to utilize the context from retrieval or the context itself is insufficient to answer the query. To shed light on this, we develop a new notion of sufficient context, along with a way to classify instances that have enough information to answer the query. We then use sufficient context to analyze several models and datasets. By stratifying errors based on context sufficiency, we find that proprietary LLMs (Gemini, GPT, Claude) excel at answering queries when the context is sufficient, but often output incorrect answers instead of abstaining when the context is not. On the other hand, open-source LLMs (Llama, Mistral, Gemma) hallucinate or abstain often, even with sufficient context. We further categorize cases when the context is useful, and improves accuracy, even though it does not fully answer the query and the model errs without the context. Building on our findings, we explore ways to reduce hallucinations in RAG systems, including a new selective generation method that leverages sufficient context information for guided abstention. Our method improves the fraction of correct answers among times where the model responds by 2-10% for Gemini, GPT, and Gemma.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-11-09",
    "authors": [
      {
        "authorId": "2220231789",
        "name": "Hailey Joren"
      },
      {
        "authorId": "2330232188",
        "name": "Jianyi Zhang"
      },
      {
        "authorId": "3340602",
        "name": "Chun-Sung Ferng"
      },
      {
        "authorId": "50270386",
        "name": "Da-Cheng Juan"
      },
      {
        "authorId": "2313338667",
        "name": "Ankur Taly"
      },
      {
        "authorId": "3125805",
        "name": "Cyrus Rashtchian"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "3e59c8f47b211bf82a1b0fa886fa399ec25044c7",
    "url": "https://www.semanticscholar.org/paper/3e59c8f47b211bf82a1b0fa886fa399ec25044c7",
    "title": "Emergency Patient Triage Improvement through a Retrieval-Augmented Generation Enhanced Large-Scale Language Model.",
    "abstract": "Objectives: Emergency medical triage is crucial for prioritizing patient care in emergency situations, yet its effectiveness can vary significantly based on the experience and training of the personnel involved. This study aims to evaluate the efficacy of integrating Retrieval Augmented Generation (RAG) with Large Language Models (LLMs), specifically OpenAI's GPT models, to standardize triage procedures and reduce variability in emergency care.Methods: We created 100 simulated triage scenarios based on modified cases from the Japanese National Examination for Emergency Medical Technicians. These scenarios were processed by the RAG-enhanced LLMs, and the models were given patient vital signs, symptoms, and observations from emergency medical services (EMS) teams as inputs. The primary outcome was the accuracy of triage classifications, which was used to compare the performance of the RAG-enhanced LLMs to that of emergency medical technicians and emergency physicians. Secondary outcomes included the rates of under-triage and over-triage.Results: The Generative Pre-trained Transformer 3.5 (GPT-3.5) with RAG model achieved a correct triage rate of 70%, significantly outperforming Emergency Medical Technicians (EMTs) with 35% and 38% correct rates, and emergency physicians with 50% and 47% correct rates (p < 0.05). Additionally, this model demonstrated a substantial reduction in under-triage rates to 8%, compared to 33% for GPT-3.5 without RAG, and 39% for GPT-4 without RAG.Conclusions: The integration of RAG with LLMs shows promise in improving the accuracy and consistency of medical assessments in emergency settings. Further validation in diverse medical settings with broader datasets is necessary to confirm the effectiveness and adaptability of these technologies in live environments.",
    "venue": "Prehospital Emergency Care",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-01",
    "authors": [
      {
        "authorId": "2309234093",
        "name": "Megumi Yazaki"
      },
      {
        "authorId": "145967080",
        "name": "S. Maki"
      },
      {
        "authorId": "40124964",
        "name": "T. Furuya"
      },
      {
        "authorId": "2309240234",
        "name": "Ken Inoue"
      },
      {
        "authorId": "2309237600",
        "name": "Ko Nagai"
      },
      {
        "authorId": "2185291545",
        "name": "Yuki Nagashima"
      },
      {
        "authorId": "2237224620",
        "name": "Juntaro Maruyama"
      },
      {
        "authorId": "40162960",
        "name": "Yasunori Toki"
      },
      {
        "authorId": "2193550808",
        "name": "Kyota Kitagawa"
      },
      {
        "authorId": "2151281299",
        "name": "Shuhei Iwata"
      },
      {
        "authorId": "2146876872",
        "name": "Takaki Kitamura"
      },
      {
        "authorId": "2309234080",
        "name": "Sho Gushiken"
      },
      {
        "authorId": "2123929595",
        "name": "Yuji Noguchi"
      },
      {
        "authorId": "2107707735",
        "name": "M. Inoue"
      },
      {
        "authorId": "36650273",
        "name": "Yasuhiro Shiga"
      },
      {
        "authorId": "6056327",
        "name": "K. Inage"
      },
      {
        "authorId": "143892751",
        "name": "S. Orita"
      },
      {
        "authorId": "2309236432",
        "name": "Takaaki Nakada"
      },
      {
        "authorId": "6933526",
        "name": "S. Ohtori"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "26420d986b441066a8350d445a6b7f46af93e952",
    "url": "https://www.semanticscholar.org/paper/26420d986b441066a8350d445a6b7f46af93e952",
    "title": "TRACE the Evidence: Constructing Knowledge-Grounded Reasoning Chains for Retrieval-Augmented Generation",
    "abstract": "Retrieval-augmented generation (RAG) offers an effective approach for addressing question answering (QA) tasks. However, the imperfections of the retrievers in RAG models often result in the retrieval of irrelevant information, which could introduce noises and degrade the performance, especially when handling multi-hop questions that require multiple steps of reasoning. To enhance the multi-hop reasoning ability of RAG models, we propose TRACE. TRACE constructs knowledge-grounded reasoning chains, which are a series of logically connected knowledge triples, to identify and integrate supporting evidence from the retrieved documents for answering questions. Specifically, TRACE employs a KG Generator to create a knowledge graph (KG) from the retrieved documents, and then uses an Autoregressive Reasoning Chain Constructor to build reasoning chains. Experimental results on three multi-hop QA datasets show that TRACE achieves an average performance improvement of up to 14.03% compared to using all the retrieved documents. Moreover, the results indicate that using reasoning chains as context, rather than the entire documents, is often sufficient to correctly answer questions.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-17",
    "authors": [
      {
        "authorId": "1384241384",
        "name": "Jinyuan Fang"
      },
      {
        "authorId": "3451645",
        "name": "Zaiqiao Meng"
      },
      {
        "authorId": "2260652308",
        "name": "Craig Macdonald"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "3676e7dffe9ca779a0fbf03fda53f2a90018a15e",
    "url": "https://www.semanticscholar.org/paper/3676e7dffe9ca779a0fbf03fda53f2a90018a15e",
    "title": "Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented Generation in Niche Domains, Exemplified by Korean Medicine",
    "abstract": "We propose a natural language prompt-based retrieval augmented generation (Prompt-RAG), a novel approach to enhance the performance of generative large language models (LLMs) in niche domains. Conventional RAG methods mostly require vector embeddings, yet the suitability of generic LLM-based embedding representations for specialized domains remains uncertain. To explore and exemplify this point, we compared vector embeddings from Korean Medicine (KM) and Conventional Medicine (CM) documents, finding that KM document embeddings correlated more with token overlaps and less with human-assessed document relatedness, in contrast to CM embeddings. Prompt-RAG, distinct from conventional RAG models, operates without the need for embedding vectors. Its performance was assessed through a Question-Answering (QA) chatbot application, where responses were evaluated for relevance, readability, and informativeness. The results showed that Prompt-RAG outperformed existing models, including ChatGPT and conventional vector embedding-based RAGs, in terms of relevance and informativeness. Despite challenges like content structuring and response latency, the advancements in LLMs are expected to encourage the use of Prompt-RAG, making it a promising tool for other domains in need of RAG methods.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-20",
    "authors": [
      {
        "authorId": "2280230665",
        "name": "Bongsu Kang"
      },
      {
        "authorId": "2280174830",
        "name": "Jundong Kim"
      },
      {
        "authorId": "2267240750",
        "name": "Tae-Rim Yun"
      },
      {
        "authorId": "2280253945",
        "name": "Chang-Eop Kim"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "fd4575eec66953a204ad11d34f23f72f02b14088",
    "url": "https://www.semanticscholar.org/paper/fd4575eec66953a204ad11d34f23f72f02b14088",
    "title": "Mix-of-Granularity: Optimize the Chunking Granularity for Retrieval-Augmented Generation",
    "abstract": "Integrating information from various reference databases is a major challenge for Retrieval-Augmented Generation (RAG) systems because each knowledge source adopts a unique data structure and follows different conventions. Retrieving from multiple knowledge sources with one fixed strategy usually leads to under-exploitation of information. To mitigate this drawback, inspired by Mix-of-Expert, we introduce Mix-of-Granularity (MoG), a method that dynamically determines the optimal granularity of a knowledge source based on input queries using a router. The router is efficiently trained with a newly proposed loss function employing soft labels. We further extend MoG to MoG-Graph (MoGG), where reference documents are pre-processed as graphs, enabling the retrieval of distantly situated snippets. Experiments demonstrate that MoG and MoGG effectively predict optimal granularity levels, significantly enhancing the performance of the RAG system in downstream tasks. The code of both MoG and MoGG are released in https://github.com/ZGChung/Mix-of-Granularity.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-01",
    "authors": [
      {
        "authorId": "2283454315",
        "name": "Zijie Zhong"
      },
      {
        "authorId": "2304625092",
        "name": "Hanwen Liu"
      },
      {
        "authorId": "2304762297",
        "name": "Xiaoya Cui"
      },
      {
        "authorId": "2304524376",
        "name": "Xiaofan Zhang"
      },
      {
        "authorId": "2283547609",
        "name": "Zengchang Qin"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "444aa31192c87f996bb01fa856cb765a19cd5323",
    "url": "https://www.semanticscholar.org/paper/444aa31192c87f996bb01fa856cb765a19cd5323",
    "title": "Robust Implementation of Retrieval-Augmented Generation on Edge-based Computing-in-Memory Architectures",
    "abstract": "Large Language Models (LLMs) deployed on edge devices learn through fine-tuning and updating a certain portion of their parameters. Although such learning methods can be optimized to reduce resource utilization, the overall required resources remain a heavy burden on edge devices. Instead, Retrieval-Augmented Generation (RAG), a resource-efficient LLM learning method, can improve the quality of the LLM-generated content without updating model parameters. However, the RAG-based LLM may involve repetitive searches on the profile data in every user-LLM interaction. This search can lead to significant latency along with the accumulation of user data. Conventional efforts to decrease latency result in restricting the size of saved user data, thus reducing the scalability of RAG as user data continuously grows. It remains an open question: how to free RAG from the constraints of latency and scalability on edge devices? In this paper, we propose a novel framework to accelerate RAG via Computing-in-Memory (CiM) architectures. It accelerates matrix multiplications by performing in-situ computation inside the memory while avoiding the expensive data transfer between the computing unit and memory. Our framework, Robust CiM-backed RAG (RoCR), utilizing a novel contrastive learning-based training method and noise-aware training, can enable RAG to efficiently search profile data with CiM. To the best of our knowledge, this is the first work utilizing CiM to accelerate RAG.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-07",
    "authors": [
      {
        "authorId": "2037425350",
        "name": "Ruiyang Qin"
      },
      {
        "authorId": "153165397",
        "name": "Zheyu Yan"
      },
      {
        "authorId": "2051047670",
        "name": "Dewen Zeng"
      },
      {
        "authorId": "153862908",
        "name": "Zhenge Jia"
      },
      {
        "authorId": "2280142133",
        "name": "Dancheng Liu"
      },
      {
        "authorId": "2300330540",
        "name": "Jianbo Liu"
      },
      {
        "authorId": "2300426940",
        "name": "Zhi Zheng"
      },
      {
        "authorId": "31716183",
        "name": "N. Cao"
      },
      {
        "authorId": "2300286479",
        "name": "Kai Ni"
      },
      {
        "authorId": "2280070683",
        "name": "Jinjun Xiong"
      },
      {
        "authorId": "2269417488",
        "name": "Yiyu Shi"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "32115a8efe68b54b210ebab24f9b165628e61f06",
    "url": "https://www.semanticscholar.org/paper/32115a8efe68b54b210ebab24f9b165628e61f06",
    "title": "Nanjing Yunjin intelligent question-answering system based on knowledge graphs and retrieval augmented generation technology",
    "abstract": null,
    "venue": "Heritage Science",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "https://heritagesciencejournal.springeropen.com/counter/pdf/10.1186/s40494-024-01231-3",
      "status": "GOLD"
    },
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-09",
    "authors": [
      {
        "authorId": "2259997355",
        "name": "Liang Xu"
      },
      {
        "authorId": "2215267926",
        "name": "Lu Lu"
      },
      {
        "authorId": "2260641330",
        "name": "Minglu Liu"
      },
      {
        "authorId": "2296032461",
        "name": "Chengxuan Song"
      },
      {
        "authorId": "2295927926",
        "name": "Lizhen Wu"
      }
    ],
    "source": "semantic_scholar",
    "score": 84.1415686865115
  },
  {
    "paperId": "32b9f9e2654b7280671a2be355cbd04c1b7cc2ea",
    "url": "https://www.semanticscholar.org/paper/32b9f9e2654b7280671a2be355cbd04c1b7cc2ea",
    "title": "FeB4RAG: Evaluating Federated Search in the Context of Retrieval Augmented Generation",
    "abstract": "Federated search systems aggregate results from multiple search engines, selecting appropriate sources to enhance result quality and align with user intent. With the increasing uptake of Retrieval-Augmented Generation (RAG) pipelines, federated search can play a pivotal role in sourcing relevant information across heterogeneous data sources to generate informed responses. However, existing datasets, such as those developed in the past TREC FedWeb tracks, predate the RAG paradigm shift and lack representation of modern information retrieval challenges. To bridge this gap, we present FeB4RAG, a novel dataset specifically designed for federated search within RAG frameworks. This dataset, derived from 16 sub-collections of the widely used \\beir benchmarking collection, includes 790 information requests (akin to conversational queries) tailored for chatbot applications, along with top results returned by each resource and associated LLM-derived relevance judgements. Additionally, to support the need for this collection, we demonstrate the impact on response generation of a high quality federated search system for RAG compared to a naive approach to federated search. We do so by comparing answers generated through the RAG pipeline through a qualitative side-by-side comparison. Our collection fosters and supports the development and evaluation of new federated search methods, especially in the context of RAG pipelines.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-19",
    "authors": [
      {
        "authorId": "2146514461",
        "name": "Shuai Wang"
      },
      {
        "authorId": "2051655939",
        "name": "Ekaterina Khramtsova"
      },
      {
        "authorId": "1630489015",
        "name": "Shengyao Zhuang"
      },
      {
        "authorId": "1692855",
        "name": "G. Zuccon"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.87639203842082
  },
  {
    "paperId": "13eda6151efbb2ba10a879e21e31a118094fbe27",
    "url": "https://www.semanticscholar.org/paper/13eda6151efbb2ba10a879e21e31a118094fbe27",
    "title": "Question-Answering Based Summarization of Electronic Health Records using Retrieval Augmented Generation",
    "abstract": "Summarization of electronic health records (EHRs) can substantially minimize 'screen time' for both patients as well as medical personnel. In recent years summarization of EHRs have employed machine learning pipelines using state of the art neural models. However, these models have produced less than adequate results that are attributed to the difficulty of obtaining sufficient annotated data for training. Moreover, the requirement to consider the entire content of an EHR in summarization has resulted in poor performance due to the fact that attention mechanisms in modern large language models (LLMs) adds a quadratic complexity in terms of the size of the input. We propose here a method that mitigates these shortcomings by combining semantic search, retrieval augmented generation (RAG) and question-answering using the latest LLMs. In our approach summarization is the extraction of answers to specific questions that are deemed important by subject-matter experts (SMEs). Our approach is quite efficient; requires minimal to no training; does not suffer from the 'hallucination' problem of LLMs; and it ensures diversity, since the summary will not have repeated content but diverse answers to specific questions.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-03",
    "authors": [
      {
        "authorId": "2277599672",
        "name": "Walid Saba"
      },
      {
        "authorId": "2277600739",
        "name": "Suzanne Wendelken"
      },
      {
        "authorId": "2277602186",
        "name": "James. Shanahan"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "680824bef5d6f98d669c49246363f0894a678e3b",
    "url": "https://www.semanticscholar.org/paper/680824bef5d6f98d669c49246363f0894a678e3b",
    "title": "Ragnar√∂k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track",
    "abstract": "Did you try out the new Bing Search? Or maybe you fiddled around with Google AI~Overviews? These might sound familiar because the modern-day search stack has recently evolved to include retrieval-augmented generation (RAG) systems. They allow searching and incorporating real-time data into large language models (LLMs) to provide a well-informed, attributed, concise summary in contrast to the traditional search paradigm that relies on displaying a ranked list of documents. Therefore, given these recent advancements, it is crucial to have an arena to build, test, visualize, and systematically evaluate RAG-based search systems. With this in mind, we propose the TREC 2024 RAG Track to foster innovation in evaluating RAG systems. In our work, we lay out the steps we've made towards making this track a reality -- we describe the details of our reusable framework, Ragnar\\\"ok, explain the curation of the new MS MARCO V2.1 collection choice, release the development topics for the track, and standardize the I/O definitions which assist the end user. Next, using Ragnar\\\"ok, we identify and provide key industrial baselines such as OpenAI's GPT-4o or Cohere's Command R+. Further, we introduce a web-based user interface for an interactive arena allowing benchmarking pairwise RAG systems by crowdsourcing. We open-source our Ragnar\\\"ok framework and baselines to achieve a unified standard for future RAG systems.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-06-24",
    "authors": [
      {
        "authorId": "1816753042",
        "name": "Ronak Pradeep"
      },
      {
        "authorId": "47583894",
        "name": "Nandan Thakur"
      },
      {
        "authorId": "71076877",
        "name": "Sahel Sharifymoghaddam"
      },
      {
        "authorId": "2308035714",
        "name": "Eric Zhang"
      },
      {
        "authorId": "2308034071",
        "name": "Ryan Nguyen"
      },
      {
        "authorId": "2308033528",
        "name": "Daniel Campos"
      },
      {
        "authorId": "2182067638",
        "name": "Nick Craswell"
      },
      {
        "authorId": "2300329796",
        "name": "Jimmy Lin"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "bd370edcfe7020663a05649e6f37d1c963dcdf80",
    "url": "https://www.semanticscholar.org/paper/bd370edcfe7020663a05649e6f37d1c963dcdf80",
    "title": "LLMs Know What They Need: Leveraging a Missing Information Guided Framework to Empower Retrieval-Augmented Generation",
    "abstract": "Retrieval-Augmented Generation (RAG) demonstrates great value in alleviating outdated knowledge or hallucination by supplying LLMs with updated and relevant knowledge. However, there are still several difficulties for RAG in understanding complex multi-hop query and retrieving relevant documents, which require LLMs to perform reasoning and retrieve step by step. Inspired by human's reasoning process in which they gradually search for the required information, it is natural to ask whether the LLMs could notice the missing information in each reasoning step. In this work, we first experimentally verified the ability of LLMs to extract information as well as to know the missing. Based on the above discovery, we propose a Missing Information Guided Retrieve-Extraction-Solving paradigm (MIGRES), where we leverage the identification of missing information to generate a targeted query that steers the subsequent knowledge retrieval. Besides, we design a sentence-level re-ranking filtering approach to filter the irrelevant content out from document, along with the information extraction capability of LLMs to extract useful information from cleaned-up documents, which in turn to bolster the overall efficacy of RAG. Extensive experiments conducted on multiple public datasets reveal the superiority of the proposed MIGRES method, and analytical experiments demonstrate the effectiveness of our proposed modules.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-22",
    "authors": [
      {
        "authorId": "2234024810",
        "name": "Keheng Wang"
      },
      {
        "authorId": "2290402867",
        "name": "Feiyu Duan"
      },
      {
        "authorId": "2298213582",
        "name": "Peiguang Li"
      },
      {
        "authorId": "2295934207",
        "name": "Sirui Wang"
      },
      {
        "authorId": "2290035990",
        "name": "Xunliang Cai"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "b71141dbd0e4827156c696e05ba1fa068ad43ee1",
    "url": "https://www.semanticscholar.org/paper/b71141dbd0e4827156c696e05ba1fa068ad43ee1",
    "title": "GastroBot: a Chinese gastrointestinal disease chatbot based on the retrieval-augmented generation",
    "abstract": "Introduction Large Language Models (LLMs) play a crucial role in clinical information processing, showcasing robust generalization across diverse language tasks. However, existing LLMs, despite their significance, lack optimization for clinical applications, presenting challenges in terms of illusions and interpretability. The Retrieval-Augmented Generation (RAG) model addresses these issues by providing sources for answer generation, thereby reducing errors. This study explores the application of RAG technology in clinical gastroenterology to enhance knowledge generation on gastrointestinal diseases. Methods We fine-tuned the embedding model using a corpus consisting of 25 guidelines on gastrointestinal diseases. The fine-tuned model exhibited an 18% improvement in hit rate compared to its base model, gte-base-zh. Moreover, it outperformed OpenAI‚Äôs Embedding model by 20%. Employing the RAG framework with the llama-index, we developed a Chinese gastroenterology chatbot named ‚ÄúGastroBot,‚Äù which significantly improves answer accuracy and contextual relevance, minimizing errors and the risk of disseminating misleading information. Results When evaluating GastroBot using the RAGAS framework, we observed a context recall rate of 95%. The faithfulness to the source, stands at 93.73%. The relevance of answers exhibits a strong correlation, reaching 92.28%. These findings highlight the effectiveness of GastroBot in providing accurate and contextually relevant information about gastrointestinal diseases. During manual assessment of GastroBot, in comparison with other models, our GastroBot model delivers a substantial amount of valuable knowledge while ensuring the completeness and consistency of the results. Discussion Research findings suggest that incorporating the RAG method into clinical gastroenterology can enhance the accuracy and reliability of large language models. Serving as a practical implementation of this method, GastroBot has demonstrated significant enhancements in contextual comprehension and response quality. Continued exploration and refinement of the model are poised to drive forward clinical information processing and decision support in the gastroenterology field.",
    "venue": "Frontiers in Medicine",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "https://www.frontiersin.org/articles/10.3389/fmed.2024.1392555/pdf?isPublishedV2=False",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-22",
    "authors": [
      {
        "authorId": "2291873253",
        "name": "Qingqing Zhou"
      },
      {
        "authorId": "2291855485",
        "name": "Can Liu"
      },
      {
        "authorId": "2292054714",
        "name": "Yuchen Duan"
      },
      {
        "authorId": "2292068852",
        "name": "Kaijie Sun"
      },
      {
        "authorId": "2303286334",
        "name": "Yu Li"
      },
      {
        "authorId": "2291812967",
        "name": "Hongxing Kan"
      },
      {
        "authorId": "2292371407",
        "name": "Zongyun Gu"
      },
      {
        "authorId": "2151503729",
        "name": "Jianhua Shu"
      },
      {
        "authorId": "2291867054",
        "name": "Jili Hu"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "9b7854829ae4d4653a56ba04880aff848d70fc42",
    "url": "https://www.semanticscholar.org/paper/9b7854829ae4d4653a56ba04880aff848d70fc42",
    "title": "Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models",
    "abstract": "The robustness of large language models (LLMs) becomes increasingly important as their use rapidly grows in a wide range of domains. Retrieval-Augmented Generation (RAG) is considered as a means to improve the trustworthiness of text generation from LLMs. However, how the outputs from RAG-based LLMs are affected by slightly different inputs is not well studied. In this work, we find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers. We systematically evaluate the effect of such prefixes on RAG by introducing a novel optimization technique called Gradient Guided Prompt Perturbation (GGPP). GGPP achieves a high success rate in steering outputs of RAG-based LLMs to targeted wrong answers. It can also cope with instructions in the prompts requesting to ignore irrelevant context. We also exploit LLMs' neuron activation difference between prompts with and without GGPP perturbations to give a method that improves the robustness of RAG-based LLMs through a highly effective detector trained on neuron activation triggered by GGPP generated prompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of our methods.",
    "venue": "Knowledge Discovery and Data Mining",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-11",
    "authors": [
      {
        "authorId": "2283875900",
        "name": "Zhibo Hu"
      },
      {
        "authorId": "2109117515",
        "name": "Chen Wang"
      },
      {
        "authorId": "2283842221",
        "name": "Yanfeng Shu"
      },
      {
        "authorId": "2283841553",
        "name": "Helen Paik"
      },
      {
        "authorId": "2145199748",
        "name": "Liming Zhu"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "6bdb704aa7f99a3d9899532c547616767bbf8302",
    "url": "https://www.semanticscholar.org/paper/6bdb704aa7f99a3d9899532c547616767bbf8302",
    "title": "MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented Generation via Knowledge-enhanced Reranking and Noise-injected Training",
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in processing and generating content across multiple data modalities. However, a significant drawback of MLLMs is their reliance on static training data, leading to outdated information and limited contextual awareness. This static nature hampers their ability to provide accurate and up-to-date responses, particularly in dynamic or rapidly evolving contexts. Though integrating Multimodal Retrieval-augmented Generation (Multimodal RAG) offers a promising solution, the system would inevitably encounter the multi-granularity noisy correspondence (MNC) problem, which hinders accurate retrieval and generation. In this work, we propose RagVL, a novel framework with knowledge-enhanced reranking and noise-injected training, to address these limitations. We instruction-tune the MLLM with a simple yet effective instruction template to induce its ranking ability and serve it as a reranker to precisely filter the top-k retrieved images. For generation, we inject visual noise during training at the data and token levels to enhance the generator's robustness. Extensive experiments on the subsets of two datasets that require retrieving and reasoning over images to answer a given query verify the effectiveness of our method. Code and models are available at https://github.com/IDEA-FinAI/RagVL.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-31",
    "authors": [
      {
        "authorId": "2314696037",
        "name": "Zhanpeng Chen"
      },
      {
        "authorId": "2250617116",
        "name": "Chengjin Xu"
      },
      {
        "authorId": "2276514316",
        "name": "Yiyan Qi"
      },
      {
        "authorId": "2284217200",
        "name": "Jian Guo"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "68acf42708fd3bfad548872d04330ed089019d45",
    "url": "https://www.semanticscholar.org/paper/68acf42708fd3bfad548872d04330ed089019d45",
    "title": "Hierarchical Retrieval-Augmented Generation Model with Rethink for Multi-hop Question Answering",
    "abstract": "Multi-hop Question Answering (QA) necessitates complex reasoning by integrating multiple pieces of information to resolve intricate questions. However, existing QA systems encounter challenges such as outdated information, context window length limitations, and an accuracy-quantity trade-off. To address these issues, we propose a novel framework, the Hierarchical Retrieval-Augmented Generation Model with Rethink (HiRAG), comprising Decomposer, Definer, Retriever, Filter, and Summarizer five key modules. We introduce a new hierarchical retrieval strategy that incorporates both sparse retrieval at the document level and dense retrieval at the chunk level, effectively integrating their strengths. Additionally, we propose a single-candidate retrieval method to mitigate the limitations of multi-candidate retrieval. We also construct two new corpora, Indexed Wikicorpus and Profile Wikicorpus, to address the issues of outdated and insufficient knowledge. Our experimental results on four datasets demonstrate that HiRAG outperforms state-of-the-art models across most metrics, and our Indexed Wikicorpus is effective. The code for HiRAG is available at https://github.com/2282588541a/HiRAG",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-20",
    "authors": [
      {
        "authorId": "2287879683",
        "name": "Xiaoming Zhang"
      },
      {
        "authorId": "2249764692",
        "name": "Ming Wang"
      },
      {
        "authorId": "2135971356",
        "name": "Xiaocui Yang"
      },
      {
        "authorId": "2111226672",
        "name": "Daling Wang"
      },
      {
        "authorId": "2293320087",
        "name": "Shi Feng"
      },
      {
        "authorId": "2108463824",
        "name": "Yifei Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "1d1beece295703c0cb3e545edaa12a4336b407bc",
    "url": "https://www.semanticscholar.org/paper/1d1beece295703c0cb3e545edaa12a4336b407bc",
    "title": "Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models",
    "abstract": "Iterative retrieval refers to the process in which the model continuously queries the retriever during generation to enhance the relevance of the retrieved knowledge, thereby improving the performance of Retrieval-Augmented Generation (RAG). Existing work typically employs few-shot prompting or manually constructed rules to implement iterative retrieval. This introduces additional inference overhead and overlooks the remarkable reasoning capabilities of Large Language Models (LLMs). In this paper, we introduce Auto-RAG, an autonomous iterative retrieval model centered on the LLM's powerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues with the retriever, systematically planning retrievals and refining queries to acquire valuable knowledge. This process continues until sufficient external information is gathered, at which point the results are presented to the user. To this end, we develop a method for autonomously synthesizing reasoning-based decision-making instructions in iterative retrieval and fine-tuned the latest open-source LLMs. The experimental results indicate that Auto-RAG is capable of autonomous iterative interaction with the retriever, effectively leveraging the remarkable reasoning and decision-making abilities of LLMs, which lead to outstanding performance across six benchmarks. Further analysis reveals that Auto-RAG can autonomously adjust the number of iterations based on the difficulty of the questions and the utility of the retrieved knowledge, without requiring any human intervention. Moreover, Auto-RAG expresses the iterative retrieval process in natural language, enhancing interpretability while providing users with a more intuitive experience\\footnote{Code is available at \\url{https://github.com/ictnlp/Auto-RAG}.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-11-29",
    "authors": [
      {
        "authorId": "2288191839",
        "name": "Tian Yu"
      },
      {
        "authorId": "2480521",
        "name": "Shaolei Zhang"
      },
      {
        "authorId": "2261199971",
        "name": "Yang Feng"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "0cfeaa745a07a3494c7e3faa4e24c7071e50c177",
    "url": "https://www.semanticscholar.org/paper/0cfeaa745a07a3494c7e3faa4e24c7071e50c177",
    "title": "Retrieval Augmented Generation for Dynamic Graph Modeling",
    "abstract": "Dynamic graph modeling is crucial for analyzing evolving patterns in various applications. Existing approaches often integrate graph neural networks with temporal modules or redefine dynamic graph modeling as a generative sequence task. However, these methods typically rely on isolated historical contexts of the target nodes from a narrow perspective, neglecting occurrences of similar patterns or relevant cases associated with other nodes. In this work, we introduce the Retrieval-Augmented Generation for Dynamic Graph Modeling (RAG4DyG) framework, which leverages guidance from contextually and temporally analogous examples to broaden the perspective of each node. This approach presents two critical challenges: (1) How to identify and retrieve high-quality demonstrations that are contextually and temporally analogous to dynamic graph samples? (2) How can these demonstrations be effectively integrated to improve dynamic graph modeling? To address these challenges, we propose RAG4DyG, which enriches the understanding of historical contexts by retrieving and learning from contextually and temporally pertinent demonstrations. Specifically, we employ a time- and context-aware contrastive learning module to identify and retrieve relevant cases for each query sequence. Moreover, we design a graph fusion strategy to integrate the retrieved cases, thereby augmenting the inherent historical contexts for improved prediction. Extensive experiments on real-world datasets across different domains demonstrate the effectiveness of RAG4DyG for dynamic graph modeling.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-26",
    "authors": [
      {
        "authorId": "2301261715",
        "name": "Yuxia Wu"
      },
      {
        "authorId": "2281289986",
        "name": "Yuan Fang"
      },
      {
        "authorId": "2281037015",
        "name": "Lizi Liao"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "e40b6f84ebb817159122519b2606b47a2e3e0308",
    "url": "https://www.semanticscholar.org/paper/e40b6f84ebb817159122519b2606b47a2e3e0308",
    "title": "Towards Retrieval Augmented Generation over Large Video Libraries",
    "abstract": "Video content creators need efficient tools to repurpose content, a task that often requires complex manual or automated searches. Crafting a new video from large video libraries remains a challenge. In this paper we introduce the task of Video Library Question Answering (VLQA) through an interoperable architecture that applies Retrieval Augmented Generation (RAG) to video libraries. We propose a system that uses large language models (LLMs) to generate search queries, retrieving relevant video moments indexed by speech and visual metadata. An answer generation module then integrates user queries with this metadata to produce responses with specific video timestamps. This approach shows promise in multimedia content retrieval, and AI-assisted video content creation.",
    "venue": "International Conference on Human System Interaction",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-21",
    "authors": [
      {
        "authorId": "2202022815",
        "name": "Yannis Tevissen"
      },
      {
        "authorId": "2300092511",
        "name": "Khalil Guetari"
      },
      {
        "authorId": "2202022543",
        "name": "Fr'ed'eric Petitpont"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "a685613b78f58aca734790bc212622cd5f8da3d6",
    "url": "https://www.semanticscholar.org/paper/a685613b78f58aca734790bc212622cd5f8da3d6",
    "title": "SRAG: Speech Retrieval Augmented Generation for Spoken Language Understanding",
    "abstract": "Retrieval augmented generation (RAG) has shown promise for enhancing natural language understanding (NLU) capabilities of large language models (LLMs) by retrieving relevant knowledge as prompts. Extending RAG to spoken language understanding (SLU) represents an important research direction. This paper proposes a RAG approach for improving SLU. First, the encoder of a pretrained automatic speech recognition model is utilized for speech retrieval over the training set. The corresponding texts and intent labels are then formulated as prompts to guide the SLU decoder. Furthermore, a prompt attention mechanism is introduced to strengthen the attention between generation and prompts. Experiments demonstrate that the proposed speech RAG approach substantially outperforms conventional end-to-end and cascaded SLU models in intent prediction from speech. This highlights the efficacy of leveraging retrieval-based prompting to incorporate external knowledge for advancing SLU.",
    "venue": "2024 IEEE 2nd International Conference on Control, Electronics and Computer Technology (ICCECT)",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2024-04-26",
    "authors": [
      {
        "authorId": "2292942411",
        "name": "Hao Yang"
      },
      {
        "authorId": "2293704687",
        "name": "Min Zhang"
      },
      {
        "authorId": "8884457",
        "name": "Daimeng Wei"
      },
      {
        "authorId": "1838933693",
        "name": "Jiaxin Guo"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.39720770839918
  },
  {
    "paperId": "3eeb6829db131c59558bff33f05aa26891245680",
    "url": "https://www.semanticscholar.org/paper/3eeb6829db131c59558bff33f05aa26891245680",
    "title": "Carbon Footprint Accounting Driven by Large Language Models and Retrieval-augmented Generation",
    "abstract": "Carbon footprint accounting is crucial for quantifying greenhouse gas emissions and achieving carbon neutrality.The dynamic nature of processes, accounting rules, carbon-related policies, and energy supply structures necessitates real-time updates of CFA. Traditional life cycle assessment methods rely heavily on human expertise, making near-real-time updates challenging. This paper introduces a novel approach integrating large language models (LLMs) with retrieval-augmented generation technology to enhance the real-time, professional, and economical aspects of carbon footprint information retrieval and analysis. By leveraging LLMs' logical and language understanding abilities and RAG's efficient retrieval capabilities, the proposed method LLMs-RAG-CFA can retrieve more relevant professional information to assist LLMs, enhancing the model's generative abilities. This method offers broad professional coverage, efficient real-time carbon footprint information acquisition and accounting, and cost-effective automation without frequent LLMs' parameter updates. Experimental results across five industries(primary aluminum, lithium battery, photovoltaic, new energy vehicles, and transformers)demonstrate that the LLMs-RAG-CFA method outperforms traditional methods and other LLMs, achieving higher information retrieval rates and significantly lower information deviations and carbon footprint accounting deviations. The economically viable design utilizes RAG technology to balance real-time updates with cost-effectiveness, providing an efficient, reliable, and cost-saving solution for real-time carbon emission management, thereby enhancing environmental sustainability practices.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-19",
    "authors": [
      {
        "authorId": "2289318607",
        "name": "Haijin Wang"
      },
      {
        "authorId": "2316512589",
        "name": "Mianrong Zhang"
      },
      {
        "authorId": "2316588996",
        "name": "Zheng Chen"
      },
      {
        "authorId": "2316427861",
        "name": "Nan Shang"
      },
      {
        "authorId": "2308032355",
        "name": "Shangheng Yao"
      },
      {
        "authorId": "2316429681",
        "name": "Fushuan Wen"
      },
      {
        "authorId": "2316727946",
        "name": "Junhua Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "fe50c82de31bce81d8777d335c667a50efcd2f39",
    "url": "https://www.semanticscholar.org/paper/fe50c82de31bce81d8777d335c667a50efcd2f39",
    "title": "Leveraging Chemistry Foundation Models to Facilitate Structure Focused Retrieval Augmented Generation in Multi-Agent Workflows for Catalyst and Materials Design",
    "abstract": "Molecular property prediction and generative design via deep learning models has been the subject of intense research given its potential to accelerate development of new, high-performance materials. More recently, these workflows have been significantly augmented with the advent of large language models (LLMs) and systems of autonomous agents capable of utilizing pre-trained models to make predictions in the context of more complex research tasks. While effective, there is still room for substantial improvement within agentic systems on the retrieval of salient information for material design tasks. Within this context, alternative uses of predictive deep learning models, such as leveraging their latent representations to facilitate cross-modal retrieval augmented generation within agentic systems for task-specific materials design, has remained unexplored. Herein, we demonstrate that large, pre-trained chemistry foundation models can serve as a basis for enabling structure-focused, semantic chemistry information retrieval for both small-molecules, complex polymeric materials, and reactions. Additionally, we show the use of chemistry foundation models in conjunction with multi-modal models such as OpenCLIP facilitate unprecedented queries and information retrieval across multiple characterization data domains. Finally, we demonstrate the integration of these models within multi-agent systems to facilitate structure and topological-based natural language queries and information retrieval for different research tasks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-21",
    "authors": [
      {
        "authorId": "2290645158",
        "name": "Nathaniel H. Park"
      },
      {
        "authorId": "2316557847",
        "name": "Tiffany J. Callahan"
      },
      {
        "authorId": "2268784439",
        "name": "James L. Hedrick"
      },
      {
        "authorId": "2220395834",
        "name": "Tim Erdmann"
      },
      {
        "authorId": "2316558120",
        "name": "Sara Capponi"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "70544849186a588287a2a1604903892c05749f62",
    "url": "https://www.semanticscholar.org/paper/70544849186a588287a2a1604903892c05749f62",
    "title": "Assessing ChatGPT4 with and without retrieval-augmented generation in anticoagulation management for gastrointestinal procedures",
    "abstract": "Background In view of the growing complexity of managing anticoagulation for patients undergoing gastrointestinal (GI) procedures, this study evaluated ChatGPT-4‚Äôs ability to provide accurate medical guidance, comparing it with its prior artificial intelligence (AI) models (ChatGPT-3.5) and the retrieval-augmented generation (RAG)-supported model (ChatGPT4-RAG). Methods Thirty-six anticoagulation-related questions, based on professional guidelines, were answered by ChatGPT-4. Nine gastroenterologists assessed these responses for accuracy and relevance. ChatGPT-4‚Äôs performance was also compared to that of ChatGPT-3.5 and ChatGPT4-RAG. Additionally, a survey was conducted to understand gastroenterologists‚Äô perceptions of ChatGPT-4. Results ChatGPT-4‚Äôs responses showed significantly better accuracy and coherence compared to ChatGPT-3.5, with 30.5% of responses fully accurate and 47.2% generally accurate. ChatGPT4-RAG demonstrated a higher ability to integrate current information, achieving 75% full accuracy. Notably, for diagnostic and therapeutic esophagogastroduodenoscopy, 51.8% of responses were fully accurate; for endoscopic retrograde cholangiopancreatography with and without stent placement, 42.8% were fully accurate; and for diagnostic and therapeutic colonoscopy, 50% were fully accurate. Conclusions ChatGPT4-RAG significantly advances anticoagulation management in endoscopic procedures, offering reliable and precise medical guidance. However, medicolegal considerations mean that a 75% full accuracy rate remains inadequate for independent clinical decision-making. AI may be more appropriately utilized to support and confirm clinicians‚Äô decisions, rather than replace them. Further evaluation is essential to maintain patient confidentiality and the integrity of the physician‚Äìpatient relationship.",
    "venue": "Annals of gastroenterology : quarterly publication of the Hellenic Society of Gastroenterology",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-08-19",
    "authors": [
      {
        "authorId": "2150368819",
        "name": "Sheza Malik"
      },
      {
        "authorId": "2269066296",
        "name": "Himal Kharel"
      },
      {
        "authorId": "1657604107",
        "name": "D. Dahiya"
      },
      {
        "authorId": "2306766933",
        "name": "Hassam Ali"
      },
      {
        "authorId": "2299986514",
        "name": "Hanna Blaney"
      },
      {
        "authorId": "2274716413",
        "name": "Achintya Singh"
      },
      {
        "authorId": "67220930",
        "name": "J. Dhar"
      },
      {
        "authorId": "13637423",
        "name": "A. Perisetti"
      },
      {
        "authorId": "5009600",
        "name": "A. Facciorusso"
      },
      {
        "authorId": "47648670",
        "name": "S. Chandan"
      },
      {
        "authorId": "2267947425",
        "name": "Babu P. Mohan"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "6fff65981e79981e47ab219fd12ccc824d47d6ce",
    "url": "https://www.semanticscholar.org/paper/6fff65981e79981e47ab219fd12ccc824d47d6ce",
    "title": "Adaptive Control of Retrieval-Augmented Generation for Large Language Models Through Reflective Tags",
    "abstract": "While retrieval-augmented generation (RAG) enhances large language models (LLMs), it also introduces challenges that can impact accuracy and performance. In practice, RAG can obscure the intrinsic strengths of LLMs. Firstly, LLMs may become too reliant on external retrieval, underutilizing their own knowledge and reasoning, which can diminish responsiveness. Secondly, RAG may introduce irrelevant or low-quality data, adding noise that disrupts generation, especially with complex tasks. This paper proposes an RAG framework that uses reflective tags to manage retrieval, evaluating documents in parallel and applying the chain-of-thought (CoT) technique for step-by-step generation. The model selects the highest quality content for final output. The key contributions are as follows: (1) reducing hallucinations by focusing on high-scoring documents; (2) improving real-time performance through efficient retrieval; and (3) mitigating negative effects by filtering out irrelevant information using parallel generation and reflective tagging. These innovations aim to optimize RAG for more reliable, high-quality results.",
    "venue": "Electronics",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-11-25",
    "authors": [
      {
        "authorId": "2332684729",
        "name": "Chengyuan Yao"
      },
      {
        "authorId": "2275134724",
        "name": "Satoshi Fujita"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "852f307fb0cd2ccd3edd4ee00dc60af64655841c",
    "url": "https://www.semanticscholar.org/paper/852f307fb0cd2ccd3edd4ee00dc60af64655841c",
    "title": "Mask-based Membership Inference Attacks for Retrieval-Augmented Generation",
    "abstract": "Retrieval-Augmented Generation (RAG) has been an effective approach to mitigate hallucinations in large language models (LLMs) by incorporating up-to-date and domain-specific knowledge. Recently, there has been a trend of storing up-to-date or copyrighted data in RAG knowledge databases instead of using it for LLM training. This practice has raised concerns about Membership Inference Attacks (MIAs), which aim to detect if a specific target document is stored in the RAG system's knowledge database so as to protect the rights of data producers. While research has focused on enhancing the trustworthiness of RAG systems, existing MIAs for RAG systems remain largely insufficient. Previous work either relies solely on the RAG system's judgment or is easily influenced by other documents or the LLM's internal knowledge, which is unreliable and lacks explainability. To address these limitations, we propose a Mask-Based Membership Inference Attacks (MBA) framework. Our framework first employs a masking algorithm that effectively masks a certain number of words in the target document. The masked text is then used to prompt the RAG system, and the RAG system is required to predict the mask values. If the target document appears in the knowledge database, the masked text will retrieve the complete target document as context, allowing for accurate mask prediction. Finally, we adopt a simple yet effective threshold-based method to infer the membership of target document by analyzing the accuracy of mask prediction. Our mask-based approach is more document-specific, making the RAG system's generation less susceptible to distractions from other documents or the LLM's internal knowledge. Extensive experiments demonstrate the effectiveness of our approach compared to existing baseline models.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-26",
    "authors": [
      {
        "authorId": "2328072859",
        "name": "Mingrui Liu"
      },
      {
        "authorId": "2328224602",
        "name": "Sixiao Zhang"
      },
      {
        "authorId": "2328011945",
        "name": "Cheng Long"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "53dc93dfbc27ac6351a1c3af5498bc701cf7f6dd",
    "url": "https://www.semanticscholar.org/paper/53dc93dfbc27ac6351a1c3af5498bc701cf7f6dd",
    "title": "Integrating Graph Retrieval Augmented Generation with Large Language Models for Supplier Discovery",
    "abstract": "\n In an era where supply chain complexity and dynamism challenge traditional management approaches, integrating Large Language Models (LLMs) and Knowledge Graphs (KGs) emerges as a groundbreaking solution for advancing supply chain analytics. This paper presents a novel methodology crafted to harness the synergies between LLMs and KGs, with a particular focus on enhancing supplier discovery. The primary goal is to transform and integrate a vast body of unstructured supplier capability data into a harmonized KG, thus streamlining the supplier discovery process and enhancing the accessibility and findability of manufacturing suppliers. Through an ontology-driven graph construction process and automatic scaling up facilitated by the system, the presented methodology integrates KGs and Retrieval Augmented Generation with advanced LLM-based natural language processing techniques. With the aid of a detailed case study, we showcase how this integrated approach not only enhances the quality of answers and increases visibility for small and medium-sized manufacturers, but also amplifies agility and provides strategic insights in supply chain management.",
    "venue": "Journal of Computing and Information Science in Engineering",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-12-11",
    "authors": [
      {
        "authorId": "2291438142",
        "name": "Yunqing Li"
      },
      {
        "authorId": "2334919574",
        "name": "Hyunwoong Ko"
      },
      {
        "authorId": "2291418751",
        "name": "Farhad Ameri"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.39720770839918
  },
  {
    "paperId": "fe21b2d8005e0757fe76abb2dfcd09634cdf10cd",
    "url": "https://www.semanticscholar.org/paper/fe21b2d8005e0757fe76abb2dfcd09634cdf10cd",
    "title": "SBI-RAG: Enhancing Math Word Problem Solving for Students through Schema-Based Instruction and Retrieval-Augmented Generation",
    "abstract": "Many students struggle with math word problems (MWPs), often finding it difficult to identify key information and select the appropriate mathematical operations. Schema-based instruction (SBI) is an evidence-based strategy that helps students categorize problems based on their structure, improving problem-solving accuracy. Building on this, we propose a Schema-Based Instruction Retrieval-Augmented Generation (SBI-RAG) framework that incorporates a large language model (LLM). Our approach emphasizes step-by-step reasoning by leveraging schemas to guide solution generation. We evaluate its performance on the GSM8K dataset, comparing it with GPT-4 and GPT-3.5 Turbo, and introduce a\"reasoning score\"metric to assess solution quality. Our findings suggest that SBI-RAG enhances reasoning clarity and facilitates a more structured problem-solving process potentially providing educational benefits for students.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-17",
    "authors": [
      {
        "authorId": "2326298804",
        "name": "Prakhar Dixit"
      },
      {
        "authorId": "2326301434",
        "name": "Tim Oates"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "89729cdfe0f71ad7a04c73e9167c2b266ee0ee8c",
    "url": "https://www.semanticscholar.org/paper/89729cdfe0f71ad7a04c73e9167c2b266ee0ee8c",
    "title": "Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models",
    "abstract": "Retrieval-Augmented Generation (RAG) is applied to solve hallucination problems and real-time constraints of large language models, but it also induces vulnerabilities against retrieval corruption attacks. Existing research mainly explores the unreliability of RAG in white-box and closed-domain QA tasks. In this paper, we aim to reveal the vulnerabilities of Retrieval-Enhanced Generative (RAG) models when faced with black-box attacks for opinion manipulation. We explore the impact of such attacks on user cognition and decision-making, providing new insight to enhance the reliability and security of RAG models. We manipulate the ranking results of the retrieval model in RAG with instruction and use these results as data to train a surrogate model. By employing adversarial retrieval attack methods to the surrogate model, black-box transfer attacks on RAG are further realized. Experiments conducted on opinion datasets across multiple topics show that the proposed attack strategy can significantly alter the opinion polarity of the content generated by RAG. This demonstrates the model's vulnerability and, more importantly, reveals the potential negative impact on user cognition and decision-making, making it easier to mislead users into accepting incorrect or biased information.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-18",
    "authors": [
      {
        "authorId": "2287816650",
        "name": "Zhuo Chen"
      },
      {
        "authorId": "2287859943",
        "name": "Jiawei Liu"
      },
      {
        "authorId": "2289252486",
        "name": "Haotan Liu"
      },
      {
        "authorId": "2283732",
        "name": "Qikai Cheng"
      },
      {
        "authorId": "2312054192",
        "name": "Fan Zhang"
      },
      {
        "authorId": "2260835264",
        "name": "Wei Lu"
      },
      {
        "authorId": "2247614916",
        "name": "Xiaozhong Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "34c4e345b8971d185c2f9dfa384fe2eced8cc62e",
    "url": "https://www.semanticscholar.org/paper/34c4e345b8971d185c2f9dfa384fe2eced8cc62e",
    "title": "Bridging Relevance and Reasoning: Rationale Distillation in Retrieval-Augmented Generation",
    "abstract": "The reranker and generator are two critical components in the Retrieval-Augmented Generation (i.e., RAG) pipeline, responsible for ranking relevant documents and generating responses. However, due to differences in pre-training data and objectives, there is an inevitable gap between the documents ranked as relevant by the reranker and those required by the generator to support answering the query. To address this gap, we propose RADIO, a novel and practical preference alignment framework with RAtionale DIstillatiOn. Specifically, We first propose a rationale extraction method that leverages the reasoning capabilities of Large Language Models (LLMs) to extract the rationales necessary for answering the query. Subsequently, a rationale-based alignment process is designed to rerank the documents based on the extracted rationales, and fine-tune the reranker to align the preferences. We conduct extensive experiments on two tasks across three datasets to demonstrate the effectiveness of our approach compared to baseline methods. Our code is released online to ease reproduction.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-12-11",
    "authors": [
      {
        "authorId": "2264224432",
        "name": "Pengyue Jia"
      },
      {
        "authorId": "2262514619",
        "name": "Derong Xu"
      },
      {
        "authorId": "2238125533",
        "name": "Xiaopeng Li"
      },
      {
        "authorId": "2174600044",
        "name": "Zhaochen Du"
      },
      {
        "authorId": "2181637944",
        "name": "Xiangyang Li"
      },
      {
        "authorId": "2238104000",
        "name": "Xiangyu Zhao"
      },
      {
        "authorId": "2262403807",
        "name": "Yichao Wang"
      },
      {
        "authorId": "2223878432",
        "name": "Yuhao Wang"
      },
      {
        "authorId": "3339005",
        "name": "Huifeng Guo"
      },
      {
        "authorId": "2334741329",
        "name": "Ruiming Tang"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "650f7db2b37069614c0fb04ea77f099bb5d4efa5",
    "url": "https://www.semanticscholar.org/paper/650f7db2b37069614c0fb04ea77f099bb5d4efa5",
    "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed KV Caches for Chunked Text",
    "abstract": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process numerous retrieved document chunks for prefill which requires a large volume of computation, therefore leading to significant latency in time-to-first-token (TTFT). To reduce the computation overhead as well as TTFT, we introduce TurboRAG, a novel RAG system that redesigns the inference paradigm of the current RAG system by first pre-computing and storing the key-value (KV) caches of documents offline, and then directly retrieving the saved KV cache for prefill. Hence, online computation of KV caches is eliminated during inference. In addition, we provide a number of insights into the mask matrix and positional embedding mechanisms, plus fine-tune a pretrained language model to maintain model accuracy of TurboRAG. Our approach is applicable to most existing large language models and their applications without any requirement in modification of models and inference systems. Experimental results across a suite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x compared to the conventional RAG systems (on an average of 8.6x), but reserving comparable performance to the standard RAG systems.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-10",
    "authors": [
      {
        "authorId": "2303957265",
        "name": "Songshuo Lu"
      },
      {
        "authorId": "2325461964",
        "name": "Hua Wang"
      },
      {
        "authorId": "2325157287",
        "name": "Yutian Rong"
      },
      {
        "authorId": "2325197565",
        "name": "Zhi Chen"
      },
      {
        "authorId": "2304014129",
        "name": "Yaohua Tang"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "10acaa03a4c36aae8dc261fd3e43ddfc31180ba2",
    "url": "https://www.semanticscholar.org/paper/10acaa03a4c36aae8dc261fd3e43ddfc31180ba2",
    "title": "A Retrieval-Augmented Generation Approach for Data-Driven Energy Infrastructure Digital Twins",
    "abstract": "Digital-twin platforms are increasingly adopted in energy infrastructure management for smart grids. Novel opportunities arise from emerging artificial intelligence technologies to increase user trust by enhancing predictive and prescriptive analytics capabilities and by improving user interaction paradigms. This paper presents a novel data-driven and knowledge-based energy digital-twin framework and architecture. Data integration and mining based on machine learning are integrated into a knowledge graph annotating asset status data, prediction outcomes, and background domain knowledge in order to support a retrieval-augmented generation approach, which enhances a conversational virtual assistant based on a large language model to provide user decision support in asset management and maintenance. Components of the proposed architecture have been mapped to commercial-off-the-shelf tools to implement a prototype framework, exploited in a case study on the management of a section of the high-voltage energy infrastructure in central Italy.",
    "venue": "Smart Cities",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-24",
    "authors": [
      {
        "authorId": "3011421",
        "name": "S. Ieva"
      },
      {
        "authorId": "2147159664",
        "name": "Davide Loconte"
      },
      {
        "authorId": "3205335",
        "name": "G. Loseto"
      },
      {
        "authorId": "1686717",
        "name": "M. Ruta"
      },
      {
        "authorId": "1696528",
        "name": "Floriano Scioscia"
      },
      {
        "authorId": "2327748995",
        "name": "Davide Marche"
      },
      {
        "authorId": "2327744543",
        "name": "Marianna Notarnicola"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "99b364eaf55295f53f6afaf6fce7c61dcd567eb9",
    "url": "https://www.semanticscholar.org/paper/99b364eaf55295f53f6afaf6fce7c61dcd567eb9",
    "title": "Generating Is Believing: Membership Inference Attacks against Retrieval-Augmented Generation",
    "abstract": "Retrieval-Augmented Generation (RAG) is a state-of-the-art technique that mitigates issues such as hallucinations and knowledge staleness in Large Language Models (LLMs) by retrieving relevant knowledge from an external database to assist in content generation. Existing research has demonstrated potential privacy risks associated with the LLMs of RAG. However, the privacy risks posed by the integration of an external database, which often contains sensitive data such as medical records or personal identities, have remained largely unexplored. In this paper, we aim to bridge this gap by focusing on membership privacy of RAG's external database, with the aim of determining whether a given sample is part of the RAG's database. Our basic idea is that if a sample is in the external database, it will exhibit a high degree of semantic similarity to the text generated by the RAG system. We present S$^2$MIA, a \\underline{M}embership \\underline{I}nference \\underline{A}ttack that utilizes the \\underline{S}emantic \\underline{S}imilarity between a given sample and the content generated by the RAG system. With our proposed S$^2$MIA, we demonstrate the potential to breach the membership privacy of the RAG database. Extensive experiment results demonstrate that S$^2$MIA can achieve a strong inference performance compared with five existing MIAs, and is able to escape from the protection of three representative defenses.",
    "venue": "",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-06-27",
    "authors": [
      {
        "authorId": "2315443068",
        "name": "Yuying Li"
      },
      {
        "authorId": "2019143562",
        "name": "Gaoyang Liu"
      },
      {
        "authorId": "2323160757",
        "name": "Chen Wang"
      },
      {
        "authorId": "2320837306",
        "name": "Yang Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "915539c51252649cf7d09e25dea3af199235ebed",
    "url": "https://www.semanticscholar.org/paper/915539c51252649cf7d09e25dea3af199235ebed",
    "title": "Adaptive Contrastive Decoding in Retrieval-Augmented Generation for Handling Noisy Contexts",
    "abstract": "When using large language models (LLMs) in knowledge-intensive tasks, such as open-domain question answering, external context can bridge the gap between external knowledge and the LLMs' parametric knowledge. Recent research has been developed to amplify contextual knowledge over the parametric knowledge of LLMs with contrastive decoding approaches. While these approaches could yield truthful responses when relevant context is provided, they are prone to vulnerabilities when faced with noisy contexts. We extend the scope of previous studies to encompass noisy contexts and propose adaptive contrastive decoding (ACD) to leverage contextual influence effectively. ACD demonstrates improvements in open-domain question answering tasks compared to baselines, especially in robustness by remaining undistracted by noisy contexts in retrieval-augmented generation.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-08-02",
    "authors": [
      {
        "authorId": "2189657321",
        "name": "Youna Kim"
      },
      {
        "authorId": "2166352356",
        "name": "Hyuhng Joon Kim"
      },
      {
        "authorId": "2284731913",
        "name": "Cheonbok Park"
      },
      {
        "authorId": "2188345697",
        "name": "Choonghyun Park"
      },
      {
        "authorId": "2259114784",
        "name": "Hyunsoo Cho"
      },
      {
        "authorId": "2109166096",
        "name": "Junyeob Kim"
      },
      {
        "authorId": "31760501",
        "name": "Kang Min Yoo"
      },
      {
        "authorId": "2261393515",
        "name": "Sang-goo Lee"
      },
      {
        "authorId": "5041757",
        "name": "Taeuk Kim"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "fd11941be0d7c5f6d25d5cde3d60598f268fa51d",
    "url": "https://www.semanticscholar.org/paper/fd11941be0d7c5f6d25d5cde3d60598f268fa51d",
    "title": "A Method for Parsing and Vectorization of Semi-structured Data used in Retrieval Augmented Generation",
    "abstract": "This paper presents a novel method for parsing and vectorizing semi-structured data to enhance the functionality of Retrieval-Augmented Generation (RAG) within Large Language Models (LLMs). We developed a comprehensive pipeline for converting various data formats into .docx, enabling efficient parsing and structured data extraction. The core of our methodology involves the construction of a vector database using Pinecone, which integrates seamlessly with LLMs to provide accurate, context-specific responses, particularly in environmental management and wastewater treatment operations. Through rigorous testing with both English and Chinese texts in diverse document formats, our results demonstrate a marked improvement in the precision and reliability of LLMs outputs. The RAG-enhanced models displayed enhanced ability to generate contextually rich and technically accurate responses, underscoring the potential of vector knowledge bases in significantly boosting the performance of LLMs in specialized domains. This research not only illustrates the effectiveness of our method but also highlights its potential to revolutionize data processing and analysis in environmental sciences, setting a precedent for future advancements in AI-driven applications. Our code is available at https://github.com/linancn/TianGong-AI-Unstructure.git.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-07",
    "authors": [
      {
        "authorId": "2277232606",
        "name": "Hang Yang"
      },
      {
        "authorId": "2277216790",
        "name": "Jing Guo"
      },
      {
        "authorId": "2112611806",
        "name": "J. Qi"
      },
      {
        "authorId": "2293178818",
        "name": "Jinliang Xie"
      },
      {
        "authorId": "2277447299",
        "name": "Si Zhang"
      },
      {
        "authorId": "2300245689",
        "name": "Siqi Yang"
      },
      {
        "authorId": "2277452325",
        "name": "Nan Li"
      },
      {
        "authorId": "2277425927",
        "name": "Ming Xu"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "f22dc44ba7eb8cd27655c2ba2090636cb16ffb87",
    "url": "https://www.semanticscholar.org/paper/f22dc44ba7eb8cd27655c2ba2090636cb16ffb87",
    "title": "Self-explanatory Retrieval-Augmented Generation for SDG Evidence Identification",
    "abstract": ". With the establishment of the Sustainable Development Goals (SDG) framework, practitioners in environmental impact assessment have an increasing requirement to detect relevant information centered on this frame of reference. The task of automatically identifying evidence that supports the project actually addressing a particular SDG target becomes crucial for enabling assessment digitalization across long, heterogeneous documents. In this work, we tackle SDG evidence identification via the well-suited Retrieval-augmented Generation (RAG) approach powered by Large Language Models (LLM). The identified evidence may also support further related tasks in conceptual modeling where reports or parts of their content are to be assigned to entries in a structured resource such as a domain-specific ontology. Beyond the measurement of performance of a series of method configurations on this task, we also assess RAG abilities for making this kind of decisions when the LLM is requested to explain its own mechanisms alongside the answer it generates. Our evaluation resources are made publicly available.",
    "venue": "International Conference on Conceptual Modeling",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2315308998",
        "name": "Dar√≠o Garigliotti"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "a546fa44c110c33a3280f31090f96f5b886ac44f",
    "url": "https://www.semanticscholar.org/paper/a546fa44c110c33a3280f31090f96f5b886ac44f",
    "title": "Mindful-RAG: A Study of Points of Failure in Retrieval Augmented Generation",
    "abstract": "Large Language Models (LLMs) are proficient at generating coherent and contextually relevant text but face challenges when addressing knowledge-intensive queries in domain-specific and factual question-answering tasks. Retrieval-augmented generation (RAG) systems mitigate this by incorporating external knowledge sources, such as structured knowledge graphs (KGs). However, LLMs often struggle to produce accurate answers despite access to KG-extracted information containing necessary facts. Our study investigates this dilemma by analyzing error patterns in existing KG-based RAG methods and identifying eight critical failure points. We observed that these errors predominantly occur due to insufficient focus on discerning the question's intent and adequately gathering relevant context from the knowledge graph facts. Drawing on this analysis, we propose the Mindful-RAG approach, a framework designed for intent-based and contextually aligned knowledge retrieval. This method explicitly targets the identified failures and offers improvements in the correctness and relevance of responses provided by LLMs, representing a significant step forward from existing methods.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-16",
    "authors": [
      {
        "authorId": "145363324",
        "name": "Garima Agrawal"
      },
      {
        "authorId": "40899329",
        "name": "Tharindu Kumarage"
      },
      {
        "authorId": "2185743354",
        "name": "Zeyad Alghamdi"
      },
      {
        "authorId": "2146398603",
        "name": "Huanmin Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "272d0cfef44320feb482c8013c51efcb9c6f9448",
    "url": "https://www.semanticscholar.org/paper/272d0cfef44320feb482c8013c51efcb9c6f9448",
    "title": "CaseGPT: a case reasoning framework based on language models and retrieval-augmented generation",
    "abstract": "This paper presents CaseGPT, an innovative approach that combines Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) technology to enhance case-based reasoning in the healthcare and legal sectors. The system addresses the challenges of traditional database queries by enabling fuzzy searches based on imprecise descriptions, thereby improving data searchability and usability. CaseGPT not only retrieves relevant case data but also generates insightful suggestions and recommendations based on patterns discerned from existing case data. This functionality proves especially valuable for tasks such as medical diagnostics, legal precedent research, and case strategy formulation. The paper includes an in-depth discussion of the system's methodology, its performance in both medical and legal domains, and its potential for future applications. Our experiments demonstrate that CaseGPT significantly outperforms traditional keyword-based and simple LLM-based systems in terms of precision, recall, and efficiency.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-04",
    "authors": [
      {
        "authorId": "2310912282",
        "name": "Rui Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "fc409c663357758248eea787afd1c7809f30c6f3",
    "url": "https://www.semanticscholar.org/paper/fc409c663357758248eea787afd1c7809f30c6f3",
    "title": "P-RAG: Progressive Retrieval Augmented Generation For Planning on Embodied Everyday Task",
    "abstract": "Embodied Everyday Task is a popular task in the embodied AI community, requiring agents to make a sequence of actions based on natural language instructions and visual observations. Traditional learning-based approaches face two challenges. Firstly, natural language instructions often lack explicit task planning. Secondly, extensive training is required to equip models with knowledge of the task environment. Previous works based on Large Language Model (LLM) either suffer from poor performance due to the lack of task-specific knowledge or rely on ground truth as few-shot samples. To address the above limitations, we propose a novel approach called Progressive Retrieval Augmented Generation (P-RAG), which not only effectively leverages the powerful language processing capabilities of LLMs but also progressively accumulates task-specific knowledge without ground-truth. Compared to the conventional RAG methods, which retrieve relevant information from the database in a one-shot manner to assist generation, P-RAG introduces an iterative approach to progressively update the database. In each iteration, P-RAG retrieves the latest database and obtains historical information from the previous interaction as experiential references for the current interaction. Moreover, we also introduce a more granular retrieval scheme that not only retrieves similar tasks but also incorporates retrieval of similar situations to provide more valuable reference experiences. Extensive experiments reveal that P-RAG achieves competitive results without utilizing ground truth and can even further improve performance through self-iterations.",
    "venue": "ACM Multimedia",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-17",
    "authors": [
      {
        "authorId": "2321554031",
        "name": "Weiye Xu"
      },
      {
        "authorId": "2145298424",
        "name": "Min Wang"
      },
      {
        "authorId": "38272296",
        "name": "Wen-gang Zhou"
      },
      {
        "authorId": "2210048071",
        "name": "Houqiang Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.39720770839918
  },
  {
    "paperId": "269427c4a82eaed464e20fdb26cbcc23d9472083",
    "url": "https://www.semanticscholar.org/paper/269427c4a82eaed464e20fdb26cbcc23d9472083",
    "title": "Report Generation from X-Ray imaging by Retrieval-Augmented Generation and improved Image-Text Matching",
    "abstract": "Creating radiology reports is a vital but time-intensive task that involves analyzing images, consulting documents, and evaluating data. This process, heavily reliant on human effort, is prone to errors that can vary with the radiologists experience. Consequently, automating the generation of radiology reports is a key research goal due to its potential impact on medical procedures and patient care.This work proposes a multimodal approach specifically designed for generating radiological reports from chest X-rays (CXRs). Our method integrates a LLaMa large language model with Retrieval Augmented Generation (RAG), enhanced by a modified ALBEF embedding model that exploits efficient organ semantic segmentation and triple contrastive loss (called EALBEF). The combination of these two components allows radiological report generation that surpasses current state-of-the-art methods in terms of quality and accuracy. Our approach demonstrates a significant enhancement in the radiologist-specific metrics (e.g., RadCliQ), as well as across various generic lexical-based metrics (e.g., GLEU). Quantitative analyses of the models outputs reveal a notable increase in fluency and accuracy, with a marked reduction in issues such as hallucinations and source-reference divergences in the generated reports.",
    "venue": "IEEE International Joint Conference on Neural Network",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-30",
    "authors": [
      {
        "authorId": "1689571",
        "name": "M. Bernardi"
      },
      {
        "authorId": "1706324",
        "name": "Marta Cimitile"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.39720770839918
  },
  {
    "paperId": "b2e81e974f7bdfbf195a11e6bd7a15077069730c",
    "url": "https://www.semanticscholar.org/paper/b2e81e974f7bdfbf195a11e6bd7a15077069730c",
    "title": "AI Assistants for Spaceflight Procedures: Combining Generative Pre-Trained Transformer and Retrieval-Augmented Generation on Knowledge Graphs With Augmented Reality Cues",
    "abstract": "This paper describes the capabilities and potential of the intelligent personal assistant (IPA) CORE (Checklist Organizer for Research and Exploration), designed to support astronauts during procedures onboard the International Space Station (ISS), the Lunar Gateway station, and beyond. We reflect on the importance of a reliable and flexible assistant capable of offline operation and highlight the usefulness of audiovisual interaction using augmented reality elements to intuitively display checklist information. We argue that current approaches to the design of IPAs in space operations fall short of meeting these criteria. Therefore, we propose CORE as an assistant that combines Knowledge Graphs (KGs), Retrieval-Augmented Generation (RAG) for a Generative Pre-Trained Transformer (GPT), and Augmented Reality (AR) elements to ensure an intuitive understanding of procedure steps, reliability, offline availability, and flexibility in terms of response style and procedure updates.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-21",
    "authors": [
      {
        "authorId": "2282467405",
        "name": "Oliver Bensch"
      },
      {
        "authorId": "2266841010",
        "name": "Leonie Bensch"
      },
      {
        "authorId": "2266840816",
        "name": "Tommy Nilsson"
      },
      {
        "authorId": "2322443824",
        "name": "Florian Saling"
      },
      {
        "authorId": "2322446002",
        "name": "Bernd Bewer"
      },
      {
        "authorId": "2282467369",
        "name": "Sophie Jentzsch"
      },
      {
        "authorId": "2282467403",
        "name": "Tobias Hecking"
      },
      {
        "authorId": "144484982",
        "name": "J. Kutz"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "ff3edac95dcc82eae72cd25519e5dff075fc58be",
    "url": "https://www.semanticscholar.org/paper/ff3edac95dcc82eae72cd25519e5dff075fc58be",
    "title": "Retrieval Augmented Generation in Prompt-based Text-to-Speech Synthesis with Context-Aware Contrastive Language-Audio Pretraining",
    "abstract": "Recent prompt-based text-to-speech (TTS) models can clone an unseen speaker using only a short speech prompt. They leverage a strong in-context ability to mimic the speech prompts, including speaker style, prosody, and emotion. Therefore, the selection of a speech prompt greatly influences the generated speech, akin to the importance of a prompt in large language models (LLMs). However, current prompt-based TTS models choose the speech prompt manually or simply at random. Hence, in this paper, we adapt retrieval augmented generation (RAG) from LLMs to prompt-based TTS. Unlike traditional RAG methods, we additionally consider contextual information during the retrieval process and present a Context-Aware Contrastive Language-Audio Pre-training (CA-CLAP) model to extract context-aware, style-related features. The objective and subjective evaluations demonstrate that our proposed RAG method outperforms baselines, and our CA-CLAP achieves better results than text-only retrieval methods.",
    "venue": "Interspeech",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-06",
    "authors": [
      {
        "authorId": "2159562819",
        "name": "Jinlong Xue"
      },
      {
        "authorId": "2111214299",
        "name": "Yayue Deng"
      },
      {
        "authorId": "2261912430",
        "name": "Yingming Gao"
      },
      {
        "authorId": "2161324824",
        "name": "Ya Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "19edfb636a14f058f5d87db9f0519b393829ed6c",
    "url": "https://www.semanticscholar.org/paper/19edfb636a14f058f5d87db9f0519b393829ed6c",
    "title": "HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation System for AI Legal and Policy Applications",
    "abstract": "While Large Language Models (LLMs) excel in text generation and question-answering, their effectiveness in AI legal and policy applications is limited by outdated knowledge, hallucinations, and inadequate reasoning in complex contexts. Retrieval-Augmented Generation (RAG) systems improve response accuracy by integrating external knowledge but struggle with retrieval errors, poor context integration, and high costs, particularly in interpreting AI legal texts. This paper introduces a Hybrid Parameter-Adaptive RAG (HyPA-RAG) system tailored for AI legal and policy, exemplified by NYC Local Law 144 (LL144). HyPA-RAG uses a query complexity classifier for adaptive parameter tuning, a hybrid retrieval strategy combining dense, sparse, and knowledge graph methods, and an evaluation framework with specific question types and metrics. By dynamically adjusting parameters, HyPA-RAG significantly improves retrieval accuracy and response fidelity. Testing on LL144 shows enhanced correctness, faithfulness, and contextual precision, addressing the need for adaptable NLP systems in complex, high-stakes AI legal and policy applications.",
    "venue": "CUSTOMNLP4U",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-29",
    "authors": [
      {
        "authorId": "80605886",
        "name": "Rishi Kalra"
      },
      {
        "authorId": "2284029866",
        "name": "Zekun Wu"
      },
      {
        "authorId": "2321025778",
        "name": "Ayesha Gulley"
      },
      {
        "authorId": "2128410682",
        "name": "Airlie Hilliard"
      },
      {
        "authorId": "2308070610",
        "name": "Xin Guan"
      },
      {
        "authorId": "2268316579",
        "name": "A. Koshiyama"
      },
      {
        "authorId": "2244619460",
        "name": "Philip C. Treleaven"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "758881985475e137439da465fadf968aead68c4c",
    "url": "https://www.semanticscholar.org/paper/758881985475e137439da465fadf968aead68c4c",
    "title": "AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation",
    "abstract": "Recent advancements in Large Language Models have transformed ML/AI development, necessitating a reevaluation of AutoML principles for the Retrieval-Augmented Generation (RAG) systems. To address the challenges of hyper-parameter optimization and online adaptation in RAG, we propose the AutoRAG-HP framework, which formulates the hyper-parameter tuning as an online multi-armed bandit (MAB) problem and introduces a novel two-level Hierarchical MAB (Hier-MAB) method for efficient exploration of large search spaces. We conduct extensive experiments on tuning hyper-parameters, such as top-k retrieved documents, prompt compression ratio, and embedding methods, using the ALCE-ASQA and Natural Questions datasets. Our evaluation from jointly optimization all three hyper-parameters demonstrate that MAB-based online learning methods can achieve Recall@5 $\\approx 0.8$ for scenarios with prominent gradients in search space, using only $\\sim20\\%$ of the LLM API calls required by the Grid Search approach. Additionally, the proposed Hier-MAB approach outperforms other baselines in more challenging optimization scenarios. The code will be made available at https://aka.ms/autorag.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-27",
    "authors": [
      {
        "authorId": "2310195282",
        "name": "Jia Fu"
      },
      {
        "authorId": "2227603238",
        "name": "Xiaoting Qin"
      },
      {
        "authorId": "2261394438",
        "name": "Fangkai Yang"
      },
      {
        "authorId": "2163383329",
        "name": "Lu Wang"
      },
      {
        "authorId": "2163389931",
        "name": "Jue Zhang"
      },
      {
        "authorId": "2303522901",
        "name": "Qingwei Lin"
      },
      {
        "authorId": "2308546942",
        "name": "Yubo Chen"
      },
      {
        "authorId": "2307618254",
        "name": "Dongmei Zhang"
      },
      {
        "authorId": "148121358",
        "name": "S. Rajmohan"
      },
      {
        "authorId": "2279711828",
        "name": "Qi Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "2d362392fb0e13baf77e8ee35b5543e08207e97e",
    "url": "https://www.semanticscholar.org/paper/2d362392fb0e13baf77e8ee35b5543e08207e97e",
    "title": "Causal Reasoning in Large Language Models using Causal Graph Retrieval Augmented Generation",
    "abstract": "Large Language Models (LLMs) are leading the Generative Artificial Intelligence transformation in natural language understanding. Beyond language understanding, LLMs have demonstrated capabilities in reasoning tasks, including commonsense, logical, and mathematical reasoning. However, their proficiency in causal understanding has been limited due to the complex nature of causal reasoning. Several recent studies have discussed the role of external causal models for improved causal understanding. Building on the success of Retrieval-Augmented Generation (RAG) for factual reasoning in LLMs, this paper introduces a novel approach that utilizes Causal Graphs as external sources for establishing causal relationships between complex vectors. This method is empirically evaluated using two benchmark datasets across the metrics of Context Relevance, Answer Relevance, and Grounding, in its ability to retrieve relevant context with causal alignment. The retrieval effectiveness is further compared with traditional RAG methods that are based on semantic proximity.",
    "venue": "International Conference on Human System Interaction",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-08",
    "authors": [
      {
        "authorId": "51437913",
        "name": "Chamod Samarajeewa"
      },
      {
        "authorId": "144286739",
        "name": "Daswin De Silva"
      },
      {
        "authorId": "2276845387",
        "name": "Evgeny Osipov"
      },
      {
        "authorId": "143775049",
        "name": "D. Alahakoon"
      },
      {
        "authorId": "2185595070",
        "name": "Milos Manic"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "30394de373b65aeda84e2bd100e8efe38f4d1a8c",
    "url": "https://www.semanticscholar.org/paper/30394de373b65aeda84e2bd100e8efe38f4d1a8c",
    "title": "Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection",
    "abstract": "Large language models (LLMs) augmented with retrieval exhibit robust performance and extensive versatility by incorporating external contexts. However, the input length grows linearly in the number of retrieved documents, causing a dramatic increase in latency. In this paper, we propose a novel paradigm named Sparse RAG, which seeks to cut computation costs through sparsity. Specifically, Sparse RAG encodes retrieved documents in parallel, which eliminates latency introduced by long-range attention of retrieved documents. Then, LLMs selectively decode the output by only attending to highly relevant caches auto-regressively, which are chosen via prompting LLMs with special control tokens. It is notable that Sparse RAG combines the assessment of each individual document and the generation of the response into a single process. The designed sparse mechanism in a RAG system can facilitate the reduction of the number of documents loaded during decoding for accelerating the inference of the RAG system. Additionally, filtering out undesirable contexts enhances the model's focus on relevant context, inherently improving its generation quality. Evaluation results of two datasets show that Sparse RAG can strike an optimal balance between generation quality and computational efficiency, demonstrating its generalizability across both short- and long-form generation tasks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-25",
    "authors": [
      {
        "authorId": "2281839893",
        "name": "Yun Zhu"
      },
      {
        "authorId": "2308241925",
        "name": "Jia-Chen Gu"
      },
      {
        "authorId": "2303406428",
        "name": "Caitlin Sikora"
      },
      {
        "authorId": "2303403653",
        "name": "Ho Ko"
      },
      {
        "authorId": "2257122720",
        "name": "Yinxiao Liu"
      },
      {
        "authorId": "2266757860",
        "name": "Chu-Cheng Lin"
      },
      {
        "authorId": "2257004117",
        "name": "Lei Shu"
      },
      {
        "authorId": "2256991052",
        "name": "Liangchen Luo"
      },
      {
        "authorId": "2218226973",
        "name": "Lei Meng"
      },
      {
        "authorId": "2303517615",
        "name": "Bang Liu"
      },
      {
        "authorId": "2261676962",
        "name": "Jindong Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "2a7720c878c20ed21608978e31507ca9d9936d1c",
    "url": "https://www.semanticscholar.org/paper/2a7720c878c20ed21608978e31507ca9d9936d1c",
    "title": "SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation",
    "abstract": "This paper introduces Self-aware Knowledge Retrieval (SeaKR), a novel adaptive RAG model that extracts self-aware uncertainty of LLMs from their internal states. SeaKR activates retrieval when the LLMs present high self-aware uncertainty for generation. To effectively integrate retrieved knowledge snippets, SeaKR re-ranks them based on LLM's self-aware uncertainty to preserve the snippet that reduces their uncertainty to the utmost. To facilitate solving complex tasks that require multiple retrievals, SeaKR utilizes their self-aware uncertainty to choose among different reasoning strategies. Our experiments on both complex and simple Question Answering datasets show that SeaKR outperforms existing adaptive RAG methods. We release our code at https://github.com/THU-KEG/SeaKR.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-27",
    "authors": [
      {
        "authorId": "2273946831",
        "name": "Zijun Yao"
      },
      {
        "authorId": "2149667177",
        "name": "Weijian Qi"
      },
      {
        "authorId": "2308643805",
        "name": "Liangming Pan"
      },
      {
        "authorId": "1712738522",
        "name": "S. Cao"
      },
      {
        "authorId": "2282510494",
        "name": "Linmei Hu"
      },
      {
        "authorId": "2308556832",
        "name": "Weichuan Liu"
      },
      {
        "authorId": "2284777109",
        "name": "Lei Hou"
      },
      {
        "authorId": "2133353675",
        "name": "Juanzi Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "7496f2f13961f2e80c596573e51dd255091be536",
    "url": "https://www.semanticscholar.org/paper/7496f2f13961f2e80c596573e51dd255091be536",
    "title": "Retrieval-Augmented Generation‚ÄìEnabled GPT-4 for Clinical Trial Screening",
    "abstract": null,
    "venue": "NEJM AI",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://ai.nejm.org/doi/pdf/10.1056/AIoa2400181",
      "status": "BRONZE"
    },
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-17",
    "authors": [
      {
        "authorId": "14579749",
        "name": "Ozan Unlu"
      },
      {
        "authorId": "2283302172",
        "name": "Jiyeon Shin"
      },
      {
        "authorId": "123271410",
        "name": "Charlotte J. Mailly"
      },
      {
        "authorId": "2283261639",
        "name": "Michael Oates"
      },
      {
        "authorId": "2190273257",
        "name": "Michela R. Tucci"
      },
      {
        "authorId": "3040805",
        "name": "Matthew Varugheese"
      },
      {
        "authorId": "2727465",
        "name": "K. Wagholikar"
      },
      {
        "authorId": "2283311300",
        "name": "Fei Wang"
      },
      {
        "authorId": "2274921165",
        "name": "Benjamin M. Scirica"
      },
      {
        "authorId": "2305088945",
        "name": "Alexander J. Blood"
      },
      {
        "authorId": "2275613770",
        "name": "Samuel J. Aronson"
      }
    ],
    "source": "semantic_scholar",
    "score": 81.19162312519754
  },
  {
    "paperId": "a1f3aac8462a709a7c73484699f513a92f443927",
    "url": "https://www.semanticscholar.org/paper/a1f3aac8462a709a7c73484699f513a92f443927",
    "title": "Transforming Healthcare Education: Harnessing Large Language Models for Frontline Health Worker Capacity Building using Retrieval-Augmented Generation",
    "abstract": "In recent years, large language models (LLMs) have emerged as a transformative force in several domains, including medical education and healthcare. This paper presents a case study on the practical application of using retrieval-augmented generation (RAG) based models for enhancing healthcare education in low- and middle-income countries. The model described in this paper, SMARThealth GPT, stems from the necessity for accessible and locally relevant medical information to aid community health workers in delivering high-quality maternal care. We describe the development process of the complete RAG pipeline, including the creation of a knowledge base of Indian pregnancy-related guidelines, knowledge embedding retrieval, parameter selection and optimization, and answer generation. This case study highlights the potential of LLMs in building frontline healthcare worker capacity and enhancing guideline-based health education; and offers insights for similar applications in resource-limited settings. It serves as a reference for machine learning scientists, educators, healthcare professionals, and policymakers aiming to harness the power of LLMs for substantial educational improvement.",
    "venue": "medRxiv",
    "year": 2023,
    "citationCount": 13,
    "openAccessPdf": {
      "url": "https://www.medrxiv.org/content/medrxiv/early/2023/12/17/2023.12.15.23300009.full.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-12-17",
    "authors": [
      {
        "authorId": "2152252759",
        "name": "Yasmina Al Ghadban"
      },
      {
        "authorId": "2275031544",
        "name": "Yvonne Lu"
      },
      {
        "authorId": "2274942448",
        "name": "Uday Adavi"
      },
      {
        "authorId": "2275032104",
        "name": "Ankita Sharma"
      },
      {
        "authorId": "2274941067",
        "name": "Sridevi Gara"
      },
      {
        "authorId": "2274942153",
        "name": "Neelanjana Das"
      },
      {
        "authorId": "2275901455",
        "name": "Bhaskar Kumar"
      },
      {
        "authorId": "2274943561",
        "name": "Renu John"
      },
      {
        "authorId": "2274941480",
        "name": "Praveen Devarsetty"
      },
      {
        "authorId": "2274942234",
        "name": "Jane E. Hirst"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "0b8003d97669b43b7e0d792d1f4042373da1a632",
    "url": "https://www.semanticscholar.org/paper/0b8003d97669b43b7e0d792d1f4042373da1a632",
    "title": "RealGen: Retrieval Augmented Generation for Controllable Traffic Scenarios",
    "abstract": "Simulation plays a crucial role in the development of autonomous vehicles (AVs) due to the potential risks associated with real-world testing. Although significant progress has been made in the visual aspects of simulators, generating complex behavior among agents remains a formidable challenge. It is not only imperative to ensure realism in the scenarios generated but also essential to incorporate preferences and conditions to facilitate controllable generation for AV training and evaluation. Traditional methods, mainly relying on memorizing the distribution of training datasets, often fall short in generating unseen scenarios. Inspired by the success of retrieval augmented generation in large language models, we present RealGen, a novel retrieval-based in-context learning framework for traffic scenario generation. RealGen synthesizes new scenarios by combining behaviors from multiple retrieved examples in a gradient-free way, which may originate from templates or tagged scenarios. This in-context learning framework endows versatile generative capabilities, including the ability to edit scenarios, compose various behaviors, and produce critical scenarios. Evaluations show that RealGen offers considerable flexibility and controllability, marking a new direction in the field of controllable traffic scenario generation. Check our project website for more information: https://realgen.github.io.",
    "venue": "European Conference on Computer Vision",
    "year": 2023,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-19",
    "authors": [
      {
        "authorId": "152425748",
        "name": "Wenhao Ding"
      },
      {
        "authorId": "2146176464",
        "name": "Yulong Cao"
      },
      {
        "authorId": "2258097326",
        "name": "Ding Zhao"
      },
      {
        "authorId": "2254275777",
        "name": "Chaowei Xiao"
      },
      {
        "authorId": "2237790577",
        "name": "Marco Pavone"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "1148782ef3582e18d32c0e354f8e88b8d7e71cc9",
    "url": "https://www.semanticscholar.org/paper/1148782ef3582e18d32c0e354f8e88b8d7e71cc9",
    "title": "Retrieval Augmented Generation with Rich Answer Encoding",
    "abstract": "Knowledge-intensive generation tasks like generative question answering require models to retrieve appropriate passages from external knowledge sources to support answer generation. The generation quality relies heavily on the retrieved passages, which serve as contextual information. State-of-the-art Retrieval Augmented Generation models with marginalized output dominate this area but focus too much on label-relevant passages, rather than question-relevant passages and answers. This work addresses this issue by incorporating rich answer encoding through Dense Knowledge Similarity (DKS) and Retriever as Answer Classifier (RAC). We demonstrate the advantages of our proposed approach in open domain question answering (MSMARCO) and conversation (Wizard of Wikipedia) datasets, reporting both generation and retrieval metrics. In the MSMARCO development set, our best model achieves 12.1% relative improvement 1 on Recall@1 and 4.5% relative improvement on BLEU-4 compared to the baseline model. In the KILT-WoW leaderboard, our best model achieves 8.9% relative improvement on R-Precision and 13.3% relative improvement on KILT-RL compared to the baseline model. Our codes and models are available at https://github.com/hwy9855/rag-ae .",
    "venue": "International Joint Conference on Natural Language Processing",
    "year": 2023,
    "citationCount": 12,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.ijcnlp-main.65.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1896870270",
        "name": "Wenyu Huang"
      },
      {
        "authorId": "1747893",
        "name": "Mirella Lapata"
      },
      {
        "authorId": "7631872",
        "name": "P. Vougiouklis"
      },
      {
        "authorId": "3451396",
        "name": "Nikos Papasarantopoulos"
      },
      {
        "authorId": "2280059593",
        "name": "Jeff Z. Pan"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "5c54f1f38974c896ebbf6494a2ac0d9b1ba21b33",
    "url": "https://www.semanticscholar.org/paper/5c54f1f38974c896ebbf6494a2ac0d9b1ba21b33",
    "title": "MKRAG: Medical Knowledge Retrieval Augmented Generation for Medical Question Answering",
    "abstract": "Large Language Models (LLMs), although powerful in general domains, often perform poorly on domain-specific tasks such as medical question answering (QA). In addition, LLMs tend to function as\"black-boxes\", making it challenging to modify their behavior. To address the problem, our work employs a transparent process of retrieval augmented generation (RAG), aiming to improve LLM responses without the need for fine-tuning or retraining. Specifically, we propose a comprehensive retrieval strategy to extract medical facts from an external knowledge base, and then inject them into the LLM's query prompt. Focusing on medical QA, we evaluate the impact of different retrieval models and the number of facts on LLM performance using the MedQA-SMILE dataset. Notably, our retrieval-augmented Vicuna-7B model exhibited an accuracy improvement from 44.46% to 48.54%. This work underscores the potential of RAG to enhance LLM performance, offering a practical approach to mitigate the challenges posed by black-box LLMs.",
    "venue": "",
    "year": 2023,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-09-27",
    "authors": [
      {
        "authorId": "2249001715",
        "name": "Yucheng Shi"
      },
      {
        "authorId": "2211904452",
        "name": "Shaochen Xu"
      },
      {
        "authorId": "2145977326",
        "name": "Zheng Liu"
      },
      {
        "authorId": "2115345993",
        "name": "Tianming Liu"
      },
      {
        "authorId": "47057650",
        "name": "Xiang Li"
      },
      {
        "authorId": "2238404369",
        "name": "Ninghao Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "63a1f6f6fafef55ac70af3adff5c353bce9a21ff",
    "url": "https://www.semanticscholar.org/paper/63a1f6f6fafef55ac70af3adff5c353bce9a21ff",
    "title": "CodeGRAG: Bridging the Gap between Natural Language and Programming Language via Graphical Retrieval Augmented Generation",
    "abstract": "Utilizing large language models to generate codes has shown promising meaning in software development revolution. Despite the intelligence shown by the general large language models, their specificity in code generation can still be improved due to the syntactic gap and mismatched vocabulary existing among natural language and different programming languages. In this paper, we propose CodeGRAG, a Graphical Retrieval Augmented Code Generation framework to enhance the performance of LLMs. CodeGRAG builds the graphical view of code blocks based on the control flow and data flow of them to fill the gap between programming languages and natural language, which can facilitate natural language based LLMs for better understanding of code syntax and serve as a bridge among different programming languages. To take the extracted structural knowledge into the foundation models, we propose 1) a hard meta-graph prompt template to transform the challenging graphical representation into informative knowledge for tuning-free models and 2) a soft prompting technique that injects the domain knowledge of programming languages into the model parameters via finetuning the models with the help of a pretrained GNN expert model. Various experiments and ablations are done on four datasets including both the C++ and python languages to validate the hard meta-graph prompt, the soft prompting technique, and the effectiveness of the objectives for pretrained GNN expert. CodeGRAG improves the code generation ability of LLMs and can even offer performance gain for cross-lingual code generation. Code is available at https://anonymous.4open.science/r/Code-5970/.",
    "venue": "",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-05-03",
    "authors": [
      {
        "authorId": "1780721965",
        "name": "Kounianhua Du"
      },
      {
        "authorId": "2304517128",
        "name": "Jizheng Chen"
      },
      {
        "authorId": "2153850099",
        "name": "Renting Rui"
      },
      {
        "authorId": "2220553287",
        "name": "Huacan Chai"
      },
      {
        "authorId": "2171109846",
        "name": "Lingyue Fu"
      },
      {
        "authorId": "2154454480",
        "name": "Wei Xia"
      },
      {
        "authorId": "2282544603",
        "name": "Yasheng Wang"
      },
      {
        "authorId": "2257180930",
        "name": "Ruiming Tang"
      },
      {
        "authorId": "2237958078",
        "name": "Yong Yu"
      },
      {
        "authorId": "2240768092",
        "name": "Weinan Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "e4be17af0075f47b6bc259044b14243663fd3ea0",
    "url": "https://www.semanticscholar.org/paper/e4be17af0075f47b6bc259044b14243663fd3ea0",
    "title": "Retrieval-Augmented Generation for Large Language Models in Radiology: Another Leap Forward in Board Examination Performance.",
    "abstract": "Supplemental material is available for this article. See also the editorial by Forghani in this issue.",
    "venue": "Radiology",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-01",
    "authors": [
      {
        "authorId": "2290161110",
        "name": "Rajesh Bhayana"
      },
      {
        "authorId": "2290047926",
        "name": "Aly Fawzy"
      },
      {
        "authorId": "2279259326",
        "name": "Yangqing Deng"
      },
      {
        "authorId": "2252403231",
        "name": "Robert Bleakney"
      },
      {
        "authorId": "2278806549",
        "name": "Satheesh Krishna"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.87639203842082
  },
  {
    "paperId": "71371b95a66cfc86d6f8f67119a01f7dad1bf8d4",
    "url": "https://www.semanticscholar.org/paper/71371b95a66cfc86d6f8f67119a01f7dad1bf8d4",
    "title": "\"Knowing When You Don't Know\": A Multilingual Relevance Assessment Dataset for Robust Retrieval-Augmented Generation",
    "abstract": "Retrieval-Augmented Generation (RAG) grounds Large Language Model (LLM) output by leveraging external knowledge sources to reduce factual hallucinations. However, prior work lacks a comprehensive evaluation of different language families, making it challenging to evaluate LLM robustness against errors in external retrieved knowledge. To overcome this, we establish NoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across 18 typologically diverse languages. NoMIRACL includes both a non-relevant and a relevant subset. Queries in the non-relevant subset contain passages judged as non-relevant, whereas queries in the relevant subset include at least a single judged relevant passage. We measure relevance assessment using: (i) hallucination rate, measuring model tendency to hallucinate, when the answer is not present in passages in the non-relevant subset, and (ii) error rate, measuring model inaccuracy to recognize relevant passages in the relevant subset.In our work, we observe that most models struggle to balance the two capacities. Models such as LLAMA-2 and Orca-2 achieve over 88% hallucination rate on the non-relevant subset. Mistral and LLAMA-3 hallucinate less but can achieve up to a 74.9% error rate on the relevant subset. Overall, GPT-4 is observed to provide the best tradeoff on both subsets, highlighting future work necessary to improve LLM robustness. NoMIRACL dataset and evaluation code are available at: https://github.com/project-miracl/nomiracl.",
    "venue": "",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-12-18",
    "authors": [
      {
        "authorId": "47583894",
        "name": "Nandan Thakur"
      },
      {
        "authorId": "2275196928",
        "name": "Luiz Bonifacio"
      },
      {
        "authorId": "2118895402",
        "name": "Xinyu Crystina Zhang"
      },
      {
        "authorId": "2166106776",
        "name": "Odunayo Ogundepo"
      },
      {
        "authorId": "2023642",
        "name": "Ehsan Kamalloo"
      },
      {
        "authorId": "1419474794",
        "name": "David Alfonso-Hermelo"
      },
      {
        "authorId": "2238110973",
        "name": "Xiaoguang Li"
      },
      {
        "authorId": "2275118289",
        "name": "Qun Liu"
      },
      {
        "authorId": "2237517964",
        "name": "Boxing Chen"
      },
      {
        "authorId": "2066076226",
        "name": "Mehdi Rezagholizadeh"
      },
      {
        "authorId": "145580839",
        "name": "Jimmy J. Lin"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "11ce7cd954d4691e2b3e1fa6563e1377d0cfb28f",
    "url": "https://www.semanticscholar.org/paper/11ce7cd954d4691e2b3e1fa6563e1377d0cfb28f",
    "title": "Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for Retrieval Augmented Generation",
    "abstract": "Large Language Models (LLMs) are smart but forgetful. Recent studies, (e.g., (Bubeck et al., 2023)) on modern LLMs have shown that they are capable of performing amazing tasks typically necessitating human-level intelligence. However, unlike humans, frozen LLMs do not improve over time; they neither acquire new knowledge nor learn from their successes or failures. Some approaches to improving the intelligence of LLMs include fine-tuning models based on problem-solving performance (Zelikman et al., 2022), and building bigger and more sophisticated models (Bubeck et al., 2023). However, these methods have the drawback of requiring substantial data and computational resources to retrain existing models. In this paper, we explore the use of Retrieval Augmented Generation, also known as RAG (Lewis et al., 2021) to improve problem-solving performance. We propose ARM-RAG (Auxiliary Rationale Memory for Retrieval Augmented Generation), a system that learns from its successes without incurring high training costs. We demonstrate that the storage and subsequent retrieval of reasoning chains have a positive influence on performance in grade-school math problems.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-07",
    "authors": [
      {
        "authorId": "2265581318",
        "name": "Eric Melz"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "d704f7c327968963e6f3bf7b02cc2189564b2d02",
    "url": "https://www.semanticscholar.org/paper/d704f7c327968963e6f3bf7b02cc2189564b2d02",
    "title": "RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment",
    "abstract": "Despite the significant progress made by existing retrieval augmented language models (RALMs) in providing trustworthy responses and grounding in reliable sources, they often overlook effective alignment with human preferences. In the alignment process, reward models (RMs) act as a crucial proxy for human values to guide optimization. However, it remains unclear how to evaluate and select a reliable RM for preference alignment in RALMs. To this end, we propose RAG-RewardBench, the first benchmark for evaluating RMs in RAG settings. First, we design four crucial and challenging RAG-specific scenarios to assess RMs, including multi-hop reasoning, fine-grained citation, appropriate abstain, and conflict robustness. Then, we incorporate 18 RAG subsets, six retrievers, and 24 RALMs to increase the diversity of data sources. Finally, we adopt an LLM-as-a-judge approach to improve preference annotation efficiency and effectiveness, exhibiting a strong correlation with human annotations. Based on the RAG-RewardBench, we conduct a comprehensive evaluation of 45 RMs and uncover their limitations in RAG scenarios. Additionally, we also reveal that existing trained RALMs show almost no improvement in preference alignment, highlighting the need for a shift towards preference-aligned training.We release our benchmark and code publicly at https://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/ for future work.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-12-18",
    "authors": [
      {
        "authorId": "2152843772",
        "name": "Zhuoran Jin"
      },
      {
        "authorId": "2165224410",
        "name": "Hongbang Yuan"
      },
      {
        "authorId": "2165227300",
        "name": "Tianyi Men"
      },
      {
        "authorId": "49776272",
        "name": "Pengfei Cao"
      },
      {
        "authorId": "1763402",
        "name": "Yubo Chen"
      },
      {
        "authorId": "2284814101",
        "name": "Kang Liu"
      },
      {
        "authorId": "2269147239",
        "name": "Jun Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "2fd11a0fb0b654a380a7d0d53f30fe266b64347c",
    "url": "https://www.semanticscholar.org/paper/2fd11a0fb0b654a380a7d0d53f30fe266b64347c",
    "title": "RAGLog: Log Anomaly Detection using Retrieval Augmented Generation",
    "abstract": "The ability to detect log anomalies from system logs is a vital activity needed to ensure cyber resiliency of systems. It is applied for fault identification or facilitate cyber investigation and digital forensics. However, as logs belonging to different systems and components differ significantly, the challenge to perform such analysis is humanly challenging from the volume, variety and velocity of logs. This is further complicated by the lack or unavailability of anomalous log entries to develop trained machine learning or artificial intelligence models for such purposes. In this research work, we explore the use of a Retrieval Augmented Large Language Model that leverages a vector database to detect anomalies from logs. We used a Question and Answer configuration pipeline. To the best of our knowledge, our experiment which we called RAGLog is a novel one and the experimental results show much promise.",
    "venue": "2024 IEEE World Forum on Public Safety Technology (WFPST)",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": "2024-05-14",
    "authors": [
      {
        "authorId": "2314142900",
        "name": "Jonathan Pan"
      },
      {
        "authorId": "2314547379",
        "name": "Wong Swee Liang"
      },
      {
        "authorId": "2313965881",
        "name": "Yuan Yidi"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.39720770839918
  },
  {
    "paperId": "0504c5d96ebf7be01e2b622ff3a4bf155f2b0a41",
    "url": "https://www.semanticscholar.org/paper/0504c5d96ebf7be01e2b622ff3a4bf155f2b0a41",
    "title": "Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models",
    "abstract": "Causal graph recovery is traditionally done using statistical estimation-based methods or based on individual's knowledge about variables of interests. They often suffer from data collection biases and limitations of individuals' knowledge. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that leverages LLMs to deduce causal relationships in general causal graph recovery tasks. This method leverages knowledge compressed in LLMs and knowledge LLMs extracted from scientific publication database as well as experiment data about factors of interest to achieve this goal. Our method gives a prompting strategy to extract associational relationships among those factors and a mechanism to perform causality verification for these associations. Comparing to other LLM-based methods that directly instruct LLMs to do the highly complex causal reasoning, our method shows clear advantage on causal graph quality on benchmark datasets. More importantly, as causality among some factors may change as new research results emerge, our method show sensitivity to new evidence in the literature and can provide useful information for updating causal graphs accordingly.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-23",
    "authors": [
      {
        "authorId": "2258602359",
        "name": "Yuzhe Zhang"
      },
      {
        "authorId": "2286591780",
        "name": "Yipeng Zhang"
      },
      {
        "authorId": "2286307576",
        "name": "Yidong Gan"
      },
      {
        "authorId": "2258412350",
        "name": "Lina Yao"
      },
      {
        "authorId": "2286410001",
        "name": "Chen Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "271abb481fff11002b92e295d56e88df70541595",
    "url": "https://www.semanticscholar.org/paper/271abb481fff11002b92e295d56e88df70541595",
    "title": "Combining Retrieval-Augmented Generation and Few-Shot Learning for Model Synthesis of Uncommon DSLs",
    "abstract": ",",
    "venue": "Modellierung",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2190291951",
        "name": "Nils Baumann"
      },
      {
        "authorId": "2308447948",
        "name": "Juan Sebastian Diaz"
      },
      {
        "authorId": "144089484",
        "name": "Judith Michael"
      },
      {
        "authorId": "46642485",
        "name": "Lukas Netz"
      },
      {
        "authorId": "2248807437",
        "name": "Haron Nqiri"
      },
      {
        "authorId": "2308280052",
        "name": "Jan Reimer"
      },
      {
        "authorId": "2257229410",
        "name": "Bernhard Rumpe"
      }
    ],
    "source": "semantic_scholar",
    "score": 84.1415686865115
  },
  {
    "paperId": "78937c571ad2ee71288f087df466ff1e9c335c83",
    "url": "https://www.semanticscholar.org/paper/78937c571ad2ee71288f087df466ff1e9c335c83",
    "title": "Improving Assessment of Tutoring Practices using Retrieval-Augmented Generation",
    "abstract": "One-on-one tutoring is an effective instructional method for enhancing learning, yet its efficacy hinges on tutor competencies. Novice math tutors often prioritize content-specific guidance, neglecting aspects such as social-emotional learning. Social-emotional learning promotes equity and inclusion and nurturing relationships with students, which is crucial for holistic student development. Assessing the competencies of tutors accurately and efficiently can drive the development of tailored tutor training programs. However, evaluating novice tutor ability during real-time tutoring remains challenging as it typically requires experts-in-the-loop. To address this challenge, this preliminary study aims to harness Generative Pre-trained Transformers (GPT), such as GPT-3.5 and GPT-4 models, to automatically assess tutors' ability of using social-emotional tutoring strategies. Moreover, this study also reports on the financial dimensions and considerations of employing these models in real-time and at scale for automated assessment. The current study examined four prompting strategies: two basic Zero-shot prompt strategies, Tree of Thought prompt, and Retrieval-Augmented Generator (RAG) based prompt. The results indicate that the RAG prompt demonstrated more accurate performance (assessed by the level of hallucination and correctness in the generated assessment texts) and lower financial costs than the other strategies evaluated. These findings inform the development of personalized tutor training interventions to enhance the the educational effectiveness of tutored learning.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-04",
    "authors": [
      {
        "authorId": "2286712187",
        "name": "Zifei Han"
      },
      {
        "authorId": "2248806319",
        "name": "Jionghao Lin"
      },
      {
        "authorId": "2275144600",
        "name": "Ashish Gurung"
      },
      {
        "authorId": "2209057634",
        "name": "Danielle R. Thomas"
      },
      {
        "authorId": "2284986483",
        "name": "Eason Chen"
      },
      {
        "authorId": "83716891",
        "name": "Conrad Borchers"
      },
      {
        "authorId": "2167469364",
        "name": "Shivang Gupta"
      },
      {
        "authorId": "1718810",
        "name": "K. Koedinger"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "8772b7deb7f4c250d6a49a10f77f1d976440ee9b",
    "url": "https://www.semanticscholar.org/paper/8772b7deb7f4c250d6a49a10f77f1d976440ee9b",
    "title": "Towards Cross-Cultural Machine Translation with Retrieval-Augmented Generation from Multilingual Knowledge Graphs",
    "abstract": "Translating text that contains entity names is a challenging task, as cultural-related references can vary significantly across languages. These variations may also be caused by transcreation, an adaptation process that entails more than transliteration and word-for-word translation. In this paper, we address the problem of cross-cultural translation on two fronts: (i) we introduce XC-Translate, the first large-scale, manually-created benchmark for machine translation that focuses on text that contains potentially culturally-nuanced entity names, and (ii) we propose KG-MT, a novel end-to-end method to integrate information from a multilingual knowledge graph into a neural machine translation model by leveraging a dense retrieval mechanism. Our experiments and analyses show that current machine translation systems and large language models still struggle to translate texts containing entity names, whereas KG-MT outperforms state-of-the-art approaches by a large margin, obtaining a 129% and 62% relative improvement compared to NLLB-200 and GPT-4, respectively.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-10-17",
    "authors": [
      {
        "authorId": "2268401404",
        "name": "Simone Conia"
      },
      {
        "authorId": "2268434271",
        "name": "Daniel Lee"
      },
      {
        "authorId": "2268432243",
        "name": "Min Li"
      },
      {
        "authorId": "1856878",
        "name": "U. F. Minhas"
      },
      {
        "authorId": "2294572235",
        "name": "Saloni Potdar"
      },
      {
        "authorId": "2316362057",
        "name": "Yunyao Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "5cde9876766a6bf8ab12329fad8f634ea38992a3",
    "url": "https://www.semanticscholar.org/paper/5cde9876766a6bf8ab12329fad8f634ea38992a3",
    "title": "Retrieval Augmented Generation and Representative Vector Summarization for large unstructured textual data in Medical Education",
    "abstract": "Large Language Models are increasingly being used for various tasks including content generation and as chatbots. Despite their impressive performances in general tasks, LLMs need to be aligned when applying for domain specific tasks to mitigate the problems of hallucination and producing harmful answers. Retrieval Augmented Generation (RAG) allows to easily attach and manipulate a non-parametric knowledgebases to LLMs. Applications of RAG in the field of medical education are discussed in this paper. A combined extractive and abstractive summarization method for large unstructured textual data using representative vectors is proposed.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 9,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.00479",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-08-01",
    "authors": [
      {
        "authorId": "2306048251",
        "name": "S. S. Manathunga"
      },
      {
        "authorId": "2226256197",
        "name": "Y. A. Illangasekara"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.53877639491068
  },
  {
    "paperId": "fcefbb496545d2174270f27183cefa14f2830892",
    "url": "https://www.semanticscholar.org/paper/fcefbb496545d2174270f27183cefa14f2830892",
    "title": "Retrieval-augmented Generation across Heterogeneous Knowledge",
    "abstract": "Retrieval-augmented generation (RAG) methods have been receiving increasing attention from the NLP community and achieved state-of-the-art performance on many NLP downstream tasks. Compared with conventional pre-trained generation models, RAG methods have remarkable advantages such as easy knowledge acquisition, strong scalability, and low training cost. Although existing RAG models have been applied to various knowledge-intensive NLP tasks, such as open-domain QA and dialogue systems, most of the work has focused on retrieving unstructured text documents from Wikipedia. In this paper, I first elaborate on the current obstacles to retrieving knowledge from a single-source homogeneous corpus. Then, I demonstrate evidence from both existing literature and my experiments, and provide multiple solutions on retrieval-augmented generation methods across heterogeneous knowledge.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2022,
    "citationCount": 28,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2022.naacl-srw.7.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "38767143",
        "name": "W. Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 120.5094374497971
  },
  {
    "paperId": "9091ebda1f66e10487ae3240e041984eba42ec75",
    "url": "https://www.semanticscholar.org/paper/9091ebda1f66e10487ae3240e041984eba42ec75",
    "title": "Efficient Retrieval Augmented Generation from Unstructured Knowledge for Task-Oriented Dialog",
    "abstract": "This paper summarizes our work on the first track of the ninth Dialog System Technology Challenge (DSTC 9),\"Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access\". The goal of the task is to generate responses to user turns in a task-oriented dialog that require knowledge from unstructured documents. The task is divided into three subtasks: detection, selection and generation. In order to be compute efficient, we formulate the selection problem in terms of hierarchical classification steps. We achieve our best results with this model. Alternatively, we employ siamese sequence embedding models, referred to as Dense Knowledge Retrieval, to retrieve relevant documents. This method further reduces the computation time by a factor of more than 100x at the cost of degradation in R@1 of 5-6% compared to the first model. Then for either approach, we use Retrieval Augmented Generation to generate responses based on multiple selected snippets and we show how the method can be used to fine-tune trained embeddings.",
    "venue": "arXiv.org",
    "year": 2021,
    "citationCount": 43,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-02-09",
    "authors": [
      {
        "authorId": "17687726",
        "name": "David Thulke"
      },
      {
        "authorId": "2048028927",
        "name": "Nico Daheim"
      },
      {
        "authorId": "32703822",
        "name": "Christian Dugast"
      },
      {
        "authorId": "145322333",
        "name": "H. Ney"
      }
    ],
    "source": "semantic_scholar",
    "score": 126.76284450877392
  },
  {
    "paperId": "901440d0bded2b52bb62b873dcfd884df70b3e74",
    "url": "https://www.semanticscholar.org/paper/901440d0bded2b52bb62b873dcfd884df70b3e74",
    "title": "Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented Generation and Soft-Prompting for Non-Specialist LLM Users",
    "abstract": "Research into methods for improving the performance of large language models (LLMs) through fine-tuning, retrieval-augmented generation (RAG) and soft-prompting has tended to focus on the use of highly technical or high-cost techniques, making many of the newly discovered approaches comparatively inaccessible to non-technical users. In this paper we tested an unmodified version of GPT 3.5, a fine-tuned version, and the same unmodified model when given access to a vectorised RAG database, both in isolation and in combination with a basic, non-algorithmic soft prompt. In each case we tested the model's ability to answer a set of 100 questions relating primarily to events that occurred after September 2021 (the point at which GPT 3.5's training data set ends). We found that if commercial platforms are used and default settings are applied with no iteration in order to establish a baseline set of outputs, a fine-tuned model outperforms GPT 3.5 Turbo, while the RAG approach out-performed both. The application of a soft prompt significantly improved the performance of each approach.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-10",
    "authors": [
      {
        "authorId": "2266237514",
        "name": "Jennifer Dodgson"
      },
      {
        "authorId": "2266281858",
        "name": "Nanzheng Lin"
      },
      {
        "authorId": "2266237540",
        "name": "Julian Peh"
      },
      {
        "authorId": "2266241068",
        "name": "Akira Rafhael Janson Pattirane"
      },
      {
        "authorId": "2047388958",
        "name": "Alfath Daryl Alhajir"
      },
      {
        "authorId": "2266238002",
        "name": "Eko Ridho Dinarto"
      },
      {
        "authorId": "2266365094",
        "name": "Joseph Lim"
      },
      {
        "authorId": "2266346433",
        "name": "Syed Danyal Ahmad"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "3c4bdce3931f19cbe04f9959ed4177069ed6b674",
    "url": "https://www.semanticscholar.org/paper/3c4bdce3931f19cbe04f9959ed4177069ed6b674",
    "title": "Design and Implementation of an Interactive Question-Answering System with Retrieval-Augmented Generation for Personalized Databases",
    "abstract": "This study introduces a novel approach to personalized information retrieval by integrating retrieval augmentation generation (RAG) with a personalized database system. Recent advancements in large language models (LLMs) have shown impressive text generation capabilities but face limitations in knowledge accuracy and hallucinations. Our research addresses these challenges by combining LLMs with structured, personalized data to enhance search precision and relevance. By tagging keywords within personal documents and organizing information into context-based categories, users can conduct efficient searches within their data repositories. We conducted experiments using the GPT-3.5 and text-embedding-ada-002 models and evaluated the RAG assessment framework with five different language models and two embedding models. Our results indicate that the combination of GPT-3.5 and text-embedding-ada-002 is effective for a personalized database question-answering system, with potential for various language models depending on the application. Our approach offers improved accuracy, real-time data updates, and enhanced user experience, making a significant contribution to information retrieval by LLMs and impacting various artificial intelligence applications.",
    "venue": "Applied Sciences",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-06",
    "authors": [
      {
        "authorId": "2320037647",
        "name": "Jaeyeon Byun"
      },
      {
        "authorId": "2320314577",
        "name": "Bokyeong Kim"
      },
      {
        "authorId": "2318560378",
        "name": "Kyung-Ae Cha"
      },
      {
        "authorId": "2320848355",
        "name": "Eunhyung Lee"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.39720770839918
  },
  {
    "paperId": "dd40511ffd92fd377f029e6d4c79c90bb2b10376",
    "url": "https://www.semanticscholar.org/paper/dd40511ffd92fd377f029e6d4c79c90bb2b10376",
    "title": "Context Tuning for Retrieval Augmented Generation",
    "abstract": "Large language models (LLMs) have the remarkable ability to solve new tasks with just a few examples, but they need access to the right tools. Retrieval Augmented Generation (RAG) addresses this problem by retrieving a list of relevant tools for a given task. However, RAG‚Äôs tool retrieval step requires all the required information to be explicitly present in the query. This is a limitation, as semantic search, the widely adopted tool retrieval method, can fail when the query is incomplete or lacks context. To address this limitation, we propose Context Tuning for RAG, which employs a smart context retrieval system to fetch relevant information that improves both tool retrieval and plan generation. Our lightweight context retrieval model uses numerical, categorical, and habitual usage signals to retrieve and rank context items. Our empirical results demonstrate that context tuning significantly enhances semantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for context retrieval and tool retrieval tasks respectively, and resulting in an 11.6% increase in LLM-based planner accuracy. Additionally, we show that our proposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART outperforms GPT-4 based retrieval. Moreover, we observe context augmentation at plan generation, even after tool retrieval, reduces hallucination.",
    "venue": "UNCERTAINLP",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-09",
    "authors": [
      {
        "authorId": "1666320488",
        "name": "R. Anantha"
      },
      {
        "authorId": "2273547577",
        "name": "Tharun Bethi"
      },
      {
        "authorId": "2273532499",
        "name": "Danil Vodianik"
      },
      {
        "authorId": "1667831197",
        "name": "Srinivas Chappidi"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "800b396437db5844b5d5ddd08e46b15b8910a49d",
    "url": "https://www.semanticscholar.org/paper/800b396437db5844b5d5ddd08e46b15b8910a49d",
    "title": "How to Build an AI Tutor that Can Adapt to Any Course and Provide Accurate Answers Using Large Language Model and Retrieval-Augmented Generation",
    "abstract": ": This paper proposes a low-code solution to build an AI tutor that leverages advanced AI techniques to provide accurate and contextually relevant responses in a personalized learning environment. The OpenAI Assistants API allows AI Tutor to easily embed, store, retrieve, and manage files and chat history, enabling a low-code solution. Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) technology generate sophisticated answers based on course-specific materials. The application efficiently organizes and retrieves relevant information through vector embedding and similarity-based retrieval algorithms. The AI Tutor prototype demonstrates its ability to generate relevant, accurate answers with source citations. It represents a significant advancement in technology-enhanced tutoring systems, democratizing access to high-quality, customized educational support in higher education.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2269438579",
        "name": "Chenxi Dong"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "01f284c732124dd0c6e4a7983f491b8f2f34778f",
    "url": "https://www.semanticscholar.org/paper/01f284c732124dd0c6e4a7983f491b8f2f34778f",
    "title": "Large Language Models with Retrieval-Augmented Generation for Zero-Shot Disease Phenotyping",
    "abstract": "Identifying disease phenotypes from electronic health records (EHRs) is critical for numerous secondary uses. Manually encoding physician knowledge into rules is particularly challenging for rare diseases due to inadequate EHR coding, necessitating review of clinical notes. Large language models (LLMs) offer promise in text understanding but may not efficiently handle real-world clinical documentation. We propose a zero-shot LLM-based method enriched by retrieval-augmented generation and MapReduce, which pre-identifies disease-related text snippets to be used in parallel as queries for the LLM to establish diagnosis. We show that this method as applied to pulmonary hypertension (PH), a rare disease characterized by elevated arterial pressures in the lungs, significantly outperforms physician logic rules ($F_1$ score of 0.62 vs. 0.75). This method has the potential to enhance rare disease cohort identification, expanding the scope of robust clinical research and care gap identification.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-12-11",
    "authors": [
      {
        "authorId": "2221629510",
        "name": "Will Thompson"
      },
      {
        "authorId": "2273550708",
        "name": "D. Vidmar"
      },
      {
        "authorId": "2273558164",
        "name": "Jessica K. De Freitas"
      },
      {
        "authorId": "1690887087",
        "name": "John M. Pfeifer"
      },
      {
        "authorId": "5732263",
        "name": "Brandon K. Fornwalt"
      },
      {
        "authorId": "2273656828",
        "name": "Ruijun Chen"
      },
      {
        "authorId": "2165227550",
        "name": "Gabriel Altay"
      },
      {
        "authorId": "52197490",
        "name": "Kabir Manghnani"
      },
      {
        "authorId": "2245091842",
        "name": "Andrew C. Nelsen"
      },
      {
        "authorId": "2273567195",
        "name": "Kellie Morland"
      },
      {
        "authorId": "47175997",
        "name": "M. Stumpe"
      },
      {
        "authorId": "2273567170",
        "name": "Riccardo Miotto"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "ed16f6feda941164e6370638ebd98b81c13d2c4b",
    "url": "https://www.semanticscholar.org/paper/ed16f6feda941164e6370638ebd98b81c13d2c4b",
    "title": "RAG-Ex: A Generic Framework for Explaining Retrieval Augmented Generation",
    "abstract": null,
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-10",
    "authors": [
      {
        "authorId": "2121276445",
        "name": "Viju Sudhi"
      },
      {
        "authorId": "2258714509",
        "name": "Sinchana Ramakanth Bhat"
      },
      {
        "authorId": "2310821671",
        "name": "Max Rudat"
      },
      {
        "authorId": "2176809559",
        "name": "Roman Teucher"
      }
    ],
    "source": "semantic_scholar",
    "score": 76.79441541679836
  },
  {
    "paperId": "3120c4b739ed703beebf628fe545bc8692228ec3",
    "url": "https://www.semanticscholar.org/paper/3120c4b739ed703beebf628fe545bc8692228ec3",
    "title": "Exploring a learning-to-rank approach to enhance the Retrieval Augmented Generation (RAG)-based electronic medical records search engines",
    "abstract": null,
    "venue": "Informatics and Health",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-01",
    "authors": [
      {
        "authorId": "2312895293",
        "name": "Cheng Ye"
      }
    ],
    "source": "semantic_scholar",
    "score": 70.79441541679836
  },
  {
    "paperId": "5640189617e43d6d4cf9130fea9a88c93ad55779",
    "url": "https://www.semanticscholar.org/paper/5640189617e43d6d4cf9130fea9a88c93ad55779",
    "title": "Custom Large Language Models Improve Accuracy: Comparing Retrieval Augmented Generation and Artificial Intelligence Agents to Non-Custom Models for Evidence-Based Medicine.",
    "abstract": null,
    "venue": "Arthroscopy: The Journal of Arthroscopy And Related",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-11-01",
    "authors": [
      {
        "authorId": "2140527876",
        "name": "Joshua J. Woo"
      },
      {
        "authorId": "2329884310",
        "name": "Andrew J. Yang"
      },
      {
        "authorId": "2117576297",
        "name": "Reena J. Olsen"
      },
      {
        "authorId": "2283374939",
        "name": "Sayyida S. Hasan"
      },
      {
        "authorId": "3559939",
        "name": "D. Nawabi"
      },
      {
        "authorId": "5969055",
        "name": "Benedict U. Nwachukwu"
      },
      {
        "authorId": "2329883240",
        "name": "Riley J. Williams"
      },
      {
        "authorId": "2283367093",
        "name": "Prem N. Ramkumar"
      }
    ],
    "source": "semantic_scholar",
    "score": 70.79441541679836
  },
  {
    "paperId": "35c2340aacb51fb9283c59bb916c860f77bc68b6",
    "url": "https://www.semanticscholar.org/paper/35c2340aacb51fb9283c59bb916c860f77bc68b6",
    "title": "Semantic Embeddings for Arabic Retrieval Augmented Generation (ARAG)",
    "abstract": "‚ÄîIn recent times, Retrieval Augmented Generation (RAG) models have garnered considerable attention, primarily due to the impressive capabilities exhibited by Large Language Models (LLMs). Nevertheless, the Arabic language, despite its significance and widespread use, has received relatively less research emphasis in this field. A critical element within RAG systems is the Information Retrieval component, and at its core lies the vector embedding process commonly referred to as ‚Äúsemantic embedding‚Äù. This study encompasses an array of multilingual semantic embedding models, intending to enhance the model‚Äôs ability to comprehend and generate Arabic text effectively. We conducted an extensive evaluation of the performance of ten cutting-edge Multilingual Semantic embedding models, employing a publicly available ARCD dataset as a benchmark and assessing their performance using the average Recall@k metric. The results showed that the Microsoft E5 sentence embedding model outperformed all other models on the ARCD dataset, with Recall@10 exceeding 90%",
    "venue": "International Journal of Advanced Computer Science and Applications",
    "year": 2023,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "http://thesai.org/Downloads/Volume14No11/Paper_135-Semantic_Embeddings_for_Arabic_Retrieval_Augmented_Generation.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2269250335",
        "name": "Hazem Abdelazim"
      },
      {
        "authorId": "2148715555",
        "name": "Mohamed Tharwat"
      },
      {
        "authorId": "2269771285",
        "name": "Ammar Mohamed"
      }
    ],
    "source": "semantic_scholar",
    "score": 106.87639203842082
  },
  {
    "paperId": "27d603a67481594eafcce9818a68f44159d192fb",
    "url": "https://www.semanticscholar.org/paper/27d603a67481594eafcce9818a68f44159d192fb",
    "title": "Zero-Shot ECG Diagnosis with Large Language Models and Retrieval-Augmented Generation",
    "abstract": "Recently, Large Language Models (LLMs) have become essential players in the deep learning domain. While their capabilities are evident across various textual tasks, this study aims to bridge the gap and explore the potential of leveraging LLMs in diagnosing cardiac diseases and sleep apnea from Electrocardiography (ECG). Earlier work touched on converting ECG signals into text for LLMs, but a comprehensive LLM-based approach for dealing with more complicated symptoms remains relatively unexplored. To investigate the ECG diagnosis with an LLM-based approach, our research introduces a zero-shot retrieval-augmented diagnosis technique. We have built databases filled with specific domain knowledge for cardiac symptom and sleep apnea diagnosis, which encourages the LLMs from merely relying on the inherent LLM knowledge to a more holistic pipeline from carefully crafting prompts and infusing expert knowledge to guide LLMs. We evaluate the proposed approach on two datasets for diagnosing arrhythmia and sleep apnea, respectively. The evaluation results indicate that our zero-shot approach not only surpasses previous few-shot LLM-based methods but is also competitive with supervised learning techniques fully trained on extensive datasets.",
    "venue": "ML4H@NeurIPS",
    "year": 2023,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2110983770",
        "name": "Han Yu"
      },
      {
        "authorId": "2242796946",
        "name": "Peikun Guo"
      },
      {
        "authorId": "2804453",
        "name": "Akane Sano"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "76f3261de17913b4145a81c746703469324cfb8d",
    "url": "https://www.semanticscholar.org/paper/76f3261de17913b4145a81c746703469324cfb8d",
    "title": "Wiping out the limitations of Large Language Models - A Taxonomy for Retrieval Augmented Generation",
    "abstract": "Current research on RAGs is distributed across various disciplines, and since the technology is evolving very quickly, its unit of analysis is mostly on technological innovations, rather than applications in business contexts. Thus, in this research, we aim to create a taxonomy to conceptualize a comprehensive overview of the constituting characteristics that define RAG applications, facilitating the adoption of this technology in the IS community. To the best of our knowledge, no RAG application taxonomies have been developed so far. We describe our methodology for developing the taxonomy, which includes the criteria for selecting papers, an explanation of our rationale for employing a Large Language Model (LLM)-supported approach to extract and identify initial characteristics, and a concise overview of our systematic process for conceptualizing the taxonomy. Our systematic taxonomy development process includes four iterative phases designed to refine and enhance our understanding and presentation of RAG's core dimensions. We have developed a total of five meta-dimensions and sixteen dimensions to comprehensively capture the concept of Retrieval-Augmented Generation (RAG) applications. When discussing our findings, we also detail the specific research areas and pose key research questions to guide future information system researchers as they explore the emerging topics of RAG systems.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-08-05",
    "authors": [
      {
        "authorId": "2263394895",
        "name": "Mahei Manhai Li"
      },
      {
        "authorId": "2293940599",
        "name": "Irina Nikishina"
      },
      {
        "authorId": "27076821",
        "name": "√ñzge Sevgili"
      },
      {
        "authorId": "2142790279",
        "name": "Martin Semmann"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "3e9d7ed15eabb577324d7596f47f045ae3aa8751",
    "url": "https://www.semanticscholar.org/paper/3e9d7ed15eabb577324d7596f47f045ae3aa8751",
    "title": "Enhancing Retrieval-Augmented Generation Models with Knowledge Graphs: Innovative Practices Through a Dual-Pathway Approach",
    "abstract": null,
    "venue": "International Conference on Intelligent Computing",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2297144575",
        "name": "Sheng Xu"
      },
      {
        "authorId": "2297056196",
        "name": "Mike Chen"
      },
      {
        "authorId": "2297142188",
        "name": "Shuwen Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 66.47918433002164
  },
  {
    "paperId": "ef76276f9ef929496f03282fa85ae1bbcdc69767",
    "url": "https://www.semanticscholar.org/paper/ef76276f9ef929496f03282fa85ae1bbcdc69767",
    "title": "Robust Retrieval Augmented Generation for Zero-shot Slot Filling",
    "abstract": "Automatically inducing high quality knowledge graphs from a given collection of documents still remains a challenging problem in AI. One way to make headway for this problem is through advancements in a related task known as slot filling. In this task, given an entity query in form of [Entity, Slot, ?], a system is asked to ‚Äòfill‚Äô the slot by generating or extracting the missing value exploiting evidence extracted from relevant passage(s) in the given document collection. The recent works in the field try to solve this task in an end-to-end fashion using retrieval-based language models. In this paper, we present a novel approach to zero-shot slot filling that extends dense passage retrieval with hard negatives and robust training procedures for retrieval augmented generation models. Our model reports large improvements on both T-REx and zsRE slot filling datasets, improving both passage retrieval and slot value generation, and ranking at the top-1 position in the KILT leaderboard. Moreover, we demonstrate the robustness of our system showing its domain adaptation capability on a new variant of the TACRED dataset for slot filling, through a combination of zero/few-shot learning. We release the source code and pre-trained models.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "citationCount": 33,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2021.emnlp-main.148.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-08-31",
    "authors": [
      {
        "authorId": "143742133",
        "name": "Michael R. Glass"
      },
      {
        "authorId": "3415700",
        "name": "Gaetano Rossiello"
      },
      {
        "authorId": "8576392",
        "name": "Md. Faisal Mahbub Chowdhury"
      },
      {
        "authorId": "1711133",
        "name": "A. Gliozzo"
      }
    ],
    "source": "semantic_scholar",
    "score": 122.89540786924243
  },
  {
    "paperId": "5d2a03d679e0cc540d839a6e82d9000a9cab90c9",
    "url": "https://www.semanticscholar.org/paper/5d2a03d679e0cc540d839a6e82d9000a9cab90c9",
    "title": "War of Words: Harnessing the Potential of Large Language Models and Retrieval Augmented Generation to Classify, Counter and Diffuse Hate Speech",
    "abstract": "This paper explores the emergence of divergent narratives in the wake of the Russian-Ukraine war, which began on February 24, 2022, and the innovative application of AI language models, specifically RetrievalAugmented Generation (RAG) and instruction-based large language models (LLMs), in countering hateful speech on social media. We design a pipeline to automatically discover and then respond to hateful content trending on social media platforms. Monitoring via traditional topic/narrative modeling often focuses on lowlevel content, which is difficult to interpret. In addition, workflows for prioritization and response generation are often highly manual. We utilize several large language models (LLMs) throughout our pipeline to detect and summarize topics, to determine whether tweets contain hate speech and to generate counter narratives. We test our approach on Ukraine Bio-Lab Tweet Corpus of 500k Tweets and evaluate the counter-narrative generation performance across several dimensions: relevance, grammaticality, factuality, and diversity. Our approach outperforms existing state of the art algorithms for hate speech detection and promising counter-narrative generation performance scores across our metrics reflect effectiveness of our pipeline in addressing hateful social media posts",
    "venue": "The Florida AI Research Society",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-13",
    "authors": [
      {
        "authorId": "1396922854",
        "name": "R. Leekha"
      },
      {
        "authorId": "2301372784",
        "name": "Olga Simek"
      },
      {
        "authorId": "2301368497",
        "name": "Charlie Dagli"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "bca1bd285477efe9694384765b96d043ddd50d37",
    "url": "https://www.semanticscholar.org/paper/bca1bd285477efe9694384765b96d043ddd50d37",
    "title": "Pistis-RAG: Enhancing Retrieval-Augmented Generation with Human Feedback",
    "abstract": "RAG systems face limitations when semantic relevance alone does not guarantee improved generation quality. This issue becomes particularly evident due to the sensitivity of large language models (LLMs) to the ordering of few-shot prompts, which can affect model performance. To address this challenge, aligning LLM outputs with human preferences using structured feedback, such as options to copy, regenerate, or dislike, offers a promising method for improvement. This feedback is applied to the entire list of inputs rather than giving specific ratings for individual documents, making it a Listwide Labels Learning-to-Rank task. To address this task, we propose Pistis-RAG, a new RAG framework designed with a content-centric approach to better align LLMs with human preferences. Pistis-RAG effectively utilizes human feedback, enhancing content ranking and generation quality. To validate our framework, we use public datasets to simulate human feedback, allowing us to evaluate and refine our method effectively. Experimental results indicate that Pistis-RAG improves alignment with human preferences relative to the baseline RAG system, showing a 6.06% increase in MMLU (English) and a 7.08% increase in C-EVAL (Chinese) accuracy metrics. These results highlight Pistis-RAG's effectiveness in overcoming the limitations associated with traditional RAG approaches.",
    "venue": "",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-06-21",
    "authors": [
      {
        "authorId": "2239392496",
        "name": "Yu Bai"
      },
      {
        "authorId": "2239095099",
        "name": "Yukai Miao"
      },
      {
        "authorId": "2239195243",
        "name": "Li Chen"
      },
      {
        "authorId": "2278392505",
        "name": "Dawei Wang"
      },
      {
        "authorId": "2239099410",
        "name": "Dan Li"
      },
      {
        "authorId": "2267684690",
        "name": "Yanyu Ren"
      },
      {
        "authorId": "2312101660",
        "name": "Hongtao Xie"
      },
      {
        "authorId": "2309213959",
        "name": "Ce Yang"
      },
      {
        "authorId": "2310400489",
        "name": "Xuhui Cai"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "c594c52da2e7146339f9558db1a2b057577ef557",
    "url": "https://www.semanticscholar.org/paper/c594c52da2e7146339f9558db1a2b057577ef557",
    "title": "Using retrieval-augmented generation to elevate low-code developer skills",
    "abstract": "This article proposes applying retrieval-augmented generation (RAG) to improve the skills of low-code developers by augmenting large language models with up-to-date domain-specific knowledge. As low-code development requires combining multiple systems into a final product, developers must consult several sources of documentation and various articles, videos, and forum threads. Such a process may be time-consuming, prompting the use of an LLM for the authoritative answer. However, LLMs often lack knowledge of low-code platforms, leading to hallucinations and superficial responses. RAG utilizes the benefits of LLMs on relevant information, suggesting a presumption that it may be effectively applied in low-code development. Heterogeneous data sources concerning low-code systems are converted to a text representation, split into logical chunks, and stored in a vector database. During the exploitation of the model, cosine similarity is used to retrieve top-K documents and concatenate them with user query, using the produced text as a prompt to an LLM. The results support the hypothesis that RAG models outperform standard LLMs in knowledge retrieval in this domain",
    "venue": "Artificial Intelligence",
    "year": 2023,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-30",
    "authors": [
      {
        "authorId": "2273963117",
        "name": "Nakhod O"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "db354b82d3129bdf6bd54e6f8aa983a20a898794",
    "url": "https://www.semanticscholar.org/paper/db354b82d3129bdf6bd54e6f8aa983a20a898794",
    "title": "Retrieval Augmented Generation using Engineering Design Knowledge",
    "abstract": "Aiming to support Retrieval Augmented Generation (RAG) in the design process, we present a method to identify explicit, engineering design facts - {head entity :: relationship :: tail entity} from patented artefact descriptions. Given a sentence with a pair of entities (based on noun phrases) marked in a unique manner, our method extracts the relationship that is explicitly communicated in the sentence. For this task, we create a dataset of 375,084 examples and fine-tune language models for relation identification (token classification) and elicitation (sequence-to-sequence). The token classification approach achieves up to 99.7 % accuracy. Upon applying the method to a domain of 4,870 fan system patents, we populate a knowledge base of over 2.93 million facts. Using this knowledge base, we demonstrate how Large Language Models (LLMs) are guided by explicit facts to synthesise knowledge and generate technical and cohesive responses when sought out for knowledge retrieval tasks in the design process.",
    "venue": "",
    "year": 2023,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-07-13",
    "authors": [
      {
        "authorId": "51471694",
        "name": "L. Siddharth"
      },
      {
        "authorId": "145990580",
        "name": "Jianxi Luo"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "6633c4b3ec2008b7ed77f5293c1bd4fd4f66cc79",
    "url": "https://www.semanticscholar.org/paper/6633c4b3ec2008b7ed77f5293c1bd4fd4f66cc79",
    "title": "Evaluation of Orca 2 Against Other LLMs for Retrieval Augmented Generation",
    "abstract": null,
    "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2304300042",
        "name": "Donghao Huang"
      },
      {
        "authorId": "2301266496",
        "name": "Zhaoxia Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 60.39720770839918
  },
  {
    "paperId": "483cccd73494ebb9a34dc3825f21e0ef92665cc0",
    "url": "https://www.semanticscholar.org/paper/483cccd73494ebb9a34dc3825f21e0ef92665cc0",
    "title": "A Retrieval-Augmented Generation Strategy to Enhance Medical Chatbot Reliability",
    "abstract": null,
    "venue": "Conference on Artificial Intelligence in Medicine in Europe",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2315310497",
        "name": "Saba Ghanbari Haez"
      },
      {
        "authorId": "2302561013",
        "name": "Marina Segala"
      },
      {
        "authorId": "51891870",
        "name": "Patrizio Bellan"
      },
      {
        "authorId": "2273097074",
        "name": "Simone Magnolini"
      },
      {
        "authorId": "2302560427",
        "name": "Leonardo Sanna"
      },
      {
        "authorId": "2273094860",
        "name": "Monica Consolandi"
      },
      {
        "authorId": "2273097195",
        "name": "Mauro Dragoni"
      }
    ],
    "source": "semantic_scholar",
    "score": 60.39720770839918
  },
  {
    "paperId": "42494c88cb0b242274f96c19f3d160380410a3fe",
    "url": "https://www.semanticscholar.org/paper/42494c88cb0b242274f96c19f3d160380410a3fe",
    "title": "SRAG: Speech Retrieval Augmented Generation for Spoken Language Understanding",
    "abstract": "Retrieval augmented generation (RAG) has shown promise for enhancing natural language understanding (NLU) capabilities of large language models (LLMs) by retrieving relevant knowledge as prompts. Extending RAG to spoken language understanding (SLU) represents an important research direction. This paper proposes a RAG approach for improving SLU. First, the encoder of a pretrained automatic speech recognition model is utilized for speech retrieval over the training set. The corresponding texts and intent labels are then formulated as prompts to guide the SLU decoder. Furthermore, a prompt attention mechanism is introduced to strengthen the attention between generation and prompts. Experiments demonstrate that the proposed speech RAG approach substantially outperforms conventional end-to-end and cascaded SLU models in intent prediction from speech. This highlights the efficacy of leveraging retrieval-based prompting to incorporate external knowledge for advancing SLU.",
    "venue": "2023 6th International Seminar on Research of Information Technology and Intelligent Systems (ISRITI)",
    "year": 2023,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": "2023-12-11",
    "authors": [
      {
        "authorId": "2292942411",
        "name": "Hao Yang"
      },
      {
        "authorId": "2293704687",
        "name": "Min Zhang"
      },
      {
        "authorId": "8884457",
        "name": "Daimeng Wei"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "ed795e4913449e29ed57fdf92ccd8c7d109e1cfb",
    "url": "https://www.semanticscholar.org/paper/ed795e4913449e29ed57fdf92ccd8c7d109e1cfb",
    "title": "RIGHT: Retrieval-augmented Generation for Mainstream Hashtag Recommendation",
    "abstract": "Automatic mainstream hashtag recommendation aims to accurately provide users with concise and popular topical hashtags before publication. Generally, mainstream hashtag recommendation faces challenges in the comprehensive difficulty of newly posted tweets in response to new topics, and the accurate identification of mainstream hashtags beyond semantic correctness. However, previous retrieval-based methods based on a fixed predefined mainstream hashtag list excel in producing mainstream hashtags, but fail to understand the constant flow of up-to-date information. Conversely, generation-based methods demonstrate a superior ability to comprehend newly posted tweets, but their capacity is constrained to identifying mainstream hashtags without additional features. Inspired by the recent success of the retrieval-augmented technique, in this work, we attempt to adopt this framework to combine the advantages of both approaches. Meantime, with the help of the generator component, we could rethink how to further improve the quality of the retriever component at a low cost. Therefore, we propose RetrIeval-augmented Generative Mainstream HashTag Recommender (RIGHT), which consists of three components: 1) a retriever seeks relevant hashtags from the entire tweet-hashtags set; 2) a selector enhances mainstream identification by introducing global signals; and 3) a generator incorporates input tweets and selected hashtags to directly generate the desired hashtags. The experimental results show that our method achieves significant improvements over state-of-the-art baselines. Moreover, RIGHT can be easily integrated into large language models, improving the performance of ChatGPT by more than 10%.",
    "venue": "European Conference on Information Retrieval",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-16",
    "authors": [
      {
        "authorId": "1657674644",
        "name": "Run-Ze Fan"
      },
      {
        "authorId": "7888704",
        "name": "Yixing Fan"
      },
      {
        "authorId": "2108313363",
        "name": "Jiangui Chen"
      },
      {
        "authorId": "1777025",
        "name": "J. Guo"
      },
      {
        "authorId": "2109960367",
        "name": "Ruqing Zhang"
      },
      {
        "authorId": "2244825947",
        "name": "Xueqi Cheng"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "2c40a40cbb4a86f0dc0d3036fd6f17c7f9e32129",
    "url": "https://www.semanticscholar.org/paper/2c40a40cbb4a86f0dc0d3036fd6f17c7f9e32129",
    "title": "Optimizing Science Question Ranking through Model and Retrieval-Augmented Generation",
    "abstract": "This paper delves into the challenges of discerning optimal answers from science-based questions generated by large language models (LLM), particularly emphasizing the intricate task of ranking. Employing the MAP@3 evaluation metric and drawing from the OpenBookQA dataset, the study explores modeling strategies and highlights the exceptional performance of the Platypus2-70B model. Equipped with a state-of-the-art text encoder, Platypus2-70B achieves an impressive score of 0.909904, setting a benchmark for excellence in future large language model competitions. The paper goes beyond a mere description of model architectures and experimental results, offering a comprehensive journey that envisions the transformative impact of large-scale language models on the landscape of natural language understanding, especially within the intricate domains of scientific exploration.",
    "venue": "International Journal of Computer Science & Information Technology (IJCSIT)",
    "year": 2023,
    "citationCount": 14,
    "openAccessPdf": {
      "url": "http://wepub.org/index.php/IJCSIT/article/download/606/576",
      "status": "HYBRID"
    },
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-30",
    "authors": [
      {
        "authorId": "2281786127",
        "name": "Ye Zhang"
      },
      {
        "authorId": "2283279768",
        "name": "Mengran Zhu"
      },
      {
        "authorId": "2281799931",
        "name": "Yulu Gong"
      },
      {
        "authorId": "2283195050",
        "name": "Rui Ding"
      }
    ],
    "source": "semantic_scholar",
    "score": 120.62075301653314
  },
  {
    "paperId": "87dde6e5f221bf697d79b74f2efafaca9da220fd",
    "url": "https://www.semanticscholar.org/paper/87dde6e5f221bf697d79b74f2efafaca9da220fd",
    "title": "RAGLog: Log Anomaly Detection using Retrieval Augmented Generation",
    "abstract": "The ability to detect log anomalies from system logs is a vital activity needed to ensure cyber resiliency of systems. It is applied for fault identification or facilitate cyber investigation and digital forensics. However, as logs belonging to different systems and components differ significantly, the challenge to perform such analysis is humanly challenging from the volume, variety and velocity of logs. This is further complicated by the lack or unavailability of anomalous log entries to develop trained machine learning or artificial intelligence models for such purposes. In this research work, we explore the use of a Retrieval Augmented Large Language Model that leverages a vector database to detect anomalies from logs. We used a Question and Answer configuration pipeline. To the best of our knowledge, our experiment which we called RAGLog is a novel one and the experimental results show much promise.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-09",
    "authors": [
      {
        "authorId": "2213687575",
        "name": "Jonathan Pan"
      },
      {
        "authorId": "2266142024",
        "name": "Swee Liang Wong"
      },
      {
        "authorId": "2265932326",
        "name": "Yidi Yuan"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "c311a89de3f869ec97973ad68c7a267cbfd23971",
    "url": "https://www.semanticscholar.org/paper/c311a89de3f869ec97973ad68c7a267cbfd23971",
    "title": "DuetRAG: Collaborative Retrieval-Augmented Generation",
    "abstract": "Retrieval-Augmented Generation (RAG) methods augment the input of Large Language Models (LLMs) with relevant retrieved passages, reducing factual errors in knowledge-intensive tasks. However, contemporary RAG approaches suffer from irrelevant knowledge retrieval issues in complex domain questions (e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to low-quality generations. To address this issue, we propose a novel Collaborative Retrieval-Augmented Generation framework, DuetRAG. Our bootstrapping philosophy is to simultaneously integrate the domain fintuning and RAG models to improve the knowledge retrieval quality, thereby enhancing generation quality. Finally, we demonstrate DuetRAG' s matches with expert human researchers on HotPot QA.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-12",
    "authors": [
      {
        "authorId": "2302798653",
        "name": "Dian Jiao"
      },
      {
        "authorId": "2303434387",
        "name": "Li Cai"
      },
      {
        "authorId": "2303044665",
        "name": "Jingsheng Huang"
      },
      {
        "authorId": "2108125912",
        "name": "Wenqiao Zhang"
      },
      {
        "authorId": "2118071462",
        "name": "Siliang Tang"
      },
      {
        "authorId": "2253660817",
        "name": "Yueting Zhuang"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "1dbc0e65604e34d87e6ffda47f35b6fc792af10b",
    "url": "https://www.semanticscholar.org/paper/1dbc0e65604e34d87e6ffda47f35b6fc792af10b",
    "title": "Accelerating Retrieval-Augmented Generation",
    "abstract": "An evolving solution to address hallucination and enhance accuracy in large language models (LLMs) is Retrieval-Augmented Generation (RAG), which involves augmenting LLMs with information retrieved from an external knowledge source, such as the web. This paper profiles several RAG execution pipelines and demystifies the complex interplay between their retrieval and generation phases. We demonstrate that while exact retrieval schemes are expensive, they can reduce inference time compared to approximate retrieval variants because an exact retrieval model can send a smaller but more accurate list of documents to the generative model while maintaining the same end-to-end accuracy. This observation motivates the acceleration of the exact nearest neighbor search for RAG. In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL device that implements a scale-out near-memory acceleration architecture with a novel cache-coherent interface between the host CPU and near-memory accelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a 512GB vector database compared with executing the search on Intel Sapphire Rapids CPUs. This higher search performance translates to 1.7-26.3x lower end-to-end inference time for representative RAG applications. IKS is inherently a memory expander; its internal DRAM can be disaggregated and used for other applications running on the server to prevent DRAM, which is the most expensive component in today's servers, from being stranded.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-12-14",
    "authors": [
      {
        "authorId": "2271710074",
        "name": "Derrick Quinn"
      },
      {
        "authorId": "2292049291",
        "name": "Mohammad Nouri"
      },
      {
        "authorId": "2273672376",
        "name": "Neel Patel"
      },
      {
        "authorId": "2336731280",
        "name": "John Salihu"
      },
      {
        "authorId": "2073044451",
        "name": "Alireza Salemi"
      },
      {
        "authorId": "2336831464",
        "name": "Sukhan Lee"
      },
      {
        "authorId": "2295731593",
        "name": "Hamed Zamani"
      },
      {
        "authorId": "2273183386",
        "name": "Mohammad Alian"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "5eced62c6e4b68068939283afba0375e97ce90a1",
    "url": "https://www.semanticscholar.org/paper/5eced62c6e4b68068939283afba0375e97ce90a1",
    "title": "LI-RAGE: Late Interaction Retrieval Augmented Generation with Explicit Signals for Open-Domain Table Question Answering",
    "abstract": "Recent open-domain TableQA models are typically implemented as retriever-reader pipelines. The retriever component is usually a variant of the Dense Passage Retriever, which computes the similarities between questions and tables based on a single representation of each.These fixed vectors can be insufficient to capture fine-grained features of potentially very big tables with heterogeneous row/column information. We address this limitation by 1) applying late interaction models which enforce a finer-grained interaction between question and table embeddings at retrieval time. In addition, we 2) incorporate a joint training scheme of the retriever and reader with explicit table-level signals, and 3) embed a binary relevance token as a prefix to the answer generated by the reader, so we can determine at inference time whether the table used to answer the question is reliable and filter accordingly. The combined strategies set a new state-to-the-art performance on two public open-domain TableQA datasets.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.acl-short.133.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1454363000",
        "name": "Weizhe Lin"
      },
      {
        "authorId": "2221287630",
        "name": "Rexhina Blloshmi"
      },
      {
        "authorId": "36126076",
        "name": "B. Byrne"
      },
      {
        "authorId": "1738657356",
        "name": "A. de Gispert"
      },
      {
        "authorId": "145833974",
        "name": "Gonzalo Iglesias"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "beb3389ded23688da387f5ed027a52da06b54e17",
    "url": "https://www.semanticscholar.org/paper/beb3389ded23688da387f5ed027a52da06b54e17",
    "title": "A Retrieval-Augmented Generation Based Large Language Model Benchmarked On a Novel Dataset",
    "abstract": "The evolution of natural language processing has seen marked advancements, particularly with the advent of models like BERT, Transformers, and GPT variants, with recent additions like GPT and Bard. This paper investigates the Retrieval-Augmented Generation (RAG) framework, providing insights into its modular design and the impact of its constituent modules on performance. Leveraging a unique dataset from Amazon Rainforest natives and biologists, our research demonstrates the significance of preserving indigenous cultures and biodiversity. The experiment employs a customizable RAG methodology, allowing for the interchangeability of various components, such as the base language model and similarity score tools. Findings indicate that while GPT performs slightly better when given context, Palm exhibits superior performance without context. The results also suggest that models tend to perform optimally when paired with similarity scores from their native platforms. Conclusively, our approach showcases the potential of a modular RAG design in optimizing language models, presenting it as a more advantageous strategy compared to traditional fine-tuning of large language models.",
    "venue": "Journal of student-scientists' research",
    "year": 2023,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "https://www.jsr.org/hs/index.php/path/article/download/6213/2485",
      "status": "GOLD"
    },
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-30",
    "authors": [
      {
        "authorId": "2190868176",
        "name": "Kieran Pichai"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "1a58d23c65d23be3e47cfa83762d1eb272ea5511",
    "url": "https://www.semanticscholar.org/paper/1a58d23c65d23be3e47cfa83762d1eb272ea5511",
    "title": "Towards a Retrieval Augmented Generation System for Information on Suicide Prevention*",
    "abstract": "Suicide is a leading cause of mortality worldwide with more than 700 000 per year. Hence, it is instrumental that the general public can access to reliable information that help in suicide prevention. In this work, we have tackled this problem by developing a question-answering system based on retrieval augmented generation ‚Äî this approach allows the system to generate answers based on a corpus of documents curated by psychologists and psychiatrists. Several alternatives have been tested for the two main components of the system: an embedding model to retrieve relevant contexts from the corpus of documents to answer a given question (the best option was a BERT-based model), and a language model to generate a response from the extracted contexts (the best alternative was the Bertin model). The developed system is a first step towards helping in one of the greatest global public health concerns.Clinical Relevance: The system suggested in this work will offer a reliable and accessible resource that could aid in providing timely information and support to individuals at risk and their families, thereby potentially enhancing clinical interventions and patient outcomes in the realm of mental health.",
    "venue": "2023 IEEE EMBS Special Topic Conference on Data Science and Engineering in Healthcare, Medicine and Biology",
    "year": 2023,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2023-12-07",
    "authors": [
      {
        "authorId": "2261747390",
        "name": "Pablo Ascorbe"
      },
      {
        "authorId": "2256507724",
        "name": "Mar√≠a S. Campos"
      },
      {
        "authorId": "2261746542",
        "name": "C√©sar Dom√≠nguez"
      },
      {
        "authorId": "2193190796",
        "name": "J√≥nathan Heras"
      },
      {
        "authorId": "2261746803",
        "name": "Ana Rosa Terroba-Reinares"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.39720770839918
  },
  {
    "paperId": "b462417b40852a5a1e2d8862e5d5b464242ff902",
    "url": "https://www.semanticscholar.org/paper/b462417b40852a5a1e2d8862e5d5b464242ff902",
    "title": "Dynamic Retrieval-Augmented Generation",
    "abstract": "Current state-of-the-art large language models are effective in generating high-quality text and encapsulating a broad spectrum of world knowledge. These models, however, often hallucinate and lack locally relevant factual data. Retrieval-augmented approaches were introduced to overcome these problems and provide more accurate responses. Typically, the retrieved information is simply appended to the main request, restricting the context window size of the model. We propose a novel approach for the Dynamic Retrieval-Augmented Generation (DRAG), based on the entity-augmented generation, which injects compressed embeddings of the retrieved entities into the generative model. The proposed pipeline was developed for code-generation tasks, yet can be transferred to some domains of natural language processing. To train the model, we collect and publish a new project-level code generation dataset. We use it for the evaluation along with publicly available datasets. Our approach achieves several targets: (1) lifting the length limitations of the context window, saving on the prompt size; (2) allowing huge expansion of the number of retrieval entities available for the context; (3) alleviating the problem of misspelling or failing to find relevant entity names. This allows the model to beat all baselines (except GPT-3.5) with a strong margin.",
    "venue": "",
    "year": 2023,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-12-14",
    "authors": [
      {
        "authorId": "2274107562",
        "name": "Anton Shapkin"
      },
      {
        "authorId": "2274786287",
        "name": "Denis Litvinov"
      },
      {
        "authorId": "2284684195",
        "name": "Yaroslav Zharov"
      },
      {
        "authorId": "137361658",
        "name": "Egor Bogomolov"
      },
      {
        "authorId": "2284760809",
        "name": "Timur Galimzyanov"
      },
      {
        "authorId": "2851011",
        "name": "T. Bryksin"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "5fdb4ca353809326ae5f5296a448c40ccbdc0b4b",
    "url": "https://www.semanticscholar.org/paper/5fdb4ca353809326ae5f5296a448c40ccbdc0b4b",
    "title": "Retrieval-Augmented Generation with Quantized Large Language Models: A Comparative Analysis",
    "abstract": "Large language models have demonstrated emergent intelligence and ability to handle a wide array of tasks. However, the reliability of these models in terms of factual accuracy and timely knowledge acquisition remains a challenge. Researchers explore the implementation of retrieval-augmented generation methods, aiming to enhance the authenticity and specificity in knowledge-intensive tasks. This paper discusses the practical application in industrial settings, particularly in assisting design personnel with navigating complex standards and quality manuals. Utilizing an open-source model with 6 billion parameters, the study employs quantization technology for local deployment, addressing computational challenges. The retrieval-augmented generation framework is analyzed, emphasizing the integration of document parsing, vector databases, and text embedding models. Experimental results compare models at different quantization levels, revealing trade-offs between response time, model size, and performance metrics. The findings suggest that 4-bit integer quantization is optimal for standard document retrieval and question-answering tasks, highlighting practical considerations for CPU inference. The paper concludes with insights into hyper-parameter tuning, model comparisons, and future optimizations for enhanced performance in edge device deployments of large language models.",
    "venue": "IoTAAI",
    "year": 2023,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2023-11-24",
    "authors": [
      {
        "authorId": "2300128478",
        "name": "Shanglin Yang"
      },
      {
        "authorId": "2300045418",
        "name": "Jialin Zhu"
      },
      {
        "authorId": "2300129153",
        "name": "Jialin Wang"
      },
      {
        "authorId": "2241951660",
        "name": "Xiaohan Xu"
      },
      {
        "authorId": "2300003699",
        "name": "Zihang Shao"
      },
      {
        "authorId": "2300244859",
        "name": "Liwei Yao"
      },
      {
        "authorId": "2112670948",
        "name": "Benchang Zheng"
      },
      {
        "authorId": "2299984697",
        "name": "Hu Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "e0fd38e8bf98e9bdb292e40e463433c981c1f985",
    "url": "https://www.semanticscholar.org/paper/e0fd38e8bf98e9bdb292e40e463433c981c1f985",
    "title": "Harnessing Retrieval-Augmented Generation (RAG) for Uncovering Knowledge Gaps",
    "abstract": "The paper presents a methodology for uncovering knowledge gaps on the internet using the Retrieval Augmented Generation (RAG) model. By simulating user search behaviour, the RAG system identifies and addresses gaps in information retrieval systems. The study demonstrates the effectiveness of the RAG system in generating relevant suggestions with a consistent accuracy of 93%. The methodology can be applied in various fields such as scientific discovery, educational enhancement, research development, market analysis, search engine optimisation, and content development. The results highlight the value of identifying and understanding knowledge gaps to guide future endeavours.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-12",
    "authors": [
      {
        "authorId": "49885837",
        "name": "J. Hurtado"
      }
    ],
    "source": "semantic_scholar",
    "score": 75.39720770839918
  },
  {
    "paperId": "b721887f271df727bec5783b49de91ba5fc9a1cc",
    "url": "https://www.semanticscholar.org/paper/b721887f271df727bec5783b49de91ba5fc9a1cc",
    "title": "Retrieval Augmented Generation of Symbolic Music with LLMs",
    "abstract": "We explore the use of large language models (LLMs) for music generation using a retrieval system to select relevant examples. We find promising initial results for music generation in a dialogue with the user, especially considering the ease with which such a system can be implemented. The code is available online.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-17",
    "authors": [
      {
        "authorId": "101602321",
        "name": "Nicolas Jonason"
      },
      {
        "authorId": "2267243551",
        "name": "Luca Casini"
      },
      {
        "authorId": "2059983409",
        "name": "Carl Thom'e"
      },
      {
        "authorId": "2267247063",
        "name": "Bob L. T. Sturm"
      }
    ],
    "source": "semantic_scholar",
    "score": 81.47918433002164
  },
  {
    "paperId": "e2907d8a966ba44022c76ddc17d9d0a1ce5b9205",
    "url": "https://www.semanticscholar.org/paper/e2907d8a966ba44022c76ddc17d9d0a1ce5b9205",
    "title": "End-to-End Table Question Answering via Retrieval-Augmented Generation",
    "abstract": "Most existing end-to-end Table Question Answering (Table QA) models consist of a two-stage framework with a retriever to select relevant table candidates from a corpus and a reader to locate the correct answers from table candidates. Even though the accuracy of the reader models is significantly improved with the recent transformer-based approaches, the overall performance of such frameworks still suffers from the poor accuracy of using traditional information retrieval techniques as retrievers. To alleviate this problem, we introduce T-RAG, an end-to-end Table QA model, where a non-parametric dense vector index is fine-tuned jointly with BART, a parametric sequence-to-sequence model to generate answer tokens. Given any natural language question, T-RAG utilizes a unified pipeline to automatically search through a table corpus to directly locate the correct answer from the table cells. We apply T-RAG to recent open-domain Table QA benchmarks and demonstrate that the fine-tuned T-RAG model is able to achieve state-of-the-art performance in both the end-to-end Table QA and the table retrieval tasks.",
    "venue": "arXiv.org",
    "year": 2022,
    "citationCount": 13,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2203.16714",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-03-30",
    "authors": [
      {
        "authorId": "2068792892",
        "name": "Feifei Pan"
      },
      {
        "authorId": "1888104",
        "name": "Mustafa Canim"
      },
      {
        "authorId": "143742133",
        "name": "Michael R. Glass"
      },
      {
        "authorId": "1711133",
        "name": "A. Gliozzo"
      },
      {
        "authorId": "1701341",
        "name": "J. Hendler"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "75dd700df42523996e161341fa80f99566e705a2",
    "url": "https://www.semanticscholar.org/paper/75dd700df42523996e161341fa80f99566e705a2",
    "title": "FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented Generation with an LLM",
    "abstract": "Fast disaster impact reporting is crucial in planning humanitarian assistance. Large Language Models (LLMs) are well known for their ability to write coherent text and fulfill a variety of tasks relevant to impact reporting, such as question answering or text summarization. However, LLMs are constrained by the knowledge within their training data and are prone to generating inaccurate, or\"hallucinated\", information. To address this, we introduce a sophisticated pipeline embodied in our tool FloodBrain (floodbrain.com), specialized in generating flood disaster impact reports by extracting and curating information from the web. Our pipeline assimilates information from web search results to produce detailed and accurate reports on flood events. We test different LLMs as backbones in our tool and compare their generated reports to human-written reports on different metrics. Similar to other studies, we find a notable correlation between the scores assigned by GPT-4 and the scores given by human evaluators when comparing our generated reports to human-authored ones. Additionally, we conduct an ablation study to test our single pipeline components and their relevancy for the final reports. With our tool, we aim to advance the use of LLMs for disaster impact reporting and reduce the time for coordination of humanitarian efforts in the wake of flood disasters.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-05",
    "authors": [
      {
        "authorId": "2320299063",
        "name": "Grace Colverd"
      },
      {
        "authorId": "2325803070",
        "name": "Paul Darm"
      },
      {
        "authorId": "2265494916",
        "name": "Leonard Silverberg"
      },
      {
        "authorId": "2157857749",
        "name": "Noah Kasmanoff"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "9a948203f7cb6cf87fcd33c4e08a6ea300a64bab",
    "url": "https://www.semanticscholar.org/paper/9a948203f7cb6cf87fcd33c4e08a6ea300a64bab",
    "title": "Rethinking Relevance: How Noise and Distractors Impact Retrieval-Augmented Generation",
    "abstract": null,
    "venue": "Italian Information Retrieval Workshop",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2281641042",
        "name": "Florin Cuconasu"
      },
      {
        "authorId": "120709579",
        "name": "Giovanni Trappolini"
      },
      {
        "authorId": "1752951302",
        "name": "F. Siciliano"
      },
      {
        "authorId": "2281640399",
        "name": "Simone Filice"
      },
      {
        "authorId": "2061745788",
        "name": "Cesare Campagnano"
      },
      {
        "authorId": "1781257",
        "name": "Y. Maarek"
      },
      {
        "authorId": "2281641625",
        "name": "Nicola Tonellotto"
      },
      {
        "authorId": "2276426254",
        "name": "Fabrizio Silvestri"
      }
    ],
    "source": "semantic_scholar",
    "score": 50
  },
  {
    "paperId": "61a955799c7429efabc46a60812ec13cb1b3fc26",
    "url": "https://www.semanticscholar.org/paper/61a955799c7429efabc46a60812ec13cb1b3fc26",
    "title": "Chain-of-Retrieval Augmented Generation",
    "abstract": "This paper introduces an approach for training o1-like RAG models that retrieve and reason over relevant information step by step before generating the final answer. Conventional RAG methods usually perform a single retrieval step before the generation process, which limits their effectiveness in addressing complex queries due to imperfect retrieval results. In contrast, our proposed method, CoRAG (Chain-of-Retrieval Augmented Generation), allows the model to dynamically reformulate the query based on the evolving state. To train CoRAG effectively, we utilize rejection sampling to automatically generate intermediate retrieval chains, thereby augmenting existing RAG datasets that only provide the correct final answer. At test time, we propose various decoding strategies to scale the model's test-time compute by controlling the length and number of sampled retrieval chains. Experimental results across multiple benchmarks validate the efficacy of CoRAG, particularly in multi-hop question answering tasks, where we observe more than 10 points improvement in EM score compared to strong baselines. On the KILT benchmark, CoRAG establishes a new state-of-the-art performance across a diverse range of knowledge-intensive tasks. Furthermore, we offer comprehensive analyses to understand the scaling behavior of CoRAG, laying the groundwork for future research aimed at developing factual and grounded foundation models.",
    "venue": "",
    "year": 2025,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2025-01-24",
    "authors": [
      {
        "authorId": null,
        "name": "Liang Wang"
      },
      {
        "authorId": "2280839621",
        "name": "Haonan Chen"
      },
      {
        "authorId": "2242947624",
        "name": "Nan Yang"
      },
      {
        "authorId": "2116768132",
        "name": "Xiaolong Huang"
      },
      {
        "authorId": "2273086037",
        "name": "Zhicheng Dou"
      },
      {
        "authorId": "2257346447",
        "name": "Furu Wei"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "7bd78700829c69a480895346a8d916cf4e93f435",
    "url": "https://www.semanticscholar.org/paper/7bd78700829c69a480895346a8d916cf4e93f435",
    "title": "Parametric Retrieval Augmented Generation",
    "abstract": "Retrieval-augmented generation (RAG) techniques have emerged as a promising solution to enhance the reliability of large language models (LLMs) by addressing issues like hallucinations, outdated knowledge, and domain adaptation. In particular, existing RAG methods append relevant documents retrieved from external corpus or databases to the input of LLMs to guide their generation process, which we refer to as the in-context knowledge injection method. While this approach is simple and often effective, it has inherent limitations. Firstly, increasing the context length and number of relevant documents can lead to higher computational overhead and degraded performance, especially in complex reasoning tasks. More importantly, in-context knowledge injection operates primarily at the input level, but LLMs store their internal knowledge in their parameters. This gap fundamentally limits the capacity of in-context methods. To this end, we introduce Parametric retrieval-augmented generation (Parametric RAG), a new RAG paradigm that integrates external knowledge directly into the parameters of feed-forward networks (FFN) of an LLM through document parameterization. This approach not only saves online computational costs by eliminating the need to inject multiple documents into the LLMs' input context, but also deepens the integration of external knowledge into the parametric knowledge space of the LLM. Experimental results demonstrate that Parametric RAG substantially enhances both the effectiveness and efficiency of knowledge augmentation in LLMs. Also, it can be combined with in-context RAG methods to achieve even better performance. We have open-sourced all the code, data, and models in the following anonymized GitHub link: https://github.com/oneal2000/PRAG",
    "venue": "",
    "year": 2025,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2025-01-27",
    "authors": [
      {
        "authorId": null,
        "name": "Weihang Su"
      },
      {
        "authorId": "2291993837",
        "name": "Yichen Tang"
      },
      {
        "authorId": "2256982003",
        "name": "Qingyao Ai"
      },
      {
        "authorId": null,
        "name": "Junxi Yan"
      },
      {
        "authorId": null,
        "name": "Changyue Wang"
      },
      {
        "authorId": "2342438811",
        "name": "Hongning Wang"
      },
      {
        "authorId": null,
        "name": "Ziyi Ye"
      },
      {
        "authorId": "2290870875",
        "name": "Yujia Zhou"
      },
      {
        "authorId": "2260835922",
        "name": "Yiqun Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "94034fd2ed4b6cf41113abb7adc9ae469313c958",
    "url": "https://www.semanticscholar.org/paper/94034fd2ed4b6cf41113abb7adc9ae469313c958",
    "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
    "abstract": "Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but possibly incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 21,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-04-17",
    "authors": [
      {
        "authorId": "2260272949",
        "name": "Yizheng Huang"
      },
      {
        "authorId": "2259653248",
        "name": "Jimmy X. Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 116.36563680037474
  },
  {
    "paperId": "871961e2ff2a9f7d7673f5d7a712064089e13e25",
    "url": "https://www.semanticscholar.org/paper/871961e2ff2a9f7d7673f5d7a712064089e13e25",
    "title": "Phantom: General Trigger Attacks on Retrieval Augmented Language Generation",
    "abstract": "Retrieval Augmented Generation (RAG) expands the capabilities of modern large language models (LLMs), by anchoring, adapting, and personalizing their responses to the most relevant knowledge sources. It is particularly useful in chatbot applications, allowing developers to customize LLM output without expensive retraining. Despite their significant utility in various applications, RAG systems present new security risks. In this work, we propose new attack vectors that allow an adversary to inject a single malicious document into a RAG system's knowledge base, and mount a backdoor poisoning attack. We design Phantom, a general two-stage optimization framework against RAG systems, that crafts a malicious poisoned document leading to an integrity violation in the model's output. First, the document is constructed to be retrieved only when a specific trigger sequence of tokens appears in the victim's queries. Second, the document is further optimized with crafted adversarial text that induces various adversarial objectives on the LLM output, including refusal to answer, reputation damage, privacy violations, and harmful behaviors. We demonstrate our attacks on multiple LLM architectures, including Gemma, Vicuna, and Llama, and show that they transfer to GPT-3.5 Turbo and GPT-4. Finally, we successfully conducted a Phantom attack on NVIDIA's black-box production RAG system,\"Chat with RTX\".",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-30",
    "authors": [
      {
        "authorId": "81386254",
        "name": "Harsh Chaudhari"
      },
      {
        "authorId": "1411279560",
        "name": "Giorgio Severi"
      },
      {
        "authorId": "32820056",
        "name": "John Abascal"
      },
      {
        "authorId": "40844378",
        "name": "Matthew Jagielski"
      },
      {
        "authorId": "1415982317",
        "name": "Christopher A. Choquette-Choo"
      },
      {
        "authorId": "3490923",
        "name": "Milad Nasr"
      },
      {
        "authorId": "1398550766",
        "name": "C. Nita-Rotaru"
      },
      {
        "authorId": "2288696989",
        "name": "Alina Oprea"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "dc256e179d4e8eff48879a40ddc414b15b0b2300",
    "url": "https://www.semanticscholar.org/paper/dc256e179d4e8eff48879a40ddc414b15b0b2300",
    "title": "RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation",
    "abstract": "We explore how iterative revising a chain of thoughts with the help of information retrieval significantly improves large language models' reasoning and generation ability in long-horizon generation tasks, while hugely mitigating hallucination. In particular, the proposed method -- *retrieval-augmented thoughts* (RAT) -- revises each thought step one by one with retrieved information relevant to the task query, the current and the past thought steps, after the initial zero-shot CoT is generated. Applying RAT to GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on various long-horizon generation tasks; on average of relatively increasing rating scores by 13.63% on code generation, 16.96% on mathematical reasoning, 19.2% on creative writing, and 42.78% on embodied task planning. The demo page can be found at https://craftjarvis.github.io/RAT",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 29,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-08",
    "authors": [
      {
        "authorId": "47196237",
        "name": "Zihao Wang"
      },
      {
        "authorId": "70097297",
        "name": "Anji Liu"
      },
      {
        "authorId": "2257447835",
        "name": "Haowei Lin"
      },
      {
        "authorId": "2290567624",
        "name": "Jiaqi Li"
      },
      {
        "authorId": "2257636629",
        "name": "Xiaojian Ma"
      },
      {
        "authorId": "2257367774",
        "name": "Yitao Liang"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.01796072493232
  },
  {
    "paperId": "32c260ddd7e2d0b05c58f2e67d244b8036699db4",
    "url": "https://www.semanticscholar.org/paper/32c260ddd7e2d0b05c58f2e67d244b8036699db4",
    "title": "C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models",
    "abstract": "Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk functions under test distribution shifts. We prove that RAG achieves a lower conformal generation risk than that of a single LLM when the quality of the retrieval model and transformer is non-trivial. Our intensive empirical results demonstrate the soundness and tightness of our conformal generation risk guarantees across four widely-used NLP datasets on four state-of-the-art retrieval models.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 19,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-05",
    "authors": [
      {
        "authorId": "2153110066",
        "name": "Mintong Kang"
      },
      {
        "authorId": "51274710",
        "name": "Nezihe Merve Gurel"
      },
      {
        "authorId": "2282538184",
        "name": "Ning Yu"
      },
      {
        "authorId": "2242706269",
        "name": "D. Song"
      },
      {
        "authorId": "2267398406",
        "name": "Bo Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.93598410330986
  },
  {
    "paperId": "8f02e0ac92a0b34360eb12105f4b8d98ab91d45a",
    "url": "https://www.semanticscholar.org/paper/8f02e0ac92a0b34360eb12105f4b8d98ab91d45a",
    "title": "Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation",
    "abstract": "The proliferation of online misinformation has posed significant threats to public interest. While numerous online users actively participate in the combat against misinformation, many of such responses can be characterized by the lack of politeness and supporting facts. As a solution, text generation approaches are proposed to automatically produce counter-misinformation responses. Nevertheless, existing methods are often trained end-to-end without leveraging external knowledge, resulting in subpar text quality and excessively repetitive responses. In this paper, we propose retrieval augmented response generation for online misinformation (RARG), which collects supporting evidence from scientific sources and generates counter-misinformation responses based on the evidences. In particular, our RARG consists of two stages: (1) evidence collection, where we design a retrieval pipeline to retrieve and rerank evidence documents using a database comprising over 1M academic articles; (2) response generation, in which we align large language models (LLMs) to generate evidence-based responses via reinforcement learning from human feedback (RLHF). We propose a reward function to maximize the utilization of the retrieved evidence while maintaining the quality of the generated text, which yields polite and factual responses that clearly refutes misinformation. To demonstrate the effectiveness of our method, we study the case of COVID-19 and perform extensive experiments with both in- and cross-domain datasets, where RARG consistently outperforms baselines by generating high-quality counter-misinformation responses.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-22",
    "authors": [
      {
        "authorId": "2028213158",
        "name": "Zhenrui Yue"
      },
      {
        "authorId": "2113559163",
        "name": "Huimin Zeng"
      },
      {
        "authorId": "2293323154",
        "name": "Yimeng Lu"
      },
      {
        "authorId": "65855502",
        "name": "Lanyu Shang"
      },
      {
        "authorId": "2145953897",
        "name": "Yang Zhang"
      },
      {
        "authorId": "2254248851",
        "name": "Dong Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "a1675f47125aa409525c5f759b5e6bcc1c8831aa",
    "url": "https://www.semanticscholar.org/paper/a1675f47125aa409525c5f759b5e6bcc1c8831aa",
    "title": "Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy",
    "abstract": "Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 144,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.15294",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-24",
    "authors": [
      {
        "authorId": "144485528",
        "name": "Zhihong Shao"
      },
      {
        "authorId": "2171182",
        "name": "Yeyun Gong"
      },
      {
        "authorId": "1752875",
        "name": "Yelong Shen"
      },
      {
        "authorId": "1730108",
        "name": "Minlie Huang"
      },
      {
        "authorId": "46429989",
        "name": "Nan Duan"
      },
      {
        "authorId": "2109136147",
        "name": "Weizhu Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 144.6510061363086
  },
  {
    "paperId": "41b796b026a1d322de6ef0b280d3e2e68eee65bd",
    "url": "https://www.semanticscholar.org/paper/41b796b026a1d322de6ef0b280d3e2e68eee65bd",
    "title": "Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory",
    "abstract": "With direct access to human-written reference as memory, retrieval-augmented generation has achieved much progress in a wide range of text generation tasks. Since better memory would typically prompt better generation~(we define this as primal problem). The traditional approach for memory retrieval involves selecting memory that exhibits the highest similarity to the input. However, this method is constrained by the quality of the fixed corpus from which memory is retrieved. In this paper, by exploring the duality of the primal problem: better generation also prompts better memory, we propose a novel framework, selfmem, which addresses this limitation by iteratively employing a retrieval-augmented generator to create an unbounded memory pool and using a memory selector to choose one output as memory for the subsequent generation round. This enables the model to leverage its own output, referred to as self-memory, for improved generation. We evaluate the effectiveness of selfmem on three distinct text generation tasks: neural machine translation, abstractive text summarization, and dialogue generation, under two generation paradigms: fine-tuned small model and few-shot LLM. Our approach achieves state-of-the-art results in four directions in JRC-Acquis, XSum (50.3 ROUGE-1), and BigPatent (62.9 ROUGE-1), demonstrating the potential of self-memory in enhancing retrieval-augmented generation models. Furthermore, we conduct thorough analyses of each component in the selfmem framework to identify bottlenecks and provide insights for future research.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 66,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.02437",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-03",
    "authors": [
      {
        "authorId": "2193630544",
        "name": "Xin Cheng"
      },
      {
        "authorId": "2215612529",
        "name": "Di Luo"
      },
      {
        "authorId": "2116950235",
        "name": "Xiuying Chen"
      },
      {
        "authorId": "2978364",
        "name": "Lemao Liu"
      },
      {
        "authorId": "144060462",
        "name": "Dongyan Zhao"
      },
      {
        "authorId": "144539156",
        "name": "Rui Yan"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.07038929086448
  },
  {
    "paperId": "388fedd2e549e6f51c107162830c38c8f32902e1",
    "url": "https://www.semanticscholar.org/paper/388fedd2e549e6f51c107162830c38c8f32902e1",
    "title": "Ground Every Sentence: Improving Retrieval-Augmented LLMs with Interleaved Reference-Claim Generation",
    "abstract": "Retrieval-Augmented Generation (RAG) has been widely adopted to enhance Large Language Models (LLMs) in knowledge-intensive tasks. Recently, Attributed Text Generation (ATG) has attracted growing attention, which provides citations to support the model's responses in RAG, so as to enhance the credibility of LLM-generated content and facilitate verification. Prior methods mainly adopt coarse-grained attributions, linking to passage-level references or providing paragraph-level citations. However, these methods still fall short in verifiability and require certain time costs for fact checking. This paper proposes a fine-grained ATG method called ReClaim(Refer&Claim), which alternates the generation of references and answers step by step. Unlike traditional coarse-grained attribution, ReClaim allows the model to add sentence-level fine-grained citations to each answer sentence in long-form question-answering tasks. Our experiments encompass various training and inference methods and multiple LLMs, verifying the effectiveness of our approach.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-01",
    "authors": [
      {
        "authorId": "2309175505",
        "name": "Sirui Xia"
      },
      {
        "authorId": "2108003209",
        "name": "Xintao Wang"
      },
      {
        "authorId": "3366523",
        "name": "Jiaqing Liang"
      },
      {
        "authorId": "2309213407",
        "name": "Yifei Zhang"
      },
      {
        "authorId": "2309277002",
        "name": "Weikang Zhou"
      },
      {
        "authorId": "2310294929",
        "name": "Jiaji Deng"
      },
      {
        "authorId": "2310860800",
        "name": "Fei Yu"
      },
      {
        "authorId": "2267005966",
        "name": "Yanghua Xiao"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "6fad4b822db2fb02122575b8b7dd51089f6691df",
    "url": "https://www.semanticscholar.org/paper/6fad4b822db2fb02122575b8b7dd51089f6691df",
    "title": "Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records",
    "abstract": "Electronic health records (EHR) and claims data are rich sources of real-world data that reflect patient health status and healthcare utilization. Querying these databases to answer epidemiological questions is challenging due to the intricacy of medical terminology and the need for complex SQL queries. Here, we introduce an end-to-end methodology that combines text-to-SQL generation with retrieval augmented generation (RAG) to answer epidemiological questions using EHR and claims data. We show that our approach, which integrates a medical coding step into the text-to-SQL process, significantly improves the performance over simple prompting. Our findings indicate that although current language models are not yet sufficiently accurate for unsupervised use, RAG offers a promising direction for improving their capabilities, as shown in a realistic industry setting.",
    "venue": "Clinical Natural Language Processing Workshop",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-14",
    "authors": [
      {
        "authorId": "2291136353",
        "name": "Angelo Ziletti"
      },
      {
        "authorId": "1399705292",
        "name": "Leonardo D'Ambrosi"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "3e231405a4ab25a3c7510c69a69d91e99d7cce76",
    "url": "https://www.semanticscholar.org/paper/3e231405a4ab25a3c7510c69a69d91e99d7cce76",
    "title": "CLERC: A Dataset for Legal Case Retrieval and Retrieval-Augmented Analysis Generation",
    "abstract": "Legal professionals need to write analyses that rely on citations to relevant precedents, i.e., previous case decisions. Intelligent systems assisting legal professionals in writing such documents provide great benefits but are challenging to design. Such systems need to help locate, summarize, and reason over salient precedents in order to be useful. To enable systems for such tasks, we work with legal professionals to transform a large open-source legal corpus into a dataset supporting two important backbone tasks: information retrieval (IR) and retrieval-augmented generation (RAG). This dataset CLERC (Case Law Evaluation Retrieval Corpus), is constructed for training and evaluating models on their ability to (1) find corresponding citations for a given piece of legal analysis and to (2) compile the text of these citations (as well as previous context) into a cogent analysis that supports a reasoning goal. We benchmark state-of-the-art models on CLERC, showing that current approaches still struggle: GPT-4o generates analyses with the highest ROUGE F-scores but hallucinates the most, while zero-shot IR models only achieve 48.3% recall@1000.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-24",
    "authors": [
      {
        "authorId": "2257002716",
        "name": "A. Hou"
      },
      {
        "authorId": "47433471",
        "name": "Orion Weller"
      },
      {
        "authorId": "38733050",
        "name": "Guanghui Qin"
      },
      {
        "authorId": "2296599846",
        "name": "Eugene Yang"
      },
      {
        "authorId": "2261984405",
        "name": "Dawn Lawrie"
      },
      {
        "authorId": "51261844",
        "name": "Nils Holzenberger"
      },
      {
        "authorId": "1403840964",
        "name": "Andrew Blair-Stanek"
      },
      {
        "authorId": "2292194313",
        "name": "Benjamin Van Durme"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "9343a0211c31dc7587139bcdb0e88ca36f65a88c",
    "url": "https://www.semanticscholar.org/paper/9343a0211c31dc7587139bcdb0e88ca36f65a88c",
    "title": "LLM-based and Retrieval-Augmented Control Code Generation",
    "abstract": "Control code is designed and implemented for industrial automation applications that manage power plants, petrochemical processes, or steel production. Popular large language models (LLM) can synthesize low-level control code in the Structured Text programming notation according to the standard IEC 61131-3, but are not aware of proprietary control code function block libraries, which are often used in practice. To automate control logic implementation tasks, we proposed a retrieval-augmented control code generation method that can integrate such function blocks into the generated code. With this method control engineers can benefit from the code generation capabilities of LLMs, re-use proprietary and well-tested function blocks, and speed up typical programming tasks significantly. We have evaluated the method using a prototypical implementation based on GPT-4, LangChain, Open-PLC, and the open-source OSCAT function block library. In several spot sample tests, we successfully generated IEC 61131-3 ST code that integrated the desired function blocks, could be compiled, and validated through simulations.CCS CONCEPTS‚Ä¢ Software and its engineering Automatic programming; Command and control languages; ‚Ä¢ Applied computing ‚Üí Computer-aided design; ‚Ä¢ Computing methodologies ‚Üí Natural language processing.",
    "venue": "2024 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code)",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-20",
    "authors": [
      {
        "authorId": "2313002919",
        "name": "Heiko Koziolek"
      },
      {
        "authorId": "2239560299",
        "name": "Sten Gr√ºner"
      },
      {
        "authorId": "1776025",
        "name": "Rhaban Hark"
      },
      {
        "authorId": "2142749704",
        "name": "Virendra Ashiwal"
      },
      {
        "authorId": "2316752931",
        "name": "Sofia Linsbauer"
      },
      {
        "authorId": "1397375971",
        "name": "Nafise Eskandani"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.53877639491068
  },
  {
    "paperId": "d99fcc9136b60a824752a0ccb656bb9ae0c1c4ea",
    "url": "https://www.semanticscholar.org/paper/d99fcc9136b60a824752a0ccb656bb9ae0c1c4ea",
    "title": "BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering",
    "abstract": "Retrieval-augmented Large Language Models (LLMs) offer substantial benefits in enhancing performance across knowledge-intensive scenarios. However, these methods often struggle with complex inputs and encounter difficulties due to noisy knowledge retrieval, notably hindering model effectiveness. To address this issue, we introduce BlendFilter, a novel approach that elevates retrieval-augmented LLMs by integrating query generation blending with knowledge filtering. BlendFilter proposes the blending process through its query generation method, which integrates both external and internal knowledge augmentation with the original query, ensuring comprehensive information gathering. Additionally, our distinctive knowledge filtering module capitalizes on the intrinsic capabilities of the LLM, effectively eliminating extraneous data. We conduct extensive experiments on three open-domain question answering benchmarks, and the findings clearly indicate that our innovative BlendFilter surpasses state-of-the-art baselines significantly.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-16",
    "authors": [
      {
        "authorId": "51225422",
        "name": "Haoyu Wang"
      },
      {
        "authorId": "2273928648",
        "name": "Tuo Zhao"
      },
      {
        "authorId": "2284861474",
        "name": "Jing Gao"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "55c3095681acc82780508b0e484dba0c30cf1caa",
    "url": "https://www.semanticscholar.org/paper/55c3095681acc82780508b0e484dba0c30cf1caa",
    "title": "Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation",
    "abstract": "We propose a new method to measure the task-specific accuracy of Retrieval-Augmented Large Language Models (RAG). Evaluation is performed by scoring the RAG on an automatically-generated synthetic exam composed of multiple choice questions based on the corpus of documents associated with the task. Our method is an automated, cost-efficient, interpretable, and robust strategy to select the optimal components for a RAG system. We leverage Item Response Theory (IRT) to estimate the quality of an exam and its informativeness on task-specific accuracy. IRT also provides a natural way to iteratively improve the exam by eliminating the exam questions that are not sufficiently informative about a model's ability. We demonstrate our approach on four new open-ended Question-Answering tasks based on Arxiv abstracts, StackExchange questions, AWS DevOps troubleshooting guides, and SEC filings. In addition, our experiments reveal more general insights into factors impacting RAG performance like size, retrieval mechanism, prompting and fine-tuning. Most notably, our findings show that choosing the right retrieval algorithms often leads to bigger performance gains than simply using a larger language model.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-22",
    "authors": [
      {
        "authorId": "2028200300",
        "name": "Gauthier Guinet"
      },
      {
        "authorId": "1403851743",
        "name": "Behrooz Omidvar-Tehrani"
      },
      {
        "authorId": "1713801",
        "name": "Anoop Deoras"
      },
      {
        "authorId": "2302802307",
        "name": "Laurent Callot"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "c77e8affde18b46295b3cb153937bb32b3eaf933",
    "url": "https://www.semanticscholar.org/paper/c77e8affde18b46295b3cb153937bb32b3eaf933",
    "title": "Enhancing Distractor Generation for Multiple-Choice Questions with Retrieval Augmented Pretraining and Knowledge Graph Integration",
    "abstract": "In this paper, we tackle the task of distractor generation (DG) for multiple-choice questions. Our study introduces two key designs. First, we propose \\textit{retrieval augmented pretraining}, which involves refining the language model pretraining to align it more closely with the downstream task of DG. Second, we explore the integration of knowledge graphs to enhance the performance of DG. Through experiments with benchmarking datasets, we show that our models significantly outperform the state-of-the-art results. Our best-performing model advances the F1@3 score from 14.80 to 16.47 in MCQ dataset and from 15.92 to 16.50 in Sciq dataset.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-19",
    "authors": [
      {
        "authorId": "2203132690",
        "name": "Han-Cheng Yu"
      },
      {
        "authorId": "2203084106",
        "name": "Yu-An Shih"
      },
      {
        "authorId": "2307463292",
        "name": "Kin-Man Law"
      },
      {
        "authorId": "2203068386",
        "name": "Kai-Yu Hsieh"
      },
      {
        "authorId": "2307482289",
        "name": "Yu-Chen Cheng"
      },
      {
        "authorId": "2307450965",
        "name": "Hsin-Chih Ho"
      },
      {
        "authorId": "2275772050",
        "name": "Zih-An Lin"
      },
      {
        "authorId": "2307465155",
        "name": "Wen-Chuan Hsu"
      },
      {
        "authorId": "2275767125",
        "name": "Yao-Chung Fan"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1886522358297
  },
  {
    "paperId": "3675ae068acb1bd60e9c880c763150253ad1daef",
    "url": "https://www.semanticscholar.org/paper/3675ae068acb1bd60e9c880c763150253ad1daef",
    "title": "Retrieval-Augmented Score Distillation for Text-to-3D Generation",
    "abstract": "Text-to-3D generation has achieved significant success by incorporating powerful 2D diffusion models, but insufficient 3D prior knowledge also leads to the inconsistency of 3D geometry. Recently, since large-scale multi-view datasets have been released, fine-tuning the diffusion model on the multi-view datasets becomes a mainstream to solve the 3D inconsistency problem. However, it has confronted with fundamental difficulties regarding the limited quality and diversity of 3D data, compared with 2D data. To sidestep these trade-offs, we explore a retrieval-augmented approach tailored for score distillation, dubbed ReDream. We postulate that both expressiveness of 2D diffusion models and geometric consistency of 3D assets can be fully leveraged by employing the semantically relevant assets directly within the optimization process. To this end, we introduce novel framework for retrieval-based quality enhancement in text-to-3D generation. We leverage the retrieved asset to incorporate its geometric prior in the variational objective and adapt the diffusion model's 2D prior toward view consistency, achieving drastic improvements in both geometry and fidelity of generated scenes. We conduct extensive experiments to demonstrate that ReDream exhibits superior quality with increased geometric consistency. Project page is available at https://ku-cvlab.github.io/ReDream/.",
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-05",
    "authors": [
      {
        "authorId": "49867112",
        "name": "Junyoung Seo"
      },
      {
        "authorId": "2186865215",
        "name": "Susung Hong"
      },
      {
        "authorId": "8310837",
        "name": "Wooseok Jang"
      },
      {
        "authorId": "2130270152",
        "name": "Ines Hyeonsu Kim"
      },
      {
        "authorId": "2202704871",
        "name": "Minseop Kwak"
      },
      {
        "authorId": "2282554412",
        "name": "Doyup Lee"
      },
      {
        "authorId": "2257312084",
        "name": "Seungryong Kim"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "59bf9d56d57386aac83926da4b4c22a3ae6d1804",
    "url": "https://www.semanticscholar.org/paper/59bf9d56d57386aac83926da4b4c22a3ae6d1804",
    "title": "LAKE-RED: Camouflaged Images Generation by Latent Background Knowledge Retrieval-Augmented Diffusion",
    "abstract": "Camouflaged vision perception is an important vision task with numerous practical applications. Due to the expensive collection and labeling costs, this community struggles with a major bottleneck that the species category of its datasets is limited to a small number of object species. However, the existing camouflaged generation methods require specifying the background manually, thus failing to extend the camouflaged sample diversity in a low-cost manner. In this paper, we propose a Latent Background Knowledge Retrieval-Augmented Diffusion (LAKE-RED) for camouflaged image generation. To our knowledge, our contributions mainly include: (1) For the first time, we propose a camouflaged generation paradigm that does not need to re-eive any background inputs. (2) Our LAKE-RED is the first knowledge retrieval-augmented method with interpretability for camouflaged generation, in which we propose an idea that knowledge retrieval and reasoning enhancement are separated explicitly, to alleviate the task-specific chal-lenges. Moreover, our method is not restricted to specific foreground targets or backgrounds, offering a potential for extending camouflaged vision perception to more diverse domains. (3) Experimental results demonstrate that our method outperforms the existing approaches, generating more realistic camouflage images. Our source code is released on https://github.com/PanchengZhaoILAKE-RED.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2404.00292",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-30",
    "authors": [
      {
        "authorId": "2295200178",
        "name": "Pancheng Zhao"
      },
      {
        "authorId": "2268685797",
        "name": "Peng Xu"
      },
      {
        "authorId": "2268674626",
        "name": "Pengda Qin"
      },
      {
        "authorId": "2268675384",
        "name": "Deng-Ping Fan"
      },
      {
        "authorId": "2213702340",
        "name": "Zhicheng Zhang"
      },
      {
        "authorId": "2273925124",
        "name": "Guoli Jia"
      },
      {
        "authorId": "2269188963",
        "name": "Bowen Zhou"
      },
      {
        "authorId": "2279760632",
        "name": "Jufeng Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "471f3012cee44684aa2e193373391d96a580e9fd",
    "url": "https://www.semanticscholar.org/paper/471f3012cee44684aa2e193373391d96a580e9fd",
    "title": "PropertyGPT: LLM-driven Formal Verification of Smart Contracts through Retrieval-Augmented Property Generation",
    "abstract": "With recent advances in large language models (LLMs), this paper explores the potential of leveraging state-of-the-art LLMs,such as GPT-4, to transfer existing human-written properties (e.g.,those from Certora auditing reports) and automatically generate customized properties for unknown code. To this end, we embed existing properties into a vector database and retrieve a reference property for LLM-based in-context learning to generate a new property for a given code. While this basic process is relatively straightforward, ensuring that the generated properties are (i) compilable, (ii) appropriate, and (iii) verifiable presents challenges. To address (i), we use the compilation and static analysis feedback as an external oracle to guide LLMs in iteratively revising the generated properties. For (ii), we consider multiple dimensions of similarity to rank the properties and employ a weighted algorithm to identify the top-K properties as the final result. For (iii), we design a dedicated prover to formally verify the correctness of the generated properties. We have implemented these strategies into a novel LLM-based property generation tool called PropertyGPT. Our experiments show that PropertyGPT can generate comprehensive and high-quality properties, achieving an 80% recall compared to the ground truth. It successfully detected 26 CVEs/attack incidents out of 37 tested and also uncovered 12 zero-day vulnerabilities, leading to $8,256 in bug bounty rewards.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-04",
    "authors": [
      {
        "authorId": "2291057369",
        "name": "Ye Liu"
      },
      {
        "authorId": "2228257773",
        "name": "Yue Xue"
      },
      {
        "authorId": "2253859864",
        "name": "Daoyuan Wu"
      },
      {
        "authorId": "2281788746",
        "name": "Yuqiang Sun"
      },
      {
        "authorId": "2268952326",
        "name": "Yi Li"
      },
      {
        "authorId": "2281822756",
        "name": "Miaolei Shi"
      },
      {
        "authorId": "2281790097",
        "name": "Yang Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "1169c485293107ba0192770c6e40562392f4c520",
    "url": "https://www.semanticscholar.org/paper/1169c485293107ba0192770c6e40562392f4c520",
    "title": "REANO: Optimising Retrieval-Augmented Reader Models through Knowledge Graph Generation",
    "abstract": "Open domain question answering (ODQA) aims to answer questions with knowledge from an external corpus. Fusion-in-Decoder (FiD) is an effective retrieval-augmented reader model to address this task. Given that FiD independently encodes passages, which overlooks the semantic relationships between passages, some studies use knowledge graphs (KGs) to establish dependencies among passages. However, they only leverage knowledge triples from existing KGs, which suffer from incompleteness and may lack certain information critical for answering given questions. To this end, in order to capture the dependencies between passages while tacking the issue of incompleteness in existing KGs, we propose to enhance the retrieval-augmented reader model with a knowledge graph generation module ( REANO ). Specifically, REANO consists of a KG generator and an answer predictor . The KG generator aims to generate KGs from the passages; the answer predictor then generates answers based on the passages and the generated KGs. Experimental results on five ODQA datasets indicate that compared with baselines, REANO 1 can improve the exact match score by up to 2 . 7% on the EntityQuestion dataset, with an average improvement of 1 . 8% across all the datasets.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1384241384",
        "name": "Jinyuan Fang"
      },
      {
        "authorId": "3451645",
        "name": "Zaiqiao Meng"
      },
      {
        "authorId": "2260652308",
        "name": "Craig Macdonald"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "88bce6c2e44a4a60fe1136e4f340636a60ef0964",
    "url": "https://www.semanticscholar.org/paper/88bce6c2e44a4a60fe1136e4f340636a60ef0964",
    "title": "Retrieval Augmented Structured Generation: Business Document Information Extraction as Tool Use",
    "abstract": "Business Document Information Extraction (BDIE) is the problem of transforming a blob of unstructured information (raw text, scanned documents, etc.) into a structured format that downstream systems can parse and use. It has two main tasks: Key-Information Extraction (KIE) and Line Items Recognition (LIR). In this paper, we argue that BDIE is best modeled as a Tool Use problem, where the tools are these downstream systems. We then present Retrieval Augmented Structured Generation (RASG), a novel general framework for BDIE that achieves state of the art (SOTA) results on both KIE and LIR tasks on BDIE benchmarks. The contributions of this paper are threefold: (1) We show, with ablation benchmarks, that Large Language Models (LLMs) with RASG are already competitive with or surpasses current SOTA Large Multimodal Models (LMMs) without RASG on BDIE benchmarks. (2) We propose a new metric class for Line Items Recognition, General Line Items Recognition Metric (GLIRM), that is more aligned with practical BDIE use cases compared to existing metrics, such as ANLS*, DocILE, and GriTS. (3) We provide a heuristic algorithm for backcalculating bounding boxes of predicted line items and tables without the need for vision encoders. Finally, we claim that, while LMMs might sometimes offer marginal performance benefits, LLMs + RASG is oftentimes superior given real-world applications and constraints of BDIE.",
    "venue": "Conference on Multimedia Information Processing and Retrieval",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-30",
    "authors": [
      {
        "authorId": "2303850298",
        "name": "Franz Louis Cesista"
      },
      {
        "authorId": "2303849937",
        "name": "Rui Aguiar"
      },
      {
        "authorId": "2303890835",
        "name": "Jason Kim"
      },
      {
        "authorId": "2303850067",
        "name": "Paolo Acilo"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "7a6fc147f2c0cc43c253778bd712191b10911b9c",
    "url": "https://www.semanticscholar.org/paper/7a6fc147f2c0cc43c253778bd712191b10911b9c",
    "title": "Groundedness in Retrieval-augmented Long-form Generation: An Empirical Study",
    "abstract": "We present an empirical study of groundedness in long-form question answering (LFQA) by retrieval-augmented large language models (LLMs). In particular, we evaluate whether every generated sentence is grounded in the retrieved documents or the model's pre-training data. Across 3 datasets and 4 model families, our findings reveal that a significant fraction of generated sentences are consistently ungrounded, even when those sentences contain correct ground-truth answers. Additionally, we examine the impacts of factors such as model size, decoding strategy, and instruction tuning on groundedness. Our results show that while larger models tend to ground their outputs more effectively, a significant portion of correct answers remains compromised by hallucinations. This study provides novel insights into the groundedness challenges in LFQA and underscores the necessity for more robust mechanisms in LLMs to mitigate the generation of ungrounded content.",
    "venue": "NAACL-HLT",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-10",
    "authors": [
      {
        "authorId": "2175480389",
        "name": "Alessandro Stolfo"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "4341e28b9778d9eb77de2f423a60c952e34aa3ce",
    "url": "https://www.semanticscholar.org/paper/4341e28b9778d9eb77de2f423a60c952e34aa3ce",
    "title": "Retrieval-Augmented Modular Prompt Tuning for Low-Resource Data-to-Text Generation",
    "abstract": "Data-to-text (D2T) generation describes the task of verbalizing data, often given as attribute-value pairs. While this task is relevant for many different data domains beyond the traditionally well-explored tasks of weather forecasting, restaurant recommendations, and sports reporting, a major challenge to the applicability of data-to-text generation methods is typically data sparsity. For many applications, there is extremely little training data in terms of attribute-value inputs and target language outputs available for training a model. Given the sparse data setting, recently developed prompting methods seem most suitable for addressing D2T tasks since they do not require substantial amounts of training data, unlike finetuning approaches. However, prompt-based approaches are also challenging, as a) the design and search of prompts are non-trivial; and b) hallucination problems may occur because of the strong inductive bias of these models. In this paper, we propose a retrieval-augmented modular prompt tuning () method, which constructs prompts that fit the input data closely, thereby bridging the domain gap between the large-scale language model and the structured input data. Experiments show that our method generates texts with few hallucinations and achieves state-of-the-art performance on a dataset for drone handover message generation.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2301584522",
        "name": "Ruitao Feng"
      },
      {
        "authorId": "2615648",
        "name": "Xudong Hong"
      },
      {
        "authorId": "46183473",
        "name": "Mayank Jobanputra"
      },
      {
        "authorId": "2301582796",
        "name": "Mattes Warning"
      },
      {
        "authorId": "2253593393",
        "name": "Vera Demberg"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "b9632d1acbcb78926fb2c449a267bdd2645161b3",
    "url": "https://www.semanticscholar.org/paper/b9632d1acbcb78926fb2c449a267bdd2645161b3",
    "title": "RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic Program Repair",
    "abstract": "Automatic program repair (APR) is crucial to reduce manual debugging efforts for developers and improve software reliability. While conventional search-based techniques typically rely on heuristic rules or a redundancy assumption to mine fix patterns, recent years have witnessed the surge of deep learning (DL) based approaches to automate the program repair process in a data-driven manner. However, their performance is often limited by a fixed set of parameters to model the highly complex search space of APR. To ease such burden on the parametric models, in this work, we propose a novel Retrieval-Augmented Patch Generation framework (RAP-Gen) by explicitly leveraging relevant fix patterns retrieved from a codebase of previous bug-fix pairs. Specifically, we build a hybrid patch retriever to account for both lexical and semantic matching based on the raw source code in a language-agnostic manner, which does not rely on any code-specific features. In addition, we adapt a code-aware language model CodeT5 as our foundation model to facilitate both patch retrieval and generation tasks in a unified manner. We adopt a stage-wise approach where the patch retriever first retrieves a relevant external bug-fix pair to augment the buggy input for the CodeT5 patch generator, which synthesizes a ranked list of repair patch candidates. Notably, RAP-Gen is a generic APR framework that can flexibly integrate different patch retrievers and generators to repair various types of bugs. We thoroughly evaluate RAP-Gen on three benchmarks in two programming languages, including the TFix benchmark in JavaScript, and Code Refinement and Defects4J benchmarks in Java, where the bug localization information may or may not be provided. Experimental results show that RAP-Gen significantly outperforms previous state-of-the-art (SoTA) approaches on all benchmarks, e.g., boosting the accuracy of T5-large on TFix from 49.70% to 54.15% (repairing 478 more bugs) and repairing 15 more bugs on 818 Defects4J bugs. Further analysis reveals that our patch retriever can search for relevant fix patterns to guide the APR systems.",
    "venue": "ESEC/SIGSOFT FSE",
    "year": 2023,
    "citationCount": 41,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.06057",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2023-09-12",
    "authors": [
      {
        "authorId": "2108528154",
        "name": "Weishi Wang"
      },
      {
        "authorId": "49416727",
        "name": "Yue Wang"
      },
      {
        "authorId": "2708940",
        "name": "Shafiq R. Joty"
      },
      {
        "authorId": "2184854289",
        "name": "Steven C. H. Hoi"
      }
    ],
    "source": "semantic_scholar",
    "score": 126.06504427425052
  },
  {
    "paperId": "78ea1cb2de8960fe0aa1913dfb5b22b148ab2bef",
    "url": "https://www.semanticscholar.org/paper/78ea1cb2de8960fe0aa1913dfb5b22b148ab2bef",
    "title": "GarmentAligner: Text-to-Garment Generation via Retrieval-augmented Multi-level Corrections",
    "abstract": "General text-to-image models bring revolutionary innovation to the fields of arts, design, and media. However, when applied to garment generation, even the state-of-the-art text-to-image models suffer from fine-grained semantic misalignment, particularly concerning the quantity, position, and interrelations of garment components. Addressing this, we propose GarmentAligner, a text-to-garment diffusion model trained with retrieval-augmented multi-level corrections. To achieve semantic alignment at the component level, we introduce an automatic component extraction pipeline to obtain spatial and quantitative information of garment components from corresponding images and captions. Subsequently, to exploit component relationships within the garment images, we construct retrieval subsets for each garment by retrieval augmentation based on component-level similarity ranking and conduct contrastive learning to enhance the model perception of components from positive and negative samples. To further enhance the alignment of components across semantic, spatial, and quantitative granularities, we propose the utilization of multi-level correction losses that leverage detailed component information. The experimental findings demonstrate that GarmentAligner achieves superior fidelity and fine-grained semantic alignment when compared to existing competitors.",
    "venue": "European Conference on Computer Vision",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-08-22",
    "authors": [
      {
        "authorId": "2220617619",
        "name": "Shiyue Zhang"
      },
      {
        "authorId": "2312407610",
        "name": "Zheng Chong"
      },
      {
        "authorId": "2107974344",
        "name": "Xujie Zhang"
      },
      {
        "authorId": "2276604489",
        "name": "Hanhui Li"
      },
      {
        "authorId": "2298644153",
        "name": "Yuhao Cheng"
      },
      {
        "authorId": "144880586",
        "name": "Yiqiang Yan"
      },
      {
        "authorId": "2291389227",
        "name": "Xiaodan Liang"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "18d8adbe20aa337fbc8bf1459d2389c7b3487169",
    "url": "https://www.semanticscholar.org/paper/18d8adbe20aa337fbc8bf1459d2389c7b3487169",
    "title": "DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation",
    "abstract": "Storytelling video generation (SVG) has recently emerged as a task to create long, multi-motion, multi-scene videos that consistently represent the story described in the input text script. SVG holds great potential for diverse content creation in media and entertainment; however, it also presents significant challenges: (1) objects must exhibit a range of fine-grained, complex motions, (2) multiple objects need to appear consistently across scenes, and (3) subjects may require multiple motions with seamless transitions within a single scene. To address these challenges, we propose DreamRunner, a novel story-to-video generation method: First, we structure the input script using a large language model (LLM) to facilitate both coarse-grained scene planning as well as fine-grained object-level layout and motion planning. Next, DreamRunner presents retrieval-augmented test-time adaptation to capture target motion priors for objects in each scene, supporting diverse motion customization based on retrieved videos, thus facilitating the generation of new videos with complex, scripted motions. Lastly, we propose a novel spatial-temporal region-based 3D attention and prior injection module SR3AI for fine-grained object-motion binding and frame-by-frame semantic control. We compare DreamRunner with various SVG baselines, demonstrating state-of-the-art performance in character consistency, text alignment, and smooth transitions. Additionally, DreamRunner exhibits strong fine-grained condition-following ability in compositional text-to-video generation, significantly outperforming baselines on T2V-ComBench. Finally, we validate DreamRunner's robust ability to generate multi-object interactions with qualitative examples.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-11-25",
    "authors": [
      {
        "authorId": "2310518904",
        "name": "Zun Wang"
      },
      {
        "authorId": "2108961694",
        "name": "Jialu Li"
      },
      {
        "authorId": "2260201860",
        "name": "Han Lin"
      },
      {
        "authorId": "13563486",
        "name": "Jaehong Yoon"
      },
      {
        "authorId": "2276608813",
        "name": "Mohit Bansal"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "305bbb460af5f27a279c85d7f57231fd1871f64f",
    "url": "https://www.semanticscholar.org/paper/305bbb460af5f27a279c85d7f57231fd1871f64f",
    "title": "Retrieval-Augmented Code Generation for Universal Information Extraction",
    "abstract": "Information Extraction (IE) aims to extract structural knowledge (e.g., entities, relations, events) from natural language texts, which brings challenges to existing methods due to task-specific schemas and complex text expressions. Code, as a typical kind of formalized language, is capable of describing structural knowledge under various schemas in a universal way. On the other hand, Large Language Models (LLMs) trained on both codes and texts have demonstrated powerful capabilities of transforming texts into codes, which provides a feasible solution to IE tasks. Therefore, in this paper, we propose a universal retrieval-augmented code generation framework based on LLMs, called Code4UIE, for IE tasks. Specifically, Code4UIE adopts Python classes to define task-specific schemas of various structural knowledge in a universal way. By so doing, extracting knowledge under these schemas can be transformed into generating codes that instantiate the predefined Python classes with the information in texts. To generate these codes more precisely, Code4UIE adopts the in-context learning mechanism to instruct LLMs with examples. In order to obtain appropriate examples for different tasks, Code4UIE explores several example retrieval strategies, which can retrieve examples semantically similar to the given texts. Extensive experiments on five representative IE tasks across nine datasets demonstrate the effectiveness of the Code4UIE framework.",
    "venue": "Natural Language Processing and Chinese Computing",
    "year": 2023,
    "citationCount": 24,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-06",
    "authors": [
      {
        "authorId": "2265608871",
        "name": "Yucan Guo"
      },
      {
        "authorId": "46947005",
        "name": "Zixuan Li"
      },
      {
        "authorId": "2149111400",
        "name": "Xiaolong Jin"
      },
      {
        "authorId": "2244697902",
        "name": "Yantao Liu"
      },
      {
        "authorId": "122290781",
        "name": "Yutao Zeng"
      },
      {
        "authorId": "2265516393",
        "name": "Wenxuan Liu"
      },
      {
        "authorId": "2266006660",
        "name": "Xiang Li"
      },
      {
        "authorId": "2265533684",
        "name": "Pan Yang"
      },
      {
        "authorId": "2075398318",
        "name": "Long Bai"
      },
      {
        "authorId": "1777025",
        "name": "J. Guo"
      },
      {
        "authorId": "2244825947",
        "name": "Xueqi Cheng"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.28313737302301
  },
  {
    "paperId": "e6770e3f5e74210c6863aaeed527ac4c1da419d7",
    "url": "https://www.semanticscholar.org/paper/e6770e3f5e74210c6863aaeed527ac4c1da419d7",
    "title": "A Survey on Retrieval-Augmented Text Generation",
    "abstract": "Recently, retrieval-augmented text generation attracted increasing attention of the computational linguistics community. Compared with conventional generation models, retrieval-augmented text generation has remarkable advantages and particularly has achieved state-of-the-art performance in many NLP tasks. This paper aims to conduct a survey about retrieval-augmented text generation. It firstly highlights the generic paradigm of retrieval-augmented generation, and then it reviews notable approaches according to different tasks including dialogue response generation, machine translation, and other generation tasks. Finally, it points out some important directions on top of recent methods to facilitate future research.",
    "venue": "arXiv.org",
    "year": 2022,
    "citationCount": 153,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2022-02-02",
    "authors": [
      {
        "authorId": "91956362",
        "name": "Huayang Li"
      },
      {
        "authorId": "50087162",
        "name": "Yixuan Su"
      },
      {
        "authorId": "2053327987",
        "name": "Deng Cai"
      },
      {
        "authorId": "2152546690",
        "name": "Yan Wang"
      },
      {
        "authorId": "2978364",
        "name": "Lemao Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.55428903620444
  },
  {
    "paperId": "f659db03dcec59885ff52bcd193d8e2aa9228895",
    "url": "https://www.semanticscholar.org/paper/f659db03dcec59885ff52bcd193d8e2aa9228895",
    "title": "Retrieval-Augmented Feature Generation for Domain-Specific Classification",
    "abstract": "Feature generation can significantly enhance learning outcomes, particularly for tasks with limited data. An effective way to improve feature generation is by expanding the current feature space using existing features and enriching the informational content. However, generating new, interpretable features in application fields often requires domain-specific knowledge about the existing features. This paper introduces a new method RAFG for generating reasonable and explainable features specific to domain classification tasks. To generate new features with interpretability in domain knowledge, we perform information retrieval on existing features to identify potential feature associations, and utilize these associations to generate meaningful features. Furthermore, we develop a Large Language Model (LLM)-based framework for feature generation with reasoning to verify and filter features during the generation process. Experiments across several datasets in medical, economic, and geographic domains show that our RAFG method produces high-quality, meaningful features and significantly improves classification performance compared with baseline methods.",
    "venue": "",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-06-17",
    "authors": [
      {
        "authorId": "2130031226",
        "name": "XinHao Zhang"
      },
      {
        "authorId": "2304897819",
        "name": "Jinghan Zhang"
      },
      {
        "authorId": "2307000104",
        "name": "Fengran Mo"
      },
      {
        "authorId": "2307032534",
        "name": "Yuzhong Chen"
      },
      {
        "authorId": "2293571072",
        "name": "Kunpeng Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "77040969110fab39a55699cb06f9edf68789445a",
    "url": "https://www.semanticscholar.org/paper/77040969110fab39a55699cb06f9edf68789445a",
    "title": "Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation",
    "abstract": "Generating videos for visual storytelling can be a tedious and complex process that typically requires either live-action filming or graphics animation rendering. To bypass these challenges, our key idea is to utilize the abundance of existing video clips and synthesize a coherent storytelling video by customizing their appearances. We achieve this by developing a framework comprised of two functional modules: (i) Motion Structure Retrieval, which provides video candidates with desired scene or motion context described by query texts, and (ii) Structure-Guided Text-to-Video Synthesis, which generates plot-aligned videos under the guidance of motion structure and text prompts. For the first module, we leverage an off-the-shelf video retrieval system and extract video depths as motion structure. For the second module, we propose a controllable video generation model that offers flexible controls over structure and characters. The videos are synthesized by following the structural guidance and appearance instruction. To ensure visual consistency across clips, we propose an effective concept personalization approach, which allows the specification of the desired character identities through text prompts. Extensive experiments demonstrate that our approach exhibits significant advantages over various existing baselines.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 46,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.06940",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-13",
    "authors": [
      {
        "authorId": "2154314501",
        "name": "Yin-Yin He"
      },
      {
        "authorId": "2904307",
        "name": "Menghan Xia"
      },
      {
        "authorId": "2149052351",
        "name": "Haoxin Chen"
      },
      {
        "authorId": "30176430",
        "name": "Xiaodong Cun"
      },
      {
        "authorId": "2114983337",
        "name": "Yuan Gong"
      },
      {
        "authorId": "2087273800",
        "name": "Jinbo Xing"
      },
      {
        "authorId": "33603226",
        "name": "Yong Zhang"
      },
      {
        "authorId": "47119707",
        "name": "Xintao Wang"
      },
      {
        "authorId": "2211001131",
        "name": "Chao-Liang Weng"
      },
      {
        "authorId": "2187307826",
        "name": "Ying Shan"
      },
      {
        "authorId": "2257095769",
        "name": "Qifeng Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 127.75221402565089
  },
  {
    "paperId": "9f9b96e48670477640c12930fca69fee8afcfaff",
    "url": "https://www.semanticscholar.org/paper/9f9b96e48670477640c12930fca69fee8afcfaff",
    "title": "Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT models",
    "abstract": "We propose Retrieval Augmented Generation (RAG) as an approach for automated radiology report writing that leverages multimodally aligned embeddings from a contrastively pretrained vision language model for retrieval of relevant candidate radiology text for an input radiology image and a general domain generative model like OpenAI text-davinci-003, gpt-3.5-turbo and gpt-4 for report generation using the relevant radiology text retrieved. This approach keeps hallucinated generations under check and provides capabilities to generate report content in the format we desire leveraging the instruction following capabilities of these generative models. Our approach achieves better clinical metrics with a BERTScore of 0.2865 ({\\Delta}+ 25.88%) and Semb score of 0.4026 ({\\Delta}+ 6.31%). Our approach can be broadly relevant for different clinical settings as it allows to augment the automated radiology report generation process with content relevant for that setting while also having the ability to inject user intents and requirements in the prompts as part of the report generation process to modulate the content and format of the generated reports as applicable for that clinical setting.",
    "venue": "Machine Learning in Health Care",
    "year": 2023,
    "citationCount": 19,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.03660",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-05",
    "authors": [
      {
        "authorId": "1381199788",
        "name": "M. Ranjit"
      },
      {
        "authorId": "1768952",
        "name": "G. Ganapathy"
      },
      {
        "authorId": "1661031443",
        "name": "R. Manuel"
      },
      {
        "authorId": "1785978",
        "name": "T. Ganu"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.93598410330986
  },
  {
    "paperId": "4976f7d7f97a83ab255a57d266c19cf094f2652a",
    "url": "https://www.semanticscholar.org/paper/4976f7d7f97a83ab255a57d266c19cf094f2652a",
    "title": "Interaction-based Retrieval-augmented Diffusion Models for Protein-specific 3D Molecule Generation",
    "abstract": null,
    "venue": "International Conference on Machine Learning",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2277790317",
        "name": "Zhilin Huang"
      },
      {
        "authorId": "2289007265",
        "name": "Ling Yang"
      },
      {
        "authorId": "2315255726",
        "name": "Xiangxin Zhou"
      },
      {
        "authorId": "2047319007",
        "name": "C. Qin"
      },
      {
        "authorId": "2299258293",
        "name": "Yijie Yu"
      },
      {
        "authorId": "2288182667",
        "name": "Xiawu Zheng"
      },
      {
        "authorId": "2297830139",
        "name": "Zikun Zhou"
      },
      {
        "authorId": "2277807793",
        "name": "Wentao Zhang"
      },
      {
        "authorId": "2288406848",
        "name": "Yu Wang"
      },
      {
        "authorId": "2289113963",
        "name": "Wenming Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 70.79441541679836
  },
  {
    "paperId": "e2b57df28c8edf25bf8adb86ca5a9e7fe61eea0b",
    "url": "https://www.semanticscholar.org/paper/e2b57df28c8edf25bf8adb86ca5a9e7fe61eea0b",
    "title": "Retrieval-Augmented Code Generation for Situated Action Generation: A Case Study on Minecraft",
    "abstract": "In the Minecraft Collaborative Building Task, two players collaborate: an Architect (A) provides instructions to a Builder (B) to assemble a specified structure using 3D blocks. In this work, we investigate the use of large language models (LLMs) to predict the sequence of actions taken by the Builder. Leveraging LLMs' in-context learning abilities, we use few-shot prompting techniques, that significantly improve performance over baseline methods. Additionally, we present a detailed analysis of the gaps in performance for future work",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-25",
    "authors": [
      {
        "authorId": "2046531185",
        "name": "Kranti Chalamalasetti"
      },
      {
        "authorId": "2079305",
        "name": "Sherzod Hakimov"
      },
      {
        "authorId": "2257274000",
        "name": "David Schlangen"
      }
    ],
    "source": "semantic_scholar",
    "score": 75.39720770839918
  },
  {
    "paperId": "df19bd0bb788474a578fac33d6cff9867af3eead",
    "url": "https://www.semanticscholar.org/paper/df19bd0bb788474a578fac33d6cff9867af3eead",
    "title": "FABULA: Intelligence Report Generation Using Retrieval-Augmented Narrative Construction",
    "abstract": "Narrative construction is the process of representing disparate event information into a logical plot structure that models an end to end story. Intelligence analysis is an example of a domain that can benefit tremendously from narrative construction techniques, particularly in aiding analysts during the largely manual and costly process of synthesizing event information into comprehensive intelligence reports. Manual intelligence report generation is often prone to challenges such as integrating dynamic event information, writing fine-grained queries, and closing information gaps. This motivates the development of a system that retrieves and represents critical aspects of events in a form that aids in automatic generation of intelligence reports. We introduce a Retrieval Augmented Generation (RAG) approach to augment prompting of an autoregressive decoder by retrieving structured information asserted in a knowledge graph to generate targeted information based on a narrative plot model. We apply our approach to the problem of neural intelligence report generation and introduce FABULA, framework to augment intelligence analysis workflows using RAG. An analyst can use FABULA to query an Event Plot Graph (EPG) to retrieve relevant event plot points, which can be used to augment prompting of a Large Language Model (LLM) during intelligence report generation. Our evaluation studies show that the plot points included in the generated intelligence reports have high semantic relevance, high coherency, and low data redundancy.",
    "venue": "International Conference on Advances in Social Networks Analysis and Mining",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3625007.3627505",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2023-10-20",
    "authors": [
      {
        "authorId": "51092885",
        "name": "P. Ranade"
      },
      {
        "authorId": "2269936199",
        "name": "Anupam Joshi"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "db72305f709bb73e8d641ef2d71bca7c7fc950fc",
    "url": "https://www.semanticscholar.org/paper/db72305f709bb73e8d641ef2d71bca7c7fc950fc",
    "title": "Retrieval-Augmented Text-to-Audio Generation",
    "abstract": "Despite recent progress in text-to-audio (TTA) generation, we show that the state-of-the-art models, such as AudioLDM, trained on datasets with an imbalanced class distribution, such as AudioCaps, are biased in their generation performance. Specifically, they excel in generating common audio classes while underperforming in the rare ones, thus degrading the overall generation performance. We refer to this problem as long-tailed text-to-audio generation. To address this issue, we propose a simple retrieval-augmented approach for TTA models. Specifically, given an input text prompt, we first leverage a Contrastive Language Audio Pretraining (CLAP) model to retrieve relevant text-audio pairs. The features of the retrieved audio-text data are then used as additional conditions to guide the learning of TTA models. We enhance AudioLDM with our proposed approach and denote the resulting augmented system as Re-AudioLDM. On the AudioCaps dataset, Re-AudioLDM achieves a state-of-the-art Frechet Audio Distance (FAD) of 1.37, outperforming the existing approaches by a large margin. Furthermore, we show that Re-AudioLDM can generate realistic audio for complex scenes, rare audio classes, and even unseen audio types, indicating its potential in TTA tasks.",
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year": 2023,
    "citationCount": 20,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.08051",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-09-14",
    "authors": [
      {
        "authorId": "152338671",
        "name": "Yiitan Yuan"
      },
      {
        "authorId": "151503689",
        "name": "Haohe Liu"
      },
      {
        "authorId": "2110814131",
        "name": "Xubo Liu"
      },
      {
        "authorId": "2110992465",
        "name": "Qiushi Huang"
      },
      {
        "authorId": "1804703",
        "name": "Mark D. Plumbley"
      },
      {
        "authorId": "2239051433",
        "name": "Wenwu Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.66783656585135
  },
  {
    "paperId": "ff9545fb640071eadbfb40111318d7bc9c083e07",
    "url": "https://www.semanticscholar.org/paper/ff9545fb640071eadbfb40111318d7bc9c083e07",
    "title": "AI Can Look Up StackOverflow too: Retrieval-Augmented Code Generation",
    "abstract": "This research paper investigates the effectiveness of retrieval-augmented generation for generating code from natural language queries for specific domains, without the need for fine-tuning a model on a domain-specific corpus. Code generation from natural language is a challenging task with numerous applications in software engineering and data science. Retraining or fine-tuning Large Language Models (LLMs) is an expensive undertaking, and LLMs have been shown to often effectively use in-context examples to arrive at the correct answer. The paper specifically explores using retrieval from a bimodal corpus with snippets consisting of description and code, to enhance prompting for code generation for data science problems in the Python programming language. Retrieval is performed against a corpus of StackOverflow posts using a weighted ensemble of 2 models, one for ranking query-code similarity and the other for query-description similarity. We compare our approach to 3 baselines using 2 widely used metrics on the DS-1000 data set. Experimental results show that (1) Our method is able to generate more correct programs, improving the pass @5 score by up to 10% compared to the baseline(sans retrieval) score of generative models such as codex-davinci-002 from OpenAI, achieving the new state-of-the-art. (2) Our method enhances robustness to semantic perturbation of the query text.",
    "venue": "",
    "year": 2023,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2220695012",
        "name": "Shreyas Vinayakumar"
      },
      {
        "authorId": "2220692985",
        "name": "Swagata Ashwani"
      },
      {
        "authorId": "2286582747",
        "name": "Minh-Tue Vo-Thanh"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "c56aced0f0c5cfebefadb530cb08d736c3ac5c05",
    "url": "https://www.semanticscholar.org/paper/c56aced0f0c5cfebefadb530cb08d736c3ac5c05",
    "title": "Retrieval Augmented Code Generation and Summarization",
    "abstract": "Software developers write a lot of source code and documentation during software development. Intrinsically, developers often recall parts of source code or code summaries that they had written in the past while implementing software or documenting them. To mimic developers' code or summary generation behavior, we propose a retrieval augmented framework, REDCODER, that retrieves relevant code or summaries from a retrieval database and provides them as a supplement to code generation or summarization models. REDCODER has a couple of uniqueness. First, it extends the state-of-the-art dense retrieval technique to search for relevant code or summaries. Second, it can work with retrieval databases that include unimodal (only code or natural language description) or bimodal instances (code-description pairs). We conduct experiments and extensive analysis on two benchmark datasets of code generation and summarization in Java and Python, and the promising results endorse the effectiveness of our proposed retrieval augmented framework.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "citationCount": 143,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2021.findings-emnlp.232.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-08-26",
    "authors": [
      {
        "authorId": "3405393",
        "name": "Md. Rizwan Parvez"
      },
      {
        "authorId": "2125973767",
        "name": "W. Ahmad"
      },
      {
        "authorId": "47570053",
        "name": "Saikat Chakraborty"
      },
      {
        "authorId": "31631000",
        "name": "Baishakhi Ray"
      },
      {
        "authorId": "2782886",
        "name": "Kai-Wei Chang"
      }
    ],
    "source": "semantic_scholar",
    "score": 144.54719949364
  },
  {
    "paperId": "fc74fe92fb9e34d3ae1237bf9a6e718c723f4f3e",
    "url": "https://www.semanticscholar.org/paper/fc74fe92fb9e34d3ae1237bf9a6e718c723f4f3e",
    "title": "FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation",
    "abstract": "Retrieval-augmented generation models offer many benefits over standalone language models: besides a textual answer to a given query they provide provenance items retrieved from an updateable knowledge base. However, they are also more complex systems and need to handle long inputs. In this work, we introduce FiD-Light to strongly increase the efficiency of the state-of-the-art retrieval-augmented FiD model, while maintaining the same level of effectiveness. Our FiD-Light model constrains the information flow from the encoder (which encodes passages separately) to the decoder (using concatenated encoded representations). Furthermore, we adapt FiD-Light with re-ranking capabilities through textual source pointers, to improve the top-ranked provenance precision. Our experiments on a diverse set of seven knowledge intensive tasks (KILT) show FiD-Light consistently improves the Pareto frontier between query latency and effectiveness. FiD-Light with source pointing sets substantial new state-of-the-art results on six KILT tasks for combined text generation and provenance retrieval evaluation, while maintaining high efficiency.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2022,
    "citationCount": 65,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2209.14290",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2022-09-28",
    "authors": [
      {
        "authorId": "97393346",
        "name": "Sebastian Hofst√§tter"
      },
      {
        "authorId": "2809410",
        "name": "Jiecao Chen"
      },
      {
        "authorId": "2062947723",
        "name": "K. Raman"
      },
      {
        "authorId": "2499986",
        "name": "Hamed Zamani"
      }
    ],
    "source": "semantic_scholar",
    "score": 138.84482113039638
  },
  {
    "paperId": "5585cb971a55f12ca6f6be4231d4db0c5c84c97e",
    "url": "https://www.semanticscholar.org/paper/5585cb971a55f12ca6f6be4231d4db0c5c84c97e",
    "title": "ReZG: Retrieval-Augmented Zero-Shot Counter Narrative Generation for Hate Speech",
    "abstract": "The proliferation of hate speech (HS) on social media poses a serious threat to societal security. Automatic counter narrative (CN) generation, as an active strategy for HS intervention, has garnered increasing attention in recent years. Existing methods for automatically generating CNs mainly rely on re-training or fine-tuning pre-trained language models (PLMs) on human-curated CN corpora. Unfortunately, the annotation speed of CN corpora cannot keep up with the growth of HS targets, while generating specific and effective CNs for unseen targets remains a significant challenge for the model. To tackle this issue, we propose Retrieval-Augmented Zero-shot Generation (ReZG) to generate CNs with high-specificity for unseen targets. Specifically, we propose a multi-dimensional hierarchical retrieval method that integrates stance, semantics, and fitness, extending the retrieval metric from single dimension to multiple dimensions suitable for the knowledge that refutes HS. Then, we implement an energy-based constrained decoding mechanism that enables PLMs to use differentiable knowledge preservation, countering, and fluency constraint functions instead of in-target CNs as control signals for generation, thereby achieving zero-shot CN generation. With the above techniques, ReZG can integrate external knowledge flexibly and improve the specificity of CNs. Experimental results show that ReZG exhibits stronger generalization capabilities and outperforms strong baselines with significant improvements of 2.0%+ in the relevance and 4.5%+ in the countering success rate metrics.",
    "venue": "",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-10-09",
    "authors": [
      {
        "authorId": "7027584",
        "name": "Shuyu Jiang"
      },
      {
        "authorId": "2257383244",
        "name": "Wenyi Tang"
      },
      {
        "authorId": "2257126773",
        "name": "Xingshu Chen"
      },
      {
        "authorId": "2258720735",
        "name": "Rui Tang"
      },
      {
        "authorId": "48016101",
        "name": "Haizhou Wang"
      },
      {
        "authorId": "2108595924",
        "name": "Wenxian Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "ed4c4ca57c805d215a955377817ffb51242c044f",
    "url": "https://www.semanticscholar.org/paper/ed4c4ca57c805d215a955377817ffb51242c044f",
    "title": "From Classification to Generation: Insights into Crosslingual Retrieval Augmented ICL",
    "abstract": "The remarkable ability of Large Language Models (LLMs) to understand and follow instructions has sometimes been limited by their in-context learning (ICL) performance in low-resource languages. To address this, we introduce a novel approach that leverages cross-lingual retrieval-augmented in-context learning (CREA-ICL). By extracting semantically similar prompts from high-resource languages, we aim to improve the zero-shot performance of multilingual pre-trained language models (MPLMs) across diverse tasks. Though our approach yields steady improvements in classification tasks, it faces challenges in generation tasks. Our evaluation offers insights into the performance dynamics of retrieval-augmented in-context learning across both classification and generation domains.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-11",
    "authors": [
      {
        "authorId": "2264327809",
        "name": "Xiaoqian Li"
      },
      {
        "authorId": "2197254657",
        "name": "Ercong Nie"
      },
      {
        "authorId": "2114787903",
        "name": "Sheng Liang"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.19162312519754
  },
  {
    "paperId": "e17109b4ca2688753948c02e37912834b5c76fad",
    "url": "https://www.semanticscholar.org/paper/e17109b4ca2688753948c02e37912834b5c76fad",
    "title": "Diversify Question Generation with Retrieval-Augmented Style Transfer",
    "abstract": "Given a textual passage and an answer, humans are able to ask questions with various expressions, but this ability is still challenging for most question generation (QG) systems. Existing solutions mainly focus on the internal knowledge within the given passage or the semantic word space for diverse content planning. These methods, however, have not considered the potential of external knowledge for expression diversity. To bridge this gap, we propose RAST, a framework for Retrieval-Augmented Style Transfer, where the objective is to utilize the style of diverse templates for question generation. For training RAST, we develop a novel Reinforcement Learning (RL) based approach that maximizes a weighted combination of diversity reward and consistency reward. Here, the consistency reward is computed by a Question-Answering (QA) model, whereas the diversity reward measures how much the final output mimics the retrieved template. Experimental results show that our method outperforms previous diversity-driven baselines on diversity while being comparable in terms of consistency scores. Our code is available at https://github.com/gouqi666/RAST.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-23",
    "authors": [
      {
        "authorId": "2261086202",
        "name": "Qi Gou"
      },
      {
        "authorId": "2072880957",
        "name": "Zehua Xia"
      },
      {
        "authorId": "2249451832",
        "name": "Bowen Yu"
      },
      {
        "authorId": "46493167",
        "name": "Haiyang Yu"
      },
      {
        "authorId": "2257407873",
        "name": "Fei Huang"
      },
      {
        "authorId": "2253881638",
        "name": "Yongbin Li"
      },
      {
        "authorId": "2261285247",
        "name": "Nguyen Cam-Tu"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "2c4f456b167cb55331078b1262f470a6af5e9045",
    "url": "https://www.semanticscholar.org/paper/2c4f456b167cb55331078b1262f470a6af5e9045",
    "title": "Syntax-Aware Retrieval Augmented Code Generation",
    "abstract": "Neural code generation models are nowadays widely adopted to generate code from natural language descriptions automatically. Recently, pre-trained neural models equipped with token-level retrieval capabilities have exhibited great potentials in neural machine translation. However, applying them directly to code generation experience challenges: the use of the retrieval-based mechanism inevitably introduces extra-neous noise to the generation process, resulting in even syntactically incorrect code. Com-putationally, such models necessitate frequent searches of the cached datastore, which turns out to be time-consuming. To address these issues, we propose k NN-TRANX, a token-level retrieval augmented code generation method. k NN-TRANX allows for searches in smaller datastores tailored for the code generation task. It leverages syntax constraints for the retrieval of datastores, which reduces the impact of retrieve noise. We evaluate k NN-TRANX on two public datasets and the experimental re-sults confirm the effectiveness of our approach.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.findings-emnlp.90.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2262845618",
        "name": "Xiangyu Zhang"
      },
      {
        "authorId": "144452434",
        "name": "Yu Zhou"
      },
      {
        "authorId": "2273904837",
        "name": "Guang Yang"
      },
      {
        "authorId": "2273533801",
        "name": "Taolue Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "cd6be2cc157b9cae0f0c8f492da2f539111db399",
    "url": "https://www.semanticscholar.org/paper/cd6be2cc157b9cae0f0c8f492da2f539111db399",
    "title": "Retrieval-Augmented Layout Transformer for Content-Aware Layout Generation",
    "abstract": "Content-aware graphic layout generation aims to automatically arrange visual elements along with a given content, such as an e-commerce product image. In this paper, we argue that the current layout generation approaches suffer from the limited training data for the high-dimensional layout structure. We show that a simple retrieval augmentation can significantly improve the generation quality. Our model, which is named Retrieval-Augmented Layout Transformer (RALF),retrieves nearest neighbor layout examples based on an input image and feeds these results into an autoregressive generator. Our model can apply retrieval augmentation to various controllable generation tasks and yield high-quality layouts within a unified architecture. Our extensive experiments show that RALF successfully generates content-aware layouts in both constrained and unconstrained settings and significantly outperforms the baselines. 1",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2023,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2311.13602",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-22",
    "authors": [
      {
        "authorId": "35687142",
        "name": "Daichi Horita"
      },
      {
        "authorId": "47036547",
        "name": "Naoto Inoue"
      },
      {
        "authorId": "2053130054",
        "name": "Kotaro Kikuchi"
      },
      {
        "authorId": "2292144660",
        "name": "Kota Yamaguchi"
      },
      {
        "authorId": "2237794190",
        "name": "Kiyoharu Aizawa"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "56e586035249e247959891422d36f727a6aad661",
    "url": "https://www.semanticscholar.org/paper/56e586035249e247959891422d36f727a6aad661",
    "title": "Retrieval-Augmented Neural Response Generation Using Logical Reasoning and Relevance Scoring",
    "abstract": "Constructing responses in task-oriented dialogue systems typically relies on information sources such the current dialogue state or external databases. This paper presents a novel approach to knowledge-grounded response generation that combines retrieval-augmented language models with logical reasoning. The approach revolves around a knowledge graph representing the current dialogue state and background information, and proceeds in three steps. The knowledge graph is first enriched with logically derived facts inferred using probabilistic logical programming. A neural model is then employed at each turn to score the conversational relevance of each node and edge of this extended graph. Finally, the elements with highest relevance scores are converted to a natural language form, and are integrated into the prompt for the neural conversational model employed to generate the system response. We investigate the benefits of the proposed approach on two datasets (KVRET and GraphWOZ) along with a human evaluation. Experimental results show that the combination of (probabilistic) logical reasoning with conversational relevance scoring does increase both the factuality and fluency of the responses.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-20",
    "authors": [
      {
        "authorId": "2190503824",
        "name": "N. Walker"
      },
      {
        "authorId": "2042754248",
        "name": "Stefan Ultes"
      },
      {
        "authorId": "2261081553",
        "name": "Pierre Lison"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "dd7a2613ee15cd10938e7ad40f3aa052e98fda3c",
    "url": "https://www.semanticscholar.org/paper/dd7a2613ee15cd10938e7ad40f3aa052e98fda3c",
    "title": "GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence",
    "abstract": "Conditional story generation is significant in human-machine interaction, particularly in producing stories with complex plots. While Large language models (LLMs) perform well on multiple NLP tasks, including story generation, it is challenging to generate stories with both complex and creative plots. Existing methods often rely on detailed prompts to guide LLMs to meet target conditions, which inadvertently restrict the creative potential of the generated stories. We argue that leveraging information from exemplary human-written stories facilitates generating more diverse plotlines. Delving deeper into story details helps build complex and credible plots. In this paper, we propose a retrieval-au\\textbf{G}mented sto\\textbf{R}y generation framework with a f\\textbf{O}rest of e\\textbf{V}id\\textbf{E}nce (GROVE) to enhance stories' complexity. We build a retrieval repository for target conditions to produce few-shot examples to prompt LLMs. Additionally, we design an ``asking-why'' prompting scheme that extracts a forest of evidence, providing compensation for the ambiguities that may occur in the generated story. This iterative process uncovers underlying story backgrounds. Finally, we select the most fitting chains of evidence from the evidence forest and integrate them into the generated story, thereby enhancing the narrative's complexity and credibility. Experimental results and numerous examples verify the effectiveness of our method.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 9,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.05388",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-09",
    "authors": [
      {
        "authorId": "2256993729",
        "name": "Zhihua Wen"
      },
      {
        "authorId": "2257366341",
        "name": "Zhiliang Tian"
      },
      {
        "authorId": "2257335291",
        "name": "Wei Wu"
      },
      {
        "authorId": "2257088560",
        "name": "Yuxin Yang"
      },
      {
        "authorId": "2218437825",
        "name": "Yanqi Shi"
      },
      {
        "authorId": "2257117406",
        "name": "Zhen Huang"
      },
      {
        "authorId": "2163335292",
        "name": "Dongsheng Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "8d57faec0792c0df36f9d63810cb4a551cfffb42",
    "url": "https://www.semanticscholar.org/paper/8d57faec0792c0df36f9d63810cb4a551cfffb42",
    "title": "On the Relevant Set of Contexts for Evaluating Retrieval-Augmented Generation Systems",
    "abstract": "The recent interest in approaching more language and knowledge processing tasks via the Retrieval-Augmented Generation (RAG) framework allows for the consideration of evaluation criteria that can lead to a discrepancy in the way that the set of relevant results is determined for assessing retrieval-based performances. In this work, we describe and reflect on the consequences of such a discrepancy, and present basic results from experimentation over a RAG-based benchmark for Question Answering.",
    "venue": "",
    "year": null,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2327982454",
        "name": "Dar√≠o Garigliotti"
      }
    ],
    "source": "semantic_scholar",
    "score": 55.39720770839918
  },
  {
    "paperId": "735bf29aaf13c9420653e271db37614be55154d7",
    "url": "https://www.semanticscholar.org/paper/735bf29aaf13c9420653e271db37614be55154d7",
    "title": "Recent Advances in Retrieval-Augmented Text Generation",
    "abstract": "Recently retrieval-augmented text generation has achieved state-of-the-art performance in many NLP tasks and has attracted increasing attention of the NLP and IR community, this tutorial thereby aims to present recent advances in retrieval-augmented text generation comprehensively and comparatively. It firstly highlights the generic paradigm of retrieval-augmented text generation, then reviews notable works for different text generation tasks including dialogue generation, machine translation, and other generation tasks, and finally points out some limitations and shortcomings to facilitate future research.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2022,
    "citationCount": 66,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference",
      "Review"
    ],
    "publicationDate": "2022-07-06",
    "authors": [
      {
        "authorId": "2053327987",
        "name": "Deng Cai"
      },
      {
        "authorId": "2152546690",
        "name": "Yan Wang"
      },
      {
        "authorId": "2978364",
        "name": "Lemao Liu"
      },
      {
        "authorId": "2072684668",
        "name": "Shuming Shi"
      }
    ],
    "source": "semantic_scholar",
    "score": 134.07038929086448
  },
  {
    "paperId": "f6a957deade29c17033cb6693a5c65360f42b47c",
    "url": "https://www.semanticscholar.org/paper/f6a957deade29c17033cb6693a5c65360f42b47c",
    "title": "Revisiting and Improving Retrieval-Augmented Deep Assertion Generation",
    "abstract": "Unit testing validates the correctness of the unit under test and has become an essential activity in software development process. A unit test consists of a test prefix that drives the unit under test into a particular state, and a test oracle (e.g., assertion), which specifies the behavior in that state. To reduce manual efforts in conducting unit testing, Yu et al. proposed an integrated approach (integration for short), combining information retrieval with a deep learning-based approach, to generate assertions for a unit test. Despite being promising, there is still a knowledge gap as to why or where integration works or does not work. In this paper, we describe an in-depth analysis of the effectiveness of integration. Our analysis shows that: ‚ë† The overall performance of integration is mainly due to its success in retrieving assertions. ‚ë° integration struggles to understand the semantic differences between the retrieved focal-test (focal-test includes a test prefix and a unit under test) and the input focal-test, resulting in many tokens being incorrectly modified; ‚ë¢ integration is limited to specific types of edit operations (i.e., replacement) and cannot handle token addition or deletion. To improve the effectiveness of assertion generation, this paper proposes a novel retrieve-and-edit approach named EDITAS. Specifically, Editas first retrieves a similar focal-test from a pre-defined corpus and treats its assertion as a prototype. Then, Editas reuses the information in the prototype and edits the prototype automatically. Editas is more generalizable than integration because it can ‚ù∂ comprehensively understand the semantic differences between input and similar focal-tests; ‚ù∑ apply appropriate assertion edit patterns with greater flexibility; and ‚ù∏ generate more diverse edit actions than just replacement operations. We conduct experiments on two large-scale datasets and the experimental results demonstrate that Editas outperforms the state-of-the-art approaches, with an average improvement of 10.00%-87.48% and 3.30%-42.65% in accuracy and BLEU score, respectively.",
    "venue": "International Conference on Automated Software Engineering",
    "year": 2023,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.10264",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-09-11",
    "authors": [
      {
        "authorId": "2215273274",
        "name": "Weifeng Sun"
      },
      {
        "authorId": "2215368637",
        "name": "Hongyan Li"
      },
      {
        "authorId": "2215290031",
        "name": "Meng Yan"
      },
      {
        "authorId": "2242125639",
        "name": "Yan Lei"
      },
      {
        "authorId": "2242122772",
        "name": "Hongyu Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "bd0fe06bdd608e5e46f9fcfdf829b34cfb200e4a",
    "url": "https://www.semanticscholar.org/paper/bd0fe06bdd608e5e46f9fcfdf829b34cfb200e4a",
    "title": "Improving Retrieval Augmented Generation",
    "abstract": "With the recent popularity of large language models (LLMs), there are some limitations, namely, information being outdated due to a data cutoff. Retrieval augmented generation proves to be a good solution to this, providing the LLM access to recent information with the use of databases. We test a combination of methods like chunking, query re-ranking, query expansion, and knowledge graphs. Chunking tests what chunk sizes provide the best context for the LLM, resulting in single-sentence chunks being the most effective. Furthermore, query re-ranking is where the ordering of documents is modified to include the most relevant documents at the top of a search. We find that query re-ranking has mixed results compared to the baseline, where some methods perform better with certain k values. Next, we looked at query expansion, which uses an LLM to generate another response to further expand on the question. Using three different methods of query expansion, we find that they underperforming compared to the baseline. Finally, we use a knowledge graph to model relationships between different points of data to better understand and connect information. With this, results were a little more complicated due to the nature of the testing. Optimizations of the retrieval augmented generation pipeline is a promising way of showing effective methods to increase the accuracy and efficiency for real-world scenarios, however, in our testing, we find that these optimization methods are not as clear cut as they are expected to be.",
    "venue": "",
    "year": null,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [],
    "source": "semantic_scholar",
    "score": 50
  },
  {
    "paperId": "9038f40c43e7d62d8f1dc4819093083090911f7a",
    "url": "https://www.semanticscholar.org/paper/9038f40c43e7d62d8f1dc4819093083090911f7a",
    "title": "Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning",
    "abstract": "Paraphrase generation is a fundamental and long-standing task in natural language processing. In this paper, we concentrate on two contributions to the task: (1) we propose Retrieval Augmented Prompt Tuning (RAPT) as a parameter-efficient method to adapt large pre-trained language models for paraphrase generation; (2) we propose Novelty Conditioned RAPT (NC-RAPT) as a simple model-agnostic method of using specialized prompt tokens for controlled paraphrase generation with varying levels of lexical novelty. By conducting extensive experiments on four datasets, we demonstrate the effectiveness of the proposed approaches for retaining the semantic content of the original text while inducing lexical novelty in the generation.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2022,
    "citationCount": 36,
    "openAccessPdf": {
      "url": "https://ojs.aaai.org/index.php/AAAI/article/download/21297/21046",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-02-01",
    "authors": [
      {
        "authorId": "123467107",
        "name": "Jishnu Ray Chowdhury"
      },
      {
        "authorId": "2152482425",
        "name": "Yong Zhuang"
      },
      {
        "authorId": "2152375661",
        "name": "Shuyi Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.16376868966336
  },
  {
    "paperId": "d80241e05947581719bf2839e1621875890a12b0",
    "url": "https://www.semanticscholar.org/paper/d80241e05947581719bf2839e1621875890a12b0",
    "title": "RACE: Retrieval-augmented Commit Message Generation",
    "abstract": "Commit messages are important for software development and maintenance. Many neural network-based approaches have been proposed and shown promising results on automatic commit message generation. However, the generated commit messages could be repetitive or redundant. In this paper, we propose RACE, a new retrieval-augmented neural commit message generation method, which treats the retrieved similar commit as an exemplar and leverages it to generate an accurate commit message. As the retrieved commit message may not always accurately describe the content/intent of the current code diff, we also propose an exemplar guider, which learns the semantic similarity between the retrieved and current code diff and then guides the generation of commit message based on the similarity. We conduct extensive experiments on a large public dataset with five programming languages. Experimental results show that RACE can outperform all baselines. Furthermore, RACE can boost the performance of existing Seq2Seq models in commit message generation.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "citationCount": 32,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2022.emnlp-main.372.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-03-05",
    "authors": [
      {
        "authorId": "2006371687",
        "name": "Ensheng Shi"
      },
      {
        "authorId": "2108975906",
        "name": "Yanlin Wang"
      },
      {
        "authorId": "2112528845",
        "name": "Wei Tao"
      },
      {
        "authorId": "12723949",
        "name": "Lun Du"
      },
      {
        "authorId": "46702864",
        "name": "Hongyu Zhang"
      },
      {
        "authorId": "2109750123",
        "name": "Shi Han"
      },
      {
        "authorId": "2140415600",
        "name": "Dongmei Zhang"
      },
      {
        "authorId": "2152992779",
        "name": "Hongbin Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 122.4476134219972
  },
  {
    "paperId": "75e59a0b5173c9e7bd6fb89e457884984dbc9ab1",
    "url": "https://www.semanticscholar.org/paper/75e59a0b5173c9e7bd6fb89e457884984dbc9ab1",
    "title": "Multi-Task Retrieval-Augmented Text Generation with Relevance Sampling",
    "abstract": "This paper studies multi-task training of retrieval-augmented generation models for knowledge-intensive tasks. We propose to clean the training set by utilizing a distinct property of knowledge-intensive generation: The connection of query-answer pairs to items in the knowledge base. We filter training examples via a threshold of confidence on the relevance labels, whether a pair is answerable by the knowledge base or not. We train a single Fusion-in-Decoder (FiD) generator on seven combined tasks of the KILT benchmark. The experimental results suggest that our simple yet effective approach substantially improves competitive baselines on two strongly imbalanced tasks; and shows either smaller improvements or no significant regression on the remaining tasks. Furthermore, we demonstrate our multi-task training with relevance label sampling scales well with increased model capacity and achieves state-of-the-art results in five out of seven KILT tasks.",
    "venue": "arXiv.org",
    "year": 2022,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2207.03030",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-07-07",
    "authors": [
      {
        "authorId": "97393346",
        "name": "Sebastian Hofst√§tter"
      },
      {
        "authorId": "2809410",
        "name": "Jiecao Chen"
      },
      {
        "authorId": "2062947723",
        "name": "K. Raman"
      },
      {
        "authorId": "2499986",
        "name": "Hamed Zamani"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "1c62647bb8971105c77c1d642991cb1b92a52214",
    "url": "https://www.semanticscholar.org/paper/1c62647bb8971105c77c1d642991cb1b92a52214",
    "title": "Retrieval-Augmented Multilingual Keyphrase Generation with Retriever-Generator Iterative Training",
    "abstract": "Keyphrase generation is the task of automatically predicting keyphrases given a piece of long text. Despite its recent flourishing, keyphrase generation on non-English languages haven't been vastly investigated. In this paper, we call attention to a new setting named multilingual keyphrase generation and we contribute two new datasets, EcommerceMKP and AcademicMKP, covering six languages. Technically, we propose a retrieval-augmented method for multilingual keyphrase generation to mitigate the data shortage problem in non-English languages. The retrieval-augmented model leverages keyphrase annotations in English datasets to facilitate generating keyphrases in low-resource languages. Given a non-English passage, a cross-lingual dense passage retrieval module finds relevant English passages. Then the associated English keyphrases serve as external knowledge for keyphrase generation in the current language. Moreover, we develop a retriever-generator iterative training algorithm to mine pseudo parallel passage pairs to strengthen the cross-lingual passage retriever. Comprehensive experiments and ablations show that the proposed approach outperforms all baselines.",
    "venue": "NAACL-HLT",
    "year": 2022,
    "citationCount": 17,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2205.10471",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-05-21",
    "authors": [
      {
        "authorId": "1921742",
        "name": "Yifan Gao"
      },
      {
        "authorId": "3393799",
        "name": "Qingyu Yin"
      },
      {
        "authorId": "2146249169",
        "name": "Zheng Li"
      },
      {
        "authorId": "1400438808",
        "name": "Rui Meng"
      },
      {
        "authorId": "2088210968",
        "name": "Tong Zhao"
      },
      {
        "authorId": "2021632793",
        "name": "Bing Yin"
      },
      {
        "authorId": "145310663",
        "name": "Irwin King"
      },
      {
        "authorId": "145609003",
        "name": "M. Lyu"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.35557636844247
  },
  {
    "paperId": "5b3284875c425488abf126ab9b5e483986b6796d",
    "url": "https://www.semanticscholar.org/paper/5b3284875c425488abf126ab9b5e483986b6796d",
    "title": "Retrieval-Augmented Response Generation for Knowledge-Grounded Conversation in the Wild",
    "abstract": "Users on the internet usually have conversations on interesting facts or topics along with diverse knowledge from the web. However, most existing knowledge-grounded conversation models consider only a single document regarding the topic of a conversation. The recently proposed retrieval-augmented models generate a response based on multiple documents; however, they ignore the given topic and use only the local context of the conversation. To this end, we introduce a novel retrieval-augmented response generation model that retrieves an appropriate range of documents relevant to both the topic and local context of a conversation and uses them for generating a knowledge-grounded response. Our model first accepts both topic words extracted from the whole conversation and the tokens before the response to yield multiple representations. It then chooses representations of the first N token and ones of keywords from the conversation and document encoders and compares the two groups of representation from the conversation with those groups of the document, respectively. For training, we introduce a new data-weighting scheme to encourage the model to produce knowledge-grounded responses without ground truth knowledge. Both automatic and human evaluation results with a large-scale dataset show that our models can generate more knowledgeable, diverse, and relevant responses compared to the state-of-the-art models.",
    "venue": "IEEE Access",
    "year": 2022,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/09982598.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2563707",
        "name": "Yeonchan Ahn"
      },
      {
        "authorId": "73124469",
        "name": "Sang-goo Lee"
      },
      {
        "authorId": "46934982",
        "name": "Junho Shim"
      },
      {
        "authorId": "2052729",
        "name": "Jaehui Park"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.1886522358297
  },
  {
    "paperId": "ed88efa03fb9b486b72b8ba8e8b3470b0b097689",
    "url": "https://www.semanticscholar.org/paper/ed88efa03fb9b486b72b8ba8e8b3470b0b097689",
    "title": "SEScore2: Retrieval Augmented Pretraining for Text Generation Evaluation",
    "abstract": "Is it possible to leverage large scale raw and raw parallel corpora to build a general learned metric? Existing learned metrics have gaps to human judgements, are model-dependent or are limited to the domains or tasks where human ratings are available. In this paper, we propose SEScore2, a model-based metric pretrained over million-scale synthetic dataset constructed by our novel retrieval augmented data synthesis pipeline. SEScore2 achieves high correlation to human judgements without any human rating supervisions. Importantly, our unsupervised SEScore2 can outperform supervised metrics, which are trained on the News human ratings, at the TED domain. We evaluate SEScore2 over four text generation tasks across three languages. SEScore2 outperforms all prior unsupervised evaluation metrics in machine translation, speech translation, data-to-text and dialogue generation, with average Kendall improvements 0 . 158 . SEScore2 even outperforms SOTA supervised BLEURT at data-to-text, dialogue generation and overall correlation 1 .",
    "venue": "arXiv.org",
    "year": 2022,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2212.09305",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "145738382",
        "name": "Wenda Xu"
      },
      {
        "authorId": "2067733840",
        "name": "Xian Qian"
      },
      {
        "authorId": "50468534",
        "name": "Mingxuan Wang"
      },
      {
        "authorId": "143900005",
        "name": "Lei Li"
      },
      {
        "authorId": "1682479",
        "name": "William Yang Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "ecd5d43a13d3e74d5fbbb8eef18b269fc9ca5fb0",
    "url": "https://www.semanticscholar.org/paper/ecd5d43a13d3e74d5fbbb8eef18b269fc9ca5fb0",
    "title": "BashExplainer: Retrieval-Augmented Bash Code Comment Generation based on Fine-tuned CodeBERT",
    "abstract": "Developers use shell commands for many tasks, such as file system management, network control, and process management. Bash is one of the most commonly used shells and plays an important role in Linux system development and maintenance. Due to the language flexibility of Bash code, developers who are not familiar with Bash often have difficulty understanding the purpose and functionality of Bash code. In this study, we study Bash code comment generation problem and proposed an automatic method BASHEXPLAINER based on two-stage training strategy. In the first stage, we train a Bash encoder by fine-tuning CodeBERT on our constructed Bash code corpus. In the second stage, we first retrieve the most similar code from the code repository for the target code based on semantic and lexical similarity. Then we use the trained Bash encoder to generate two vector representations. Finally, we fuse these two vector representations via the fusion layer and generate the code comment through the decoder. To show the competitiveness of our proposed method, we construct a high-quality corpus by combining the corpus shared in the previous NL2Bash study and the corpus shared in the NLC2CMD competition. This corpus contains 10,592 Bash codes and corresponding comments. Then we selected ten baselines from previous studies on automatic code comment generation, which cover information retrieval methods, deep learning methods, and hybrid methods. The experimental results show that in terms of the performance measures BLEU-3/4, METEOR, and ROUGR-L, BASHEXPLAINER can outperform all baselines by at least 8.75%, 9.29%, 4.77% and 3.86%. Then we design ablation experiments to show the component setting rationality of BASHEXPLAINER. Later, we conduct a human study to further show the competitiveness of BASHEXPLAINER. Finally, we develop a browser plug-in based on BASHEXPLAINER to facilitate the understanding of the Bash code for developers.",
    "venue": "IEEE International Conference on Software Maintenance and Evolution",
    "year": 2022,
    "citationCount": 20,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2206.13325",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-06-27",
    "authors": [
      {
        "authorId": "2110721439",
        "name": "Chi Yu"
      },
      {
        "authorId": "2149522863",
        "name": "Guang Yang"
      },
      {
        "authorId": "2143738759",
        "name": "Xiang Chen"
      },
      {
        "authorId": "2152353244",
        "name": "Ke Liu"
      },
      {
        "authorId": "2144211882",
        "name": "Yanlin Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.66783656585135
  },
  {
    "paperId": "44855f05648a7b3a0d78729959f900de629c686f",
    "url": "https://www.semanticscholar.org/paper/44855f05648a7b3a0d78729959f900de629c686f",
    "title": "Retrieval-Augmented Transformer-XL for Close-Domain Dialog Generation",
    "abstract": "\n \n \nTransformer-based models have demonstrated excellent capabilities of capturing patterns and structures in natural language generation and achieved state-of-the-art results in many tasks. In this paper we present a transformer-based model for multi-turn dialog response generation. Our solution is based on a hybrid approach which augments a transformer-based generative model with a novel retrieval mechanism, which leverages the memorized information in the training data via k-Nearest Neighbor search. Our system is evaluated on two datasets made by customer/assistant dialogs: the Taskmaster-1, released by Google and holding high quality, goal-oriented conversational data and a proprietary dataset collected from a real customer service call center. Both achieve better BLEU scores over strong baselines. \n \n \n",
    "venue": "The Florida AI Research Society",
    "year": 2021,
    "citationCount": 13,
    "openAccessPdf": {
      "url": "https://journals.flvc.org/FLAIRS/article/download/128369/130118",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-05-19",
    "authors": [
      {
        "authorId": "66951248",
        "name": "Giovanni Bonetta"
      },
      {
        "authorId": "2853804",
        "name": "R. Cancelliere"
      },
      {
        "authorId": "2115295588",
        "name": "Ding Liu"
      },
      {
        "authorId": "1752755",
        "name": "Paul Vozila"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "eb9c4a07a336e8deefe7b399c550d3af0241238e",
    "url": "https://www.semanticscholar.org/paper/eb9c4a07a336e8deefe7b399c550d3af0241238e",
    "title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
    "abstract": "As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations, such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the generation quality of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: architectures, training strategies, and applications. As the preliminary knowledge, we briefly introduce the foundations and recent advances of LLMs. Then, to illustrate the practical significance of RAG for LLMs, we systematically review mainstream relevant work by their architectures, training strategies, and application areas, detailing specifically the challenges of each and the corresponding capabilities of RA-LLMs. Finally, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/",
    "venue": "Knowledge Discovery and Data Mining",
    "year": 2024,
    "citationCount": 71,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference",
      "Review"
    ],
    "publicationDate": "2024-05-10",
    "authors": [
      {
        "authorId": "2291324376",
        "name": "Wenqi Fan"
      },
      {
        "authorId": "2301103404",
        "name": "Yujuan Ding"
      },
      {
        "authorId": "2301015154",
        "name": "Liang-bo Ning"
      },
      {
        "authorId": "2266567589",
        "name": "Shijie Wang"
      },
      {
        "authorId": "2301167804",
        "name": "Hengyun Li"
      },
      {
        "authorId": "2297846971",
        "name": "Dawei Yin"
      },
      {
        "authorId": "2279753672",
        "name": "Tat-Seng Chua"
      },
      {
        "authorId": "2297955888",
        "name": "Qing Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 134.14999178524084
  },
  {
    "paperId": "ca89781d7915eac3089a7b47a065943ce722109f",
    "url": "https://www.semanticscholar.org/paper/ca89781d7915eac3089a7b47a065943ce722109f",
    "title": "Retrieval-Augmented Controllable Review Generation",
    "abstract": "In this paper, we study review generation given a set of attribute identifiers which are user ID, product ID and rating. This is a difficult subtask of natural language generation since models are limited to the given identifiers, without any specific descriptive information regarding the inputs, when generating the text. The capacity of these models is thus confined and dependent to how well the models can capture vector representations of attributes. We thus propose to additionally leverage references, which are selected from a large pool of texts labeled with one of the attributes, as textual information that enriches inductive biases of given attributes. With these references, we can now pose the problem as an instance of text-to-text generation, which makes the task easier since texts that are syntactically, semantically similar with the output text are provided as input. Using this framework, we address issues such as selecting references from a large candidate set without textual context and improving the model complexity for generation. Our experiments show that our models improve over previous approaches on both automatic and human evaluation metrics.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2020,
    "citationCount": 18,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference",
      "Review"
    ],
    "publicationDate": "2020-12-01",
    "authors": [
      {
        "authorId": "50982689",
        "name": "Jihyeok Kim"
      },
      {
        "authorId": "5841595",
        "name": "Seungtaek Choi"
      },
      {
        "authorId": "23181472",
        "name": "Reinald Kim Amplayo"
      },
      {
        "authorId": "1716415",
        "name": "Seung-won Hwang"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.1665846874966
  },
  {
    "paperId": "465471bb5bf1a945549d6291c2d23367966b4957",
    "url": "https://www.semanticscholar.org/paper/465471bb5bf1a945549d6291c2d23367966b4957",
    "title": "In-Context Retrieval-Augmented Language Models",
    "abstract": "Abstract Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access.1",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 410,
    "openAccessPdf": {
      "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-01-31",
    "authors": [
      {
        "authorId": "73775461",
        "name": "Ori Ram"
      },
      {
        "authorId": "152754428",
        "name": "Yoav Levine"
      },
      {
        "authorId": "1491822146",
        "name": "Itay Dalmedigos"
      },
      {
        "authorId": "51918041",
        "name": "Dor Muhlgay"
      },
      {
        "authorId": "3140335",
        "name": "A. Shashua"
      },
      {
        "authorId": "2066411743",
        "name": "Kevin Leyton-Brown"
      },
      {
        "authorId": "1701353",
        "name": "Y. Shoham"
      }
    ],
    "source": "semantic_scholar",
    "score": 160.2788982174435
  },
  {
    "paperId": "848772a50cee68e88988ded7522e280d1c490598",
    "url": "https://www.semanticscholar.org/paper/848772a50cee68e88988ded7522e280d1c490598",
    "title": "Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models",
    "abstract": "Abstract Summary Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its generated explanations with customized reflective tokens. Our work proves that domain-specific components, such as a retriever, domain-related document corpus, and instruction sets are necessary for adhering to domain-related instructions. Using three major medical question-answering benchmark datasets, experimental results of Self-BioRAG demonstrate significant performance gains by achieving a 7.2% absolute improvement on average over the state-of-the-art open-foundation model with a parameter size of 7B or less. Similarly, Self-BioRAG outperforms RAG by 8% Rouge-1 score in generating more proficient answers on two long-form question-answering benchmarks on average. Overall, we analyze that Self-BioRAG finds the clues in the question, retrieves relevant documents if needed, and understands how to answer with information from retrieved documents and encoded knowledge as a medical expert does. We release our data and code for training our framework components and model weights (7B and 13B) to enhance capabilities in biomedical and clinical domains. Availability and implementation Self-BioRAG is available at https://github.com/dmis-lab/self-biorag.",
    "venue": "Bioinform.",
    "year": 2024,
    "citationCount": 18,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-27",
    "authors": [
      {
        "authorId": "2281744951",
        "name": "Minbyul Jeong"
      },
      {
        "authorId": "2281744575",
        "name": "Jiwoong Sohn"
      },
      {
        "authorId": "147610425",
        "name": "Mujeen Sung"
      },
      {
        "authorId": "2281792202",
        "name": "Jaewoo Kang"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.1665846874966
  },
  {
    "paperId": "1f98af8fdca3bc61da466ef88fdc64a86de6d422",
    "url": "https://www.semanticscholar.org/paper/1f98af8fdca3bc61da466ef88fdc64a86de6d422",
    "title": "Almanac - Retrieval-Augmented Language Models for Clinical Medicine.",
    "abstract": "BACKGROUND\nLarge language models (LLMs) have recently shown impressive zero-shot capabilities, whereby they can use auxiliary data, without the availability of task-specific training examples, to complete a variety of natural language tasks, such as summarization, dialogue generation, and question answering. However, despite many promising applications of LLMs in clinical medicine, adoption of these models has been limited by their tendency to generate incorrect and sometimes even harmful statements.\n\n\nMETHODS\nWe tasked a panel of eight board-certified clinicians and two health care practitioners with evaluating Almanac, an LLM framework augmented with retrieval capabilities from curated medical resources for medical guideline and treatment recommendations. The panel compared responses from Almanac and standard LLMs (ChatGPT-4, Bing, and Bard) versus a novel data set of 314 clinical questions spanning nine medical specialties.\n\n\nRESULTS\nAlmanac showed a significant improvement in performance compared with the standard LLMs across axes of factuality, completeness, user preference, and adversarial safety.\n\n\nCONCLUSIONS\nOur results show the potential for LLMs with access to domain-specific corpora to be effective in clinical decision-making. The findings also underscore the importance of carefully testing LLMs before deployment to mitigate their shortcomings. (Funded by the National Institutes of Health, National Heart, Lung, and Blood Institute.).",
    "venue": "NEJM AI",
    "year": 2024,
    "citationCount": 38,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-25",
    "authors": [
      {
        "authorId": "2330710849",
        "name": "Cyril Zakka"
      },
      {
        "authorId": "6831979",
        "name": "R. Shad"
      },
      {
        "authorId": "51171151",
        "name": "Akash Chaurasia"
      },
      {
        "authorId": "10748418",
        "name": "Alex R. Dalal"
      },
      {
        "authorId": "2217990518",
        "name": "Jennifer L. Kim"
      },
      {
        "authorId": "2281505553",
        "name": "M. Moor"
      },
      {
        "authorId": "79890611",
        "name": "R. Fong"
      },
      {
        "authorId": "2281479776",
        "name": "Curran Phillips"
      },
      {
        "authorId": "2072267385",
        "name": "Kevin Alexander"
      },
      {
        "authorId": "2239207422",
        "name": "Euan A. Ashley"
      },
      {
        "authorId": "2217560856",
        "name": "Jack Boyd"
      },
      {
        "authorId": "2194123892",
        "name": "Kathleen Boyd"
      },
      {
        "authorId": "2217546108",
        "name": "Karen Hirsch"
      },
      {
        "authorId": "2249123829",
        "name": "Curtis P. Langlotz"
      },
      {
        "authorId": "2281635373",
        "name": "Rita Lee"
      },
      {
        "authorId": "2281509661",
        "name": "Joanna Melia"
      },
      {
        "authorId": "2217523638",
        "name": "Joanna Nelson"
      },
      {
        "authorId": "2281509382",
        "name": "Karim Sallam"
      },
      {
        "authorId": "2281508041",
        "name": "Stacey Tullis"
      },
      {
        "authorId": "5295775",
        "name": "M. Vogelsong"
      },
      {
        "authorId": "2140409003",
        "name": "J. Cunningham"
      },
      {
        "authorId": "4684172",
        "name": "W. Hiesinger"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.95342469194469
  },
  {
    "paperId": "8f06a8cff762b5ce6337d3b617442c72a46374a4",
    "url": "https://www.semanticscholar.org/paper/8f06a8cff762b5ce6337d3b617442c72a46374a4",
    "title": "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers",
    "abstract": "Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q&A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the ‚ÄòBlended RAG‚Äô method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies. Our study achieves better retrieval results and sets new benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID datasets. We further extend such a ‚ÄòBlended Retriever‚Äô to the RAG system to demonstrate far superior results on Generative Q&A datasets like SQUAD, even surpassing fine-tuning performance.",
    "venue": "Conference on Multimedia Information Processing and Retrieval",
    "year": 2024,
    "citationCount": 20,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-22",
    "authors": [
      {
        "authorId": "2003089508",
        "name": "Kunal Sawarkar"
      },
      {
        "authorId": "2295990127",
        "name": "Abhilasha Mangal"
      },
      {
        "authorId": "2295990033",
        "name": "Shivam Raj Solanki"
      }
    ],
    "source": "semantic_scholar",
    "score": 115.66783656585135
  },
  {
    "paperId": "addd475c96056491539b790c1b264d0855c80fb7",
    "url": "https://www.semanticscholar.org/paper/addd475c96056491539b790c1b264d0855c80fb7",
    "title": "Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training",
    "abstract": "Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs' capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: https://github.com/calubkk/RAAT.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-31",
    "authors": [
      {
        "authorId": "2293319089",
        "name": "Feiteng Fang"
      },
      {
        "authorId": "2287892865",
        "name": "Yuelin Bai"
      },
      {
        "authorId": "2266469238",
        "name": "Shiwen Ni"
      },
      {
        "authorId": "2301170603",
        "name": "Min Yang"
      },
      {
        "authorId": "2287803799",
        "name": "Xiaojun Chen"
      },
      {
        "authorId": "2266809482",
        "name": "Ruifeng Xu"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "8c44c3fa1d3cbc9cbde4cb028f598f1999d539d7",
    "url": "https://www.semanticscholar.org/paper/8c44c3fa1d3cbc9cbde4cb028f598f1999d539d7",
    "title": "Generative Retrieval-Augmented Ontologic Graph and Multiagent Strategies for Interpretive Large Language Model-Based Materials Design",
    "abstract": "Transformer neural networks show promising capabilities, in particular for uses in materials analysis, design, and manufacturing, including their capacity to work effectively with human language, symbols, code, and numerical data. Here, we explore the use of large language models (LLMs) as a tool that can support engineering analysis of materials, applied to retrieving key information about subject areas, developing research hypotheses, discovery of mechanistic relationships across disparate areas of knowledge, and writing and executing simulation codes for active knowledge generation based on physical ground truths. Moreover, when used as sets of AI agents with specific features, capabilities, and instructions, LLMs can provide powerful problem-solution strategies for applications in analysis and design problems. Our experiments focus on using a fine-tuned model, MechGPT, developed based on training data in the mechanics of materials domain. We first affirm how fine-tuning endows LLMs with a reasonable understanding of subject area knowledge. However, when queried outside the context of learned matter, LLMs can have difficulty recalling correct information and may hallucinate. We show how this can be addressed using retrieval-augmented Ontological Knowledge Graph strategies. The graph-based strategy helps us not only to discern how the model understands what concepts are important but also how they are related, which significantly improves generative performance and also naturally allows for injection of new and augmented data sources into generative AI algorithms. We find that the additional feature of relatedness provides advantages over regular retrieval augmentation approaches and not only improves LLM performance but also provides mechanistic insights for exploration of a material design process. Illustrated for a use case of relating distinct areas of knowledge, here, music and proteins, such strategies can also provide an interpretable graph structure with rich information at the node, edge, and subgraph level that provides specific insights into mechanisms and relationships. We discuss other approaches to improve generative qualities, including nonlinear sampling strategies and agent-based modeling that offer enhancements over single-shot generations, whereby LLMs are used to both generate content and assess content against an objective target. Examples provided include complex question answering, code generation, and execution in the context of automated force-field development from actively learned density functional theory (DFT) modeling and data analysis.",
    "venue": "ACS Engineering Au",
    "year": 2024,
    "citationCount": 21,
    "openAccessPdf": {
      "url": "https://pubs.acs.org/doi/pdf/10.1021/acsengineeringau.3c00058",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-12",
    "authors": [
      {
        "authorId": "2252265640",
        "name": "Markus J. Buehler"
      }
    ],
    "source": "semantic_scholar",
    "score": 116.36563680037474
  },
  {
    "paperId": "aa32cce28aad1d04fea026860c3e2d4a218d9a57",
    "url": "https://www.semanticscholar.org/paper/aa32cce28aad1d04fea026860c3e2d4a218d9a57",
    "title": "Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language Models for Telecommunications",
    "abstract": "The application of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems in the telecommunication domain presents unique challenges, primarily due to the complex nature of telecom standard documents and the rapid evolution of the field. The paper introduces Telco-RAG, an open-source RAG framework designed to handle the specific needs of telecommunications standards, particularly 3rd Generation Partnership Project (3GPP) documents. Telco-RAG addresses the critical challenges of implementing a RAG pipeline on highly technical content, paving the way for applying LLMs in telecommunications and offering guidelines for RAG implementation in other technical domains.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-24",
    "authors": [
      {
        "authorId": "2298273471",
        "name": "Andrei-Laurentiu Bornea"
      },
      {
        "authorId": "70486867",
        "name": "Fadhel Ayed"
      },
      {
        "authorId": "2261362548",
        "name": "Antonio De Domenico"
      },
      {
        "authorId": "3393923",
        "name": "Nicola Piovesan"
      },
      {
        "authorId": "2257807974",
        "name": "Ali Maatouk"
      }
    ],
    "source": "semantic_scholar",
    "score": 103.47424036192305
  },
  {
    "paperId": "1b7492a4b813052146300fa8c03325c4ad7a0a25",
    "url": "https://www.semanticscholar.org/paper/1b7492a4b813052146300fa8c03325c4ad7a0a25",
    "title": "RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing",
    "abstract": "Large Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge. To mitigate these, recent methodologies have integrated information retrieved from external resources with LLMs, substantially enhancing their performance across NLP tasks. This survey paper addresses the absence of a comprehensive overview on Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an in-depth examination of their paradigm, evolution, taxonomy, and applications. The paper discusses the essential components of RALMs, including Retrievers, Language Models, and Augmentations, and how their interactions lead to diverse model structures and applications. RALMs demonstrate utility in a spectrum of tasks, from translation and dialogue systems to knowledge-intensive applications. The survey includes several evaluation methods of RALMs, emphasizing the importance of robustness, accuracy, and relevance in their assessment. It also acknowledges the limitations of RALMs, particularly in retrieval quality and computational efficiency, offering directions for future research. In conclusion, this survey aims to offer a structured insight into RALMs, their potential, and the avenues for their future development in NLP. The paper is supplemented with a Github Repository containing the surveyed works and resources for further study: https://github.com/2471023025/RALM_Survey.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-04-30",
    "authors": [
      {
        "authorId": "2299263700",
        "name": "Yucheng Hu"
      },
      {
        "authorId": "2220675861",
        "name": "Yuxing Lu"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "3a6bdf724da556ca534a9786b7a9f3f0adc567f7",
    "url": "https://www.semanticscholar.org/paper/3a6bdf724da556ca534a9786b7a9f3f0adc567f7",
    "title": "AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework",
    "abstract": "The task of financial analysis primarily encompasses two key areas: stock trend prediction and the corresponding financial question answering. Currently, machine learning and deep learning algorithms (ML&DL) have been widely applied for stock trend predictions, leading to significant progress. However, these methods fail to provide reasons for predictions, lacking interpretability and reasoning processes. Also, they can not integrate textual information such as financial news or reports. Meanwhile, large language models (LLM) have remarkable textual understanding and generation ability. But due to the scarcity of financial training datasets and limited integration with real-time knowledge, LLM still suffer from hallucinations and unable to keep up with the latest information. To tackle these challenges, we first release AlphaFin datasets, combining traditional research datasets, real-time financial data, and handwritten chain-of-thought (CoT) data. It has positive impact on training LLM for completing financial analysis. We then use AlphaFin datasets to benchmark a state-of-the-art method, called Stock-Chain, for effectively tackling the financial analysis task, which integrates retrieval-augmented generation (RAG) techniques. Extensive experiments are conducted to demonstrate the effectiveness of our framework on financial analysis.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-19",
    "authors": [
      {
        "authorId": "2292290608",
        "name": "Xiang Li"
      },
      {
        "authorId": "2292305424",
        "name": "Zhenyu Li"
      },
      {
        "authorId": "2292904456",
        "name": "Chen Shi"
      },
      {
        "authorId": "2292324128",
        "name": "Yong Xu"
      },
      {
        "authorId": "2302806265",
        "name": "Qing Du"
      },
      {
        "authorId": "2257488640",
        "name": "Mingkui Tan"
      },
      {
        "authorId": "2292174629",
        "name": "Jun Huang"
      },
      {
        "authorId": "2291964148",
        "name": "Wei Lin"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "301e4ba731b703313ef24c1b6e95e9c0bc05a7dd",
    "url": "https://www.semanticscholar.org/paper/301e4ba731b703313ef24c1b6e95e9c0bc05a7dd",
    "title": "Health-LLM: Personalized Retrieval-Augmented Disease Prediction System",
    "abstract": "Recent advancements in artificial intelligence (AI), especially large language models (LLMs), have significantly advanced healthcare applications and demonstrated potentials in intelligent medical treatment. However, there are conspicuous challenges such as vast data volumes and inconsistent symptom characterization standards, preventing full integration of healthcare AI systems with individual patients' needs. To promote professional and personalized healthcare, we propose an innovative framework, Heath-LLM, which combines large-scale feature extraction and medical knowledge trade-off scoring. Compared to traditional health management applications, our system has three main advantages: (1) It integrates health reports and medical knowledge into a large model to ask relevant questions to large language model for disease prediction; (2) It leverages a retrieval augmented generation (RAG) mechanism to enhance feature extraction; (3) It incorporates a semi-automated feature updating framework that can merge and delete features to improve accuracy of disease prediction. We experiment on a large number of health reports to assess the effectiveness of Health-LLM system. The results indicate that the proposed system surpasses the existing ones and has the potential to significantly advance disease prediction and personalized health management.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-01",
    "authors": [
      {
        "authorId": "2220539385",
        "name": "Mingyu Jin"
      },
      {
        "authorId": "2220796036",
        "name": "Qinkai Yu"
      },
      {
        "authorId": "2267332168",
        "name": "Dong Shu"
      },
      {
        "authorId": "2279759627",
        "name": "Chong Zhang"
      },
      {
        "authorId": "2268855367",
        "name": "Lizhou Fan"
      },
      {
        "authorId": "2007245028",
        "name": "Wenyue Hua"
      },
      {
        "authorId": "2279758147",
        "name": "Suiyuan Zhu"
      },
      {
        "authorId": "2278984372",
        "name": "Yanda Meng"
      },
      {
        "authorId": "2292292249",
        "name": "Zhenting Wang"
      },
      {
        "authorId": "2237804196",
        "name": "Mengnan Du"
      },
      {
        "authorId": "2279766837",
        "name": "Yongfeng Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "6489640b1d30a8a3e7cb906bb6557f1ccd0d799d",
    "url": "https://www.semanticscholar.org/paper/6489640b1d30a8a3e7cb906bb6557f1ccd0d799d",
    "title": "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models",
    "abstract": "Retrieval-augmented language model (RALM) represents a significant advancement in mitigating factual hallucination by leveraging external knowledge sources. However, the reliability of the retrieved information is not always guaranteed, and the retrieval of irrelevant data can mislead the response generation. Moreover, standard RALMs frequently neglect their intrinsic knowledge due to the interference from retrieved information. In instances where the retrieved information is irrelevant, RALMs should ideally utilize their intrinsic knowledge or, in the absence of both intrinsic and retrieved knowledge, opt to respond with ‚Äúunknown‚Äù to avoid hallucination. In this paper, we introduces Chain-of-Note (CoN), a novel approach to improve robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios. The core idea of CoN is to generate sequential reading notes for each retrieved document, enabling a thorough evaluation of their relevance to the given question and integrating this information to formulate the final answer. Our experimental results show that GPT-4, when equipped with CoN, outperforms the Chain-of-Thought approach. Besides, we utilized GPT-4 to create 10K CoN data, subsequently trained on smaller models like OPT and LLaMa-2. Our experiments across four open-domain QA benchmarks show that fine-tuned RALMs equipped with CoN significantly outperform standard fine-tuned RALMs.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 79,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-15",
    "authors": [
      {
        "authorId": "38767143",
        "name": "W. Yu"
      },
      {
        "authorId": "2254831297",
        "name": "Hongming Zhang"
      },
      {
        "authorId": "2243367575",
        "name": "Xiaoman Pan"
      },
      {
        "authorId": "22244290",
        "name": "Kaixin Ma"
      },
      {
        "authorId": "2261392960",
        "name": "Hongwei Wang"
      },
      {
        "authorId": "2261392681",
        "name": "Dong Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 135.73039952010822
  },
  {
    "paperId": "a37153a5f42ee2951ad8a2c9ec86b52c4bf81c77",
    "url": "https://www.semanticscholar.org/paper/a37153a5f42ee2951ad8a2c9ec86b52c4bf81c77",
    "title": "Retrieval-Augmented Multimodal Language Modeling",
    "abstract": "Recent multimodal models such as DALL-E and\r\nCM3 have achieved remarkable progress in textto-image and image-to-text generation. However, these models store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the\r\nmodel parameters, requiring increasingly larger models and training data to capture more knowledge. To integrate knowledge in a more scalable and modular way, we propose a retrievalaugmented multimodal model, which enables a base multimodal model (generator) to refer to relevant knowledge fetched by a retriever from external memory (e.g., multimodal documents on the web). Specifically, we implement a retriever using the pretrained CLIP model and a generator using the CM3 Transformer architecture, and\r\ntrain this model using the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can retrieve and generate mixtures of text and images.\r\nWe show that RA-CM3 significantly outperforms baseline multimodal models such as DALL-E and CM3 on both image and caption generation tasks (12 FID and 17 CIDEr improvements on MSCOCO), while requiring much less compute for training (<30% of DALL-E). Moreover, we show that RA-CM3 exhibits novel capabilities such as knowledge-intensive image generation and multimodal in-context learning",
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "citationCount": 83,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2211.12561",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "19168196",
        "name": "Michihiro Yasunaga"
      },
      {
        "authorId": "2201435",
        "name": "Armen Aghajanyan"
      },
      {
        "authorId": "3040379",
        "name": "Weijia Shi"
      },
      {
        "authorId": "2191899140",
        "name": "Rich James"
      },
      {
        "authorId": "1702139",
        "name": "J. Leskovec"
      },
      {
        "authorId": "145419642",
        "name": "Percy Liang"
      },
      {
        "authorId": "35084211",
        "name": "M. Lewis"
      },
      {
        "authorId": "1982950",
        "name": "Luke Zettlemoyer"
      },
      {
        "authorId": "2072801764",
        "name": "Wen-tau Yih"
      }
    ],
    "source": "semantic_scholar",
    "score": 136.46225198264972
  },
  {
    "paperId": "430aa6966c15c4a20a4fb2d8383e136b9cb6cde7",
    "url": "https://www.semanticscholar.org/paper/430aa6966c15c4a20a4fb2d8383e136b9cb6cde7",
    "title": "Almanac: Retrieval-Augmented Language Models for Clinical Medicine",
    "abstract": "Large-language models have recently demonstrated impressive zero-shot capabilities in a variety of natural language tasks such as summarization, dialogue generation, and question-answering. Despite many promising applications in clinical medicine, adoption of these models in real-world settings has been largely limited by their tendency to generate incorrect and sometimes even toxic statements. In this study, we develop Almanac, a large language model framework augmented with retrieval capabilities for medical guideline and treatment recommendations. Performance on a novel dataset of clinical scenarios (n= 130) evaluated by a panel of 5 board-certified and resident physicians demonstrates significant increases in factuality (mean of 18% at p-value < 0.05) across all specialties, with improvements in completeness and safety. Our results demonstrate the potential for large language models to be effective tools in the clinical decision-making process, while also emphasizing the importance of careful testing and deployment to mitigate their shortcomings.",
    "venue": "Research Square",
    "year": 2023,
    "citationCount": 106,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-03-01",
    "authors": [
      {
        "authorId": "4684172",
        "name": "W. Hiesinger"
      },
      {
        "authorId": "1994286801",
        "name": "Cyril Zakka"
      },
      {
        "authorId": "51171151",
        "name": "Akash Chaurasia"
      },
      {
        "authorId": "6831979",
        "name": "R. Shad"
      },
      {
        "authorId": "10748418",
        "name": "Alex R. Dalal"
      },
      {
        "authorId": "2217990518",
        "name": "Jennifer L. Kim"
      },
      {
        "authorId": "1557487144",
        "name": "Michael Moor"
      },
      {
        "authorId": "2072267385",
        "name": "Kevin Alexander"
      },
      {
        "authorId": "2202192099",
        "name": "Euan A. Ashley"
      },
      {
        "authorId": "2217560856",
        "name": "Jack Boyd"
      },
      {
        "authorId": "2194123892",
        "name": "Kathleen Boyd"
      },
      {
        "authorId": "2217546108",
        "name": "Karen Hirsch"
      },
      {
        "authorId": "1584627064",
        "name": "C. Langlotz"
      },
      {
        "authorId": "2217523638",
        "name": "Joanna Nelson"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.09243251692857
  },
  {
    "paperId": "731ac2fd7a5586d17cb8959c190158d429821a60",
    "url": "https://www.semanticscholar.org/paper/731ac2fd7a5586d17cb8959c190158d429821a60",
    "title": "ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model",
    "abstract": "3D human motion generation is crucial for creative industry. Recent advances rely on generative models with domain knowledge for text-driven motion generation, leading to substantial progress in capturing common motions. However, the performance on more diverse motions remains unsatisfactory. In this work, we propose ReMoDiffuse, a diffusion-model-based motion generation framework that integrates a retrieval mechanism to refine the denoising process. ReMoDiffuse enhances the generalizability and diversity of text-driven motion generation with three key designs: 1) Hybrid Retrieval finds appropriate references from the database in terms of both semantic and kinematic similarities. 2) Semantic-Modulated Transformer selectively absorbs retrieval knowledge, adapting to the difference between retrieved samples and the target motion sequence. 3) Condition Mixture better utilizes the retrieval database during inference, overcoming the scale sensitivity in classifier-free guidance. Extensive experiments demonstrate that ReMoDiffuse outperforms state-of-the-art methods by balancing both text-motion consistency and motion quality, especially for more diverse motion generation. Project page: https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2023,
    "citationCount": 111,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2304.01116",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-04-03",
    "authors": [
      {
        "authorId": "2119682611",
        "name": "Mingyuan Zhang"
      },
      {
        "authorId": "2183512577",
        "name": "Xinying Guo"
      },
      {
        "authorId": "50379842",
        "name": "Liang Pan"
      },
      {
        "authorId": "66562436",
        "name": "Zhongang Cai"
      },
      {
        "authorId": "1568986485",
        "name": "Fangzhou Hong"
      },
      {
        "authorId": "47892535",
        "name": "Huirong Li"
      },
      {
        "authorId": "2165477990",
        "name": "Lei Yang"
      },
      {
        "authorId": "2164276341",
        "name": "Ziwei Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 146.7774830694264
  },
  {
    "paperId": "cfce709a65f90312d2bdc1a6cf0380c19becf694",
    "url": "https://www.semanticscholar.org/paper/cfce709a65f90312d2bdc1a6cf0380c19becf694",
    "title": "RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models",
    "abstract": "Retrieval-augmented generation (RAG) has become a main technique for alleviating hallucinations in large language models (LLMs). Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents. In order to develop effective hallucination prevention strategies under RAG, it is important to create benchmark datasets that can measure the extent of hallucination. This paper presents RAGTruth, a corpus tailored for analyzing word-level hallucinations in various domains and tasks within the standard RAG frameworks for LLM applications. RAGTruth comprises nearly 18,000 naturally generated responses from diverse LLMs using RAG. These responses have undergone meticulous manual annotations at both the individual cases and word levels, incorporating evaluations of hallucination intensity. We not only benchmark hallucination frequencies across different LLMs, but also critically assess the effectiveness of several existing hallucination detection methodologies. Furthermore, we show that using a high-quality dataset such as RAGTruth, it is possible to finetune a relatively small LLM and achieve a competitive level of performance in hallucination detection when compared to the existing prompt-based approaches using state-of-the-art large language models such as GPT-4.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 43,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-31",
    "authors": [
      {
        "authorId": "2277251434",
        "name": "Yuanhao Wu"
      },
      {
        "authorId": "2277249776",
        "name": "Juno Zhu"
      },
      {
        "authorId": "2277414822",
        "name": "Siliang Xu"
      },
      {
        "authorId": "2121340452",
        "name": "Kashun Shum"
      },
      {
        "authorId": "2277247851",
        "name": "Cheng Niu"
      },
      {
        "authorId": "2277247396",
        "name": "Randy Zhong"
      },
      {
        "authorId": "2277244244",
        "name": "Juntong Song"
      },
      {
        "authorId": "2277418669",
        "name": "Tong Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 126.76284450877392
  },
  {
    "paperId": "c4db4cfe084d0ab44955088e7e75f0704bd90c6c",
    "url": "https://www.semanticscholar.org/paper/c4db4cfe084d0ab44955088e7e75f0704bd90c6c",
    "title": "Retrieval Augmented End-to-End Spoken Dialog Models",
    "abstract": "We recently developed a joint speech and language model (SLM [1]) which fuses a pretrained foundational speech model and a large language model (LLM), while preserving the in-context learning capability intrinsic to the pretrained LLM. In this paper, we apply SLM to dialog applications where the dialog states are inferred directly from the audio signal.Task-oriented dialogs often contain domain-specific entities, i.e., restaurants, hotels, train stations, and city names, which are difficult to recognize, however, critical for the downstream applications. Inspired by the RAG (retrieval-augmented generation) models, we propose a retrieval augmented SLM (ReSLM) that overcomes this weakness. We first train a retriever to retrieve text entities given audio inputs. The retrieved entities are then added as text inputs to the underlying LLM to bias model predictions. We evaluated ReSLM on speech MultiWoz task (DSTC-11 Challenge), and found that the retrieval augmentation boosts model performance, achieving joint goal accuracy (38.6% vs 32.7%), slot error rate (20.6% vs 24.8%) and ASR word error rate (5.5% vs 6.7%). While demonstrated on dialog state tracking, our approach is broadly applicable to speech tasks requiring custom contextual information or domain-specific entities.",
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2402.01828",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-02",
    "authors": [
      {
        "authorId": "2316671524",
        "name": "Mingqiu Wang"
      },
      {
        "authorId": "1697494",
        "name": "Izhak Shafran"
      },
      {
        "authorId": "38940652",
        "name": "H. Soltau"
      },
      {
        "authorId": "2264081388",
        "name": "Wei Han"
      },
      {
        "authorId": "145144022",
        "name": "Yuan Cao"
      },
      {
        "authorId": "2256337021",
        "name": "Dian Yu"
      },
      {
        "authorId": "2121764",
        "name": "Laurent El Shafey"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.53877639491068
  },
  {
    "paperId": "901be45331739bd087ee6946da9756860e7f8d96",
    "url": "https://www.semanticscholar.org/paper/901be45331739bd087ee6946da9756860e7f8d96",
    "title": "RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots",
    "abstract": "Large language models (LLMs) like ChatGPT demonstrate the remarkable progress of artificial intelligence. However, their tendency to hallucinate -- generate plausible but false information -- poses a significant challenge. This issue is critical, as seen in recent court cases where ChatGPT's use led to citations of non-existent legal rulings. This paper explores how Retrieval-Augmented Generation (RAG) can counter hallucinations by integrating external knowledge with prompts. We empirically evaluate RAG against standard LLMs using prompts designed to induce hallucinations. Our results show that RAG increases accuracy in some cases, but can still be misled when prompts directly contradict the model's pre-trained understanding. These findings highlight the complex nature of hallucinations and the need for more robust solutions to ensure LLM reliability in real-world applications. We offer practical recommendations for RAG deployment and discuss implications for the development of more trustworthy LLMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-02",
    "authors": [
      {
        "authorId": "2294586789",
        "name": "Philip Feldman"
      },
      {
        "authorId": "40289577",
        "name": "James R. Foulds"
      },
      {
        "authorId": "2257320672",
        "name": "Shimei Pan"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "08dd6f6adf5657516d53db1946780a5d31a10676",
    "url": "https://www.semanticscholar.org/paper/08dd6f6adf5657516d53db1946780a5d31a10676",
    "title": "Retrieval-Generation Synergy Augmented Large Language Models",
    "abstract": "Large language models augmented with task-relevant documents have demonstrated impressive performance on knowledge-intensive tasks. However, regarding how to obtain effective documents, the existing methods are mainly divided into two categories. One is to retrieve from an external knowledge base, and the other is to utilize large language models to generate documents. We propose an iterative retrieval-generation collaborative framework. It is not only able to leverage both parametric and non-parametric knowledge, but also helps to find the correct reasoning path through retrieval-generation interactions, which is very important for tasks that require multi-step reasoning. We conduct experiments on four question answering datasets, including single-hop QA and multi-hop QA tasks. Empirical results show that our method significantly improves the reasoning ability of large language models and outperforms previous baselines.",
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year": 2023,
    "citationCount": 23,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.05149",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-08",
    "authors": [
      {
        "authorId": "51056532",
        "name": "Zhangyin Feng"
      },
      {
        "authorId": "2674998",
        "name": "Xiaocheng Feng"
      },
      {
        "authorId": "2258097320",
        "name": "Dezhi Zhao"
      },
      {
        "authorId": "2257128653",
        "name": "Maojin Yang"
      },
      {
        "authorId": "2257004102",
        "name": "Bing Qin"
      }
    ],
    "source": "semantic_scholar",
    "score": 123.67080745521918
  },
  {
    "paperId": "ec38c6183a5b45627e248268aff7050a95313857",
    "url": "https://www.semanticscholar.org/paper/ec38c6183a5b45627e248268aff7050a95313857",
    "title": "ERAGent: Enhancing Retrieval-Augmented Language Models with Improved Accuracy, Efficiency, and Personalization",
    "abstract": "Retrieval-augmented generation (RAG) for language models significantly improves language understanding systems. The basic retrieval-then-read pipeline of response generation has evolved into a more extended process due to the integration of various components, sometimes even forming loop structures. Despite its advancements in improving response accuracy, challenges like poor retrieval quality for complex questions that require the search of multifaceted semantic information, inefficiencies in knowledge re-retrieval during long-term serving, and lack of personalized responses persist. Motivated by transcending these limitations, we introduce ERAGent, a cutting-edge framework that embodies an advancement in the RAG area. Our contribution is the introduction of the synergistically operated module: Enhanced Question Rewriter and Knowledge Filter, for better retrieval quality. Retrieval Trigger is incorporated to curtail extraneous external knowledge retrieval without sacrificing response quality. ERAGent also personalizes responses by incorporating a learned user profile. The efficiency and personalization characteristics of ERAGent are supported by the Experiential Learner module which makes the AI assistant being capable of expanding its knowledge and modeling user profile incrementally. Rigorous evaluations across six datasets and three question-answering tasks prove ERAGent's superior accuracy, efficiency, and personalization, emphasizing its potential to advance the RAG field and its applicability in practical systems.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-06",
    "authors": [
      {
        "authorId": "2290123179",
        "name": "Yunxiao Shi"
      },
      {
        "authorId": "2301152279",
        "name": "Xing Zi"
      },
      {
        "authorId": "2301176009",
        "name": "Zijing Shi"
      },
      {
        "authorId": "46701988",
        "name": "Haimin Zhang"
      },
      {
        "authorId": "2290138621",
        "name": "Qiang Wu"
      },
      {
        "authorId": "2290224625",
        "name": "Min Xu"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "5950dec8f9ae389b6998b21833f3c4755d6f1f0f",
    "url": "https://www.semanticscholar.org/paper/5950dec8f9ae389b6998b21833f3c4755d6f1f0f",
    "title": "Metacognitive Retrieval-Augmented Large Language Models",
    "abstract": "Retrieval-augmented generation have become central in natural language processing due to their efficacy in generating factual content. While traditional methods employ single-time retrieval, more recent approaches have shifted towards multi-time retrieval for multi-hop reasoning tasks. However, these strategies are bound by predefined reasoning steps, potentially leading to inaccuracies in response generation. This paper introduces MetaRAG, an approach that combines the retrieval-augmented generation process with metacognition. Drawing from cognitive psychology, metacognition allows an entity to self-reflect and critically evaluate its cognitive processes. By integrating this, MetaRAG enables the model to monitor, evaluate, and plan its response strategies, enhancing its introspective reasoning abilities. Through a three-step metacognitive regulation pipeline, the model can identify inadequacies in initial cognitive responses and fixes them. Empirical evaluations show that MetaRAG significantly outperforms existing methods.",
    "venue": "The Web Conference",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2402.11626",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2024-02-18",
    "authors": [
      {
        "authorId": "2118788278",
        "name": "Yujia Zhou"
      },
      {
        "authorId": "2284309569",
        "name": "Zheng Liu"
      },
      {
        "authorId": "4376097",
        "name": "Jiajie Jin"
      },
      {
        "authorId": "2284686381",
        "name": "Jian-Yun Nie"
      },
      {
        "authorId": "2257039188",
        "name": "Zhicheng Dou"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "b7a9754bbe631eb5431533a7ab8c7408cafa2e0a",
    "url": "https://www.semanticscholar.org/paper/b7a9754bbe631eb5431533a7ab8c7408cafa2e0a",
    "title": "BiomedRAG: A Retrieval Augmented Large Language Model for Biomedicine",
    "abstract": "Retrieval-augmented generation (RAG) involves a solution by retrieving knowledge from an established database to enhance the performance of large language models (LLM). , these models retrieve information at the sentence or paragraph level, potentially introducing noise and affecting the generation quality. To address these issues, we propose a novel BiomedRAG framework that directly feeds automatically retrieved chunk-based documents into the LLM. Our evaluation of BiomedRAG across four biomedical natural language processing tasks using eight datasets demonstrates that our proposed framework not only improves the performance by 9.95% on average, but also achieves state-of-the-art results, surpassing various baselines by 4.97%. BiomedRAG paves the way for more accurate and adaptable LLM applications in the biomedical domain.",
    "venue": "Journal of Biomedical Informatics",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-01",
    "authors": [
      {
        "authorId": "2262448361",
        "name": "Mingchen Li"
      },
      {
        "authorId": "3358452",
        "name": "H. Kilicoglu"
      },
      {
        "authorId": "2124947799",
        "name": "Hualei Xu"
      },
      {
        "authorId": "2262893164",
        "name": "Rui Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "7e55d8701785818776323b4147cb13354c820469",
    "url": "https://www.semanticscholar.org/paper/7e55d8701785818776323b4147cb13354c820469",
    "title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research",
    "abstract": "Large Language Models (LLMs) generalize well across language tasks, but suffer from hallucinations and uninterpretability, making it difficult to assess their accuracy without ground-truth. Retrieval-Augmented Generation (RAG) models have been proposed to reduce hallucinations and provide provenance for how an answer was generated. Applying such models to the scientific literature may enable large-scale, systematic processing of scientific knowledge. We present PaperQA, a RAG agent for answering questions over the scientific literature. PaperQA is an agent that performs information retrieval across full-text scientific articles, assesses the relevance of sources and passages, and uses RAG to provide answers. Viewing this agent as a question answering model, we find it exceeds performance of existing LLMs and LLM agents on current science QA benchmarks. To push the field closer to how humans perform research on scientific literature, we also introduce LitQA, a more complex benchmark that requires retrieval and synthesis of information from full-text scientific papers across the literature. Finally, we demonstrate PaperQA's matches expert human researchers on LitQA.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 48,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-08",
    "authors": [
      {
        "authorId": "2219926382",
        "name": "Jakub L'ala"
      },
      {
        "authorId": "2258961056",
        "name": "Odhran O'Donoghue"
      },
      {
        "authorId": "2258961451",
        "name": "Aleksandar Shtedritski"
      },
      {
        "authorId": "2161337138",
        "name": "Sam Cox"
      },
      {
        "authorId": "2258964497",
        "name": "Samuel G. Rodriques"
      },
      {
        "authorId": "2273941271",
        "name": "Andrew D. White"
      }
    ],
    "source": "semantic_scholar",
    "score": 128.3773044716594
  },
  {
    "paperId": "a8b66565cdb2b8c90556bb98a7fc58ac679c2cec",
    "url": "https://www.semanticscholar.org/paper/a8b66565cdb2b8c90556bb98a7fc58ac679c2cec",
    "title": "Accelerating Retrieval-Augmented Language Model Serving with Speculation",
    "abstract": "Retrieval-augmented language models (RaLM) have demonstrated the potential to solve knowledge-intensive natural language processing (NLP) tasks by combining a non-parametric knowledge base with a parametric language model. Instead of fine-tuning a fully parametric model, RaLM excels at its low-cost adaptation to the latest data and better source attribution mechanisms. Among various RaLM approaches, iterative RaLM delivers a better generation quality due to a more frequent interaction between the retriever and the language model. Despite the benefits, iterative RaLM usually encounters high overheads due to the frequent retrieval step. To this end, we propose RaLMSpec, a speculation-inspired framework that provides generic speed-up over iterative RaLM while preserving the same model outputs through speculative retrieval and batched verification. By further incorporating prefetching, optimal speculation stride scheduler, and asynchronous verification, RaLMSpec can automatically exploit the acceleration potential to the fullest. For naive iterative RaLM serving, extensive evaluations over three language models on four downstream QA datasets demonstrate that RaLMSpec can achieve a speed-up ratio of 1.75-2.39x, 1.04-1.39x, and 1.31-1.77x when the retriever is an exact dense retriever, approximate dense retriever, and sparse retriever respectively compared with the baseline. For KNN-LM serving, RaLMSpec can achieve a speed-up ratio up to 7.59x and 2.45x when the retriever is an exact dense retriever and approximate dense retriever, respectively, compared with the baseline.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-25",
    "authors": [
      {
        "authorId": "2185953295",
        "name": "Zhihao Zhang"
      },
      {
        "authorId": "2231613073",
        "name": "Alan Zhu"
      },
      {
        "authorId": "2231664587",
        "name": "Lijie Yang"
      },
      {
        "authorId": "2281208130",
        "name": "Yihua Xu"
      },
      {
        "authorId": "2281325214",
        "name": "Lanting Li"
      },
      {
        "authorId": "2623548",
        "name": "P. Phothilimthana"
      },
      {
        "authorId": "2271058444",
        "name": "Zhihao Jia"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "486bb2d71e6430688ce3a7cc2f982835b24c71e7",
    "url": "https://www.semanticscholar.org/paper/486bb2d71e6430688ce3a7cc2f982835b24c71e7",
    "title": "MeTMaP: Metamorphic Testing for Detecting False Vector Matching Problems in LLM Augmented Generation",
    "abstract": "Augmented generation techniques such as Retrieval-Augmented Generation (RAG) and Cache-Augmented Generation (CAG) have revolutionized the field by enhancing large language model (LLM) outputs with external knowledge and cached information. However, the integration of vector databases, which serve as a backbone for these augmentations, introduces critical challenges, particularly in ensuring accurate vector matching. False vector matching in these databases can significantly compromise the integrity and reliability of LLM outputs, leading to misinformation or erroneous responses. Despite the crucial impact of these issues, there is a notable research gap in methods to effectively detect and address false vector matches in LLM-augmented generation. This paper presents MeTMaP, a metamorphic testing framework developed to identify false vector matching in LLM -augmented generation systems. We derive eight metamorphic relations (MRs) from six NLP datasets, which form our method's core, based on the idea that semantically similar texts should match and dissim-ilar ones should not. MeTMaP uses these MRs to create sentence triplets for testing, simulating real-world matching scenarios. Our evaluation of MeTMaP over 203 vector matching configurations, involving 29 embedding models and 7 distance metrics, uncovers significant inaccuracies. The results, showing a maximum accuracy of only 41.51% on our tests compared to the original datasets, em-phasize the widespread issue of false matches in vector matching methods and the critical need for effective detection and mitigation in LLM -augmented applications.",
    "venue": "2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering (Forge) Conference Acronym:",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3650105.3652297",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2024-02-22",
    "authors": [
      {
        "authorId": "2285787162",
        "name": "Guanyu Wang"
      },
      {
        "authorId": "22799258",
        "name": "Yuekang Li"
      },
      {
        "authorId": "2284875313",
        "name": "Yi Liu"
      },
      {
        "authorId": "73776889",
        "name": "Gelei Deng"
      },
      {
        "authorId": "2265665080",
        "name": "Tianlin Li"
      },
      {
        "authorId": "2284983525",
        "name": "Guosheng Xu"
      },
      {
        "authorId": "2277257260",
        "name": "Yang Liu"
      },
      {
        "authorId": "2269008788",
        "name": "Haoyu Wang"
      },
      {
        "authorId": "2284035726",
        "name": "Kailong Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.19162312519754
  },
  {
    "paperId": "717d4cc5188e06791ef2043045e6e570ae764091",
    "url": "https://www.semanticscholar.org/paper/717d4cc5188e06791ef2043045e6e570ae764091",
    "title": "Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning",
    "abstract": "Augmenting pretrained language models (LMs) with a vision encoder (e.g., Flamingo) has obtained the state-of-the-art results in image-to-text generation. However, these models store all the knowledge within their parameters, thus often requiring enormous model parameters to model the abundant visual concepts and very rich textual descriptions. Additionally, they are inefficient in incorporating new data, requiring a computational-expensive fine-tuning process. In this work, we introduce a Retrieval-augmented Visual Language Model, Re-ViLM, built upon the Flamingo, that supports retrieving the relevant knowledge from the external database for zero and in-context few-shot image-to-text generations. By storing certain knowledge explicitly in the external database, our approach reduces the number of model parameters and can easily accommodate new data during evaluation by simply updating the database. We also construct an interleaved image and text data that facilitates in-context few-shot learning capabilities. We demonstrate that Re-ViLM significantly boosts performance for image-to-text generation tasks, especially for zero-shot and few-shot generation in out-of-domain settings with 4 times less parameters compared with baseline methods.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 35,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2302.04858",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-02-09",
    "authors": [
      {
        "authorId": "2168550",
        "name": "Zhuolin Yang"
      },
      {
        "authorId": "2056440915",
        "name": "Wei Ping"
      },
      {
        "authorId": "2117941142",
        "name": "Zihan Liu"
      },
      {
        "authorId": "3111334",
        "name": "V. Korthikanti"
      },
      {
        "authorId": "2066304514",
        "name": "Weili Nie"
      },
      {
        "authorId": "38485317",
        "name": "De-An Huang"
      },
      {
        "authorId": "3275727",
        "name": "Linxi (Jim) Fan"
      },
      {
        "authorId": "1751019",
        "name": "Zhiding Yu"
      },
      {
        "authorId": "8539304",
        "name": "Shiyi Lan"
      },
      {
        "authorId": "71788673",
        "name": "Bo Li"
      },
      {
        "authorId": "2135695038",
        "name": "Mingyan Liu"
      },
      {
        "authorId": "2117748",
        "name": "Yuke Zhu"
      },
      {
        "authorId": "1911755",
        "name": "Mohammad Shoeybi"
      },
      {
        "authorId": "2301680",
        "name": "Bryan Catanzaro"
      },
      {
        "authorId": "2723309",
        "name": "Chaowei Xiao"
      },
      {
        "authorId": "47627049",
        "name": "Anima Anandkumar"
      }
    ],
    "source": "semantic_scholar",
    "score": 123.75278407684165
  },
  {
    "paperId": "8c4b0f3f69e1ed36f371d91dfab7e0d9f909311c",
    "url": "https://www.semanticscholar.org/paper/8c4b0f3f69e1ed36f371d91dfab7e0d9f909311c",
    "title": "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering",
    "abstract": "Considering the limited internal parametric knowledge, retrieval-augmented generation (RAG) has been widely used to extend the knowledge scope of large language models (LLMs). Despite the extensive efforts on RAG research, in existing methods, LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents). To address this issue, in this paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for open-domain question answering (QA). As the key motivation, we aim to enhance the self-awareness regarding the reliability of external knowledge for LLMs, so as to adaptively utilize external knowledge in RAG systems. Specially, we develop a novel architecture for LLM based RAG system, by incorporating a specially designed assessnent module that precisely assesses the relevance of retrieved documents. Furthermore, we propose an improved training method based on bi-granularity relevance fusion and noise-resistant training. By combining the improvements in both architecture and training, our proposed REAR can better utilize external knowledge by effectively perceiving the relevance of retrieved documents. Experiments on four open-domain QA tasks show that REAR significantly outperforms previous a number of competitive RAG approaches. Our codes can be accessed at https://github.com/RUCAIBox/REAR.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-27",
    "authors": [
      {
        "authorId": "2198466349",
        "name": "Yuhao Wang"
      },
      {
        "authorId": "1708171825",
        "name": "Ruiyang Ren"
      },
      {
        "authorId": "2018027",
        "name": "Junyi Li"
      },
      {
        "authorId": "2257376413",
        "name": "Wayne Xin Zhao"
      },
      {
        "authorId": "2287961461",
        "name": "Jing Liu"
      },
      {
        "authorId": "2274218622",
        "name": "Ji-Rong Wen"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "503c5cb6bfb451e38c12e5c1ba41ccf844e79fa8",
    "url": "https://www.semanticscholar.org/paper/503c5cb6bfb451e38c12e5c1ba41ccf844e79fa8",
    "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
    "abstract": "Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters. This limitation poses challenges in practical applications, especially when LLMs are already deployed, as parameter adjustments may affect their original functionality. To address this, we propose a novel method that involves learning scalable and pluggable virtual tokens for RAG. By maintaining the LLMs' original parameters and fine-tuning only the embeddings of these pluggable tokens, our approach not only enhances LLMs' performance but also preserves their general generation capabilities. Furthermore, we design several training strategies to improve the scalability, flexibility, and generalizability of our method. Comprehensive experiments across 12 question-answering tasks demonstrate the superiority of our approach.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-30",
    "authors": [
      {
        "authorId": "1900406",
        "name": "Yutao Zhu"
      },
      {
        "authorId": "2187935160",
        "name": "Zhaoheng Huang"
      },
      {
        "authorId": "1897235",
        "name": "Zhicheng Dou"
      },
      {
        "authorId": "2186578511",
        "name": "Ji-Rong Wen"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "d8cbe0bfcc9c2eb1714c0e5be18a6ecaf50ea7a5",
    "url": "https://www.semanticscholar.org/paper/d8cbe0bfcc9c2eb1714c0e5be18a6ecaf50ea7a5",
    "title": "Improving Retrieval Augmented Language Model with Self-Reasoning",
    "abstract": "The Retrieval-Augmented Language Model (RALM) has shown remarkable performance on knowledge-intensive tasks by incorporating external knowledge during inference, which mitigates the factual hallucinations inherited in large language models (LLMs). Despite these advancements, challenges persist in the implementation of RALMs, particularly concerning their reliability and traceability. To be specific, the irrelevant document retrieval may result in unhelpful response generation or even deteriorate the performance of LLMs, while the lack of proper citations in generated outputs complicates efforts to verify the trustworthiness of the models. To this end, we propose a novel self-reasoning framework aimed at improving the reliability and traceability of RALMs, whose core idea is to leverage reasoning trajectories generated by the LLM itself. The framework involves constructing self-reason trajectories with three processes: a relevance-aware process, an evidence-aware selective process, and a trajectory analysis process. We have evaluated our framework across four public datasets (two short-form QA datasets, one long-form QA dataset, and one fact verification dataset) to demonstrate the superiority of our method, which can outperform existing state-of-the-art models and can achieve comparable performance with GPT-4, while only using 2,000 training samples.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-29",
    "authors": [
      {
        "authorId": "2111275391",
        "name": "Yuan Xia"
      },
      {
        "authorId": "2314078488",
        "name": "Jingbo Zhou"
      },
      {
        "authorId": "18083770",
        "name": "Zhenhui Shi"
      },
      {
        "authorId": "2153418695",
        "name": "Jun Chen"
      },
      {
        "authorId": "30898812",
        "name": "Hai-ting Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "660db1e60311d7d2a24f39d0c96fb12b69ccdcef",
    "url": "https://www.semanticscholar.org/paper/660db1e60311d7d2a24f39d0c96fb12b69ccdcef",
    "title": "Retrieval-augmented Recommender System: Enhancing Recommender Systems with Large Language Models",
    "abstract": "Recommender Systems (RSs) play a pivotal role in delivering personalized recommendations across various domains, from e-commerce to content streaming platforms. Recent advancements in natural language processing have introduced Large Language Models (LLMs) that exhibit remarkable capabilities in understanding and generating human-like text. RS are renowned for their effectiveness and proficiency within clearly defined domains; nevertheless, they are limited in adaptability and incapable of providing recommendations for unexplored data. Conversely, LLMs exhibit contextual awareness and strong adaptability to unseen data. Combining these technologies creates a powerful tool for delivering contextual and relevant recommendations, even in cold scenarios characterized by high data sparsity. The proposal aims to explore the possibilities of integrating LLMs into RS, introducing a novel approach called Retrieval-augmented Recommender Systems, which combines the strengths of retrieval-based and generation-based models to enhance the ability of RSs to provide relevant suggestions.",
    "venue": "ACM Conference on Recommender Systems",
    "year": 2023,
    "citationCount": 29,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle"
    ],
    "publicationDate": "2023-09-14",
    "authors": [
      {
        "authorId": "2240534691",
        "name": "Dario Di Palma"
      }
    ],
    "source": "semantic_scholar",
    "score": 127.01796072493232
  },
  {
    "paperId": "edeb9ec83948942b6e2b4f88ce380ff148bfbad9",
    "url": "https://www.semanticscholar.org/paper/edeb9ec83948942b6e2b4f88ce380ff148bfbad9",
    "title": "REAL: A Retrieval-Augmented Entity Linking Approach for Biomedical Concept Recognition",
    "abstract": "Large Language Models (LLMs) offer an appealing alternative to training dedicated models for many Natural Language Processing (NLP) tasks. However, outdated knowledge and hallucination issues can be major obstacles in their application in knowledge-intensive biomedical scenarios. In this study, we consider the task of biomedical concept recognition (CR) from unstructured scientific literature and explore the use of Retrieval Augmented Generation (RAG) to improve accuracy and reliability of the LLM-based biomedical CR. Our approach, named REAL (Retrieval Augmented Entity Linking), combines the generative capabilities of LLMs with curated knowledge bases to automatically annotate natural language texts with concepts from bio-ontologies. By applying REAL to benchmark corpora on phenotype concept recognition, we show its effectiveness in improving LLM-based CR performance. This research highlights the potential of combining LLMs with external knowledge sources to advance biomedical text processing.",
    "venue": "Workshop on Biomedical Natural Language Processing",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2315312730",
        "name": "Darya Shlyk"
      },
      {
        "authorId": "1778643",
        "name": "T. Groza"
      },
      {
        "authorId": "2312764542",
        "name": "Marco Mesiti"
      },
      {
        "authorId": "2315307503",
        "name": "Stefano Montanelli"
      },
      {
        "authorId": "2221100434",
        "name": "Emanuele Cavalleri"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "f462f7b44051014e90bd125c1109b3cf5ba6f1b6",
    "url": "https://www.semanticscholar.org/paper/f462f7b44051014e90bd125c1109b3cf5ba6f1b6",
    "title": "Integrating a LLaMa-based Chatbot with Augmented Retrieval Generation as a Complementary Educational Tool for High School and College Students",
    "abstract": null,
    "venue": "International Conference on Software and Data Technologies",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2311266857",
        "name": "Dar√≠o Cabezas"
      },
      {
        "authorId": "2319671110",
        "name": "Rigoberto Fonseca"
      },
      {
        "authorId": "2311043965",
        "name": "Iv√°n Reyes-Chac√≥n"
      },
      {
        "authorId": "2310988440",
        "name": "Paulina Vizca√≠no-Imaca√±a"
      },
      {
        "authorId": "1404253416",
        "name": "Manuel Eugenio Morocho-Cayamcela"
      }
    ],
    "source": "semantic_scholar",
    "score": 70.79441541679836
  },
  {
    "paperId": "a9b04034e31a8c8ec5adff3c4665eda58b1f3013",
    "url": "https://www.semanticscholar.org/paper/a9b04034e31a8c8ec5adff3c4665eda58b1f3013",
    "title": "RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering",
    "abstract": "Question answering based on retrieval augmented generation (RAG-QA) is an important research topic in NLP and has a wide range of real-world applications. However, most existing datasets for this task are either constructed using a single source corpus or consist of short extractive answers, which fall short of evaluating large language model (LLM) based RAG-QA systems on cross-domain generalization. To address these limitations, we create Long-form RobustQA (LFRQA), a new dataset comprising human-written long-form answers that integrate short extractive answers from multiple documents into a single, coherent narrative, covering 26K queries and large corpora across seven different domains. We further propose RAG-QA Arena by directly comparing model-generated answers against LFRQA‚Äôs answers using LLMs as evaluators. We show via extensive experiments that RAG-QA Arena and human judgments on answer quality are highly correlated. Moreover, only 41.3% of the most competitive LLM‚Äôs answers are preferred to LFRQA‚Äôs answers, demonstrating RAG-QA Arena as a challenging evaluation platform for future research.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-19",
    "authors": [
      {
        "authorId": "2312206172",
        "name": "Rujun Han"
      },
      {
        "authorId": "2285401140",
        "name": "Yuhao Zhang"
      },
      {
        "authorId": "50531624",
        "name": "Peng Qi"
      },
      {
        "authorId": "2312273705",
        "name": "Yumo Xu"
      },
      {
        "authorId": "2312269233",
        "name": "Jenyuan Wang"
      },
      {
        "authorId": "2197484384",
        "name": "Lan Liu"
      },
      {
        "authorId": "2312264839",
        "name": "William Yang Wang"
      },
      {
        "authorId": "2290849554",
        "name": "Bonan Min"
      },
      {
        "authorId": "2287929856",
        "name": "Vittorio Castelli"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "1551524f2aa4d16beb40fbee44397e7ac2b06bb4",
    "url": "https://www.semanticscholar.org/paper/1551524f2aa4d16beb40fbee44397e7ac2b06bb4",
    "title": "Retrieval Augmented Thought Process for Private Data Handling in Healthcare",
    "abstract": "Large Language Models (LLMs) have demonstrated the strong potential to assist both clinicians and the general public with their extensive medical knowledge. However, their application in healthcare is constrained due to concerns about the privacy of data used in training, which prevents the integration of private and personal information because of security and ethical issues. Moreover, if their capabilities can be enhanced with information retrieval to access up-to-date knowledge, the current integration of LLMs with Information retrieval lacks robustness to imperfect retrieval, which can hinder their effectiveness and even reduce overall performance. In this work, we address this challenge by introducing the Retrieval-Augmented Thought Process (RATP). Given access to external knowledge, RATP formulates the thought generation of LLMs as a multiple-step decision process. To optimise such a thought process, RATP leverages Monte-Carlo Tree Search and learns a proxy reward function that permits cost-efficient inference. On a private dataset of electronic medical records, deliberately excluded from any LLM training set, RATP achieves 35% additional accuracy compared to in-context retrieval-augmented generation for the question-answering task.",
    "venue": "",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-02-12",
    "authors": [
      {
        "authorId": "3866909",
        "name": "T. Pouplin"
      },
      {
        "authorId": "2257404641",
        "name": "Hao Sun"
      },
      {
        "authorId": "2069144895",
        "name": "Samuel Holt"
      },
      {
        "authorId": "1729969",
        "name": "M. Schaar"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "3cf513137d5aacea8dd7e06bec7a3eaed196c724",
    "url": "https://www.semanticscholar.org/paper/3cf513137d5aacea8dd7e06bec7a3eaed196c724",
    "title": "RaDA: Retrieval-augmented Web Agent Planning with LLMs",
    "abstract": "Agents powered by large language models (LLMs) inherit important limitations, such as the restricted context length, dependency on human-engineered exemplars (e.g., for task decomposition), and insufficient generalization. To address these challenges, we propose RaDA, a novel planning method for Web agents that does not require manual exemplars, efficiently leverages the LLMs‚Äô context, and enhances generalization. RaDA disentangles planning into two stages: for a new given task, during Retrieval-augmented Task Decomposition (RaD), it decomposes tasks into high-level sub-tasks; next, during Retrieval-augmented Action Generation (RaA), it traverses the trajectory obtained with RaD to iteratively synthesize actions based on dynamically retrieved exemplars. We compare RaDA with strong baselines covering a broad space of design choices, using both GPT-3.5 and GPT-4 as backbones; and we find consistent improvements over previous SOTA in two challenging benchmarks, Com-pWoB and Mind2Web, covering settings with different complexities. We show the contributions of RaDA via ablation studies and qualitative analysis; and we discuss the structural benefits of our more compositional design.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2164352899",
        "name": "Minsoo Kim"
      },
      {
        "authorId": "2957094",
        "name": "V. Bursztyn"
      },
      {
        "authorId": "2176108906",
        "name": "E. Koh"
      },
      {
        "authorId": "30518075",
        "name": "Shunan Guo"
      },
      {
        "authorId": "2274001980",
        "name": "Seung-won Hwang"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "246ac1f2db3fdbb0ad1fe442a922c6cba3490e18",
    "url": "https://www.semanticscholar.org/paper/246ac1f2db3fdbb0ad1fe442a922c6cba3490e18",
    "title": "Retrieval-augmented large language models for clinical trial screening.",
    "abstract": "e13611 Background: Clinical trial screening is currently manual and laborious. We tested several large language models enhanced with retrieval-augmented generation (RAG-LLM) to assess their performance on this task. Methods: We extracted eligibility criteria of 184 oncology trials with FDA approval notifications between 8 January 2020 and 18 January 2024, as well as information on cancer staging and performance status scoring for the RAG-LLM vector database. A medical oncologist and 2 senior clinical trial coordinators developed a test set of 975 synthetic patient profiles which included primary site, stage, prior therapy, tumor mutations and one additional clinical feature. Each profile was paired with one of the 184 trials and annotated for ground-truth eligibility and the reason for it. This process was repeated for another validation set of 240 longer and more challenging profiles paired with 8 ongoing trials. We developed RAG-LLMs with 4 leading LLMs (Zephyr-7B, Med42, GPT 3.5, GPT4) and evaluated their accuracy in determining trial eligibility as well as retrieval-augmented generation assessment (RAGAs) metrics. A response was deemed accurate only if it correctly assigned both trial eligibility and the reason for assignment. Results: GPT4 performed best and achieved an accuracy of 95.18% and 80.00% on the test and validation set respectively with a mean inference time of 10.95 seconds. It also demonstrated the highest answer relevancy, context relevancy and faithfulness (Table). Conclusions: Our results demonstrate potential for RAG-LLMs to assist with trial screening at scale. Further evaluation in real-world cohorts utilizing electronic records and full protocol data with tracking of impact on trial enrolment can be explored within secure firewalls. [Table: see text]",
    "venue": "Journal of Clinical Oncology",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-01",
    "authors": [
      {
        "authorId": "2302984076",
        "name": "R. Tan"
      },
      {
        "authorId": "2303870298",
        "name": "Si Xian Ho"
      },
      {
        "authorId": "2303954930",
        "name": "Shiyun Vivianna Fequira Oo"
      },
      {
        "authorId": "2303957625",
        "name": "Shi Ling Chua"
      },
      {
        "authorId": "79896479",
        "name": "M. Zaw"
      },
      {
        "authorId": "2248156079",
        "name": "D. S. Tan"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "a8bbe05c47bc98ad2d623690a3b2f9b2e2f6d07f",
    "url": "https://www.semanticscholar.org/paper/a8bbe05c47bc98ad2d623690a3b2f9b2e2f6d07f",
    "title": "Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning",
    "abstract": "We focus on Text-to-SQL semantic parsing from the perspective of retrieval-augmented generation. Motivated by challenges related to the size of commercial database schemata and the deployability of business intelligence solutions, we propose $\\text{ASTReS}$ that dynamically retrieves input database information and uses abstract syntax trees to select few-shot examples for in-context learning. Furthermore, we investigate the extent to which an in-parallel semantic parser can be leveraged for generating approximated versions of the expected SQL queries, to support our retrieval. We take this approach to the extreme--we adapt a model consisting of less than $500$M parameters, to act as an extremely efficient approximator, enhancing it with the ability to process schemata in a parallelised manner. We apply $\\text{ASTReS}$ to monolingual and cross-lingual benchmarks for semantic parsing, showing improvements over state-of-the-art baselines. Comprehensive experiments highlight the contribution of modules involved in this retrieval-augmented generation setting, revealing interesting directions for future work.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-03",
    "authors": [
      {
        "authorId": "2280044353",
        "name": "Zhili Shen"
      },
      {
        "authorId": "7631872",
        "name": "P. Vougiouklis"
      },
      {
        "authorId": "2280009150",
        "name": "Chenxin Diao"
      },
      {
        "authorId": "2309478564",
        "name": "Kaustubh Vyas"
      },
      {
        "authorId": "2223861984",
        "name": "Yuanyi Ji"
      },
      {
        "authorId": "2280059593",
        "name": "Jeff Z. Pan"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "d511435015370a5ec91689cbd6d1ddb1dc9da0b4",
    "url": "https://www.semanticscholar.org/paper/d511435015370a5ec91689cbd6d1ddb1dc9da0b4",
    "title": "KG-Retriever: Efficient Knowledge Indexing for Retrieval-Augmented Large Language Models",
    "abstract": "Large language models with retrieval-augmented generation encounter a pivotal challenge in intricate retrieval tasks, e.g., multi-hop question answering, which requires the model to navigate across multiple documents and generate comprehensive responses based on fragmented information. To tackle this challenge, we introduce a novel Knowledge Graph-based RAG framework with a hierarchical knowledge retriever, termed KG-Retriever. The retrieval indexing in KG-Retriever is constructed on a hierarchical index graph that consists of a knowledge graph layer and a collaborative document layer. The associative nature of graph structures is fully utilized to strengthen intra-document and inter-document connectivity, thereby fundamentally alleviating the information fragmentation problem and meanwhile improving the retrieval efficiency in cross-document retrieval of LLMs. With the coarse-grained collaborative information from neighboring documents and concise information from the knowledge graph, KG-Retriever achieves marked improvements on five public QA datasets, showing the effectiveness and efficiency of our proposed RAG framework.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-12-07",
    "authors": [
      {
        "authorId": "2334493192",
        "name": "Weijie Chen"
      },
      {
        "authorId": "2260482811",
        "name": "Ting Bai"
      },
      {
        "authorId": "2335609031",
        "name": "Jinbo Su"
      },
      {
        "authorId": "2257013742",
        "name": "Jian Luan"
      },
      {
        "authorId": "2257333016",
        "name": "Wei Liu"
      },
      {
        "authorId": "2328349476",
        "name": "Chuan Shi"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "09131311f5f87d6db153b6be6fcf0092704eac65",
    "url": "https://www.semanticscholar.org/paper/09131311f5f87d6db153b6be6fcf0092704eac65",
    "title": "Retrieval-Augmented Machine Translation with Unstructured Knowledge",
    "abstract": "Retrieval-augmented generation (RAG) introduces additional information to enhance large language models (LLMs). In machine translation (MT), previous work typically retrieves in-context examples from paired MT corpora, or domain-specific knowledge from knowledge graphs, to enhance models' MT ability. However, a large amount of world knowledge is organized in unstructured documents, and might not be fully paired across different languages. In this paper, we study retrieval-augmented MT using unstructured documents. Specifically, we build RAGtrans, the first benchmark to train and evaluate LLMs' retrieval-augmented MT ability. RAGtrans contains 79K MT samples collected via GPT-4o and human translators. Besides, documents from different languages are also provided to supply the knowledge to these samples. Based on RAGtrans, we further propose a multi-task training method to teach LLMs how to use information from multilingual documents during their translation. The method uses existing multilingual corpora to create auxiliary training objectives without additional labeling requirements. Extensive experiments show that the method improves LLMs by 1.58-3.09 BLEU and 1.00-2.03 COMET scores.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-12-05",
    "authors": [
      {
        "authorId": "2316674293",
        "name": "Jiaan Wang"
      },
      {
        "authorId": "33427918",
        "name": "Fandong Meng"
      },
      {
        "authorId": "2265687206",
        "name": "Yingxue Zhang"
      },
      {
        "authorId": "2265517237",
        "name": "Jie Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "14b588f38a3af6ef1b0186e2bb98c77d3b650093",
    "url": "https://www.semanticscholar.org/paper/14b588f38a3af6ef1b0186e2bb98c77d3b650093",
    "title": "ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator",
    "abstract": "Large language models (LLMs) are proven to benefit a lot from retrieval-augmented generation (RAG) in alleviating hallucinations confronted with knowledge-intensive questions. RAG adopts information retrieval techniques to inject external knowledge from semantic-relevant documents as input contexts. However, due to today‚Äôs Internet being flooded with numerous noisy and fabricating content, it is inevitable that RAG systems are vulnerable to these noises and prone to respond incorrectly. To this end, we propose to optimize the retrieval-augmented Generator with a Adversarial Tuning Multi-agent system **(ATM)**. The ATM steers the Generator to have a robust perspective of useful documents for question answering with the help of an auxiliary Attacker agent. The Generator and the Attacker are tuned adversarially for several iterations. After rounds of multi-agent iterative tuning, the Generator can eventually better discriminate useful documents amongst fabrications. The experimental results verify the effectiveness of ATM and we also observe that the Generator can achieve better performance compared to state-of-the-art baselines.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-28",
    "authors": [
      {
        "authorId": "2303469564",
        "name": "Junda Zhu"
      },
      {
        "authorId": "1387839383",
        "name": "Lingyong Yan"
      },
      {
        "authorId": "2185224690",
        "name": "Haibo Shi"
      },
      {
        "authorId": "2299154851",
        "name": "Dawei Yin"
      },
      {
        "authorId": "2282962405",
        "name": "Lei Sha"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "a5b9645a78720509215b773ffd81fb9fb593406e",
    "url": "https://www.semanticscholar.org/paper/a5b9645a78720509215b773ffd81fb9fb593406e",
    "title": "Addressing the Productivity Paradox in Healthcare with Retrieval Augmented Generative AI Chatbots",
    "abstract": "Artificial Intelligence (AI) is reshaping the health-care landscape through diverse innovations, personalisations and decision-making capabilities. The human-like intelligence of Generative AI has been fundamental in driving this transformation across the sector. Despite large investments and some early successes, several studies have signalled the emergence of a productivity paradox due to inherent limitations of Generative AI that disintegrate within the complexity of healthcare systems and operations. In this study, we investigate the capabilities of Retrieval Augmented Generation (RAG) and Generative AI chatbots in addressing some of these challenges. We present the design and development of a Retrieval Augmented Generative AI Chatbot framework for consultation summaries, diagnostic insights, and emotional assessments of patients. We further demonstrate the technical value of this framework in service innovation, patient engagement and workflow efficiencies that collectively move to address the productivity paradox of AI in healthcare.",
    "venue": "International Conference on Industrial Technology",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-25",
    "authors": [
      {
        "authorId": "2066910140",
        "name": "S. Ranasinghe"
      },
      {
        "authorId": "144286739",
        "name": "Daswin De Silva"
      },
      {
        "authorId": "47528865",
        "name": "Nishan Mills"
      },
      {
        "authorId": "143775049",
        "name": "D. Alahakoon"
      },
      {
        "authorId": "2185595070",
        "name": "Milos Manic"
      },
      {
        "authorId": "2304900724",
        "name": "Yen Lim"
      },
      {
        "authorId": "144399642",
        "name": "W. Ranasinghe"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "c92ebd299b8285d6fc9a14d2e1b015bd394e1701",
    "url": "https://www.semanticscholar.org/paper/c92ebd299b8285d6fc9a14d2e1b015bd394e1701",
    "title": "Meta Knowledge for Retrieval Augmented Large Language Models",
    "abstract": "Retrieval Augmented Generation (RAG) is a technique used to augment Large Language Models (LLMs) with contextually relevant, time-critical, or domain-specific information without altering the underlying model parameters. However, constructing RAG systems that can effectively synthesize information from large and diverse set of documents remains a significant challenge. We introduce a novel data-centric RAG workflow for LLMs, transforming the traditional retrieve-then-read system into a more advanced prepare-then-rewrite-then-retrieve-then-read framework, to achieve higher domain expert-level understanding of the knowledge base. Our methodology relies on generating metadata and synthetic Questions and Answers (QA) for each document, as well as introducing the new concept of Meta Knowledge Summary (MK Summary) for metadata-based clusters of documents. The proposed innovations enable personalized user-query augmentation and in-depth information retrieval across the knowledge base. Our research makes two significant contributions: using LLMs as evaluators and employing new comparative performance metrics, we demonstrate that (1) using augmented queries with synthetic question matching significantly outperforms traditional RAG pipelines that rely on document chunking (p<0.01), and (2) meta knowledge-augmented queries additionally significantly improve retrieval precision and recall, as well as the final answers breadth, depth, relevancy, and specificity. Our methodology is cost-effective, costing less than $20 per 2000 research papers using Claude 3 Haiku, and can be adapted with any fine-tuning of either the language or embedding models to further enhance the performance of end-to-end RAG pipelines.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-16",
    "authors": [
      {
        "authorId": "2287923849",
        "name": "Laurent Mombaerts"
      },
      {
        "authorId": "2316430921",
        "name": "Terry Ding"
      },
      {
        "authorId": "2316476835",
        "name": "Adi Banerjee"
      },
      {
        "authorId": "2316429297",
        "name": "Florian Felice"
      },
      {
        "authorId": "2287922211",
        "name": "Jonathan Taws"
      },
      {
        "authorId": "3322455",
        "name": "Tarik Borogovac"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "63a1617af179ee8b5b096b3038913a19166168d4",
    "url": "https://www.semanticscholar.org/paper/63a1617af179ee8b5b096b3038913a19166168d4",
    "title": "Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models",
    "abstract": "Retrieval-Augmented Generation (RAG) has been shown to enhance the factual accuracy of Large Language Models (LLMs), but existing methods often suffer from limited reasoning capabilities in effectively using the retrieved evidence, particularly when using open-source LLMs. To mitigate this gap, we introduce a novel framework, Open-RAG, designed to enhance reasoning capabilities in RAG with open-source LLMs. Our framework transforms an arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE) model capable of handling complex reasoning tasks, including both single- and multi-hop queries. Open-RAG uniquely trains the model to navigate challenging distractors that appear relevant but are misleading. As a result, Open-RAG leverages latent learning, dynamically selecting relevant experts and integrating external knowledge effectively for more accurate and contextually relevant responses. In addition, we propose a hybrid adaptive retrieval method to determine retrieval necessity and balance the trade-off between performance gain and inference speed. Experimental results show that the Llama2-7B-based Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT, Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source our code and models at https://openragmoe.github.io/",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-10-02",
    "authors": [
      {
        "authorId": "2232783785",
        "name": "Shayekh Bin Islam"
      },
      {
        "authorId": "2323863538",
        "name": "Md Asib Rahman"
      },
      {
        "authorId": "2323786676",
        "name": "K. S. M. T. Hossain"
      },
      {
        "authorId": "2274022429",
        "name": "Enamul Hoque"
      },
      {
        "authorId": "2708940",
        "name": "Shafiq R. Joty"
      },
      {
        "authorId": "3405393",
        "name": "Md. Rizwan Parvez"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "b7fb6a0653d2ddcf7114c0b691543826b05a598c",
    "url": "https://www.semanticscholar.org/paper/b7fb6a0653d2ddcf7114c0b691543826b05a598c",
    "title": "RASU: Retrieval Augmented Speech Understanding through Generative Modeling",
    "abstract": "Large language models have benefited from retrieval augmented generation (RAG) techniques, which allow relevant knowledge to be retrieved and provided as prompts to enhance natural language understanding capabilities. Extending this promising approach to spoken language understanding (SLU) tasks represents an important area of study. This paper introduces a novel RAG framework tailored for SLU called Retrieval Augmented Speech Understanding(RASU). The proposed model first employs the encoder from a pre-trained automatic speech recognition (ASR) model to retrieve relevant speech segments and transcripts from the training data given a new spoken utterance. The retrieved text transcripts and their corresponding intent labels are then formulated as prompts to conditionally guide the SLU decoder during generation. Additionally, a prompt attention mechanism is incorporated to strengthen the interaction be-tween the generated outputs and the retrieved prompts. Empirical evaluations demonstrate that RASU substantially out-performs conventional end-to-end and cascaded SLU models on intent prediction from speech data. These results highlight the efficacy of leveraging retrieval-based prompting and exter-nal knowledge sources to markedly improve spoken language understanding performance. The RASU approach presents a promising direction for advancing SLU capabilities by bridging speech retrieval and generative language modeling.",
    "venue": "Interspeech",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": "2024-09-01",
    "authors": [
      {
        "authorId": "2292942411",
        "name": "Hao Yang"
      },
      {
        "authorId": "2293704687",
        "name": "Min Zhang"
      },
      {
        "authorId": "1628331115",
        "name": "Minghan Wang"
      },
      {
        "authorId": "1838933693",
        "name": "Jiaxin Guo"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "1bab539dd0318fe446fe50574253bdf4600b112a",
    "url": "https://www.semanticscholar.org/paper/1bab539dd0318fe446fe50574253bdf4600b112a",
    "title": "On the Role of Long-tail Knowledge in Retrieval Augmented Large Language Models",
    "abstract": "Retrieval augmented generation (RAG) exhibits outstanding performance in promoting the knowledge capabilities of large language models (LLMs) with retrieved documents related to user queries. However, RAG only focuses on improving the response quality of LLMs via enhancing queries indiscriminately with retrieved information, paying little attention to what type of knowledge LLMs really need to answer original queries more accurately. In this paper, we suggest that long-tail knowledge is crucial for RAG as LLMs have already remembered common world knowledge during large-scale pre-training. Based on our observation, we propose a simple but effective long-tail knowledge detection method for LLMs. Specifically, the novel Generative Expected Calibration Error (GECE) metric is derived to measure the ``long-tailness'' of knowledge based on both statistics and semantics. Hence, we retrieve relevant documents and infuse them into the model for patching knowledge loopholes only when the input query relates to long-tail knowledge. Experiments show that, compared to existing RAG pipelines, our method achieves over 4x speedup in average inference time and consistent performance improvement in downstream tasks.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-24",
    "authors": [
      {
        "authorId": "2257089368",
        "name": "Dongyang Li"
      },
      {
        "authorId": "2243406637",
        "name": "Junbing Yan"
      },
      {
        "authorId": "2146342371",
        "name": "Taolin Zhang"
      },
      {
        "authorId": "121899912",
        "name": "Chengyu Wang"
      },
      {
        "authorId": "2257159827",
        "name": "Xiaofeng He"
      },
      {
        "authorId": "2292090586",
        "name": "Longtao Huang"
      },
      {
        "authorId": "2292128230",
        "name": "Hui Xue"
      },
      {
        "authorId": "2272790856",
        "name": "Junyuan Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "50d31439390b3853d8b36f2e2adb5d145b56f700",
    "url": "https://www.semanticscholar.org/paper/50d31439390b3853d8b36f2e2adb5d145b56f700",
    "title": "REMED: Retrieval-Augmented Medical Document Query Responding with Embedding Fine-Tuning",
    "abstract": "While advanced Large Language Models (LLMs) exhibit considerable promise, their tendency to generate unreliable information poses significant challenges, particularly in high-risk domains like healthcare. However, the advent of Retrieval-Augmented Generation (RAG) offers a novel solution tailored for the medical realm. This study further enhances retrieval accuracy by introducing REMED, a specialized medical document retrieval framework designed to address the hallucination problem prevalent in LLMs. The REMED framework integrates dataset construction, an efficient embedding fine-tuning EM-FT model, retrieval-augmented generation, and human evaluation of LLM responses. The EM-FT model can end-to-end fine-tune the medical sentence representations in large pre-trained models through an efficient embedding fine-tuning method, thereby enhancing the performance of medical retrieval. We adopt contrastive learning as the loss function to optimize the performance of the EM-FT model, enabling it to accurately capture the similarity between query and relevant documents. This approach not only improves the retrieval accuracy of positively related contents but also effectively reduces the matching with negatively related contents. Compared to direct dense vector retrieval, fine-tuning query and content vectors first and then performing dense retrieval tasks significantly improved the performance. Through validation on two datasets, we demonstrate that our EM-FT method improves recall and precision on MMD by 3.2%-6.0% and on MPD by 14.4%-42.6% compared to using the embedding model directly for retrieval. Furthermore, through human evaluation on the PULSE-7Bv5 model, we further confirm the effectiveness of our retrieval results in improving the quality of generated text.",
    "venue": "IEEE International Joint Conference on Neural Network",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-30",
    "authors": [
      {
        "authorId": "2215451922",
        "name": "Tianqi Pang"
      },
      {
        "authorId": "2216509779",
        "name": "Kehui Tan"
      },
      {
        "authorId": "2320838988",
        "name": "Yujun Yao"
      },
      {
        "authorId": "2320579279",
        "name": "Xiangyang Liu"
      },
      {
        "authorId": "2320779859",
        "name": "Fanlong Meng"
      },
      {
        "authorId": "2320813945",
        "name": "Chenyou Fan"
      },
      {
        "authorId": "2320708996",
        "name": "Xiaofan Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.39720770839918
  },
  {
    "paperId": "d083e6eded99f1345f461766a843fae9d0fee3c4",
    "url": "https://www.semanticscholar.org/paper/d083e6eded99f1345f461766a843fae9d0fee3c4",
    "title": "HijackRAG: Hijacking Attacks against Retrieval-Augmented Large Language Models",
    "abstract": "Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge, making them adaptable and cost-effective for various applications. However, the growing reliance on these systems also introduces potential security risks. In this work, we reveal a novel vulnerability, the retrieval prompt hijack attack (HijackRAG), which enables attackers to manipulate the retrieval mechanisms of RAG systems by injecting malicious texts into the knowledge database. When the RAG system encounters target questions, it generates the attacker's pre-determined answers instead of the correct ones, undermining the integrity and trustworthiness of the system. We formalize HijackRAG as an optimization problem and propose both black-box and white-box attack strategies tailored to different levels of the attacker's knowledge. Extensive experiments on multiple benchmark datasets show that HijackRAG consistently achieves high attack success rates, outperforming existing baseline attacks. Furthermore, we demonstrate that the attack is transferable across different retriever models, underscoring the widespread risk it poses to RAG systems. Lastly, our exploration of various defense mechanisms reveals that they are insufficient to counter HijackRAG, emphasizing the urgent need for more robust security measures to protect RAG systems in real-world deployments.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-30",
    "authors": [
      {
        "authorId": "2328481464",
        "name": "Yucheng Zhang"
      },
      {
        "authorId": "2297142856",
        "name": "Qinfeng Li"
      },
      {
        "authorId": "2290912812",
        "name": "Tianyu Du"
      },
      {
        "authorId": "2290975291",
        "name": "Xuhong Zhang"
      },
      {
        "authorId": "1720282",
        "name": "Xinkui Zhao"
      },
      {
        "authorId": "2328310194",
        "name": "Zhengwen Feng"
      },
      {
        "authorId": "2291142301",
        "name": "Jianwei Yin"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "e30b976d4c7d9d023dcef102a360d7305bc6a32b",
    "url": "https://www.semanticscholar.org/paper/e30b976d4c7d9d023dcef102a360d7305bc6a32b",
    "title": "Tool Calling: Enhancing Medication Consultation via Retrieval-Augmented Large Language Models",
    "abstract": "Large-scale language models (LLMs) have achieved remarkable success across various language tasks but suffer from hallucinations and temporal misalignment. To mitigate these shortcomings, Retrieval-augmented generation (RAG) has been utilized to provide external knowledge to facilitate the answer generation. However, applying such models to the medical domain faces several challenges due to the lack of domain-specific knowledge and the intricacy of real-world scenarios. In this study, we explore LLMs with RAG framework for knowledge-intensive tasks in the medical field. To evaluate the capabilities of LLMs, we introduce MedicineQA, a multi-round dialogue benchmark that simulates the real-world medication consultation scenario and requires LLMs to answer with retrieved evidence from the medicine database. MedicineQA contains 300 multi-round question-answering pairs, each embedded within a detailed dialogue history, highlighting the challenge posed by this knowledge-intensive task to current LLMs. We further propose a new \\textit{Distill-Retrieve-Read} framework instead of the previous \\textit{Retrieve-then-Read}. Specifically, the distillation and retrieval process utilizes a tool calling mechanism to formulate search queries that emulate the keyword-based inquiries used by search engines. With experimental results, we show that our framework brings notable performance improvements and surpasses the previous counterparts in the evidence retrieval process in terms of evidence retrieval accuracy. This advancement sheds light on applying RAG to the medical domain.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-27",
    "authors": [
      {
        "authorId": "2164396303",
        "name": "Zhongzhen Huang"
      },
      {
        "authorId": "145848004",
        "name": "Kui Xue"
      },
      {
        "authorId": "2298953258",
        "name": "Yongqi Fan"
      },
      {
        "authorId": "2298907225",
        "name": "Linjie Mu"
      },
      {
        "authorId": "2298908741",
        "name": "Ruoyu Liu"
      },
      {
        "authorId": "2287928656",
        "name": "Tong Ruan"
      },
      {
        "authorId": "2267720309",
        "name": "Shaoting Zhang"
      },
      {
        "authorId": "2288039293",
        "name": "Xiaofan Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "9590a540353f22865587393efabeb3a1e040aa7f",
    "url": "https://www.semanticscholar.org/paper/9590a540353f22865587393efabeb3a1e040aa7f",
    "title": "Retrieval-Augmented Audio Deepfake Detection",
    "abstract": "With recent advances in speech synthesis including text-to-speech (TTS) and voice conversion (VC) systems enabling the generation of ultra-realistic audio deepfakes, there is growing concern about their potential misuse. However, most deepfake (DF) detection methods rely solely on the fuzzy knowledge learned by a single model, resulting in performance bottlenecks and transparency issues. Inspired by retrieval-augmented generation (RAG), we propose a retrieval-augmented detection (RAD) framework that augments test samples with similar retrieved samples for enhanced detection. We also extend the multi-fusion attentive classifier to integrate it with our proposed RAD framework. Extensive experiments show the superior performance of the proposed RAD framework over baseline methods, achieving state-of-the-art results on the ASVspoof 2021 DF set and competitive results on the 2019 and 2021 LA sets. Further sample analysis indicates that the retriever consistently retrieves samples mostly from the same speaker with acoustic characteristics highly consistent with the query audio, thereby improving detection performance.",
    "venue": "International Conference on Multimedia Retrieval",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3652583.3658086",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2024-04-22",
    "authors": [
      {
        "authorId": "73816351",
        "name": "Zuheng Kang"
      },
      {
        "authorId": "2211611702",
        "name": "Yayun He"
      },
      {
        "authorId": "2299344533",
        "name": "Botao Zhao"
      },
      {
        "authorId": "2624637",
        "name": "Xiaoyang Qu"
      },
      {
        "authorId": "2116386157",
        "name": "Junqing Peng"
      },
      {
        "authorId": "91353860",
        "name": "Jing Xiao"
      },
      {
        "authorId": "66063851",
        "name": "Jianzong Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "1bce5a6d8c037a90e9cd49d38fe6446652389674",
    "url": "https://www.semanticscholar.org/paper/1bce5a6d8c037a90e9cd49d38fe6446652389674",
    "title": "Towards a Search Engine for Machines: Unified Ranking for Multiple Retrieval-Augmented Large Language Models",
    "abstract": "This paper introduces uRAG--a framework with a unified retrieval engine that serves multiple downstream retrieval-augmented generation (RAG) systems. Each RAG system consumes the retrieval results for a unique purpose, such as open-domain question answering, fact verification, entity linking, and relation extraction. We introduce a generic training guideline that standardizes the communication between the search engine and the downstream RAG systems that engage in optimizing the retrieval model. This lays the groundwork for us to build a large-scale experimentation ecosystem consisting of 18 RAG systems that engage in training and 18 unknown RAG systems that use the uRAG as the new users of the search engine. Using this experimentation ecosystem, we answer a number of fundamental research questions that improve our understanding of promises and challenges in developing search engines for machines.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-30",
    "authors": [
      {
        "authorId": "2073044451",
        "name": "Alireza Salemi"
      },
      {
        "authorId": "2295731593",
        "name": "Hamed Zamani"
      }
    ],
    "source": "semantic_scholar",
    "score": 92.47918433002164
  },
  {
    "paperId": "0113964c1dc23c494b5ff6d9cba0ccff927d4a03",
    "url": "https://www.semanticscholar.org/paper/0113964c1dc23c494b5ff6d9cba0ccff927d4a03",
    "title": "KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation",
    "abstract": "The recently developed retrieval-augmented generation (RAG) technology has enabled the efficient construction of domain-specific applications. However, it also has limitations, including the gap between vector similarity and the relevance of knowledge reasoning, as well as insensitivity to knowledge logic, such as numerical values, temporal relations, expert rules, and others, which hinder the effectiveness of professional knowledge services. In this work, we introduce a professional domain knowledge service framework called Knowledge Augmented Generation (KAG). KAG is designed to address the aforementioned challenges with the motivation of making full use of the advantages of knowledge graph(KG) and vector retrieval, and to improve generation and reasoning performance by bidirectionally enhancing large language models (LLMs) and KGs through five key aspects: (1) LLM-friendly knowledge representation, (2) mutual-indexing between knowledge graphs and original chunks, (3) logical-form-guided hybrid reasoning engine, (4) knowledge alignment with semantic reasoning, and (5) model capability enhancement for KAG. We compared KAG with existing RAG methods in multihop question answering and found that it significantly outperforms state-of-theart methods, achieving a relative improvement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We have successfully applied KAG to two professional knowledge Q&A tasks of Ant Group, including E-Government Q&A and E-Health Q&A, achieving significant improvement in professionalism compared to RAG methods.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-10",
    "authors": [
      {
        "authorId": "2303947668",
        "name": "Lei Liang"
      },
      {
        "authorId": "2273904059",
        "name": "Mengshu Sun"
      },
      {
        "authorId": "2322445265",
        "name": "Zhengke Gui"
      },
      {
        "authorId": "2278457654",
        "name": "Zhongshu Zhu"
      },
      {
        "authorId": "2305310515",
        "name": "Zhouyu Jiang"
      },
      {
        "authorId": "2305264611",
        "name": "Ling Zhong"
      },
      {
        "authorId": "2267316531",
        "name": "Yuanyi Qu"
      },
      {
        "authorId": "2322626358",
        "name": "Peilong Zhao"
      },
      {
        "authorId": "2322446289",
        "name": "Zhongpu Bo"
      },
      {
        "authorId": "2322452210",
        "name": "Jin Yang"
      },
      {
        "authorId": "2322444122",
        "name": "Huaidong Xiong"
      },
      {
        "authorId": "2286125857",
        "name": "Lin Yuan"
      },
      {
        "authorId": "2305157852",
        "name": "Jun Xu"
      },
      {
        "authorId": "2322450745",
        "name": "Zaoyang Wang"
      },
      {
        "authorId": "2266809812",
        "name": "Zhiqiang Zhang"
      },
      {
        "authorId": "2256109363",
        "name": "Wen Zhang"
      },
      {
        "authorId": "2269769030",
        "name": "Huajun Chen"
      },
      {
        "authorId": "2322606859",
        "name": "Wenguang Chen"
      },
      {
        "authorId": "2268293957",
        "name": "Jun Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "ed54c6b73440519652566d79b0f6032307ca8a83",
    "url": "https://www.semanticscholar.org/paper/ed54c6b73440519652566d79b0f6032307ca8a83",
    "title": "Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs",
    "abstract": "Event temporal relation (TempRel) is a primary subject of the event relation extraction task. However, the inherent ambiguity of TempRel increases the difficulty of the task. With the rise of prompt engineering, it is important to design effective prompt templates and verbalizers to extract relevant knowledge. The traditional manually designed templates struggle to extract precise temporal knowledge. This paper introduces a novel retrieval-augmented TempRel extraction approach, leveraging knowledge retrieved from large language models (LLMs) to enhance prompt templates and verbalizers. Our method capitalizes on the diverse capabilities of various LLMs to generate a wide array of ideas for template and verbalizer design. Our proposed method fully exploits the potential of LLMs for generation tasks and contributes more knowledge to our design. Empirical evaluations across three widely recognized datasets demonstrate the efficacy of our method in improving the performance of event temporal relation extraction tasks.",
    "venue": "IEEE International Joint Conference on Neural Network",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2403.15273",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-22",
    "authors": [
      {
        "authorId": "2214915272",
        "name": "Xiaobin Zhang"
      },
      {
        "authorId": "3162795",
        "name": "Liangjun Zang"
      },
      {
        "authorId": "2293534767",
        "name": "Qianwen Liu"
      },
      {
        "authorId": "2214860547",
        "name": "Shuchong Wei"
      },
      {
        "authorId": "2293230209",
        "name": "Songlin Hu"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.39720770839918
  },
  {
    "paperId": "55cad5d5dacbb4ac724c86ad044493333a9256c7",
    "url": "https://www.semanticscholar.org/paper/55cad5d5dacbb4ac724c86ad044493333a9256c7",
    "title": "An Interactive Multi-modal Query Answering System with Retrieval-Augmented Large Language Models",
    "abstract": "\n Retrieval-augmented Large Language Models (LLMs) have reshaped traditional query-answering systems, offering unparalleled user experiences. However, existing retrieval techniques often struggle to handle multi-modal query contexts. In this paper, we present an interactive\n M\n ulti-modal\n Q\n uery\n A\n nswering (MQA) system, empowered by our newly developed multi-modal retrieval framework and navigation graph index, integrated with cutting-edge LLMs. It comprises five core components: Data Preprocessing, Vector Representation, Index Construction, Query Execution, and Answer Generation, all orchestrated by a dedicated coordinator to ensure smooth data flow from input to answer generation. One notable aspect of MQA is its utilization of contrastive learning to assess the significance of different modalities, facilitating precise measurement of multimodal information similarity. Furthermore, the system achieves efficient retrieval through our advanced navigation graph index, refined using computational pruning techniques. Another highlight of our system is its pluggable processing framework, allowing seamless integration of embedding models, graph indexes, and LLMs. This flexibility provides users diverse options for gaining insights from their multi-modal knowledge base. A preliminary video introduction of MQA is available at https://youtu.be/xvUuo2ZIqWk.\n",
    "venue": "Proceedings of the VLDB Endowment",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-05",
    "authors": [
      {
        "authorId": "2145364798",
        "name": "Mengzhao Wang"
      },
      {
        "authorId": "2310352314",
        "name": "Haotian Wu"
      },
      {
        "authorId": "2268397595",
        "name": "Xiangyu Ke"
      },
      {
        "authorId": "2265379431",
        "name": "Yunjun Gao"
      },
      {
        "authorId": "2273601527",
        "name": "Xiaoliang Xu"
      },
      {
        "authorId": "2257413587",
        "name": "Lu Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "7581b453d9341173eb08961cc48053e57bd0cdb5",
    "url": "https://www.semanticscholar.org/paper/7581b453d9341173eb08961cc48053e57bd0cdb5",
    "title": "Supportiveness-based Knowledge Rewriting for Retrieval-augmented Language Modeling",
    "abstract": "Retrieval-augmented language models (RALMs) have recently shown great potential in mitigating the limitations of implicit knowledge in LLMs, such as untimely updating of the latest expertise and unreliable retention of long-tail knowledge. However, since the external knowledge base, as well as the retriever, can not guarantee reliability, potentially leading to the knowledge retrieved not being helpful or even misleading for LLM generation. In this paper, we introduce Supportiveness-based Knowledge Rewriting (SKR), a robust and pluggable knowledge rewriter inherently optimized for LLM generation. Specifically, we introduce the novel concept of\"supportiveness\"--which represents how effectively a knowledge piece facilitates downstream tasks--by considering the perplexity impact of augmented knowledge on the response text of a white-box LLM. Based on knowledge supportiveness, we first design a training data curation strategy for our rewriter model, effectively identifying and filtering out poor or irrelevant rewrites (e.g., with low supportiveness scores) to improve data efficacy. We then introduce the direct preference optimization (DPO) algorithm to align the generated rewrites to optimal supportiveness, guiding the rewriter model to summarize augmented content that better improves the final response. Comprehensive evaluations across six popular knowledge-intensive tasks and four LLMs have demonstrated the effectiveness and superiority of SKR. With only 7B parameters, SKR has shown better knowledge rewriting capability over GPT-4, the current state-of-the-art general-purpose LLM.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-12",
    "authors": [
      {
        "authorId": "2184030467",
        "name": "Zile Qiao"
      },
      {
        "authorId": "145235149",
        "name": "Wei Ye"
      },
      {
        "authorId": "2256747040",
        "name": "Yong Jiang"
      },
      {
        "authorId": "2187454345",
        "name": "Tong Mo"
      },
      {
        "authorId": "35930962",
        "name": "Pengjun Xie"
      },
      {
        "authorId": "2139261376",
        "name": "Weiping Li"
      },
      {
        "authorId": "2276428076",
        "name": "Fei Huang"
      },
      {
        "authorId": "1705434",
        "name": "Shikun Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "0f4508ce635e816f54385555d75b585924ccc42b",
    "url": "https://www.semanticscholar.org/paper/0f4508ce635e816f54385555d75b585924ccc42b",
    "title": "Understanding Retrieval Robustness for Retrieval-Augmented Image Captioning",
    "abstract": "Recent advances in retrieval-augmented models for image captioning highlight the benefit of retrieving related captions for efficient, lightweight models with strong domain-transfer capabilities. While these models demonstrate the success of retrieval augmentation, retrieval models are still far from perfect in practice: the retrieved information can sometimes mislead the model, resulting in incorrect generation and worse performance. In this paper, we analyze the robustness of a retrieval-augmented captioning model SmallCap. Our analysis shows that the model is sensitive to tokens that appear in the majority of the retrieved captions, and the input attribution shows that those tokens are likely copied into the generated output. Given these findings, we propose to train the model by sampling retrieved captions from more diverse sets. This decreases the chance that the model learns to copy majority tokens, and improves both in-domain and cross-domain performance.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-04",
    "authors": [
      {
        "authorId": "2283824846",
        "name": "Wenyan Li"
      },
      {
        "authorId": "2261336713",
        "name": "Jiaang Li"
      },
      {
        "authorId": "1390025466",
        "name": "R. Ramos"
      },
      {
        "authorId": "2304553733",
        "name": "Raphael Tang"
      },
      {
        "authorId": "2304552471",
        "name": "Desmond Elliott"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "6cb1635f418e16314177fa85834ff3cec75d0a92",
    "url": "https://www.semanticscholar.org/paper/6cb1635f418e16314177fa85834ff3cec75d0a92",
    "title": "Generating Persona-Aware Empathetic Responses with Retrieval-Augmented Prompt Learning",
    "abstract": "Empathetic response generation requires perceiving and understanding the user‚Äôs emotion to deliver suitable responses. However, existing models generally lack an ability to respond in a persona-specific way, which has been shown to play a vital role in expressing appropriate empathy. To address this problem, we propose a novel Transformer-based architecture that incorporates retrieval-augmented prompt learning to generate persona-aware empathetic responses. Since personalized emotional resonance is subtle and uncontrollable, we employ dense passage retrieval to retrieve exemplary responses that reflect specific persona and context characteristics to cue the generative model on signaling empathy. Extensive experiments confirm the effectiveness of our model for persona-aware empathetic response generation.",
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-14",
    "authors": [
      {
        "authorId": "2292892379",
        "name": "Zhengjie Huang"
      },
      {
        "authorId": "47478650",
        "name": "Pingsheng Liu"
      },
      {
        "authorId": "2275242989",
        "name": "Gerard de Melo"
      },
      {
        "authorId": "2275985730",
        "name": "Liang He"
      },
      {
        "authorId": "2285613980",
        "name": "Linlin Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 92.47918433002164
  },
  {
    "paperId": "9749fe145671dc64705d7b9a3727fa67fac052b1",
    "url": "https://www.semanticscholar.org/paper/9749fe145671dc64705d7b9a3727fa67fac052b1",
    "title": "RADCoT: Retrieval-Augmented Distillation to Specialization Models for Generating Chain-of-Thoughts in Query Expansion",
    "abstract": "Large language models (LLMs) have demonstrated superior performance to that of small language models (SLM) in information retrieval for various subtasks including dense retrieval, reranking, query expansion, and pseudo-document generation. However, the parameter sizes of LLMs are extremely large, making it expensive to operate LLMs stably for providing LLM-based retrieval services. Recently, retrieval-augmented language models have been widely employed to significantly reduce the parameter size by retrieving relevant knowledge from large-scale corpora and exploiting the resulting ‚Äúin-context‚Äù knowledge as additional model input, thereby substantially reducing the burden of internalizing and retaining world knowledge in model parameters. Armed by the retrieval-augmented language models, we present a retrieval-augmented model specialization that distills the capability of LLMs to generate the chain-of-thoughts (CoT) for query expansion ‚Äì that is, injects the LLM‚Äôs capability to generate CoT into a retrieval-augmented SLM ‚Äì referred to as RADCoT. Experimental results on the MS-MARCO, TREC DL 19, 20 datasets show that RADCoT yields consistent improvements over distillation without retrieval, achieving comparable performance to that of the query expansion method using LLM-based CoTs. Our code is publicly available at https://github.com/ZIZUN/RADCoT.",
    "venue": "International Conference on Language Resources and Evaluation",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2144479686",
        "name": "Sung-min Lee"
      },
      {
        "authorId": "2165324756",
        "name": "Eunhwan Park"
      },
      {
        "authorId": "2261572149",
        "name": "Donghyeon Jeon"
      },
      {
        "authorId": "2261574115",
        "name": "Inho Kang"
      },
      {
        "authorId": "2261567172",
        "name": "Seung-Hoon Na"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "5d857d2c90d430c71a2673cc7d01e15fcd8c2e3d",
    "url": "https://www.semanticscholar.org/paper/5d857d2c90d430c71a2673cc7d01e15fcd8c2e3d",
    "title": "Evaluation of Semantic Search and its Role in Retrieved-Augmented-Generation (RAG) for Arabic Language",
    "abstract": "The latest advancements in machine learning and deep learning have brought forth the concept of semantic similarity, which has proven immensely beneficial in multiple applications and has largely replaced keyword search. However, evaluating semantic similarity and conducting searches for a specific query across various documents continue to be a complicated task. This complexity is due to the multifaceted nature of the task, the lack of standard benchmarks, whereas these challenges are further amplified for Arabic language. This paper endeavors to establish a straightforward yet potent benchmark for semantic search in Arabic. Moreover, to precisely evaluate the effectiveness of these metrics and the dataset, we conduct our assessment of semantic search within the framework of retrieval augmented generation (RAG).",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-27",
    "authors": [
      {
        "authorId": "2290185598",
        "name": "Ali Mahboub"
      },
      {
        "authorId": "1413338618",
        "name": "Muhy Eddin Za'ter"
      },
      {
        "authorId": "2290183306",
        "name": "Bashar Alfrou"
      },
      {
        "authorId": "2290183375",
        "name": "Yazan Estaitia"
      },
      {
        "authorId": "2290185563",
        "name": "Adnan Jaljuli"
      },
      {
        "authorId": "2290185070",
        "name": "Asma Hakouz"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "729199093cc98a3705867abdb3cad55ce194efb9",
    "url": "https://www.semanticscholar.org/paper/729199093cc98a3705867abdb3cad55ce194efb9",
    "title": "Reverse Image Retrieval Cues Parametric Memory in Multimodal LLMs",
    "abstract": "Despite impressive advances in recent multimodal large language models (MLLMs), state-of-the-art models such as from the GPT-4 suite still struggle with knowledge-intensive tasks. To address this, we consider Reverse Image Retrieval (RIR) augmented generation, a simple yet effective strategy to augment MLLMs with web-scale reverse image search results. RIR robustly improves knowledge-intensive visual question answering (VQA) of GPT-4V by 37-43%, GPT-4 Turbo by 25-27%, and GPT-4o by 18-20% in terms of open-ended VQA evaluation metrics. To our surprise, we discover that RIR helps the model to better access its own world knowledge. Concretely, our experiments suggest that RIR augmentation helps by providing further visual and textual cues without necessarily containing the direct answer to a query. In addition, we elucidate cases in which RIR can hurt performance and conduct a human evaluation. Finally, we find that the overall advantage of using RIR makes it difficult for an agent that can choose to use RIR to perform better than an approach where RIR is the default setting.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-29",
    "authors": [
      {
        "authorId": "2303805050",
        "name": "Jialiang Xu"
      },
      {
        "authorId": "2303651689",
        "name": "Michael Moor"
      },
      {
        "authorId": "1702139",
        "name": "J. Leskovec"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "f89ed27318cb930ae884af0c62be37f0355571b5",
    "url": "https://www.semanticscholar.org/paper/f89ed27318cb930ae884af0c62be37f0355571b5",
    "title": "RAGAR, Your Falsehood Radar: RAG-Augmented Reasoning for Political Fact-Checking using Multimodal Large Language Models",
    "abstract": "The escalating challenge of misinformation, particularly in political discourse, requires advanced fact-checking solutions; this is even clearer in the more complex scenario of multimodal claims. We tackle this issue using a multimodal large language model in conjunction with retrieval-augmented generation (RAG), and introduce two novel reasoning techniques: Chain of RAG (CoRAG) and Tree of RAG (ToRAG). They fact-check multimodal claims by extracting both textual and image content, retrieving external information, and reasoning subsequent questions to be answered based on prior evidence. We achieve a weighted F1-score of 0.85, surpassing a baseline reasoning technique by 0.14 points. Human evaluation confirms that the vast majority of our generated fact-check explanations contain all information from gold standard data.",
    "venue": "FEVER",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-18",
    "authors": [
      {
        "authorId": "2188157398",
        "name": "M. A. Khaliq"
      },
      {
        "authorId": "2297301958",
        "name": "P. Chang"
      },
      {
        "authorId": "2297666559",
        "name": "M. Ma"
      },
      {
        "authorId": "2297189460",
        "name": "B. Pflugfelder"
      },
      {
        "authorId": "2297189457",
        "name": "F. Mileti'c"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "0a4c15f50d720602524230b7ae311fc2e7e23e90",
    "url": "https://www.semanticscholar.org/paper/0a4c15f50d720602524230b7ae311fc2e7e23e90",
    "title": "Demo: Soccer Information Retrieval via Natural Queries using SoccerRAG",
    "abstract": "The rapid evolution of digital sports media necessitates sophisticated information retrieval systems that can efficiently parse extensive multimodal datasets. This paper demonstrates SoccerRAG, an innovative framework designed to harness the power of Retrieval Augmented Generation (RAG) and Large Language Models (LLMs) to extract soccer-related information through natural language queries. By leveraging a multimodal dataset, SoccerRAG supports dynamic querying and automatic data validation, enhancing user interaction and accessibility to sports archives. We present a novel interactive user interface (UI) based on the Chainlit framework which wraps around the core functionality, and enable users to interact with the SoccerRAG framework in a chatbot-like visual manner.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-03",
    "authors": [
      {
        "authorId": "2304447340",
        "name": "Aleksander Theo Strand"
      },
      {
        "authorId": "72173969",
        "name": "Sushant Gautam"
      },
      {
        "authorId": "2301155271",
        "name": "Cise Midoglu"
      },
      {
        "authorId": "153100518",
        "name": "Paal Halvorsen"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "4060f651ed012e26b4424a9b59157f60534f6051",
    "url": "https://www.semanticscholar.org/paper/4060f651ed012e26b4424a9b59157f60534f6051",
    "title": "Customising generative AI: Harnessing document retrieval and fine-tuning alternatives for dynamic marketing insights",
    "abstract": "This study delves into the transformative impact of leveraging large language models (LLMs) in marketing analytics, particularly emphasising a paradigm shift from fine-tuning models to the strategic application of document retrieval techniques and more. Focusing on innovative methods, such as retrieval augmented generation and low-rank adaptation, the paper explores how marketers can now activate against vast and unstructured datasets, such as call centre transcripts, unlocking valuable insights that were previously overlooked. By harnessing the power of document retrieval and adaptation, marketers can bring their data to life, enabling a more nuanced and adaptive approach to understanding consumer behaviour and preferences. This research contributes to the evolving landscape of applied marketing analytics by demonstrating the efficacy of document retrieval in enhancing the utilisation of LLMs for dynamic and data-driven marketing strategies.",
    "venue": "Applied marketing analytics",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-01",
    "authors": [
      {
        "authorId": "2313217778",
        "name": "Dakota Crisp"
      },
      {
        "authorId": "2313217748",
        "name": "Jacob Newsted"
      },
      {
        "authorId": "2313217794",
        "name": "Brendon Kirouac"
      },
      {
        "authorId": "2313222529",
        "name": "Danielle Barnes"
      },
      {
        "authorId": "2313125600",
        "name": "Catherine Hayes"
      },
      {
        "authorId": "1394484535",
        "name": "Jonathan Prantner"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "894d7eeb47e3bce415137f680ba29e9fa9c98b32",
    "url": "https://www.semanticscholar.org/paper/894d7eeb47e3bce415137f680ba29e9fa9c98b32",
    "title": "HIRO: Hierarchical Information Retrieval Optimization",
    "abstract": "Retrieval-Augmented Generation (RAG) has revolutionized natural language processing by dynamically integrating external knowledge into Large Language Models (LLMs), addressing their limitation of static training datasets. Recent implementations of RAG leverage hierarchical data structures, which organize documents at various levels of summarization and information density. This complexity, however, can cause LLMs to\"choke\"on information overload, necessitating more sophisticated querying mechanisms. In this context, we introduce Hierarchical Information Retrieval Optimization (HIRO), a novel querying approach that employs a Depth-First Search (DFS)-based recursive similarity score calculation and branch pruning. This method uniquely minimizes the context delivered to the LLM without informational loss, effectively managing the challenge of excessive data. HIRO's refined approach is validated by a 10.85% improvement in performance on the NarrativeQA dataset.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-14",
    "authors": [
      {
        "authorId": "2306783450",
        "name": "Krish Goel"
      },
      {
        "authorId": "2306782235",
        "name": "Mahek Chandak"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "b7102e8534fec95fde5c6e5ceba278a75758b7cf",
    "url": "https://www.semanticscholar.org/paper/b7102e8534fec95fde5c6e5ceba278a75758b7cf",
    "title": "Improve Dense Passage Retrieval with Entailment Tuning",
    "abstract": "Retrieval module can be plugged into many downstream NLP tasks to improve their performance, such as open-domain question answering and retrieval-augmented generation. The key to a retrieval system is to calculate relevance scores to query and passage pairs. However, the definition of relevance is often ambiguous. We observed that a major class of relevance aligns with the concept of entailment in NLI tasks. Based on this observation, we designed a method called entailment tuning to improve the embedding of dense retrievers. Specifically, we unify the form of retrieval data and NLI data using existence claim as a bridge. Then, we train retrievers to predict the claims entailed in a passage with a variant task of masked prediction. Our method can be efficiently plugged into current dense retrieval methods, and experiments show the effectiveness of our method.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-10-21",
    "authors": [
      {
        "authorId": "2247963050",
        "name": "Lu Dai"
      },
      {
        "authorId": "2247930299",
        "name": "Hao Liu"
      },
      {
        "authorId": "2247992230",
        "name": "Hui Xiong"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "e9a59f0319cd01cefc651d145990ef012f5b473f",
    "url": "https://www.semanticscholar.org/paper/e9a59f0319cd01cefc651d145990ef012f5b473f",
    "title": "Probing-RAG: Self-Probing to Guide Language Models in Selective Document Retrieval",
    "abstract": "Retrieval-Augmented Generation (RAG) enhances language models by retrieving and incorporating relevant external knowledge. However, traditional retrieve-and-generate processes may not be optimized for real-world scenarios, where queries might require multiple retrieval steps or none at all. In this paper, we propose a Probing-RAG, which utilizes the hidden state representations from the intermediate layers of language models to adaptively determine the necessity of additional retrievals for a given query. By employing a pre-trained prober, Probing-RAG effectively captures the model's internal cognition, enabling reliable decision-making about retrieving external documents. Experimental results across five open-domain QA datasets demonstrate that Probing-RAG outperforms previous methods while reducing the number of redundant retrieval steps.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-17",
    "authors": [
      {
        "authorId": "2311698905",
        "name": "Ingeol Baek"
      },
      {
        "authorId": "2326442662",
        "name": "Hwan Chang"
      },
      {
        "authorId": "2297374435",
        "name": "Byeongjeong Kim"
      },
      {
        "authorId": "2311822003",
        "name": "Jimin Lee"
      },
      {
        "authorId": "2311744808",
        "name": "Hwanhee Lee"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "3ddcd4d50468c9ede8887b8bf27898247128b3f9",
    "url": "https://www.semanticscholar.org/paper/3ddcd4d50468c9ede8887b8bf27898247128b3f9",
    "title": "Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search",
    "abstract": "In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy across sparse (up to 35.7%), zero-shot dense (up to 27.6%), and fine-tuned dense (up to 23.6%) retrieval settings in diverse search scenarios. To further elucidate the advantages of ReCo and stimulate research in code style normalization, we introduce Code Style Similarity, the first metric tailored to quantify stylistic similarities in code. Notably, our empirical findings reveal the inadequacy of existing metrics in capturing stylistic nuances. The source code and data are available at \\url{https://github.com/Alex-HaochenLi/ReCo}.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-01-09",
    "authors": [
      {
        "authorId": "2188740060",
        "name": "Haochen Li"
      },
      {
        "authorId": "2257593840",
        "name": "Xin Zhou"
      },
      {
        "authorId": "2261928516",
        "name": "Zhiqi Shen"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "9733ac3e100a71ed5019986efcb0538c3f081f87",
    "url": "https://www.semanticscholar.org/paper/9733ac3e100a71ed5019986efcb0538c3f081f87",
    "title": "UniRAG: Universal Retrieval Augmentation for Multi-Modal Large Language Models",
    "abstract": "Recently, Multi-Modal (MM) Large Language Models (LLMs) have unlocked many complex use-cases that require MM understanding (e.g., image captioning or visual question answering) and MM generation (e.g., text-guided image generation or editing) capabilities. To further improve the output fidelity of MM-LLMs we introduce UniRAG, a plug-and-play technique that adds relevant retrieved information to prompts as few-shot examples during inference. Unlike the common belief that Retrieval Augmentation (RA) mainly improves generation or understanding of uncommon entities, our evaluation results on the MSCOCO dataset with common entities show that both proprietary models like GPT-4o and Gemini-Pro and smaller open-source models like LLaVA, LaVIT, and Emu2 significantly enhance their generation quality when their input prompts are augmented with relevant information retrieved by MM retrievers like UniIR models.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-16",
    "authors": [
      {
        "authorId": "71076877",
        "name": "Sahel Sharifymoghaddam"
      },
      {
        "authorId": "2300284044",
        "name": "Shivani Upadhyay"
      },
      {
        "authorId": "2301504205",
        "name": "Wenhu Chen"
      },
      {
        "authorId": "2300329796",
        "name": "Jimmy Lin"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "1fd30d076074cb605587ac56e7524f79094d702c",
    "url": "https://www.semanticscholar.org/paper/1fd30d076074cb605587ac56e7524f79094d702c",
    "title": "Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning",
    "abstract": "Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, illustrating its broad applicability and efficiency.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-19",
    "authors": [
      {
        "authorId": "2108795448",
        "name": "Mingtian Zhang"
      },
      {
        "authorId": "2284682723",
        "name": "Shawn Lan"
      },
      {
        "authorId": "2067492948",
        "name": "Peter Hayes"
      },
      {
        "authorId": "2282542157",
        "name": "David Barber"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "fb4f4c64d7ed2b554dd5c45372b71871f38a1871",
    "url": "https://www.semanticscholar.org/paper/fb4f4c64d7ed2b554dd5c45372b71871f38a1871",
    "title": "MaCSC: Towards Multimodal-augmented Pre-trained Language Models via Conceptual Prototypes and Self-balancing Calibration",
    "abstract": "Pre-trained language models (PLMs) that rely solely on textual data may exhibit limitations in multimodal semantics comprehension. Existing solutions attempt to alleviate this issue by incorporating explicit image retrieval or generation techniques.However, these methods: (1) focus exclusively on the static image modality; (2) inevitably encounter modality gaps and noise; (3) indiscriminately treat all modalities.In this paper, we propose a novel multimodal-augmented framework termed MaCSC, which can infuse multimodal semantics into PLMs and facilitate a self-balancing calibration of information allocation.Specifically, MaCSC obtains modal-specific conceptual prototypes from contrastive pre-training models (e.g., CLIP),and aggregates the intra- and inter-modal semantics of the conceptual prototype to enhance PLMs.In addition, we utilize a novel self-balancing contrastive loss to achieve multi-scale self-balancing calibration of multimodal information during fine-tuning PLMs.Experimental results show that MaCSC consistently improves the performance of PLMs across various architectures and scales, and outperforms competitive baselines on multiple NLP tasks.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2293439758",
        "name": "Xianwei Zhuang"
      },
      {
        "authorId": "2301645596",
        "name": "Zhichang Wang"
      },
      {
        "authorId": null,
        "name": "Xuxin Cheng"
      },
      {
        "authorId": "2306871988",
        "name": "Yuxin Xie"
      },
      {
        "authorId": "2307892931",
        "name": "Liming Liang"
      },
      {
        "authorId": "2260859476",
        "name": "Yuexian Zou"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "d13d3c90274c2df96404fd55e84f73cbfbdd14f8",
    "url": "https://www.semanticscholar.org/paper/d13d3c90274c2df96404fd55e84f73cbfbdd14f8",
    "title": "GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval",
    "abstract": "Given a query and a document corpus, the information retrieval (IR) task is to output a ranked list of relevant documents. Combining large language models (LLMs) with embedding-based retrieval models, recent work shows promising results on the zero-shot retrieval problem, i.e., no access to labeled data from the target domain. Two such popular paradigms are generation-augmented retrieval or GAR (generate additional context for the query and then retrieve), and retrieval-augmented generation or RAG (retrieve relevant documents as context and then generate answers). The success of these paradigms hinges on (i) high-recall retrieval models, which are difficult to obtain in the zero-shot setting, and (ii) high-precision (re-)ranking models which typically need a good initialization. In this work, we propose a novel GAR-meets-RAG recurrence formulation that overcomes the challenges of existing paradigms. Our method iteratively improves retrieval (via GAR) and rewrite (via RAG) stages in the zero-shot setting. A key design principle is that the rewrite-retrieval stages improve the recall of the system and a final re-ranking stage improves the precision. We conduct extensive experiments on zero-shot passage retrieval benchmarks, BEIR and TREC-DL. Our method establishes a new state-of-the-art in the BEIR benchmark, outperforming previous best results in Recall@100 and nDCG@10 metrics on 6 out of 8 datasets, with up to 17% relative gains over the previous best.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 15,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-31",
    "authors": [
      {
        "authorId": "2138481119",
        "name": "Daman Arora"
      },
      {
        "authorId": "2257976984",
        "name": "Anush Kini"
      },
      {
        "authorId": "9575953",
        "name": "Sayak Ray Chowdhury"
      },
      {
        "authorId": "2257984983",
        "name": "Nagarajan Natarajan"
      },
      {
        "authorId": "2264187048",
        "name": "Gaurav Sinha"
      },
      {
        "authorId": "2264461062",
        "name": "Amit Sharma"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.58883083359672
  },
  {
    "paperId": "ec94157ecd55482ea4b7d66016593e3072e671f6",
    "url": "https://www.semanticscholar.org/paper/ec94157ecd55482ea4b7d66016593e3072e671f6",
    "title": "UniGen: A Unified Framework for Textual Dataset Generation Using Large Language Models",
    "abstract": "Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly impacted various fields by enabling high-quality synthetic data generation and reducing dependence on expensive human-generated datasets. Despite this, challenges remain in the areas of generalization, controllability, diversity, and truthfulness within the existing generative frameworks. To address these challenges, this paper presents UniGen, a comprehensive LLM-powered framework designed to produce diverse, accurate, and highly controllable datasets. UniGen is adaptable, supporting all types of text datasets and enhancing the generative process through innovative mechanisms. To augment data diversity, UniGen incorporates an attribute-guided generation module and a group checking feature. For accuracy, it employs a code-based mathematical assessment for label verification alongside a retrieval-augmented generation technique for factual validation. The framework also allows for user-specified constraints, enabling customization of the data generation process to suit particular requirements. Extensive experiments demonstrate the superior quality of data generated by UniGen, and each module within UniGen plays a critical role in this enhancement. Additionally, UniGen is applied in two practical scenarios: benchmarking LLMs and data augmentation. The results indicate that UniGen effectively supports dynamic and evolving benchmarking, and that data augmentation improves LLM capabilities in various domains, including agent-oriented abilities and reasoning skills.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-27",
    "authors": [
      {
        "authorId": "2254867423",
        "name": "Siyuan Wu"
      },
      {
        "authorId": "2257084278",
        "name": "Yue Huang"
      },
      {
        "authorId": "2279094112",
        "name": "Chujie Gao"
      },
      {
        "authorId": "2279219833",
        "name": "Dongping Chen"
      },
      {
        "authorId": "46324457",
        "name": "Qihui Zhang"
      },
      {
        "authorId": "2254266993",
        "name": "Yao Wan"
      },
      {
        "authorId": "2309119920",
        "name": "Tianyi Zhou"
      },
      {
        "authorId": "2307963162",
        "name": "Xiangliang Zhang"
      },
      {
        "authorId": "2288029761",
        "name": "Jianfeng Gao"
      },
      {
        "authorId": "2256992325",
        "name": "Chaowei Xiao"
      },
      {
        "authorId": "2257131651",
        "name": "Lichao Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "e3e13a31a839912aa8d0b62bbcd9906ff49512c9",
    "url": "https://www.semanticscholar.org/paper/e3e13a31a839912aa8d0b62bbcd9906ff49512c9",
    "title": "RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework",
    "abstract": "Retrieval-Augmented Generation (RAG) is a powerful approach that enables large language models (LLMs) to incorporate external knowledge. However, evaluating the effectiveness of RAG systems in specialized scenarios remains challenging due to the high costs of data construction and the lack of suitable evaluation metrics. This paper introduces RAGEval, a framework designed to assess RAG systems across diverse scenarios by generating high-quality documents, questions, answers, and references through a schema-based pipeline. With a focus on factual accuracy, we propose three novel metrics Completeness, Hallucination, and Irrelevance to rigorously evaluate LLM-generated responses. Experimental results show that RAGEval outperforms zero-shot and one-shot methods in terms of clarity, safety, conformity, and richness of generated samples. Furthermore, the use of LLMs for scoring the proposed metrics demonstrates a high level of consistency with human evaluations. RAGEval establishes a new paradigm for evaluating RAG systems in real-world applications.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-02",
    "authors": [
      {
        "authorId": "2214586034",
        "name": "Kunlun Zhu"
      },
      {
        "authorId": "2314833035",
        "name": "Yifan Luo"
      },
      {
        "authorId": "2314779448",
        "name": "Dingling Xu"
      },
      {
        "authorId": "2314784069",
        "name": "Ruobing Wang"
      },
      {
        "authorId": "2314785970",
        "name": "Shi Yu"
      },
      {
        "authorId": "2267033597",
        "name": "Shuo Wang"
      },
      {
        "authorId": "2277242040",
        "name": "Yukun Yan"
      },
      {
        "authorId": "49047064",
        "name": "Zhenghao Liu"
      },
      {
        "authorId": "48506411",
        "name": "Xu Han"
      },
      {
        "authorId": "2301534001",
        "name": "Zhiyuan Liu"
      },
      {
        "authorId": "2273551430",
        "name": "Maosong Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "e876f221c3eaf542d0d79f5f1f5c2f51ad2d0a48",
    "url": "https://www.semanticscholar.org/paper/e876f221c3eaf542d0d79f5f1f5c2f51ad2d0a48",
    "title": "Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation",
    "abstract": "The rapid development of large language models has led to the widespread adoption of Retrieval-Augmented Generation (RAG), which integrates external knowledge to alleviate knowledge bottlenecks and mitigate hallucinations. However, the existing RAG paradigm inevitably suffers from the impact of flawed information introduced during the retrieval phrase, thereby diminishing the reliability and correctness of the generated outcomes. In this paper, we propose Credibility-aware Generation (CAG), a universally applicable framework designed to mitigate the impact of flawed information in RAG. At its core, CAG aims to equip models with the ability to discern and process information based on its credibility. To this end, we propose an innovative data transformation framework that generates data based on credibility, thereby effectively endowing models with the capability of CAG. Furthermore, to accurately evaluate the models‚Äô capabilities of CAG, we construct a comprehensive benchmark covering three critical real-world scenarios. Experimental results demonstrate that our model can effectively understand and employ credibility for generation, significantly outperform other models with retrieval augmentation, and exhibit robustness despite the increasing noise in the context.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-10",
    "authors": [
      {
        "authorId": "2280143071",
        "name": "Ruotong Pan"
      },
      {
        "authorId": "2113252896",
        "name": "Boxi Cao"
      },
      {
        "authorId": "2116455765",
        "name": "Hongyu Lin"
      },
      {
        "authorId": "2118233348",
        "name": "Xianpei Han"
      },
      {
        "authorId": "2118455467",
        "name": "Jia Zheng"
      },
      {
        "authorId": "2295934207",
        "name": "Sirui Wang"
      },
      {
        "authorId": "2290035990",
        "name": "Xunliang Cai"
      },
      {
        "authorId": "2110832778",
        "name": "Le Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "15dd9ff6a609fba951e451664aa6024471282d45",
    "url": "https://www.semanticscholar.org/paper/15dd9ff6a609fba951e451664aa6024471282d45",
    "title": "JustiLM: Few-shot Justification Generation for Explainable Fact-Checking of Real-world Claims",
    "abstract": "Justification is an explanation that supports the veracity assigned to a claim in fact-checking. However, the task of justification generation has been previously oversimplified as summarization of a fact-check article authored by fact-checkers. Therefore, we propose a realistic approach to generate justification based on retrieved evidence. We present a new benchmark dataset called ExClaim (for Explainable fact-checking of real-world Claims), and introduce JustiLM, a novel few-shot Justification generation based on retrieval-augmented Language Model by using fact-check articles as an auxiliary resource during training only. Experiments show that JustiLM achieves promising performance in justification generation compared to strong baselines, and can also enhance veracity classification with a straightforward extension.1 Code and dataset are released at https://github.com/znhy1024/JustiLM.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00649/2362186/tacl_a_00649.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-16",
    "authors": [
      {
        "authorId": "1490766549",
        "name": "Fengzhu Zeng"
      },
      {
        "authorId": "2279834245",
        "name": "Wei Gao"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "d4483d3380099de6faa746342c828bfdb8456a81",
    "url": "https://www.semanticscholar.org/paper/d4483d3380099de6faa746342c828bfdb8456a81",
    "title": "Retrieval-free Knowledge Injection through Multi-Document Traversal for Dialogue Models",
    "abstract": "Dialogue models are often enriched with extensive external knowledge to provide informative responses through a retrieval-augmented pipeline.Nevertheless, retrieval-augmented approaches rely on finely annotated retrieval training data and knowledge-grounded response generation data, making it costly to transfer. To tackle this challenge, this paper proposed a retrieval-free approach, KiDG, by automatically turning knowledge documents into simulated multi-turn dialogues through a Multi-Document Traversal algorithm. The simulated knowledge-intensive dialogues constructed by KiDG in one domain can be easily used to train and enhance pre-trained dialogue models‚Äô knowledge w.r.t. this domain without costly annotation.We conduct extensive experiments comparing retrieval-augmented models and a variety of retrieval-free models. We found that dialogue models enhanced with data simulated with KiDG largely outperform state-of-the-art retrieval-free methods, and it achieves comparable performance compared to retrieval-augmented methods while being better, and cheaper at domain transfer.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.acl-long.364.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2151036536",
        "name": "Rui Wang"
      },
      {
        "authorId": "2008034310",
        "name": "Jianzhu Bao"
      },
      {
        "authorId": "33727421",
        "name": "Fei Mi"
      },
      {
        "authorId": "2165302640",
        "name": "Yi Chen"
      },
      {
        "authorId": "22642319",
        "name": "Hongru Wang"
      },
      {
        "authorId": "2136912252",
        "name": "Yasheng Wang"
      },
      {
        "authorId": "2306071677",
        "name": "Yitong Li"
      },
      {
        "authorId": "50812138",
        "name": "Lifeng Shang"
      },
      {
        "authorId": "1784988",
        "name": "Kam-Fai Wong"
      },
      {
        "authorId": "2115804042",
        "name": "Ruifeng Xu"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "7473edd6ac0313d1566344f1f7b6fad48401d28e",
    "url": "https://www.semanticscholar.org/paper/7473edd6ac0313d1566344f1f7b6fad48401d28e",
    "title": "GEAR-Up: Generative AI and External Knowledge-based Retrieval Upgrading Scholarly Article Searches for Systematic Reviews",
    "abstract": "This paper addresses the time-intensive nature of systematic reviews (SRs) and proposes a solution leveraging advancements in Generative AI (e.g., ChatGPT) and external knowledge augmentation (e.g., Retrieval-Augmented Generation). The proposed system, GEAR-Up, automates query development and translation in SRs, enhancing efficiency by enriching user queries with context from language models and knowledge graphs. Collaborating with librarians, qualitative evaluations demonstrate improved reproducibility and search strategy quality. Access the demo at https://youtu.be/zMdP56GJ9mU.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2023,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference",
      "Review"
    ],
    "publicationDate": "2023-12-15",
    "authors": [
      {
        "authorId": "2274931404",
        "name": "Kaushik Roy"
      },
      {
        "authorId": "50840713",
        "name": "Vedant Khandelwal"
      },
      {
        "authorId": "2165225140",
        "name": "Harshul Surana"
      },
      {
        "authorId": "2274936182",
        "name": "Valerie Vera"
      },
      {
        "authorId": "2274935974",
        "name": "Amit P. Sheth"
      },
      {
        "authorId": "2274935698",
        "name": "Heather Heckman"
      }
    ],
    "source": "semantic_scholar",
    "score": 89.1415686865115
  },
  {
    "paperId": "5f018ce52d471fc3b3e5f17f54ea49502a85f7f3",
    "url": "https://www.semanticscholar.org/paper/5f018ce52d471fc3b3e5f17f54ea49502a85f7f3",
    "title": "Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA",
    "abstract": "While large language models (LLMs) have demonstrated significant capabilities in text generation, their utilization in areas requiring domain-specific expertise, such as law, must be approached cautiously. This caution is warranted due to the inherent challenges associated with LLM-generated texts, including the potential presence of factual errors. Motivated by this issue, we propose Eval-RAG, a new evaluation method for LLM-generated texts. Unlike existing methods, Eval-RAG evaluates the validity of generated texts based on the related document that are collected by the retriever. In other words, Eval-RAG adopts the idea of retrieval augmented generation (RAG) for the purpose of evaluation. Our experimental results on Korean Legal Question-Answering (QA) tasks show that conventional LLM-based evaluation methods can be better aligned with Lawyers‚Äô evaluations, by combining with Eval-RAG. In addition, our qualitative analysis show that Eval-RAG successfully finds the factual errors in LLM-generated texts, while existing evaluation methods cannot.",
    "venue": "NLLP",
    "year": 2023,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.nllp-1.13.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2269459742",
        "name": "Cheol Ryu"
      },
      {
        "authorId": "2269711821",
        "name": "Seolhwa Lee"
      },
      {
        "authorId": "95026044",
        "name": "Subeen Pang"
      },
      {
        "authorId": "2269731718",
        "name": "Chanyeol Choi"
      },
      {
        "authorId": "2269704438",
        "name": "Hojun Choi"
      },
      {
        "authorId": "2269462400",
        "name": "Myeonggee Min"
      },
      {
        "authorId": "8414722",
        "name": "Jy-yong Sohn"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "fa79fc97ad43e1ea1f49a82d892e6a1134d2d844",
    "url": "https://www.semanticscholar.org/paper/fa79fc97ad43e1ea1f49a82d892e6a1134d2d844",
    "title": "LLaMA-2-Econ: Enhancing Title Generation, Abstract Classification, and Academic Q&A in Economic Research",
    "abstract": "Using Quantized Low Rank Adaptation and Parameter Efficient Fine Tuning, we fine-tuned Meta AI‚Äôs LLaMA-2-7B large language model as a research assistant in the field of economics for three different types of tasks: title generation, abstract classification, and question and answer. The model was fine-tuned on economics paper abstracts and syntheticically created question-answer dialogues based on the abstracts. For the title generation, the results of the experiment demonstrated that LLaMA-2-Econ (the fine-tuned model) surpassed the base model (7B and 13B) with few shot learning, and comparable models of similar size like Mistral-7B and Bloom-7B in the BLEU and ROUGE metrics. For abstract categorization, LLaMA-2-Econ outperformed different machine and deep learning algorithms in addition to state-of-the-art models like GPT 3.5 and GPT 4 with both single and representative few shot learning. We tested the fine-tuned Q&A model by comparing its output with the base LLaMA-2-7B-chat with a Retrieval Augmented Generation (RAG) pipeline with semantic search and dense vector indexing, and found that LLaMA-2 performed on a par with the base model with RAG.",
    "venue": "FINNLP",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1471438843",
        "name": "Onur Kele≈ü"
      },
      {
        "authorId": "2302560188",
        "name": "Omer Turan Bayraklƒ±"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "8e6d1e76f5090baa1b170d2f250523e06d82cb4a",
    "url": "https://www.semanticscholar.org/paper/8e6d1e76f5090baa1b170d2f250523e06d82cb4a",
    "title": "Explaining Autonomy: Enhancing Human-Robot Interaction through Explanation Generation with Large Language Models",
    "abstract": "This paper introduces a system designed to generate explanations for the actions performed by an autonomous robot in Human-Robot Interaction (HRI). Explainability in robotics, encapsulated within the concept of an eXplainable Autonomous Robot (XAR), is a growing research area. The work described in this paper aims to take advantage of the capabilities of Large Language Models (LLMs) in performing natural language processing tasks. This study focuses on the possibility of generating explanations using such models in combination with a Retrieval Augmented Generation (RAG) method to interpret data gathered from the logs of autonomous systems. In addition, this work also presents a formalization of the proposed explanation system. It has been evaluated through a navigation test from the European Robotics League (ERL), a Europe-wide social robotics competition. Regarding the obtained results, a validation questionnaire has been conducted to measure the quality of the explanations from the perspective of technical users. The results obtained during the experiment highlight the potential utility of LLMs in achieving explanatory capabilities in robots.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-06",
    "authors": [
      {
        "authorId": "2215807275",
        "name": "David Sobr√≠n-Hidalgo"
      },
      {
        "authorId": "2003275049",
        "name": "Miguel √Ångel Gonz√°lez Santamarta"
      },
      {
        "authorId": "2810240",
        "name": "√Ångel Manuel Guerrero Higueras"
      },
      {
        "authorId": "35089785",
        "name": "F. J. R. Lera"
      },
      {
        "authorId": "6937365",
        "name": "Vicente Matell√°n Olivera"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "6cff4f630e20b62707be257d61071c2a36b033ab",
    "url": "https://www.semanticscholar.org/paper/6cff4f630e20b62707be257d61071c2a36b033ab",
    "title": "Advanced RAG Models with Graph Structures: Optimizing Complex Knowledge Reasoning and Text Generation",
    "abstract": "This study aims to optimize the existing retrieval-augmented generation model (RAG) by introducing a graph structure to improve the performance of the model in dealing with complex knowledge reasoning tasks. The traditional RAG model has the problem of insufficient processing efficiency when facing complex graph structure information (such as knowledge graphs, hierarchical relationships, etc.), which affects the quality and consistency of the generated results. This study proposes a scheme to process graph structure data by combining graph neural network (GNN), so that the model can capture the complex relationship between entities, thereby improving the knowledge consistency and reasoning ability of the generated text. The experiment used the Natural Questions (NQ) dataset and compared it with multiple existing generation models. The results show that the graph-based RAG model proposed in this paper is superior to the traditional generation model in terms of quality, knowledge consistency, and reasoning ability, especially when dealing with tasks that require multi-dimensional reasoning. Through the combination of the enhancement of the retrieval module and the graph neural network, the model in this study can better handle complex knowledge background information and has broad potential value in multiple practical application scenarios.",
    "venue": "2024 5th International Symposium on Computer Engineering and Intelligent Communications (ISCEIC)",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2411.03572",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-11-06",
    "authors": [
      {
        "authorId": "2326920545",
        "name": "Yuxin Dong"
      },
      {
        "authorId": "2327007198",
        "name": "Shuo Wang"
      },
      {
        "authorId": "2327003963",
        "name": "Hongye Zheng"
      },
      {
        "authorId": "2322450076",
        "name": "Jiajing Chen"
      },
      {
        "authorId": "2322450970",
        "name": "Zhenhong Zhang"
      },
      {
        "authorId": "2322612972",
        "name": "Chihang Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "a849323c9c25154e4f81947ef7d20d190be093c3",
    "url": "https://www.semanticscholar.org/paper/a849323c9c25154e4f81947ef7d20d190be093c3",
    "title": "Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node",
    "abstract": "This research introduces a novel approach for assisting the creation of Asset Administration Shell (AAS) instances for digital twin modeling within the context of Industry 4.0, aiming to enhance interoperability in smart manufacturing and reduce manual eff ort. We construct a ‚Äúsemantic node‚Äù data structure to capture the semantic essence of textual data. Then, a system powered by large language models is designed and implemented to process ‚Äúsemantic node‚Äù and generate AAS instance models from textua l technical data. Our evaluation demonstrates a 62-79% effective generation rate, indicating a substantial proportion of manual creation effort can be converted into easier validation effort, thereby reducing the time and cost in creating AAS instance models. In our evaluation, a comparative analysis of different LLMs and an in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms provide insights into the effectiveness of LLM systems for interpreting technical concepts. Our findings empha size LLMs‚Äô capability in automating AAS instance creation, enhancing semantic interoperability, and contributing to the broader field of semantic interoperability for digital twins in industrial applications. The prototype implementation and evaluation results are released on our GitHub Repository: https://github.com/YuchenXia/AASbyLLM.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "152428280",
        "name": "Yuchen Xia"
      },
      {
        "authorId": "2294154000",
        "name": "Zhewen Xiao"
      },
      {
        "authorId": "2509708",
        "name": "N. Jazdi"
      },
      {
        "authorId": "2273478",
        "name": "M. Weyrich"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "7429fec29285ecc46113e05a5ab843f4036d620c",
    "url": "https://www.semanticscholar.org/paper/7429fec29285ecc46113e05a5ab843f4036d620c",
    "title": "Context Embeddings for Efficient Answer Generation in RAG",
    "abstract": "Retrieval-Augmented Generation (RAG) allows overcoming the limited knowledge of LLMs by extending the input with external information. As a consequence, the contextual inputs to the model become much longer which slows down decoding time directly translating to the time a user has to wait for an answer. We address this challenge by presenting COCOM, an effective context compression method, reducing long contexts to only a handful of Context Embeddings speeding up the generation time by a large margin. Our method allows for different compression rates trading off decoding time for answer quality. Compared to earlier methods, COCOM allows for handling multiple contexts more effectively, significantly reducing decoding time for long inputs. Our method demonstrates a speed-up of up to 5.69 $\\times$ while achieving higher performance compared to existing efficient context compression methods.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-12",
    "authors": [
      {
        "authorId": "2309172052",
        "name": "David Rau"
      },
      {
        "authorId": "2309201940",
        "name": "Shuai Wang"
      },
      {
        "authorId": "2290801744",
        "name": "Herv'e D'ejean"
      },
      {
        "authorId": "2207074",
        "name": "S. Clinchant"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "63a7b432dfa371098c63fb6784dcf0e560395012",
    "url": "https://www.semanticscholar.org/paper/63a7b432dfa371098c63fb6784dcf0e560395012",
    "title": "QUB-Cirdan at ‚ÄúDischarge Me!‚Äù: Zero shot discharge letter generation by open-source LLM",
    "abstract": "The BioNLP ACL‚Äô24 Shared Task on Streamlining Discharge Documentation aims to reduce the administrative burden on clinicians by automating the creation of critical sections of patient discharge letters. This paper presents our approach using the Llama3 8B quantized model to generate the ‚ÄúBrief Hospital Course‚Äù and ‚ÄúDischarge Instructions‚Äù sections. We employ a zero-shot method combined with Retrieval-Augmented Generation (RAG) to produce concise, contextually accurate summaries. Our contributions include the development of a curated template-based approach to ensure reliability and consistency, as well as the integration of RAG for word count prediction. We also describe several unsuccessful experiments to provide insights into our pathway for the competition. Our results demonstrate the effectiveness and efficiency of our approach, achieving high scores across multiple evaluation metrics.",
    "venue": "Workshop on Biomedical Natural Language Processing",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-27",
    "authors": [
      {
        "authorId": "2304497372",
        "name": "Rui Guo"
      },
      {
        "authorId": "2304473512",
        "name": "Greg Farnan"
      },
      {
        "authorId": "2304472538",
        "name": "Niall McLaughlin"
      },
      {
        "authorId": "2304473510",
        "name": "Barry Devereux"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "90b337077164112e81ab91e5df1b8e4352b1ff3e",
    "url": "https://www.semanticscholar.org/paper/90b337077164112e81ab91e5df1b8e4352b1ff3e",
    "title": "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation",
    "abstract": "Despite the significant progress of large language models (LLMs) in various tasks, they often produce factual errors due to their limited internal knowledge. Retrieval-Augmented Generation (RAG), which enhances LLMs with external knowledge sources, offers a promising solution. However, these methods can be misled by irrelevant paragraphs in retrieved documents. Due to the inherent uncertainty in LLM generation, inputting the entire document may introduce off-topic information, causing the model to deviate from the central topic and affecting the relevance of the generated content. To address these issues, we propose the Retrieve-Plan-Generation (RPG) framework. RPG generates plan tokens to guide subsequent generation in the plan stage. In the answer stage, the model selects relevant fine-grained paragraphs based on the plan and uses them for further answer generation. This plan-answer process is repeated iteratively until completion, enhancing generation relevance by focusing on specific topics. To implement this framework efficiently, we utilize a simple but effective multi-task prompt-tuning method, enabling the existing LLMs to handle both planning and answering. We comprehensively compare RPG with baselines across 5 knowledge-intensive generation tasks, demonstrating the effectiveness of our approach.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-21",
    "authors": [
      {
        "authorId": "2187857206",
        "name": "Yuanjie Lyu"
      },
      {
        "authorId": "2307915655",
        "name": "Zihan Niu"
      },
      {
        "authorId": "2202470155",
        "name": "Zheyong Xie"
      },
      {
        "authorId": "2260850374",
        "name": "Chao Zhang"
      },
      {
        "authorId": "2277237058",
        "name": "Tong Xu"
      },
      {
        "authorId": "2308313519",
        "name": "Yang Wang"
      },
      {
        "authorId": "2265580543",
        "name": "Enhong Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "aee0ef2356d76b5d28e167aedca0d8f1dce93419",
    "url": "https://www.semanticscholar.org/paper/aee0ef2356d76b5d28e167aedca0d8f1dce93419",
    "title": "Automated Question-Answer Generation for Evaluating RAG-based Chatbots",
    "abstract": "In this research, we propose a framework to generate human-like question-answer pairs with long or factoid answers automatically and, based on them, automatically evaluate the quality of Retrieval-Augmented Generation (RAG). Our framework can also create datasets that assess hallucination levels of Large Language Models (LLMs) by simulating unanswerable questions. We then apply the framework to create a dataset of question-answer (QA) pairs based on more than 1,000 leaflets about the medical and administrative procedures of a hospital. The dataset was evaluated by hospital specialists, who confirmed that more than 50% of the QA pairs are applicable. Finally, we show that our framework can be used to evaluate LLM performance by using Llama-2-13B fine-tuned in Dutch (Vanroy, 2023) with the generated dataset, and show the method‚Äôs use in testing models with regard to answering unanswerable and factoid questions appears promising.",
    "venue": "CL4HEALTH",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2302558582",
        "name": "Juan Jos√© Gonz√°lez Torres"
      },
      {
        "authorId": "2159008482",
        "name": "Mihai-Bogdan B√Ændila"
      },
      {
        "authorId": "2302558504",
        "name": "Sebastiaan Hofstee"
      },
      {
        "authorId": "2302558502",
        "name": "Daniel Szondy"
      },
      {
        "authorId": "2302559170",
        "name": "Quang-Hung Nguyen"
      },
      {
        "authorId": "2303033182",
        "name": "Shenghui Wang"
      },
      {
        "authorId": "2654643",
        "name": "G. Englebienne"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "f292d53ba6e79f525b55218270400b6f5078098b",
    "url": "https://www.semanticscholar.org/paper/f292d53ba6e79f525b55218270400b6f5078098b",
    "title": "Evaluating Top-k RAG-based approach for Game Review Generation",
    "abstract": "Having access to public opinion for a particular product can be a cumbersome task. There are multiple reviews for the same product. Some may be good or bad depending on the bias of the reviewer. Using LLMs for the interpretation of this data would make it easier to understand the overall perception of a product. This is a study regarding how well RAG+LLMs can be used as Game Review Generators, built using state-of-the-art open source LLM LLaMA 2 13b and the Retrieval Augmented Generation framework llamaindex. The goal here is to generate and evaluate game reviews that take elements from a set of game reviews regarding a particular game without using any form of fine-tuning. This is achieved by using a rudimentary ‚Äòquery engine‚Äô over a subset of publicly available game reviews. Game reviews are converted to vector stores which allow us to use top-k semantic retrieval for inference. This technique of providing data to an LLM from a document is called Retrieval Augmented Generation or RAG. Upon experimenting, game-specific reviews were generated (without using any form of fine-tuning) with the help of RAG combined with a top-k semantic retrieval ranking system. The application of this technique goes beyond simple game review generation, it can be used to generate and query context-specific information on any product given enough base information.",
    "venue": "2024 IEEE International Conference on Computing, Power and Communication Technologies (IC2PCT)",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "Conference",
      "Review"
    ],
    "publicationDate": "2024-02-09",
    "authors": [
      {
        "authorId": "2295816893",
        "name": "Pratyush Chauhan"
      },
      {
        "authorId": "2295828740",
        "name": "Rahul Kumar Sahani"
      },
      {
        "authorId": "2295833451",
        "name": "Soham Datta"
      },
      {
        "authorId": "2295829921",
        "name": "Ali Qadir"
      },
      {
        "authorId": "2157567994",
        "name": "Manish Raj"
      },
      {
        "authorId": "2280137940",
        "name": "Mohd Mohsin Ali"
      }
    ],
    "source": "semantic_scholar",
    "score": 92.47918433002164
  },
  {
    "paperId": "3f925e9195fdc7ea99c5d8d957f41dd57159b867",
    "url": "https://www.semanticscholar.org/paper/3f925e9195fdc7ea99c5d8d957f41dd57159b867",
    "title": "Synthetic Multimodal Question Generation",
    "abstract": "Multimodal Retrieval Augmented Generation (MMRAG) is a powerful approach to question-answering over multimodal documents. A key challenge with evaluating MMRAG is the paucity of high-quality datasets matching the question styles and modalities of interest. In light of this, we propose SMMQG, a synthetic data generation framework. SMMQG leverages interplay between a retriever, large language model (LLM) and large multimodal model (LMM) to generate question and answer pairs directly from multimodal documents, with the questions conforming to specified styles and modalities. We use SMMQG to generate an MMRAG dataset of 1024 questions over Wikipedia documents and evaluate state-of-the-art models using it, revealing insights into model performance that are attainable only through style- and modality-specific evaluation data. Next, we measure the quality of data produced by SMMQG via a human study. We find that the quality of SMMQG-generated synthetic data is on par with the quality of the crowdsourced benchmark MMQA and that downstream evaluation results using both datasets strongly concur.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-02",
    "authors": [
      {
        "authorId": "2309247511",
        "name": "Ian Wu"
      },
      {
        "authorId": "2154573994",
        "name": "Sravan Jayanthi"
      },
      {
        "authorId": "2061499362",
        "name": "Vijay Viswanathan"
      },
      {
        "authorId": "2309246846",
        "name": "Simon Rosenberg"
      },
      {
        "authorId": "2309246186",
        "name": "Sina Pakazad"
      },
      {
        "authorId": "2265722477",
        "name": "Tongshuang Wu"
      },
      {
        "authorId": "2265547593",
        "name": "Graham Neubig"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "27ee3e441f27bbfe02f0bd60b55decf4247870ac",
    "url": "https://www.semanticscholar.org/paper/27ee3e441f27bbfe02f0bd60b55decf4247870ac",
    "title": "Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos",
    "abstract": "Generating realistic audio for human actions is important for many applications, such as creating sound effects for films or virtual reality games. Existing approaches implicitly assume total correspondence between the video and audio during training, yet many sounds happen off-screen and have weak to no correspondence with the visuals -- resulting in uncontrolled ambient sounds or hallucinations at test time. We propose a novel ambient-aware audio generation model, AV-LDM. We devise a novel audio-conditioning mechanism to learn to disentangle foreground action sounds from the ambient background sounds in in-the-wild training videos. Given a novel silent video, our model uses retrieval-augmented generation to create audio that matches the visual content both semantically and temporally. We train and evaluate our model on two in-the-wild egocentric video datasets, Ego4D and EPIC-KITCHENS, and we introduce Ego4D-Sounds -- 1.2M curated clips with action-audio correspondence. Our model outperforms an array of existing methods, allows controllable generation of the ambient sound, and even shows promise for generalizing to computer graphics game clips. Overall, our approach is the first to focus video-to-audio generation faithfully on the observed visual content despite training from uncurated clips with natural background sounds.",
    "venue": "European Conference on Computer Vision",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-13",
    "authors": [
      {
        "authorId": "2268807177",
        "name": "Changan Chen"
      },
      {
        "authorId": "30598090",
        "name": "Puyuan Peng"
      },
      {
        "authorId": "2306254492",
        "name": "Ami Baid"
      },
      {
        "authorId": "2268760036",
        "name": "Zihui Xue"
      },
      {
        "authorId": "2306272639",
        "name": "Wei-Ning Hsu"
      },
      {
        "authorId": "2307105347",
        "name": "David Harwath"
      },
      {
        "authorId": "2273978066",
        "name": "Kristen Grauman"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "a33ba08c5f7f4c11205ea19d4a8718cc5bb5de98",
    "url": "https://www.semanticscholar.org/paper/a33ba08c5f7f4c11205ea19d4a8718cc5bb5de98",
    "title": "Automatic Generation of Fashion Images using Prompting in Generative Machine Learning Models",
    "abstract": "The advent of artificial intelligence has contributed in a groundbreaking transformation of the fashion industry, redefining creativity and innovation in unprecedented ways. This work investigates methodologies for generating tailored fashion descriptions using two distinct Large Language Models and a Stable Diffusion model for fashion image creation. Emphasizing adaptability in AI-driven fashion creativity, we depart from traditional approaches and focus on prompting techniques, such as zero-shot and few-shot learning, as well as Chain-of-Thought (CoT), which results in a variety of colors and textures, enhancing the diversity of the outputs. Central to our methodology is Retrieval-Augmented Generation (RAG), enriching models with insights from fashion sources to ensure contemporary representations. Evaluation combines quantitative metrics such as CLIPscore with qualitative human judgment, highlighting strengths in creativity, coherence, and aesthetic appeal across diverse styles. Among the participants, RAG and few-shot learning techniques are preferred for their ability to produce more relevant and appealing fashion descriptions. Our code is provided at https://github.com/georgiarg/AutoFashion.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-20",
    "authors": [
      {
        "authorId": "2312322836",
        "name": "Georgia Argyrou"
      },
      {
        "authorId": "2280148257",
        "name": "Angeliki Dimitriou"
      },
      {
        "authorId": "2184294391",
        "name": "Maria Lymperaiou"
      },
      {
        "authorId": "2080432906",
        "name": "Giorgos Filandrianos"
      },
      {
        "authorId": "1719165",
        "name": "G. Stamou"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "c331fbed7d2ee44ec35beed7cfb45979aed45688",
    "url": "https://www.semanticscholar.org/paper/c331fbed7d2ee44ec35beed7cfb45979aed45688",
    "title": "Don‚Äôt forget private retrieval: distributed private similarity search for large language models",
    "abstract": "While the flexible capabilities of large language models (LLMs) allow them to answer a range of queries based on existing learned knowledge, information retrieval to augment generation is an important tool to allow LLMs to answer questions on information not included in pre-training data. Such private information is increasingly being generated in a wide array of distributed contexts by organizations and individuals. Performing such information retrieval using neural embeddings of queries and documents always leaked information about queries and database content unless both were stored locally. We present Private Retrieval Augmented Generation (PRAG), an approach that uses multi-party computation (MPC) to securely transmit queries to a distributed set of servers containing a privately constructed database to return top-k and approximate top-k documents. This is a first-of-its-kind approach to dense information retrieval that ensures no server observes a client‚Äôs query or can see the database content. The approach introduces a novel MPC friendly protocol for inverted file approximate search (IVF) that allows for fast document search over distributed and private data in sublinear communication complexity. This work presents new avenues through which data for use in LLMs can be accessed and used without needing to centralize or forgo privacy.",
    "venue": "PRIVATENLP",
    "year": 2023,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-21",
    "authors": [
      {
        "authorId": "2653464",
        "name": "Guy Zyskind"
      },
      {
        "authorId": "148243135",
        "name": "Tobin South"
      },
      {
        "authorId": "2252488708",
        "name": "A. Pentland"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "05a6ef4b442a7cd37f40ef93216f68e461439b95",
    "url": "https://www.semanticscholar.org/paper/05a6ef4b442a7cd37f40ef93216f68e461439b95",
    "title": "Qlarify: Recursively Expandable Abstracts for Directed Information Retrieval over Scientific Papers",
    "abstract": "Navigating the vast scientific literature often starts with browsing a paper's abstract. However, when a reader seeks additional information, not present in the abstract, they face a costly cognitive chasm during their dive into the full text. To bridge this gap, we introduce recursively expandable abstracts, a novel interaction paradigm that dynamically expands abstracts by progressively incorporating additional information from the papers' full text. This lightweight interaction allows scholars to specify their information needs by quickly brushing over the abstract or selecting AI-suggested expandable entities. Relevant information is synthesized using a retrieval-augmented generation approach, presented as a fluid, threaded expansion of the abstract, and made efficiently verifiable via attribution to relevant source-passages in the paper. Through a series of user studies, we demonstrate the utility of recursively expandable abstracts and identify future opportunities to support low-effort and just-in-time exploration of long-form information contexts through LLM-powered interactions.",
    "venue": "",
    "year": 2023,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-10-11",
    "authors": [
      {
        "authorId": "27083453",
        "name": "Raymond Fok"
      },
      {
        "authorId": "48808806",
        "name": "Joseph Chee Chang"
      },
      {
        "authorId": "50509991",
        "name": "Tal August"
      },
      {
        "authorId": "144518215",
        "name": "Amy X. Zhang"
      },
      {
        "authorId": "1780531",
        "name": "Daniel S. Weld"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "b886c253bcc48fdfe7b838b8da71cdaa39af9487",
    "url": "https://www.semanticscholar.org/paper/b886c253bcc48fdfe7b838b8da71cdaa39af9487",
    "title": "Melody-Guided Music Generation",
    "abstract": "We present the Melody-Guided Music Generation (MG2) model, a novel approach using melody to guide the text-to-music generation that, despite a simple method and limited resources, achieves excellent performance. Specifically, we first align the text with audio waveforms and their associated melodies using the newly proposed Contrastive Language-Music Pretraining, enabling the learned text representation fused with implicit melody information. Subsequently, we condition the retrieval-augmented diffusion module on both text prompt and retrieved melody. This allows MG2 to generate music that reflects the content of the given text description, meantime keeping the intrinsic harmony under the guidance of explicit melody information. We conducted extensive experiments on two public datasets: MusicCaps and MusicBench. Surprisingly, the experimental results demonstrate that the proposed MG2 model surpasses current open-source text-to-music generation models, achieving this with fewer than 1/3 of the parameters or less than 1/200 of the training data compared to state-of-the-art counterparts. Furthermore, we conducted comprehensive human evaluations involving three types of users and five perspectives, using newly designed questionnaires to explore the potential real-world applications of MG2.",
    "venue": "",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-09-30",
    "authors": [
      {
        "authorId": "2311640882",
        "name": "Shaopeng Wei"
      },
      {
        "authorId": "2323529409",
        "name": "Manzhen Wei"
      },
      {
        "authorId": "2323530333",
        "name": "Haoyu Wang"
      },
      {
        "authorId": "2323525693",
        "name": "Yu Zhao"
      },
      {
        "authorId": "2147326459",
        "name": "Gang Kou"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "c1ff591b8c88dfebdb2f11b2bf33ded07d6c43ec",
    "url": "https://www.semanticscholar.org/paper/c1ff591b8c88dfebdb2f11b2bf33ded07d6c43ec",
    "title": "RAGDiffusion: Faithful Cloth Generation via External Knowledge Assimilation",
    "abstract": "Standard clothing asset generation involves creating forward-facing flat-lay garment images displayed on a clear background by extracting clothing information from diverse real-world contexts, which presents significant challenges due to highly standardized sampling distributions and precise structural requirements in the generated images. Existing models have limited spatial perception and often exhibit structural hallucinations in this high-specification generative task. To address this issue, we propose a novel Retrieval-Augmented Generation (RAG) framework, termed RAGDiffusion, to enhance structure determinacy and mitigate hallucinations by assimilating external knowledge from LLM and databases. RAGDiffusion consists of two core processes: (1) Retrieval-based structure aggregation, which employs contrastive learning and a Structure Locally Linear Embedding (SLLE) to derive global structure and spatial landmarks, providing both soft and hard guidance to counteract structural ambiguities; and (2) Omni-level faithful garment generation, which introduces a three-level alignment that ensures fidelity in structural, pattern, and decoding components within the diffusing. Extensive experiments on challenging real-world datasets demonstrate that RAGDiffusion synthesizes structurally and detail-faithful clothing assets with significant performance improvements, representing a pioneering effort in high-specification faithful generation with RAG to confront intrinsic hallucinations and enhance fidelity.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-11-29",
    "authors": [
      {
        "authorId": "2333979901",
        "name": "Xianfeng Tan"
      },
      {
        "authorId": "2303614448",
        "name": "Yuhan Li"
      },
      {
        "authorId": "2303464347",
        "name": "Wenxiang Shang"
      },
      {
        "authorId": "2333309732",
        "name": "Yubo Wu"
      },
      {
        "authorId": "2326532668",
        "name": "Jian Wang"
      },
      {
        "authorId": "27054843",
        "name": "Xuanhong Chen"
      },
      {
        "authorId": "2333236106",
        "name": "Yi Zhang"
      },
      {
        "authorId": "2303614290",
        "name": "Ran Lin"
      },
      {
        "authorId": "2293275659",
        "name": "Bingbing Ni"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "c58f30c054c5b48e5e4a0d1c3a20793583effa22",
    "url": "https://www.semanticscholar.org/paper/c58f30c054c5b48e5e4a0d1c3a20793583effa22",
    "title": "KITTEN: A Knowledge-Intensive Evaluation of Image Generation on Visual Entities",
    "abstract": "Recent advancements in text-to-image generation have significantly enhanced the quality of synthesized images. Despite this progress, evaluations predominantly focus on aesthetic appeal or alignment with text prompts. Consequently, there is limited understanding of whether these models can accurately represent a wide variety of realistic visual entities - a task requiring real-world knowledge. To address this gap, we propose a benchmark focused on evaluating Knowledge-InTensive image generaTion on real-world ENtities (i.e., KITTEN). Using KITTEN, we conduct a systematic study on the fidelity of entities in text-to-image generation models, focusing on their ability to generate a wide range of real-world visual entities, such as landmark buildings, aircraft, plants, and animals. We evaluate the latest text-to-image models and retrieval-augmented customization models using both automatic metrics and carefully-designed human evaluations, with an emphasis on the fidelity of entities in the generated images. Our findings reveal that even the most advanced text-to-image models often fail to generate entities with accurate visual details. Although retrieval-augmented models can enhance the fidelity of entity by incorporating reference images during testing, they often over-rely on these references and struggle to produce novel configurations of the entity as requested in creative text prompts.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-15",
    "authors": [
      {
        "authorId": "2115636909",
        "name": "Hsin-Ping Huang"
      },
      {
        "authorId": "2326171546",
        "name": "Xinyi Wang"
      },
      {
        "authorId": "1938499056",
        "name": "Yonatan Bitton"
      },
      {
        "authorId": "51258885",
        "name": "Hagai Taitelbaum"
      },
      {
        "authorId": "32012022",
        "name": "Gaurav Singh Tomar"
      },
      {
        "authorId": "2277809314",
        "name": "Ming-Wei Chang"
      },
      {
        "authorId": "2269764175",
        "name": "Xuhui Jia"
      },
      {
        "authorId": "2269731135",
        "name": "Kelvin C.K. Chan"
      },
      {
        "authorId": "2307548497",
        "name": "Hexiang Hu"
      },
      {
        "authorId": "2269866136",
        "name": "Yu-Chuan Su"
      },
      {
        "authorId": "2297139110",
        "name": "Ming-Hsuan Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "4a0c0af49996fccad71fde7aa2a6b872762228ed",
    "url": "https://www.semanticscholar.org/paper/4a0c0af49996fccad71fde7aa2a6b872762228ed",
    "title": "VIMI: Grounding Video Generation through Multi-modal Instruction",
    "abstract": "Existing text-to-video diffusion models rely solely on text-only encoders for their pretraining. This limitation stems from the absence of large-scale multimodal prompt video datasets, resulting in a lack of visual grounding and restricting their versatility and application in multimodal integration. To address this, we construct a large-scale multimodal prompt dataset by employing retrieval methods to pair in-context examples with the given text prompts and then utilize a two-stage training strategy to enable diverse video generation tasks within a model. In the first stage, we propose a multimodal conditional video generation framework for pretraining on these augmented datasets, establishing a foundational model for grounded video generation. Secondly, we fine-tune the model from the first stage on various video generation tasks, incorporating multimodal instructions. This process further refines the model‚Äôs ability to handle diverse inputs and tasks, ensuring seamless integration of multimodal information. After this two-stage training process, VIMI demonstrates multimodal understanding capabilities, producing contextually rich and personalized videos grounded in the provided inputs, as shown in Figure1. Compared to previous subject-driven video generation methods, our generator can synthesize consistent and temporally coherent videos with large motion while retaining the semantic control. Our generator also achieves state-of-the-art text-to-video generation results on UCF101 benchmark.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-08",
    "authors": [
      {
        "authorId": "2282237113",
        "name": "Yuwei Fang"
      },
      {
        "authorId": "1698103472",
        "name": "Willi Menapace"
      },
      {
        "authorId": "10753214",
        "name": "Aliaksandr Siarohin"
      },
      {
        "authorId": "2310476055",
        "name": "Tsai-Shien Chen"
      },
      {
        "authorId": "2310442187",
        "name": "Kuan-Chien Wang"
      },
      {
        "authorId": "51118864",
        "name": "Ivan Skorokhodov"
      },
      {
        "authorId": "2285194103",
        "name": "Graham Neubig"
      },
      {
        "authorId": "2292401534",
        "name": "Sergey Tulyakov"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "ed99a2572fb5f4240aa6068e3bf274832e831306",
    "url": "https://www.semanticscholar.org/paper/ed99a2572fb5f4240aa6068e3bf274832e831306",
    "title": "Recitation-Augmented Language Models",
    "abstract": "We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrieval-augmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE first recites one or several relevant passages from LLMs' own memory via sampling, and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of \\method~on four pre-trained models (PaLM, UL2, OPT, and Codex) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA). Our code is available at\"https://github.com/Edward-Sun/RECITE\".",
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "citationCount": 56,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2210.01296",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-10-04",
    "authors": [
      {
        "authorId": "48064856",
        "name": "Zhiqing Sun"
      },
      {
        "authorId": "1524732527",
        "name": "Xuezhi Wang"
      },
      {
        "authorId": "144447820",
        "name": "Yi Tay"
      },
      {
        "authorId": "46286308",
        "name": "Yiming Yang"
      },
      {
        "authorId": "65855107",
        "name": "Denny Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 130.64576901751826
  },
  {
    "paperId": "9a7b9515b66bf83c9c808626206eabe9a8837c22",
    "url": "https://www.semanticscholar.org/paper/9a7b9515b66bf83c9c808626206eabe9a8837c22",
    "title": "Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models",
    "abstract": "\n Large language models (LLMs) demonstrate impressive capabilities to generate accurate code snippets given natural language intents in a zero-shot manner,\n i.e.\n , without the need for specific fine-tuning. While prior studies have highlighted the advantages of fine-tuning LLMs, this process incurs high computational costs, making it impractical in resource-scarce environments, particularly for models with billions of parameters. To address these challenges, previous research explored in-context learning (ICL) and retrieval-augmented generation (RAG) as strategies to guide the LLM generative process with task-specific prompt examples. However, ICL and RAG introduce inconveniences, such as the need for designing contextually relevant prompts and the absence of learning task-specific parameters, thereby limiting downstream task performance. In this context, we foresee parameter-efficient fine-tuning (PEFT) as a promising approach to efficiently specialize LLMs to task-specific data while maintaining reasonable resource consumption. In this paper, we deliver a comprehensive study of PEFT techniques for LLMs in the context of automated code generation. Our comprehensive investigation of PEFT techniques for LLMs reveals their superiority and potential over ICL and RAG across a diverse set of LLMs and three representative Python code generation datasets: Conala, CodeAlpacaPy, and APPS. Furthermore, our study highlights the potential for tuning larger LLMs and significant reductions in memory usage by combining PEFT with quantization. Therefore, this study opens opportunities for broader applications of PEFT in software engineering scenarios.\n",
    "venue": "ACM Transactions on Software Engineering and Methodology",
    "year": 2023,
    "citationCount": 19,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.10462",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-08-21",
    "authors": [
      {
        "authorId": "1820831988",
        "name": "M. Weyssow"
      },
      {
        "authorId": "2148928671",
        "name": "Xin Zhou"
      },
      {
        "authorId": "35276441",
        "name": "Kisub Kim"
      },
      {
        "authorId": "2150912791",
        "name": "David Lo"
      },
      {
        "authorId": "9460712",
        "name": "H. Sahraoui"
      }
    ],
    "source": "semantic_scholar",
    "score": 120.93598410330986
  },
  {
    "paperId": "ab8ee8d06ed581c876da3a2e7a5fdb1cbec21a45",
    "url": "https://www.semanticscholar.org/paper/ab8ee8d06ed581c876da3a2e7a5fdb1cbec21a45",
    "title": "KAT: A Knowledge Augmented Transformer for Vision-and-Language",
    "abstract": "The primary focus of recent work with large-scale transformers has been on optimizing the amount of information packed into the model‚Äôs parameters. In this work, we ask a complementary question: Can multimodal transformers leverage explicit knowledge in their reasoning? Existing, primarily unimodal, methods have explored approaches under the paradigm of knowledge retrieval followed by answer prediction, but leave open questions about the quality and relevance of the retrieved knowledge used, and how the reasoning processes over implicit and explicit knowledge should be integrated. To address these challenges, we propose a - Knowledge Augmented Transformer (KAT) - which achieves a strong state-of-the-art result (+6% absolute) on the open-domain multimodal task of OK-VQA. Our approach integrates implicit and explicit knowledge in an encoder-decoder architecture, while still jointly reasoning over both knowledge sources during answer generation. Additionally, explicit knowledge integration improves interpretability of model predictions in our analysis.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2021,
    "citationCount": 135,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2022.naacl-main.70.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-12-16",
    "authors": [
      {
        "authorId": "1970583",
        "name": "Liangke Gui"
      },
      {
        "authorId": "2203187",
        "name": "Borui Wang"
      },
      {
        "authorId": "2110991956",
        "name": "Qiuyuan Huang"
      },
      {
        "authorId": "145788702",
        "name": "A. Hauptmann"
      },
      {
        "authorId": "3312309",
        "name": "Yonatan Bisk"
      },
      {
        "authorId": "48441311",
        "name": "Jianfeng Gao"
      }
    ],
    "source": "semantic_scholar",
    "score": 143.6898232860408
  },
  {
    "paperId": "eaf2f0ed4281699f52f3c03ee4a2ee411fa0aa6e",
    "url": "https://www.semanticscholar.org/paper/eaf2f0ed4281699f52f3c03ee4a2ee411fa0aa6e",
    "title": "Biomedical knowledge graph-optimized prompt generation for large language models",
    "abstract": "Abstract Motivation Large language models (LLMs) are being adopted at an unprecedented rate, yet still face challenges in knowledge-intensive domains such as biomedicine. Solutions such as pretraining and domain-specific fine-tuning add substantial computational overhead, requiring further domain-expertise. Here, we introduce a token-optimized and robust Knowledge Graph-based Retrieval Augmented Generation (KG-RAG) framework by leveraging a massive biomedical KG (SPOKE) with LLMs such as Llama-2-13b, GPT-3.5-Turbo, and GPT-4, to generate meaningful biomedical text rooted in established knowledge. Results Compared to the existing RAG technique for Knowledge Graphs, the proposed method utilizes minimal graph schema for context extraction and uses embedding methods for context pruning. This optimization in context extraction results in more than 50% reduction in token consumption without compromising the accuracy, making a cost-effective and robust RAG implementation on proprietary LLMs. KG-RAG consistently enhanced the performance of LLMs across diverse biomedical prompts by generating responses rooted in established knowledge, accompanied by accurate provenance and statistical evidence (if available) to substantiate the claims. Further benchmarking on human curated datasets, such as biomedical true/false and multiple-choice questions (MCQ), showed a remarkable 71% boost in the performance of the Llama-2 model on the challenging MCQ dataset, demonstrating the framework‚Äôs capacity to empower open-source models with fewer parameters for domain-specific questions. Furthermore, KG-RAG enhanced the performance of proprietary GPT models, such as GPT-3.5 and GPT-4. In summary, the proposed framework combines explicit and implicit knowledge of KG and LLM in a token optimized fashion, thus enhancing the adaptability of general-purpose LLMs to tackle domain-specific questions in a cost-effective fashion. Availability and implementation SPOKE KG can be accessed at https://spoke.rbvi.ucsf.edu/neighborhood.html. It can also be accessed using REST-API (https://spoke.rbvi.ucsf.edu/swagger/). KG-RAG code is made available at https://github.com/BaranziniLab/KG_RAG. Biomedical benchmark datasets used in this study are made available to the research community in the same GitHub repository.",
    "venue": "Bioinformatics",
    "year": 2023,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-29",
    "authors": [
      {
        "authorId": "48228268",
        "name": "Karthik Soman"
      },
      {
        "authorId": "2184494479",
        "name": "Peter W Rose"
      },
      {
        "authorId": "2268732062",
        "name": "John H Morris"
      },
      {
        "authorId": "2205312554",
        "name": "Rabia E Akbas"
      },
      {
        "authorId": "2241953021",
        "name": "Brett Smith"
      },
      {
        "authorId": "2268673150",
        "name": "Braian Peetoom"
      },
      {
        "authorId": "2268673146",
        "name": "Catalina Villouta-Reyes"
      },
      {
        "authorId": "2187032873",
        "name": "G. Cerono"
      },
      {
        "authorId": "2205240857",
        "name": "Yongmei Shi"
      },
      {
        "authorId": "1402246186",
        "name": "Angela Rizk-Jackson"
      },
      {
        "authorId": "52309142",
        "name": "Sharat Israni"
      },
      {
        "authorId": "32015902",
        "name": "Charlotte A. Nelson"
      },
      {
        "authorId": "2242005770",
        "name": "Sui Huang"
      },
      {
        "authorId": "2241716125",
        "name": "Sergio Baranzini"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "c80fe54fb10195c13e4087eb9712435a8d4be65a",
    "url": "https://www.semanticscholar.org/paper/c80fe54fb10195c13e4087eb9712435a8d4be65a",
    "title": "Learning Answer Generation using Supervision from Automatic Question Answering Evaluators",
    "abstract": "Recent studies show that sentence-level extractive QA, i.e., based on Answer Sentence Selection (AS2), is outperformed by Generation-based QA (GenQA) models, which generate answers using the top-k answer sentences ranked by AS2 models (a la retrieval-augmented generation style). In this paper, we propose a novel training paradigm for GenQA using supervision from automatic QA evaluation models (GAVA). Specifically, we propose three strategies to transfer knowledge from these QA evaluation models to a GenQA model: (i) augmenting training data with answers generated by the GenQA model and labelled by GAVA (either statically, before training, or (ii) dynamically, at every training epoch); and (iii) using the GAVA score for weighting the generator loss during the learning of the GenQA model. We evaluate our proposed methods on two academic and one industrial dataset, obtaining a significant improvement in answering accuracy over the previous state of the art.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.15344",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-24",
    "authors": [
      {
        "authorId": "2079392752",
        "name": "Matteo Gabburo"
      },
      {
        "authorId": "2295877",
        "name": "Siddhant Garg"
      },
      {
        "authorId": "1403698986",
        "name": "Rik Koncel-Kedziorski"
      },
      {
        "authorId": "1719404",
        "name": "Alessandro Moschitti"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "37b115139dbeff3ffbcb9927d5383084443c27e2",
    "url": "https://www.semanticscholar.org/paper/37b115139dbeff3ffbcb9927d5383084443c27e2",
    "title": "Expository Text Generation: Imitate, Retrieve, Paraphrase",
    "abstract": "Expository documents are vital resources for conveying complex information to readers. Despite their usefulness, writing expository text by hand is a challenging process that requires careful content planning, obtaining facts from multiple sources, and the ability to clearly synthesize these facts. To ease these burdens, we propose the task of expository text generation, which seeks to automatically generate an accurate and stylistically consistent expository text for a topic by intelligently searching a knowledge source. We solve our task by developing IRP, a framework that overcomes the limitations of retrieval-augmented models and iteratively performs content planning, fact retrieval, and rephrasing. Through experiments on three diverse, newly-collected datasets, we show that IRP produces factual and organized expository texts that accurately inform readers.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.03276",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-05",
    "authors": [
      {
        "authorId": "2216486213",
        "name": "Nishant Balepur"
      },
      {
        "authorId": "1490651934",
        "name": "Jie Huang"
      },
      {
        "authorId": "143922493",
        "name": "K. Chang"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "69aef44e0e39f8823c12feb09cf0f0d7f7e143dc",
    "url": "https://www.semanticscholar.org/paper/69aef44e0e39f8823c12feb09cf0f0d7f7e143dc",
    "title": "KNN-LM Does Not Improve Open-ended Text Generation",
    "abstract": "In this paper, we study the generation quality of interpolation-based retrieval-augmented language models (LMs). These methods, best exemplified by the KNN-LM, interpolate the LM's predicted distribution of the next word with a distribution formed from the most relevant retrievals for a given prefix. While the KNN-LM and related methods yield impressive decreases in perplexity, we discover that they do not exhibit corresponding improvements in open-ended generation quality, as measured by both automatic evaluation metrics (e.g., MAUVE) and human evaluations. Digging deeper, we find that interpolating with a retrieval distribution actually increases perplexity compared to a baseline Transformer LM for the majority of tokens in the WikiText-103 test set, even though the overall perplexity is lower due to a smaller number of tokens for which perplexity dramatically decreases after interpolation. However, when decoding a long sequence at inference time, significant improvements on this smaller subset of tokens are washed out by slightly worse predictions on most tokens. Furthermore, we discover that the entropy of the retrieval distribution increases faster than that of the base LM as the generated sequence becomes longer, which indicates that retrieval is less reliable when using model-generated text as queries (i.e., is subject to exposure bias). We hope that our analysis spurs future work on improved decoding algorithms and interpolation strategies for retrieval-augmented language models.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.14625",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-24",
    "authors": [
      {
        "authorId": "2051628335",
        "name": "Shufan Wang"
      },
      {
        "authorId": "8280888",
        "name": "Yixiao Song"
      },
      {
        "authorId": "32573794",
        "name": "Andrew Drozdov"
      },
      {
        "authorId": "31099365",
        "name": "Aparna Garimella"
      },
      {
        "authorId": "1977256",
        "name": "Varun Manjunatha"
      },
      {
        "authorId": "2136562",
        "name": "Mohit Iyyer"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "2a63405f1749cca44b9ade4ed4b68e155927fffc",
    "url": "https://www.semanticscholar.org/paper/2a63405f1749cca44b9ade4ed4b68e155927fffc",
    "title": "Dynamic In-Context Learning from Nearest Neighbors for Bundle Generation",
    "abstract": "Product bundling has evolved into a crucial marketing strategy in e-commerce. However, current studies are limited to generating (1) fixed-size or single bundles, and most importantly, (2) bundles that do not reflect consistent user intents, thus being less intelligible or useful to users. This paper explores two interrelated tasks, i.e., personalized bundle generation and the underlying intent inference based on users' interactions in a session, leveraging the logical reasoning capability of large language models. We introduce a dynamic in-context learning paradigm, which enables ChatGPT to seek tailored and dynamic lessons from closely related sessions as demonstrations while performing tasks in the target session. Specifically, it first harnesses retrieval augmented generation to identify nearest neighbor sessions for each target session. Then, proper prompts are designed to guide ChatGPT to perform the two tasks on neighbor sessions. To enhance reliability and mitigate the hallucination issue, we develop (1) a self-correction strategy to foster mutual improvement in both tasks without supervision signals; and (2) an auto-feedback mechanism to recurrently offer dynamic supervision based on the distinct mistakes made by ChatGPT on various neighbor sessions. Thus, the target session can receive customized and dynamic lessons for improved performance by observing the demonstrations of its neighbor sessions. Finally, experimental results on three real-world datasets verify the effectiveness of our methods on both tasks. Additionally, the inferred intents can prove beneficial for other intriguing downstream tasks, such as crafting appealing bundle names.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2023,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-26",
    "authors": [
      {
        "authorId": "2274082438",
        "name": "Zhu Sun"
      },
      {
        "authorId": "2273938691",
        "name": "Kaidong Feng"
      },
      {
        "authorId": "2276986037",
        "name": "Jie Yang"
      },
      {
        "authorId": "40507824",
        "name": "Xinghua Qu"
      },
      {
        "authorId": "2276798784",
        "name": "Hui Fang"
      },
      {
        "authorId": "8748397",
        "name": "Y. Ong"
      },
      {
        "authorId": "2276742821",
        "name": "Wenyuan Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 92.47918433002164
  },
  {
    "paperId": "4eff164ceb0b80f57c2d0cbd10241799972fa9d0",
    "url": "https://www.semanticscholar.org/paper/4eff164ceb0b80f57c2d0cbd10241799972fa9d0",
    "title": "Novel Preprocessing Technique for Data Embedding in Engineering Code Generation Using Large Language Model",
    "abstract": "We introduce four principal contributions to augment the capabilities of Large Language Models (LLMs) in generating domain-specific code: (i) leveraging LLM-based data splitting and data renovation techniques to refine the semantic representation within the embedding space; (ii) proposing an effective method for refactoring existing scripts, enabling the generation of new and high-quality scripts with the aid of LLMs; (iii) developing the Implicit Knowledge Expansion and Contemplation (IKEC) Prompt technique; and (iv) showcasing the efficacy of our data pre-processing approach through a case study using engineering simulation software RedHawk-SC. Our contributions collectively advance the Retrieval-Augmented Generation (RAG) framework, enabling more relevant and precise information retrieval. An arena-style evaluation by 28 domain experts and 182 votes confirms the significant effectiveness of our methods. Notably, our approach achieves up to 1.43 times the improvement in code generation for MapReduce applications compared to the Chain-of-Thought (CoT) technique.",
    "venue": "2024 IEEE LLM Aided Design Workshop (LAD)",
    "year": 2023,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2311.16267",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-11-27",
    "authors": [
      {
        "authorId": "2267682437",
        "name": "Yu-Chen Lin"
      },
      {
        "authorId": "1825681097",
        "name": "Akhilesh Kumar"
      },
      {
        "authorId": "2268493398",
        "name": "Norman Chang"
      },
      {
        "authorId": "2268552153",
        "name": "Wen-Liang Zhang"
      },
      {
        "authorId": "2268494226",
        "name": "Muhammad Zakir"
      },
      {
        "authorId": "153036931",
        "name": "Rucha Apte"
      },
      {
        "authorId": "2165037491",
        "name": "Haiyang He"
      },
      {
        "authorId": "2268560796",
        "name": "Chao Wang"
      },
      {
        "authorId": "2260185626",
        "name": "Jyh-Shing Roger Jang"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.39720770839918
  },
  {
    "paperId": "427939d4e1b9e12c5737244b1d9315b90497aa29",
    "url": "https://www.semanticscholar.org/paper/427939d4e1b9e12c5737244b1d9315b90497aa29",
    "title": "Resolving References in Visually-Grounded Dialogue via Text Generation",
    "abstract": "Vision-language models (VLMs) have shown to be effective at image retrieval based on simple text queries, but text-image retrieval based on conversational input remains a challenge. Consequently, if we want to use VLMs for reference resolution in visually-grounded dialogue, the discourse processing capabilities of these models need to be augmented. To address this issue, we propose fine-tuning a causal large language model (LLM) to generate definite descriptions that summarize coreferential information found in the linguistic context of references. We then use a pretrained VLM to identify referents based on the generated descriptions, zero-shot. We evaluate our approach on a manually annotated dataset of visually-grounded dialogues and achieve results that, on average, exceed the performance of the baselines we compare against. Furthermore, we find that using referent descriptions based on larger context windows has the potential to yield higher returns.",
    "venue": "SIGDIAL Conferences",
    "year": 2023,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.13430",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-23",
    "authors": [
      {
        "authorId": "34785846",
        "name": "Bram Willemsen"
      },
      {
        "authorId": "2266586888",
        "name": "Livia Qian"
      },
      {
        "authorId": "1711959",
        "name": "Gabriel Skantze"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "a5796234c33fa4e8549fe03ae1b52621817582de",
    "url": "https://www.semanticscholar.org/paper/a5796234c33fa4e8549fe03ae1b52621817582de",
    "title": "A Knowledge Augmented and Multimodal-Based Framework for Video Summarization",
    "abstract": "Video summarization aims to generate a compact version of a lengthy video that retains its primary content. In general, humans are gifted with producing a high-quality video summary, because they acquire crucial content through multiple dimensional information and own abundant background knowledge about the original video. However, existing methods rarely consider multichannel information and ignore the impact of external knowledge, resulting in the limited quality of the generated summaries. This paper proposes a knowledge augmented and multimodal-based video summarization method, termed KAMV, to address the problem above. Specifically, we design a knowledge encoder with a hybrid method consisting of generation and retrieval, to capture descriptive content and latent connections between events and entities based on the external knowledge base, which can provide rich implicit knowledge for better comprehending the video viewed. Furthermore, for the sake of exploring the interactions among visual, audio, implicit knowledge and emphasizing the content that is most relevant to the desired summary, we present a fusion module under the supervision of these multimodal information. By conducting extensive experiments on four public datasets, the results demonstrate the superior performance yielded by the proposed KAMV compared to the state-of-the-art video summarization approaches.",
    "venue": "ACM Multimedia",
    "year": 2022,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle"
    ],
    "publicationDate": "2022-10-10",
    "authors": [
      {
        "authorId": "147178951",
        "name": "Jiehang Xie"
      },
      {
        "authorId": "2028852606",
        "name": "Xuanbai Chen"
      },
      {
        "authorId": "3075033",
        "name": "Shao-Ping Lu"
      },
      {
        "authorId": "2108822506",
        "name": "Yulu Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.53877639491068
  },
  {
    "paperId": "59d440de6378de10bf8da3995e64c70be23aa88e",
    "url": "https://www.semanticscholar.org/paper/59d440de6378de10bf8da3995e64c70be23aa88e",
    "title": "Visually-augmented pretrained language models for NLP tasks without images",
    "abstract": "Although pre-trained language models (PLMs) have shown impressive performance by text-only self-supervised training, they are found lack of visual semantics or commonsense. Existing solutions often rely on explicit images for visual knowledge augmentation (requiring time-consuming retrieval or generation), and they also conduct the augmentation for the whole input text, without considering whether it is actually needed in specific inputs or tasks. To address these issues, we propose a novel **V**isually-**A**ugmented fine-tuning approach that can be generally applied to various PLMs or NLP tasks, **W**ithout using any retrieved or generated **I**mages, namely **VAWI**. Experimental results show that our approach can consistently improve the performance of BERT, RoBERTa, BART, and T5 at different scales, and outperform several competitive baselines on ten tasks. Our codes and data are publicly available at https://github.com/RUCAIBox/VAWI.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2212.07937",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-12-15",
    "authors": [
      {
        "authorId": "138404610",
        "name": "Hangyu Guo"
      },
      {
        "authorId": "1423651904",
        "name": "Kun Zhou"
      },
      {
        "authorId": "2542603",
        "name": "Wayne Xin Zhao"
      },
      {
        "authorId": "2190563903",
        "name": "Qinyu Zhang"
      },
      {
        "authorId": "153693432",
        "name": "Ji-rong Wen"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "5aaf780c63c87db6c90c7f96308cf90296c5b226",
    "url": "https://www.semanticscholar.org/paper/5aaf780c63c87db6c90c7f96308cf90296c5b226",
    "title": "Zero-shot Image Captioning by Anchor-augmented Vision-Language Space Alignment",
    "abstract": "CLIP (Contrastive Language-Image Pre-Training) has shown remarkable zero-shot transfer capabilities in cross-modal correlation tasks such as visual classification and image retrieval. However, its performance in cross-modal generation tasks like zero-shot image captioning remains unsatisfied. In this work, we discuss that directly employing CLIP for zero-shot image captioning relies more on the textual modality in context and largely ignores the visual information, which we call \\emph{contextual language prior}. To address this, we propose Cross-modal Language Models (CLMs) to facilitate unsupervised cross-modal learning. We further propose Anchor Augment to guide the generative model's attention to the fine-grained information in the representation of CLIP. Experiments on MS COCO and Flickr 30K validate the promising performance of proposed approach in both captioning quality and computational efficiency.",
    "venue": "arXiv.org",
    "year": 2022,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2211.07275",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-11-14",
    "authors": [
      {
        "authorId": "2110125710",
        "name": "Junyan Wang"
      },
      {
        "authorId": "2153912315",
        "name": "Yi Zhang"
      },
      {
        "authorId": "145929484",
        "name": "Ming Yan"
      },
      {
        "authorId": "2116921824",
        "name": "Ji Zhang"
      },
      {
        "authorId": "1798398",
        "name": "J. Sang"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "4989c08930e42d322b3bfed167d7ea434a698f2c",
    "url": "https://www.semanticscholar.org/paper/4989c08930e42d322b3bfed167d7ea434a698f2c",
    "title": "CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation",
    "abstract": "Counterfactual data augmentation (CDA) -- i.e., adding minimally perturbed inputs during training -- helps reduce model reliance on spurious correlations and improves generalization to out-of-distribution (OOD) data. Prior work on generating counterfactuals only considered restricted classes of perturbations, limiting their effectiveness. We present COunterfactual Generation via Retrieval and Editing (CORE), a retrieval-augmented generation framework for creating diverse counterfactual perturbations for CDA. For each training example, CORE first performs a dense retrieval over a task-related unlabeled text corpus using a learned bi-encoder and extracts relevant counterfactual excerpts. CORE then incorporates these into prompts to a large language model with few-shot learning capabilities, for counterfactual editing. Conditioning language model edits on naturally occurring data results in diverse perturbations. Experiments on natural language inference and sentiment analysis benchmarks show that CORE counterfactuals are more effective at improving generalization to OOD data compared to other DA approaches. We also show that the CORE retrieval framework can be used to encourage diversity in manually authored perturbations",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "citationCount": 17,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2210.04873",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-10-10",
    "authors": [
      {
        "authorId": "2126503480",
        "name": "Tanay Dixit"
      },
      {
        "authorId": "2132497930",
        "name": "Bhargavi Paranjape"
      },
      {
        "authorId": "2548384",
        "name": "Hannaneh Hajishirzi"
      },
      {
        "authorId": "1982950",
        "name": "Luke Zettlemoyer"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.35557636844247
  },
  {
    "paperId": "b164a88f49d929a81e9ad02ad50863cf5aad8b2b",
    "url": "https://www.semanticscholar.org/paper/b164a88f49d929a81e9ad02ad50863cf5aad8b2b",
    "title": "Persona-Knowledge Dialogue Multi-Context Retrieval and Enhanced Decoding Methods",
    "abstract": "Persona and Knowledge dual context open-domain chat is a novel dialogue generation task introduced recently. While Persona and Knowledge is each interesting context of open-domain dialogue, the combination of both has not been well studied. We tackle Persona-Knowledge identification and response generation tasks in this paper. We design an informed data augmentation strategy that is compatible with neural Q&A retrieval models. With the augmented data, we perform permutative Persona-Knowledge evaluation and successive Persona search fine-tuning. Furthermore, we perform dialogue generation with various decoding techniques and illustrate crucial elements. We achieve SOTA across official metrics with 93.99% Grounding accuracy average and 23.62 SacreBLEU score.",
    "venue": "arXiv.org",
    "year": 2022,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2207.13919",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-07-28",
    "authors": [
      {
        "authorId": "2162784459",
        "name": "Min Sik Oh"
      },
      {
        "authorId": "2179576783",
        "name": "Min Sang Kim"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "d7cd0fb8682bf4c018511d3d7dd8d137f1caa40a",
    "url": "https://www.semanticscholar.org/paper/d7cd0fb8682bf4c018511d3d7dd8d137f1caa40a",
    "title": "Learned hardware-in-the-loop phase retrieval for holographic near-eye displays",
    "abstract": "Holography is arguably the most promising technology to provide wide field-of-view compact eyeglasses-style near-eye displays for augmented and virtual reality. However, the image quality of existing holographic displays is far from that of current generation conventional displays, effectively making today's holographic display systems impractical. This gap stems predominantly from the severe deviations in the idealized approximations of the \"unknown\" light transport model in a real holographic display, used for computing holograms. In this work, we depart from such approximate \"ideal\" coherent light transport models for computing holograms. Instead, we learn the deviations of the real display from the ideal light transport from the images measured using a display-camera hardware system. After this unknown light propagation is learned, we use it to compensate for severe aberrations in real holographic imagery. The proposed hardware-in-the-loop approach is robust to spatial, temporal and hardware deviations, and improves the image quality of existing methods qualitatively and quantitatively in SNR and perceptual quality. We validate our approach on a holographic display prototype and show that the method can fully compensate unknown aberrations and erroneous and non-linear SLM phase delays, without explicitly modeling them. As a result, the proposed method significantly outperforms existing state-of-the-art methods in simulation and experimentation - just by observing captured holographic images.",
    "venue": "ACM Transactions on Graphics",
    "year": 2020,
    "citationCount": 86,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2020-11-26",
    "authors": [
      {
        "authorId": "51151674",
        "name": "Praneeth Chakravarthula"
      },
      {
        "authorId": "32324156",
        "name": "Ethan Tseng"
      },
      {
        "authorId": "121319612",
        "name": "Tarun Srivastava"
      },
      {
        "authorId": "145472944",
        "name": "H. Fuchs"
      },
      {
        "authorId": "49857575",
        "name": "Felix Heide"
      }
    ],
    "source": "semantic_scholar",
    "score": 142.98862177981874
  },
  {
    "paperId": "a81a09b2a4ce36ae5c847fc4e3558c523d301179",
    "url": "https://www.semanticscholar.org/paper/a81a09b2a4ce36ae5c847fc4e3558c523d301179",
    "title": "Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks",
    "abstract": "Retrieval-augmented generation models have shown state-of-the-art performance across many knowledge-intensive NLP tasks such as open-domain question answering and fact verification. These models are trained to generate a final output given retrieved passages that can be irrelevant to an input query, leading to learning spurious cues or memorization. This work introduces a method to incorporate evidentiality of passages‚Äîwhether a passage contains correct evidence to support the output‚Äîinto training the generator. We introduce a multi-task learning framework to jointly generate the final output and predict the evidentiality of each passage. Furthermore, we introduce a new task-agnostic method for obtaining high-quality silver evidentiality labels, addressing the issues of gold evidentiality labels being unavailable in most domains. Our experiments on five datasets across three knowledge-intensive tasks show that our new evidentiality-guided generator significantly outperforms its direct counterpart on all of them, and advances the state of the art on three of them. Our analysis shows that multi-task learning and silver evidentiality mining play key roles. Our code is available at https://github.com/AkariAsai/evidentiality_qa",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2021,
    "citationCount": 45,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2022.naacl-main.162.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-12-16",
    "authors": [
      {
        "authorId": "35584853",
        "name": "Akari Asai"
      },
      {
        "authorId": "40642935",
        "name": "Matt Gardner"
      },
      {
        "authorId": "2548384",
        "name": "Hannaneh Hajishirzi"
      }
    ],
    "source": "semantic_scholar",
    "score": 127.42962094733642
  },
  {
    "paperId": "8e5e94417f5495c5b017ff73aaccdc876b10120d",
    "url": "https://www.semanticscholar.org/paper/8e5e94417f5495c5b017ff73aaccdc876b10120d",
    "title": "Memory and Knowledge Augmented Language Models for Inferring Salience in Long-Form Stories",
    "abstract": "Measuring event salience is essential in the understanding of stories. This paper takes a recent unsupervised method for salience detection derived from Barthes Cardinal Functions and theories of surprise and applies it to longer narrative forms. We improve the standard transformer language model by incorporating an external knowledgebase (derived from Retrieval Augmented Generation) and adding a memory mechanism to enhance performance on longer works. We use a novel approach to derive salience annotation using chapter-aligned summaries from the Shmoop corpus for classic literary works. Our evaluation against this data demonstrates that our salience detection model improves performance over and above a non-knowledgebase and memory augmented language model, both of which are crucial to this improvement.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "citationCount": 19,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2021.emnlp-main.65.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-09-08",
    "authors": [
      {
        "authorId": "1666944787",
        "name": "David Wilmot"
      },
      {
        "authorId": "143694777",
        "name": "Frank Keller"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.93598410330986
  },
  {
    "paperId": "e0625715d17155fb91601052f5f2bc992e803d58",
    "url": "https://www.semanticscholar.org/paper/e0625715d17155fb91601052f5f2bc992e803d58",
    "title": "Long-term Control for Dialogue Generation: Methods and Evaluation",
    "abstract": "Current approaches for controlling dialogue response generation are primarily focused on high-level attributes like style, sentiment, or topic. In this work, we focus on constrained long-term dialogue generation, which involves more fine-grained control and requires a given set of control words to appear in generated responses. This setting requires a model to not only consider the generation of these control words in the immediate context, but also produce utterances that will encourage the generation of the words at some time in the (possibly distant) future. We define the problem of constrained long-term control for dialogue generation, identify gaps in current methods for evaluation, and propose new metrics that better measure long-term control. We also propose a retrieval-augmented method that improves performance of long-term controlled generation via logit modification techniques. We show through experiments on three task-oriented dialogue datasets that our metrics better assess dialogue control relative to current alternatives and that our method outperforms state-of-the-art constrained generation baselines.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2022,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2205.07352",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-05-15",
    "authors": [
      {
        "authorId": "3115147",
        "name": "Ramya Ramakrishnan"
      },
      {
        "authorId": "2165377944",
        "name": "H. Narangodage"
      },
      {
        "authorId": "120494256",
        "name": "M. Schilman"
      },
      {
        "authorId": "7446832",
        "name": "Kilian Q. Weinberger"
      },
      {
        "authorId": "143957226",
        "name": "Ryan T. McDonald"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "d15d96517370c9ed0658d176b979bcf92d1373ea",
    "url": "https://www.semanticscholar.org/paper/d15d96517370c9ed0658d176b979bcf92d1373ea",
    "title": "Reason first, then respond: Modular Generation for Knowledge-infused Dialogue",
    "abstract": "Large language models can produce fluent dialogue but often hallucinate factual inaccuracies. While retrieval-augmented models help alleviate this issue, they still face a difficult challenge of both reasoning to provide correct knowledge and generating conversation simultaneously. In this work, we propose a modular model, Knowledge to Response (K2R), for incorporating knowledge into conversational agents, which breaks down this problem into two easier steps. K2R first generates a knowledge sequence, given a dialogue context, as an intermediate step. After this\"reasoning step\", the model then attends to its own generated knowledge sequence, as well as the dialogue context, to produce a final response. In detailed experiments, we find that such a model hallucinates less in knowledge-grounded dialogue tasks, and has advantages in terms of interpretability and modularity. In particular, it can be used to fuse QA and dialogue systems together to enable dialogue agents to give knowledgeable answers, or QA models to give conversational responses in a zero-shot setting.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "citationCount": 37,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2022.findings-emnlp.527.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-11-09",
    "authors": [
      {
        "authorId": "46196592",
        "name": "Leonard Adolphs"
      },
      {
        "authorId": "35752280",
        "name": "Kurt Shuster"
      },
      {
        "authorId": "39219656",
        "name": "Jack Urbanek"
      },
      {
        "authorId": "3149531",
        "name": "Arthur Szlam"
      },
      {
        "authorId": "145183709",
        "name": "J. Weston"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.56379239589579
  },
  {
    "paperId": "c660908cf507a1a9f30a9974437170cf4c2ad766",
    "url": "https://www.semanticscholar.org/paper/c660908cf507a1a9f30a9974437170cf4c2ad766",
    "title": "Automatic Explanation Generation For Climate Science Claims",
    "abstract": "Climate change is an existential threat to humanity, the proliferation of unsubstantiated claims relating to climate science is manipulating public perception, motivating the need for fact-checking in climate science. In this work, we draw on recent work that uses retrieval-augmented generation for veracity prediction and explanation generation, in framing explanation generation as a query-focused multi-document summarization task. We adapt PRIMERA to the climate science domain by adding additional global attention on claims. Through automatic evaluation and qualitative analysis, we demonstrate that our method is effective at generating explanations.",
    "venue": "Australasian Language Technology Association Workshop",
    "year": 2022,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2209479763",
        "name": "Rui Xing"
      },
      {
        "authorId": "1984990",
        "name": "Shraey Bhatia"
      },
      {
        "authorId": "145465286",
        "name": "Timothy Baldwin"
      },
      {
        "authorId": "1800564",
        "name": "Jey Han Lau"
      }
    ],
    "source": "semantic_scholar",
    "score": 85.79441541679836
  },
  {
    "paperId": "1b46f99a9c1b481cfa4cf7cf50e994bb9cc0b2d9",
    "url": "https://www.semanticscholar.org/paper/1b46f99a9c1b481cfa4cf7cf50e994bb9cc0b2d9",
    "title": "Structural Constraints and Natural Language Inference for End-to-End Flowchart Grounded Dialog Response Generation",
    "abstract": "Flowchart grounded dialog systems converse with users by following a given flowchart and a corpus of FAQs. The existing state-of-the-art approach (Raghu et al, 2021) for learning such a dialog system, named FLONET, has two main limitations. (1) It uses a Retrieval Augmented Generation (RAG) framework which represents a flowchart as a bag of nodes. By doing so, it loses the connectivity structure between nodes that can aid in better response generation. (2) Typically dialogs progress with the agent asking polar (Y/N) questions, but users often respond indirectly without the explicit use of polar words. In such cases, it fails to understand the correct polarity of the answer. To overcome these issues, we propose Structure-Aware FLONET (SA-FLONET) which infuses structural constraints derived from the connectivity structure of flowcharts into the RAG framework. It uses natural language inference to better predict the polarity of indirect Y/N answers. We find that SA-FLONET outperforms FLONET, with a success rate improvement of 68% and 123% in flowchart grounded response generation and zero-shot flowchart grounded response generation tasks respectively.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2022.emnlp-main.739.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1916865",
        "name": "Dinesh Raghu"
      },
      {
        "authorId": "2114035331",
        "name": "Suraj Joshi"
      },
      {
        "authorId": "1703799",
        "name": "Sachindra Joshi"
      },
      {
        "authorId": "2203789510",
        "name": "M. -"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "207d75b3d6d57b28ce45860566f13eb0f1959ced",
    "url": "https://www.semanticscholar.org/paper/207d75b3d6d57b28ce45860566f13eb0f1959ced",
    "title": "Pseudo Eye: The next-generation shopping application using Augmented Reality",
    "abstract": "Nowadays, online shopping is getting increasingly popular as it refines the shopping to the comfort of the homes rather than spending money on fuel and driving to a shop. The greater usage of smartphones and tablets increased the amount of time consumers spend online. E-shopping can be taken to the next stage with convenience and innovation with the help of Augmented Reality in this fast moving world. This paper presents Pseudo Eye, an innovative shopping application using Augmented Reality and developed for various platforms like Android and Windows Phone. It enables the users to purchase with ease and full in-store experience. It uses the Content Based Image Retrieval technique to search the captured image of a product and retrieve its details from the database.",
    "venue": "International Conference for Internet Technology and Secured Transactions",
    "year": 2013,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1409904812",
        "name": "A. S"
      },
      {
        "authorId": "1393949888",
        "name": "A. G"
      },
      {
        "authorId": "10789372",
        "name": "Sree Sharmila T"
      }
    ],
    "source": "semantic_scholar",
    "score": 84.1415686865115
  },
  {
    "paperId": "3e00eceb0de85f4e5ddb714158dd858553ca7506",
    "url": "https://www.semanticscholar.org/paper/3e00eceb0de85f4e5ddb714158dd858553ca7506",
    "title": "Texture Memory-Augmented Deep Patch-Based Image Inpainting",
    "abstract": "Patch-based methods and deep networks have been employed to tackle image inpainting problem, with their own strengths and weaknesses. Patch-based methods are capable of restoring a missing region with high-quality texture through searching nearest neighbor patches from the unmasked regions. However, these methods bring problematic contents when recovering large missing regions. Deep networks, on the other hand, show promising results in completing large regions. Nonetheless, the results often lack faithful and sharp details that resemble the surrounding area. By bringing together the best of both paradigms, we propose a new deep inpainting framework where texture generation is guided by a texture memory of patch samples extracted from unmasked regions. The framework has a novel design that allows texture memory retrieval to be trained end-to-end with the deep inpainting network. In addition, we introduce a patch distribution loss to encourage high-quality patch synthesis. The proposed method shows superior performance both qualitatively and quantitatively on three challenging image benchmarks, i.e., Places, CelebA-HQ, and Paris Street-View datasets (Code will be made publicly available in https://github.com/open-mmlab/mmediting).",
    "venue": "IEEE Transactions on Image Processing",
    "year": 2020,
    "citationCount": 39,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2009.13240",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2020-09-28",
    "authors": [
      {
        "authorId": "30516781",
        "name": "Rui Xu"
      },
      {
        "authorId": "4664629",
        "name": "Minghao Guo"
      },
      {
        "authorId": "2110238778",
        "name": "Jiaqi Wang"
      },
      {
        "authorId": "2108536754",
        "name": "Xiaoxiao Li"
      },
      {
        "authorId": "145291669",
        "name": "Bolei Zhou"
      },
      {
        "authorId": "1717179",
        "name": "Chen Change Loy"
      }
    ],
    "source": "semantic_scholar",
    "score": 131.33319181170904
  },
  {
    "paperId": "d9d9c9d8a5de66302754186c7e228fd6a1a909ec",
    "url": "https://www.semanticscholar.org/paper/d9d9c9d8a5de66302754186c7e228fd6a1a909ec",
    "title": "Pseudo Eye: The next-generation shopping application using Augmented Reality",
    "abstract": "Nowadays, online shopping is getting increasingly popular as it refines the shopping to the comfort of the homes rather than spending money on fuel and driving to a shop. The greater usage of smartphones and tablets increased the amount of time consumers spend online. E-shopping can be taken to the next stage with convenience and innovation with the help of Augmented Reality in this fast moving world. This paper presents Pseudo Eye, an innovative shopping application using Augmented Reality and developed for various platforms like Android and Windows Phone. It enables the users to purchase with ease and full in-store experience. It uses the Content Based Image Retrieval technique to search the captured image of a product and retrieve its details from the database.",
    "venue": "International Conference for Internet Technology and Secured Transactions",
    "year": 2013,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2013-12-01",
    "authors": [
      {
        "authorId": "145903862",
        "name": "S. Kanagaraj"
      },
      {
        "authorId": "144305386",
        "name": "G. Arjun"
      },
      {
        "authorId": "2342660",
        "name": "T. Sharmila"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.79441541679836
  },
  {
    "paperId": "d4489958386c4ad9b7e72c957faa29a4eacc3265",
    "url": "https://www.semanticscholar.org/paper/d4489958386c4ad9b7e72c957faa29a4eacc3265",
    "title": "Efficiency Implications of Term Weighting for Passage Retrieval",
    "abstract": "Language model pre-training has spurred a great deal of attention for tasks involving natural language understanding, and has been successfully applied to many downstream tasks with impressive results. Within information retrieval, many of these solutions are too costly to stand on their own, requiring multi-stage ranking architectures. Recent work has begun to consider how to \"backport\" salient aspects of these computationally expensive models to previous stages of the retrieval pipeline. One such instance is DeepCT, which uses BERT to re-weight term importance in a given context at the passage level. This process, which is computed offline, results in an augmented inverted index with re-weighted term frequency values. In this work, we conduct an investigation of query processing efficiency over DeepCT indexes. Using a number of candidate generation algorithms, we reveal how term re-weighting can impact query processing latency, and explore how DeepCT can be used as a static index pruning technique to accelerate query processing without harming search effectiveness.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2020,
    "citationCount": 27,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2020-07-25",
    "authors": [
      {
        "authorId": "47470313",
        "name": "J. Mackenzie"
      },
      {
        "authorId": "2475437",
        "name": "Zhuyun Dai"
      },
      {
        "authorId": "35154089",
        "name": "L. Gallagher"
      },
      {
        "authorId": "144987107",
        "name": "Jamie Callan"
      }
    ],
    "source": "semantic_scholar",
    "score": 125.98306765262805
  },
  {
    "paperId": "56f254877417f134bb670f3483560ace47b8f869",
    "url": "https://www.semanticscholar.org/paper/56f254877417f134bb670f3483560ace47b8f869",
    "title": "ARSketch: Sketch-Based User Interface for Augmented Reality Glasses",
    "abstract": "Hand gesture interaction is a key component in Augmented Reality (AR) / Mixed Reality (MR). Users usually interact with AR/MR devices, e.g., Microsoft HoloLens, etc., via hand gestures to express their intentions and the devices will recognize the gestures and respond accordingly to users. However, the use of such technique so far is limited to only a few less-expressive hand gestures, which, unfortunately, are insufficient or inadequate to input complex information. To tackle this problem, we introduce a sketch-based neural network-driven user interface for AR/MR glasses, called ARSketch, which enables drawing sketches freely in air to interact with the devices. ARSketch combines: (1) hand pose estimation that estimates the egocentric hand poses in an energy-efficient way, (2) sketch generation that generates sketches using key point positions of hand poses, and (3) sketch-photo retrieval that takes sketches as inputs to retrieve relevant photos. The evaluation results on our collected sketch dataset demonstrate the efficacy of ARSketch for user interaction.",
    "venue": "ACM Multimedia",
    "year": 2020,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2020-10-12",
    "authors": [
      {
        "authorId": "2156120066",
        "name": "Zhaohui Zhang"
      },
      {
        "authorId": "2387872",
        "name": "Haichao Zhu"
      },
      {
        "authorId": "2145948441",
        "name": "Qian Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.1886522358297
  },
  {
    "paperId": "c1799bf28d1ae93e1631be5b59196ee1e568f538",
    "url": "https://www.semanticscholar.org/paper/c1799bf28d1ae93e1631be5b59196ee1e568f538",
    "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization",
    "abstract": "The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as\"What are the main themes in the dataset?\", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose a Graph RAG approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed. Our approach uses an LLM to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that Graph RAG leads to substantial improvements over a na\\\"ive RAG baseline for both the comprehensiveness and diversity of generated answers. An open-source, Python-based implementation of both global and local Graph RAG approaches is forthcoming at https://aka.ms/graphrag.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 148,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-24",
    "authors": [
      {
        "authorId": "2298275009",
        "name": "Darren Edge"
      },
      {
        "authorId": "2213073417",
        "name": "Ha Trinh"
      },
      {
        "authorId": "2298273973",
        "name": "Newman Cheng"
      },
      {
        "authorId": "2298275805",
        "name": "Joshua Bradley"
      },
      {
        "authorId": "2298274563",
        "name": "Alex Chao"
      },
      {
        "authorId": "2210994342",
        "name": "Apurva Mody"
      },
      {
        "authorId": "2298273810",
        "name": "Steven Truitt"
      },
      {
        "authorId": "2298278846",
        "name": "Jonathan Larson"
      }
    ],
    "source": "semantic_scholar",
    "score": 145.05919458918189
  },
  {
    "paperId": "ddbd8fe782ac98e9c64dd98710687a962195dd9b",
    "url": "https://www.semanticscholar.org/paper/ddbd8fe782ac98e9c64dd98710687a962195dd9b",
    "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
    "abstract": "Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 410,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-17",
    "authors": [
      {
        "authorId": "35584853",
        "name": "Akari Asai"
      },
      {
        "authorId": "7806955",
        "name": "Zeqiu Wu"
      },
      {
        "authorId": "1705260",
        "name": "Yizhong Wang"
      },
      {
        "authorId": "2707234",
        "name": "Avirup Sil"
      },
      {
        "authorId": "2548384",
        "name": "Hannaneh Hajishirzi"
      }
    ],
    "source": "semantic_scholar",
    "score": 160.2788982174435
  },
  {
    "paperId": "0e8176ecced2a01ca3c7dc31e8a3f72d0a7d3767",
    "url": "https://www.semanticscholar.org/paper/0e8176ecced2a01ca3c7dc31e8a3f72d0a7d3767",
    "title": "Generative Representational Instruction Tuning",
    "abstract": "All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by>60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available at https://github.com/ContextualAI/gritlm.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 66,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-15",
    "authors": [
      {
        "authorId": "2037383772",
        "name": "Niklas Muennighoff"
      },
      {
        "authorId": "2152173042",
        "name": "Hongjin Su"
      },
      {
        "authorId": "145769448",
        "name": "Liang Wang"
      },
      {
        "authorId": "2242947624",
        "name": "Nan Yang"
      },
      {
        "authorId": "2257346447",
        "name": "Furu Wei"
      },
      {
        "authorId": "2284337890",
        "name": "Tao Yu"
      },
      {
        "authorId": "2284268811",
        "name": "Amanpreet Singh"
      },
      {
        "authorId": "2111313627",
        "name": "Douwe Kiela"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.07038929086448
  },
  {
    "paperId": "bd0cd89337cc40d39d3a4cbe9c8709e06e877f3e",
    "url": "https://www.semanticscholar.org/paper/bd0cd89337cc40d39d3a4cbe9c8709e06e877f3e",
    "title": "Continual Learning for Large Language Models: A Survey",
    "abstract": "Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 69,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-02-02",
    "authors": [
      {
        "authorId": "1514917592",
        "name": "Tongtong Wu"
      },
      {
        "authorId": "2238130759",
        "name": "Linhao Luo"
      },
      {
        "authorId": "2256011160",
        "name": "Yuan-Fang Li"
      },
      {
        "authorId": "2265933101",
        "name": "Shirui Pan"
      },
      {
        "authorId": "122699890",
        "name": "Thuy-Trang Vu"
      },
      {
        "authorId": "2561045",
        "name": "Gholamreza Haffari"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.7274286307404
  },
  {
    "paperId": "5ff0c91997299646ab16d244bab4343be56dc0ee",
    "url": "https://www.semanticscholar.org/paper/5ff0c91997299646ab16d244bab4343be56dc0ee",
    "title": "An arabic lexicon to support information retrieval, parsing, and text generation",
    "abstract": "We developed an Arabic lexical database to support information retrieval, text generation, and parsing. It contains information about 12,500 words in the computer sublanguage. The database has a main table containing all words and then separate tables for nouns, adjectives, verbs, and particles. \nThe main table contains basic information for each Arabic word in a corpus of 242 abstracts, part of speech (noun, verb, particle, adjective), gender (masculine or feminine), number (singular, dual, plural), person (1$\\sp{\\rm st}$, 2$\\sp{\\rm nd}$, 3$\\sp{\\rm rd}$). \nThe lexical entry for the noun contains gender (masc. or fem.), person (1st, 2nd, 3rd), number (singular, dual, plural). It also places the noun in a number of syntactic/semantic categories; inert or derived, concrete or abstract, structured or declined, denuded or augmented, animate or inanimate and human or nonhuman. \nThe lexical entry for the verb tells whether it is complete or deficient, transitive or intransitive, denuded or augmented, sound or defective or mixed, and imperfect, or perfect, imperative. \nThe lexical entry for each particle tells whether it acts on nouns or verbs or both. Particles that are active on nouns are classified as letters of reduction or annulment or vocatives or letters of exclusion or conjunctions. Particles that are active on verbs are specified as different kinds of letters of elusion or opening. Particles that are active on both nouns and verbs are particles of attraction. \nThe lexical entry for the adjective tells whether they are animate/nonanimate, for animate adjectives record (human/nonhuman). \nWe implemented a lexical database system on Arabic Windows using the Microsoft Access DBMS and a Graphical User Interface (GUI). It runs on IBM/PC's and its compatibles. It is designed to be used by both programs and human endusers, with the goal of supporting natural language processing systems, ongoing research at the Arabic Language Processing Laboratory at Illinois Institute of Technology and future research in the Arabic language.",
    "venue": "",
    "year": 1996,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2186744",
        "name": "M. Evens"
      },
      {
        "authorId": "2047342",
        "name": "Khalid Alsamara"
      }
    ],
    "source": "semantic_scholar",
    "score": 91.19162312519754
  },
  {
    "paperId": "d0489fd7c5ff833e08969341ae6e5fa1d0e6dd2a",
    "url": "https://www.semanticscholar.org/paper/d0489fd7c5ff833e08969341ae6e5fa1d0e6dd2a",
    "title": "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents",
    "abstract": "Recognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of fact-checking are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to a model to check a single response. In this work, we show how to build small fact-checking models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify datasets from recent work on fact-checking and grounding LLM generations into a new benchmark, LLM-AggreFact. Our best system MiniCheck-FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for data synthesis, and models.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 47,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-16",
    "authors": [
      {
        "authorId": "46815797",
        "name": "Liyan Tang"
      },
      {
        "authorId": "46180754",
        "name": "Philippe Laban"
      },
      {
        "authorId": "1814094",
        "name": "Greg Durrett"
      }
    ],
    "source": "semantic_scholar",
    "score": 128.06801516361838
  },
  {
    "paperId": "fef0393e997ec51b184e39c712be63197d99fd46",
    "url": "https://www.semanticscholar.org/paper/fef0393e997ec51b184e39c712be63197d99fd46",
    "title": "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture",
    "abstract": "There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47% to 72%. Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 55,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-16",
    "authors": [
      {
        "authorId": "34938986",
        "name": "M. A. D. L. Balaguer"
      },
      {
        "authorId": "3456623",
        "name": "Vinamra Benara"
      },
      {
        "authorId": "2279752416",
        "name": "Renato Luiz de Freitas Cunha"
      },
      {
        "authorId": "2279750954",
        "name": "Roberto de M. Estevao Filho"
      },
      {
        "authorId": "2279749685",
        "name": "Todd Hendry"
      },
      {
        "authorId": "2279750514",
        "name": "Daniel Holstein"
      },
      {
        "authorId": "2279752770",
        "name": "Jennifer Marsman"
      },
      {
        "authorId": "2279750706",
        "name": "Nick Mecklenburg"
      },
      {
        "authorId": "145707932",
        "name": "S. Malvar"
      },
      {
        "authorId": "2256989583",
        "name": "Leonardo Nunes"
      },
      {
        "authorId": "2279548480",
        "name": "Rafael Padilha"
      },
      {
        "authorId": "2279750745",
        "name": "Morris Sharp"
      },
      {
        "authorId": "2257019569",
        "name": "B. Silva"
      },
      {
        "authorId": "2279667352",
        "name": "Swati Sharma"
      },
      {
        "authorId": "2257349985",
        "name": "Vijay Aski"
      },
      {
        "authorId": "2256993742",
        "name": "Ranveer Chandra"
      }
    ],
    "source": "semantic_scholar",
    "score": 130.38027536102726
  },
  {
    "paperId": "b76e865e070cb353b52a3cb1e50c86ec460da79e",
    "url": "https://www.semanticscholar.org/paper/b76e865e070cb353b52a3cb1e50c86ec460da79e",
    "title": "Hallucination-Free? Assessing the Reliability of Leading AI Legal Research Tools",
    "abstract": "Legal practice has witnessed a sharp rise in products incorporating artificial intelligence (AI). Such tools are designed to assist with a wide range of core legal tasks, from search and summarization of caselaw to document drafting. But the large language models used in these tools are prone to\"hallucinate,\"or make up false information, making their use risky in high-stakes domains. Recently, certain legal research providers have touted methods such as retrieval-augmented generation (RAG) as\"eliminating\"(Casetext, 2023) or\"avoid[ing]\"hallucinations (Thomson Reuters, 2023), or guaranteeing\"hallucination-free\"legal citations (LexisNexis, 2023). Because of the closed nature of these systems, systematically assessing these claims is challenging. In this article, we design and report on the first preregistered empirical evaluation of AI-driven legal research tools. We demonstrate that the providers' claims are overstated. While hallucinations are reduced relative to general-purpose chatbots (GPT-4), we find that the AI research tools made by LexisNexis (Lexis+ AI) and Thomson Reuters (Westlaw AI-Assisted Research and Ask Practical Law AI) each hallucinate between 17% and 33% of the time. We also document substantial differences between systems in responsiveness and accuracy. Our article makes four key contributions. It is the first to assess and report the performance of RAG-based proprietary legal AI tools. Second, it introduces a comprehensive, preregistered dataset for identifying and understanding vulnerabilities in these systems. Third, it proposes a clear typology for differentiating between hallucinations and accurate legal responses. Last, it provides evidence to inform the responsibilities of legal professionals in supervising and verifying AI outputs, which remains a central open question for the responsible integration of AI into law.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 42,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-30",
    "authors": [
      {
        "authorId": "2277448249",
        "name": "Varun Magesh"
      },
      {
        "authorId": "2211734790",
        "name": "Faiz Surani"
      },
      {
        "authorId": "2277446850",
        "name": "Matthew Dahl"
      },
      {
        "authorId": "51903517",
        "name": "Mirac Suzgun"
      },
      {
        "authorId": "2302743439",
        "name": "Christopher D. Manning"
      },
      {
        "authorId": "2277449380",
        "name": "Daniel E. Ho"
      }
    ],
    "source": "semantic_scholar",
    "score": 126.41800173540344
  },
  {
    "paperId": "e3411dba0c25756a4171b7cace09b7e0497bd600",
    "url": "https://www.semanticscholar.org/paper/e3411dba0c25756a4171b7cace09b7e0497bd600",
    "title": "Attention based caption augmented W2VV++ Adhoc Video Search (AVS) trecvid task",
    "abstract": "In this paper we summarize our TRECVID 2020 video retrieval. We participated in Ad-hoc Video Search (AVS) task. For the AVS task, we developed our solutions based on W2VV++, a super version of Word2VisualVec (W2VV) by attempting optimization of hyperparameters and further augmenting it with attention based caption generation based text to text matching.",
    "venue": "TREC Video Retrieval Evaluation",
    "year": 2020,
    "citationCount": 0,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2111336260",
        "name": "R. Sharma"
      },
      {
        "authorId": "2082316146",
        "name": "Deepak Mishra"
      },
      {
        "authorId": "2410655",
        "name": "Haresh S. Bhatt"
      }
    ],
    "source": "semantic_scholar",
    "score": 65
  },
  {
    "paperId": "0bf3a1867f7245b8a702093901c66b08b518eafc",
    "url": "https://www.semanticscholar.org/paper/0bf3a1867f7245b8a702093901c66b08b518eafc",
    "title": "Evaluating Very Long-Term Conversational Memory of LLM Agents",
    "abstract": "Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 36,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-27",
    "authors": [
      {
        "authorId": "8785371",
        "name": "A. Maharana"
      },
      {
        "authorId": "2266803131",
        "name": "Dong-Ho Lee"
      },
      {
        "authorId": "145582202",
        "name": "S. Tulyakov"
      },
      {
        "authorId": "2285969697",
        "name": "Mohit Bansal"
      },
      {
        "authorId": "2266751000",
        "name": "Francesco Barbieri"
      },
      {
        "authorId": "2267220081",
        "name": "Yuwei Fang"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.16376868966336
  },
  {
    "paperId": "1d650f1afd45c59ff907396fe8b678595dcb85ea",
    "url": "https://www.semanticscholar.org/paper/1d650f1afd45c59ff907396fe8b678595dcb85ea",
    "title": "Memory-Based Model Editing at Scale",
    "abstract": "Even the largest neural networks make errors, and once-correct predictions can become invalid as the world changes. Model editors make local updates to the behavior of base (pre-trained) models to inject updated knowledge or correct undesirable behaviors. Existing model editors have shown promise, but also suffer from insufficient expressiveness: they struggle to accurately model an edit's intended scope (examples affected by the edit), leading to inaccurate predictions for test inputs loosely related to the edit, and they often fail altogether after many edits. As a higher-capacity alternative, we propose Semi-Parametric Editing with a Retrieval-Augmented Counterfactual Model (SERAC), which stores edits in an explicit memory and learns to reason over them to modulate the base model's predictions as needed. To enable more rigorous evaluation of model editors, we introduce three challenging language model editing problems based on question answering, fact-checking, and dialogue generation. We find that only SERAC achieves high performance on all three problems, consistently outperforming existing approaches to model editing by a significant margin. Code, data, and additional project information will be made available at https://sites.google.com/view/serac-editing.",
    "venue": "International Conference on Machine Learning",
    "year": 2022,
    "citationCount": 267,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-06-13",
    "authors": [
      {
        "authorId": "49688913",
        "name": "E. Mitchell"
      },
      {
        "authorId": "2116721670",
        "name": "Charles Lin"
      },
      {
        "authorId": "2691021",
        "name": "Antoine Bosselut"
      },
      {
        "authorId": "144783904",
        "name": "Christopher D. Manning"
      },
      {
        "authorId": "46881670",
        "name": "Chelsea Finn"
      }
    ],
    "source": "semantic_scholar",
    "score": 153.86480470766284
  },
  {
    "paperId": "1a735015a1f7ef4f2ba2273ce5fcaaacfa9d1ea2",
    "url": "https://www.semanticscholar.org/paper/1a735015a1f7ef4f2ba2273ce5fcaaacfa9d1ea2",
    "title": "Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning",
    "abstract": "We present CM3Leon (pronounced\"Chameleon\"), a retrieval-augmented, token-based, decoder-only multi-modal language model capable of generating and infilling both text and images. CM3Leon uses the CM3 multi-modal architecture but additionally shows the extreme benefits of scaling up and tuning on more diverse instruction-style data. It is the first multi-modal model trained with a recipe adapted from text-only language models, including a large-scale retrieval-augmented pre-training stage and a second multi-task supervised fine-tuning (SFT) stage. It is also a general-purpose model that can do both text-to-image and image-to-text generation, allowing us to introduce self-contained contrastive decoding methods that produce high-quality outputs. Extensive experiments demonstrate that this recipe is highly effective for multi-modal models. CM3Leon achieves state-of-the-art performance in text-to-image generation with 5x less training compute than comparable methods (zero-shot MS-COCO FID of 4.88). After SFT, CM3Leon can also demonstrate unprecedented levels of controllability in tasks ranging from language-guided image editing to image-controlled generation and segmentation.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 118,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.02591",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-05",
    "authors": [
      {
        "authorId": "49297123",
        "name": "L. Yu"
      },
      {
        "authorId": "2261676061",
        "name": "Bowen Shi"
      },
      {
        "authorId": "10721120",
        "name": "Ramakanth Pasunuru"
      },
      {
        "authorId": "2335443692",
        "name": "Benjamin Muller"
      },
      {
        "authorId": "100664938",
        "name": "O. Yu. Golovneva"
      },
      {
        "authorId": "2238056517",
        "name": "Tianlu Wang"
      },
      {
        "authorId": "2237983657",
        "name": "Arun Babu"
      },
      {
        "authorId": "2237987675",
        "name": "Binh Tang"
      },
      {
        "authorId": "2253591308",
        "name": "Brian Karrer"
      },
      {
        "authorId": "2086827528",
        "name": "Shelly Sheynin"
      },
      {
        "authorId": "51519704",
        "name": "Candace Ross"
      },
      {
        "authorId": "33964593",
        "name": "Adam Polyak"
      },
      {
        "authorId": "2237983588",
        "name": "Russell Howes"
      },
      {
        "authorId": "2237990986",
        "name": "Vasu Sharma"
      },
      {
        "authorId": "2214843767",
        "name": "Puxin Xu"
      },
      {
        "authorId": "2040866961",
        "name": "Hovhannes Tamoyan"
      },
      {
        "authorId": "1388005058",
        "name": "Oron Ashual"
      },
      {
        "authorId": "88622696",
        "name": "Uriel Singer"
      },
      {
        "authorId": "2530311",
        "name": "Shang-Wen Li"
      },
      {
        "authorId": "2238121623",
        "name": "Susan Zhang"
      },
      {
        "authorId": "2191899140",
        "name": "Rich James"
      },
      {
        "authorId": "134007132",
        "name": "Gargi Ghosh"
      },
      {
        "authorId": "2188620",
        "name": "Yaniv Taigman"
      },
      {
        "authorId": "1399159921",
        "name": "Maryam Fazel-Zarandi"
      },
      {
        "authorId": "1709797",
        "name": "Asli Celikyilmaz"
      },
      {
        "authorId": "1982950",
        "name": "Luke Zettlemoyer"
      },
      {
        "authorId": "2201435",
        "name": "Armen Aghajanyan"
      }
    ],
    "source": "semantic_scholar",
    "score": 141.68685239667295
  },
  {
    "paperId": "0a962d107e8fee1cc97ecca2ae777b2bb48a031f",
    "url": "https://www.semanticscholar.org/paper/0a962d107e8fee1cc97ecca2ae777b2bb48a031f",
    "title": "ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence",
    "abstract": "Retrieval augmented generation (RAG) is frequently used to mitigate hallucinations and provide up-to-date knowledge for large language models (LLMs). However, given that document retrieval is an imprecise task and sometimes results in erroneous or even harmful content being presented in context, this raises the question of how LLMs handle retrieved information: If the provided content is incorrect, does the model know to ignore it, or does it recapitulate the error? Conversely, when the model's initial response is incorrect, does it always know to use the retrieved information to correct itself, or does it insist on its wrong prior response? To answer this, we curate a dataset of over 1200 questions across six domains (e.g., drug dosages, Olympic records, locations) along with content relevant to answering each question. We further apply precise perturbations to the answers in the content that range from subtle to blatant errors. We benchmark six top-performing LLMs, including GPT-4o, on this dataset and find that LLMs are susceptible to adopting incorrect retrieved content, overriding their own correct prior knowledge over 60% of the time. However, the more unrealistic the retrieved content is (i.e. more deviated from truth), the less likely the model is to adopt it. Also, the less confident a model is in its initial response (via measuring token probabilities), the more likely it is to adopt the information in the retrieved content. We exploit this finding and demonstrate simple methods for improving model accuracy where there is conflicting retrieved content. Our results highlight a difficult task and benchmark for LLMs -- namely, their ability to correctly discern when it is wrong in light of correct retrieved content and to reject cases when the provided content is incorrect.",
    "venue": "",
    "year": 2024,
    "citationCount": 25,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-04-16",
    "authors": [
      {
        "authorId": "2266355762",
        "name": "Kevin Wu"
      },
      {
        "authorId": "2253396752",
        "name": "Eric Wu"
      },
      {
        "authorId": "2293663109",
        "name": "James Zou"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.87144807032223
  },
  {
    "paperId": "fa21931a9b23a7f0b7c4975a661d9ccc8e84d059",
    "url": "https://www.semanticscholar.org/paper/fa21931a9b23a7f0b7c4975a661d9ccc8e84d059",
    "title": "ChatQA: Surpassing GPT-4 on Conversational QA and RAG",
    "abstract": "In this work, we introduce ChatQA, a suite of models that outperform GPT-4 on retrieval-augmented generation (RAG) and conversational question answering (QA). To enhance generation, we propose a two-stage instruction tuning method that significantly boosts the performance of RAG. For effective retrieval, we introduce a dense retriever optimized for conversational QA, which yields results comparable to the alternative state-of-the-art query rewriting models, while substantially reducing deployment costs. We also present the ChatRAG Bench, which encompasses ten datasets covering comprehensive evaluations on RAG, table-related QA, arithmetic calculations, and scenarios involving unanswerable questions. Our ChatQA-1.0-70B (score: 54.14), built on Llama2, a weaker foundation model than GPT-4, can slightly outperform GPT-4-0613 (score: 53.90) and GPT-4-Turbo-2024-04-09 (score: 54.03) on the ChatRAG Bench, without relying on any synthetic data from OpenAI GPT models. Notably, the Llama3-ChatQA-1.5-70B model surpasses the accuracy of GPT-4-Turbo-2024-04-09, achieving a 4.4% improvement. To advance research in this field, we open-sourced the model weights, instruction tuning data, ChatRAG Bench, and retriever for the community: https://chatqa-project.github.io/.",
    "venue": "",
    "year": 2024,
    "citationCount": 25,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-01-18",
    "authors": [
      {
        "authorId": "2256582287",
        "name": "Zihan Liu"
      },
      {
        "authorId": "2253664013",
        "name": "Wei Ping"
      },
      {
        "authorId": "2279834711",
        "name": "Rajarshi Roy"
      },
      {
        "authorId": "2254989105",
        "name": "Peng Xu"
      },
      {
        "authorId": "2254563114",
        "name": "Chankyu Lee"
      },
      {
        "authorId": "1911755",
        "name": "Mohammad Shoeybi"
      },
      {
        "authorId": "2264406909",
        "name": "Bryan Catanzaro"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.87144807032223
  },
  {
    "paperId": "b47507f12a8b7d26f78a484e32a08d61d3f87358",
    "url": "https://www.semanticscholar.org/paper/b47507f12a8b7d26f78a484e32a08d61d3f87358",
    "title": "BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack",
    "abstract": "In recent years, the input context sizes of large language models (LLMs) have increased dramatically. However, existing evaluation methods have not kept pace, failing to comprehensively assess the efficiency of models in handling long contexts. To bridge this gap, we introduce the BABILong benchmark, designed to test language models' ability to reason across facts distributed in extremely long documents. BABILong includes a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets. These tasks are challenging on their own, and even more demanding when the required facts are scattered across long natural text. Our evaluations show that popular LLMs effectively utilize only 10-20\\% of the context and their performance declines sharply with increased reasoning complexity. Among alternatives to in-context reasoning, Retrieval-Augmented Generation methods achieve a modest 60\\% accuracy on single-fact question answering, independent of context length. Among context extension methods, the highest performance is demonstrated by recurrent memory transformers after fine-tuning, enabling the processing of lengths up to 50 million tokens. The BABILong benchmark is extendable to any length to support the evaluation of new upcoming models with increased capabilities, and we provide splits up to 10 million token lengths.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 24,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-14",
    "authors": [
      {
        "authorId": "51114080",
        "name": "Yuri Kuratov"
      },
      {
        "authorId": "2176183932",
        "name": "Aydar Bulatov"
      },
      {
        "authorId": "2284590453",
        "name": "Petr Anokhin"
      },
      {
        "authorId": "2306782862",
        "name": "Ivan Rodkin"
      },
      {
        "authorId": "2284591109",
        "name": "Dmitry Sorokin"
      },
      {
        "authorId": "2284590611",
        "name": "Artyom Sorokin"
      },
      {
        "authorId": "2284592236",
        "name": "Mikhail Burtsev"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.28313737302301
  },
  {
    "paperId": "fa6618681476b6bec51b4233ab83658f1f9a6db7",
    "url": "https://www.semanticscholar.org/paper/fa6618681476b6bec51b4233ab83658f1f9a6db7",
    "title": "Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases",
    "abstract": "We proposed an end-to-end system design towards utilizing Retrieval Augmented Generation (RAG) to improve the factual accuracy of Large Language Models (LLMs) for domain-specific and time-sensitive queries related to private knowledge-bases. Our system integrates RAG pipeline with upstream datasets processing and downstream performance evaluation. Addressing the challenge of LLM hallucinations, we finetune models with a curated dataset which originates from CMU's extensive resources and annotated with the teacher model. Our experiments demonstrate the system's effectiveness in generating more accurate answers to domain-specific and time-sensitive inquiries. The results also revealed the limitations of fine-tuning LLMs with small-scale and skewed datasets. This research highlights the potential of RAG systems in augmenting LLMs with external datasets for improved performance in knowledge-intensive tasks. Our code and models are available on Github.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 27,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-15",
    "authors": [
      {
        "authorId": "2291983504",
        "name": "Jiarui Li"
      },
      {
        "authorId": "2291993398",
        "name": "Ye Yuan"
      },
      {
        "authorId": "2291997675",
        "name": "Zehua Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.98306765262805
  },
  {
    "paperId": "a72975eb88eb31f193e9587e7415cb04e7bcdbee",
    "url": "https://www.semanticscholar.org/paper/a72975eb88eb31f193e9587e7415cb04e7bcdbee",
    "title": "Generating Benchmarks for Factuality Evaluation of Language Models",
    "abstract": "Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain. Existing methods for factuality evaluation of LLM generation focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent domain specific or rare facts. We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality. FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM‚Äôs propensity to generate true facts from the corpus vs. similar but incorrect statements. We use our framework to create three benchmarks: Wiki-FACTOR, News-FACTOR and Expert-FACTOR. We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score and perplexity do not always agree on model ranking; (iii) when perplexity and benchmark score disagree, the latter better reflects factuality in open-ended generation, as measured by human annotators.",
    "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 68,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.06908",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-07-13",
    "authors": [
      {
        "authorId": "51918041",
        "name": "Dor Muhlgay"
      },
      {
        "authorId": "73775461",
        "name": "Ori Ram"
      },
      {
        "authorId": "2158996542",
        "name": "Inbal Magar"
      },
      {
        "authorId": "152754428",
        "name": "Yoav Levine"
      },
      {
        "authorId": "2148471161",
        "name": "Nir Ratner"
      },
      {
        "authorId": "2083259",
        "name": "Yonatan Belinkov"
      },
      {
        "authorId": "2769805",
        "name": "Omri Abend"
      },
      {
        "authorId": "2066411743",
        "name": "Kevin Leyton-Brown"
      },
      {
        "authorId": "3140335",
        "name": "A. Shashua"
      },
      {
        "authorId": "1701353",
        "name": "Y. Shoham"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.5115975689589
  },
  {
    "paperId": "7bc68f9133024a79d2962fada8ba319a67efa715",
    "url": "https://www.semanticscholar.org/paper/7bc68f9133024a79d2962fada8ba319a67efa715",
    "title": "Empowering Personalized Pharmacogenomics with Generative AI Solutions",
    "abstract": "Objective: This study evaluates an AI assistant developed using OpenAI's GPT-4 for interpreting pharmacogenomic (PGx) testing results, aiming to improve decision-making and knowledge sharing in clinical genetics, and to enhance patient care with equitable access. Methods: The AI assistant employs Retrieval Augmented Generation (RAG) combining retrieval and generative techniques. It employs a Knowledge Base (KB) comprising Clinical Pharmacogenetics Implementation Consortium (CPIC) data, with context-aware GPT-4 generating tailored responses to user queries from this KB, refined through prompt engineering and guardrails. Results: Evaluated against a specialized PGx question catalog, the AI assistant showed high efficacy in addressing user queries. Compared with OpenAI's ChatGPT 3.5, it demonstrated better performance, especially in provider-specific queries requiring specialized data and citations. Key areas for improvement include enhancing accuracy, relevancy, and representative language in responses. Discussion: The integration of context-aware GPT-4 with RAG significantly enhanced the AI assistant's utility. RAG's ability to incorporate domain-specific CPIC data, including recent literature, proved beneficial. Challenges persist, such as the need for specialized genetic/PGx models to improve accuracy and relevancy and addressing ethical, regulatory, and safety concerns. Conclusion: This study underscores generative AI's potential for transforming healthcare provider support and patient accessibility to complex pharmacogenomic information. While careful implementation of large language models like GPT-4 is necessary, it is clear that they can substantially improve understanding of pharmacogenomic data. With further development, these tools could augment healthcare expertise, provider productivity, and the delivery of equitable, patient-centered healthcare services.",
    "venue": "medRxiv",
    "year": 2024,
    "citationCount": 16,
    "openAccessPdf": {
      "url": "https://www.medrxiv.org/content/medrxiv/early/2024/02/27/2024.02.21.24302946.full.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-27",
    "authors": [
      {
        "authorId": "49831589",
        "name": "M. Murugan"
      },
      {
        "authorId": "2292702277",
        "name": "Bo Yuan"
      },
      {
        "authorId": "2100973",
        "name": "E. Venner"
      },
      {
        "authorId": "2272979575",
        "name": "Christie M. Ballantyne"
      },
      {
        "authorId": "2290142575",
        "name": "Katherine M Robinson"
      },
      {
        "authorId": "2290142349",
        "name": "James C. Coons"
      },
      {
        "authorId": "2287471666",
        "name": "Liwen Wang"
      },
      {
        "authorId": "3240306",
        "name": "P. Empey"
      },
      {
        "authorId": "2251203423",
        "name": "R. A. Gibbs"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.49820016084324
  },
  {
    "paperId": "95b06f4859113ccf14912b5e8e9260a69f2d37ac",
    "url": "https://www.semanticscholar.org/paper/95b06f4859113ccf14912b5e8e9260a69f2d37ac",
    "title": "The Chronicles of RAG: The Retriever, the Chunk and the Generator",
    "abstract": "Retrieval Augmented Generation (RAG) has become one of the most popular paradigms for enabling LLMs to access external data, and also as a mechanism for grounding to mitigate against hallucinations. When implementing RAG you can face several challenges like effective integration of retrieval models, efficient representation learning, data diversity, computational efficiency optimization, evaluation, and quality of text generation. Given all these challenges, every day a new technique to improve RAG appears, making it unfeasible to experiment with all combinations for your problem. In this context, this paper presents good practices to implement, optimize, and evaluate RAG for the Brazilian Portuguese language, focusing on the establishment of a simple pipeline for inference and experiments. We explored a diverse set of methods to answer questions about the first Harry Potter book. To generate the answers we used the OpenAI's gpt-4, gpt-4-1106-preview, gpt-3.5-turbo-1106, and Google's Gemini Pro. Focusing on the quality of the retriever, our approach achieved an improvement of MRR@10 by 35.4% compared to the baseline. When optimizing the input size in the application, we observed that it is possible to further enhance it by 2.4%. Finally, we present the complete architecture of the RAG with our recommendations. As result, we moved from a baseline of 57.88% to a maximum relative score of 98.61%.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-01-15",
    "authors": [
      {
        "authorId": "32264263",
        "name": "Paulo Finardi"
      },
      {
        "authorId": "2279540428",
        "name": "Leonardo Avila"
      },
      {
        "authorId": "2279540261",
        "name": "Rodrigo Castaldoni"
      },
      {
        "authorId": "2003017919",
        "name": "P. Gengo"
      },
      {
        "authorId": "150059611",
        "name": "Celio H. N. Larcher"
      },
      {
        "authorId": "1900371423",
        "name": "Marcos Piau"
      },
      {
        "authorId": "2279541193",
        "name": "Pablo B. Costa"
      },
      {
        "authorId": "2233087595",
        "name": "Vinicius Carid'a"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.49820016084324
  },
  {
    "paperId": "26702869774950e03b10a3657bca0621aa0d8d07",
    "url": "https://www.semanticscholar.org/paper/26702869774950e03b10a3657bca0621aa0d8d07",
    "title": "KG-RAG: Bridging the Gap Between Knowledge and Creativity",
    "abstract": "Ensuring factual accuracy while maintaining the creative capabilities of Large Language Model Agents (LMAs) poses significant challenges in the development of intelligent agent systems. LMAs face prevalent issues such as information hallucinations, catastrophic forgetting, and limitations in processing long contexts when dealing with knowledge-intensive tasks. This paper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation) pipeline, a novel framework designed to enhance the knowledge capabilities of LMAs by integrating structured Knowledge Graphs (KGs) with the functionalities of LLMs, thereby significantly reducing the reliance on the latent knowledge of LLMs. The KG-RAG pipeline constructs a KG from unstructured text and then performs information retrieval over the newly created graph to perform KGQA (Knowledge Graph Question Answering). The retrieval methodology leverages a novel algorithm called Chain of Explorations (CoE) which benefits from LLMs reasoning to explore nodes and relationships within the KG sequentially. Preliminary experiments on the ComplexWebQuestions dataset demonstrate notable improvements in the reduction of hallucinated content and suggest a promising path toward developing intelligent systems adept at handling knowledge-intensive tasks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 17,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-20",
    "authors": [
      {
        "authorId": "2302321748",
        "name": "Diego Sanmartin"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.35557636844247
  },
  {
    "paperId": "f65ecb65d00f2e69a49465debfdd78efa0838cec",
    "url": "https://www.semanticscholar.org/paper/f65ecb65d00f2e69a49465debfdd78efa0838cec",
    "title": "Bridging the Preference Gap between Retrievers and LLMs",
    "abstract": "Large Language Models (LLMs) have demonstrated superior results across a wide range of tasks, and Retrieval-augmented Generation (RAG) is an effective way to enhance the performance by locating relevant information and placing it into the context window of the LLM. However, the relationship between retrievers and LLMs in a RAG is still under-investigated. Most existing work treats the retriever and the LLM as independent components and leaves a gap between retrieving human-\"friendly\"information and assembling a LLM-\"friendly\"context. In this work, we examine a novel bridge mechanism. We validate the ranking and selection assumptions of retrievers in the context of RAG and propose a framework that chains together supervised and reinforcement learning to train a bridge model that optimizes the connection between the retriever and the LLM. Empirical results demonstrate the effectiveness of our method in both question-answering and personalized generation tasks.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 17,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-01-13",
    "authors": [
      {
        "authorId": "2279540527",
        "name": "Zixuan Ke"
      },
      {
        "authorId": "2275163940",
        "name": "Weize Kong"
      },
      {
        "authorId": "2249775049",
        "name": "Cheng Li"
      },
      {
        "authorId": "2249838528",
        "name": "Mingyang Zhang"
      },
      {
        "authorId": "2253452633",
        "name": "Qiaozhu Mei"
      },
      {
        "authorId": "2240516450",
        "name": "Michael Bendersky"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.35557636844247
  },
  {
    "paperId": "221a0e0c798d4bbc8a057d869b7251e03f1b0790",
    "url": "https://www.semanticscholar.org/paper/221a0e0c798d4bbc8a057d869b7251e03f1b0790",
    "title": "ChatQA: Building GPT-4 Level Conversational QA Models",
    "abstract": "In this work, we introduce ChatQA, a family of conversational question answering (QA) models that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval-augmented generation in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from Ope-nAI GPT models.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 15,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2256582287",
        "name": "Zihan Liu"
      },
      {
        "authorId": "2253664013",
        "name": "Wei Ping"
      },
      {
        "authorId": "2279834711",
        "name": "Rajarshi Roy"
      },
      {
        "authorId": "2254989105",
        "name": "Peng Xu"
      },
      {
        "authorId": "2254563114",
        "name": "Chankyu Lee"
      },
      {
        "authorId": "1911755",
        "name": "Mohammad Shoeybi"
      },
      {
        "authorId": "2264406909",
        "name": "Bryan Catanzaro"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.58883083359672
  },
  {
    "paperId": "b39aba9b515723745c994aa0fbd80a566c268282",
    "url": "https://www.semanticscholar.org/paper/b39aba9b515723745c994aa0fbd80a566c268282",
    "title": "FinBen: A Holistic Financial Benchmark for Large Language Models",
    "abstract": "LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of comprehensive evaluation benchmarks, the rapid development of LLMs, and the complexity of financial tasks. In this paper, we introduce FinBen, the first extensive open-source evaluation benchmark, including 36 datasets spanning 24 financial tasks, covering seven critical aspects: information extraction (IE), textual analysis, question answering (QA), text generation, risk management, forecasting, and decision-making. FinBen offers several key innovations: a broader range of tasks and datasets, the first evaluation of stock trading, novel agent and Retrieval-Augmented Generation (RAG) evaluation, and three novel open-source evaluation datasets for text summarization, question answering, and stock trading. Our evaluation of 15 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals several key findings: While LLMs excel in IE and textual analysis, they struggle with advanced reasoning and complex tasks like text generation and forecasting. GPT-4 excels in IE and stock trading, while Gemini is better at text generation and forecasting. Instruction-tuned LLMs improve textual analysis but offer limited benefits for complex tasks such as QA. FinBen has been used to host the first financial LLMs shared task at the FinNLP-AgentScen workshop during IJCAI-2024, attracting 12 teams. Their novel solutions outperformed GPT-4, showcasing FinBen's potential to drive innovation in financial LLMs. All datasets, results, and codes are released for the research community: https://github.com/The-FinAI/PIXIU.",
    "venue": "",
    "year": 2024,
    "citationCount": 15,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-02-20",
    "authors": [
      {
        "authorId": "2249763955",
        "name": "Qianqian Xie"
      },
      {
        "authorId": "104843747",
        "name": "Weiguang Han"
      },
      {
        "authorId": "2285168553",
        "name": "Zhengyu Chen"
      },
      {
        "authorId": "2283935820",
        "name": "Ruoyu Xiang"
      },
      {
        "authorId": "2284182151",
        "name": "Xiao Zhang"
      },
      {
        "authorId": "2284803928",
        "name": "Yueru He"
      },
      {
        "authorId": "2284762987",
        "name": "Mengxi Xiao"
      },
      {
        "authorId": "2284864662",
        "name": "Dong Li"
      },
      {
        "authorId": "2249887692",
        "name": "Yongfu Dai"
      },
      {
        "authorId": "2121560368",
        "name": "Duanyu Feng"
      },
      {
        "authorId": "2284822114",
        "name": "Yijing Xu"
      },
      {
        "authorId": "2268430363",
        "name": "Haoqiang Kang"
      },
      {
        "authorId": "13627871",
        "name": "Zi-Zhou Kuang"
      },
      {
        "authorId": "2280746100",
        "name": "Chenhan Yuan"
      },
      {
        "authorId": "2003396186",
        "name": "Kailai Yang"
      },
      {
        "authorId": "31689330",
        "name": "Zheheng Luo"
      },
      {
        "authorId": "2284778997",
        "name": "Tianlin Zhang"
      },
      {
        "authorId": "2263516877",
        "name": "Zhiwei Liu"
      },
      {
        "authorId": "2284763655",
        "name": "Guojun Xiong"
      },
      {
        "authorId": "2284946604",
        "name": "Zhiyang Deng"
      },
      {
        "authorId": "2268397502",
        "name": "Yuechen Jiang"
      },
      {
        "authorId": "2284912441",
        "name": "Zhiyuan Yao"
      },
      {
        "authorId": "2238311785",
        "name": "Haohang Li"
      },
      {
        "authorId": "2238569503",
        "name": "Yangyang Yu"
      },
      {
        "authorId": "2285246618",
        "name": "Gang Hu"
      },
      {
        "authorId": "2213444593",
        "name": "Jiajia Huang"
      },
      {
        "authorId": "2283942492",
        "name": "Xiao-Yang Liu"
      },
      {
        "authorId": "2297282347",
        "name": "Alejandro Lopez-Lira"
      },
      {
        "authorId": "2284827142",
        "name": "Benyou Wang"
      },
      {
        "authorId": "2202405535",
        "name": "Yanzhao Lai"
      },
      {
        "authorId": "2213502750",
        "name": "Hao Wang"
      },
      {
        "authorId": "2283923584",
        "name": "Min Peng"
      },
      {
        "authorId": "2240623492",
        "name": "Sophia Ananiadou"
      },
      {
        "authorId": "2555230",
        "name": "Jimin Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.58883083359672
  },
  {
    "paperId": "b6948a9e8b3eec5a56a80c69727154fcd7ececce",
    "url": "https://www.semanticscholar.org/paper/b6948a9e8b3eec5a56a80c69727154fcd7ececce",
    "title": "AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases",
    "abstract": "LLM agents have demonstrated remarkable performance across various applications, primarily due to their advanced capabilities in reasoning, utilizing external knowledge and tools, calling APIs, and executing actions to interact with environments. Current agents typically utilize a memory module or a retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and instances with similar embeddings from knowledge bases to inform task planning and execution. However, the reliance on unverified knowledge bases raises significant concerns about their safety and trustworthiness. To uncover such vulnerabilities, we propose a novel red teaming approach AgentPoison, the first backdoor attack targeting generic and RAG-based LLM agents by poisoning their long-term memory or RAG knowledge base. In particular, we form the trigger generation process as a constrained optimization to optimize backdoor triggers by mapping the triggered instances to a unique embedding space, so as to ensure that whenever a user instruction contains the optimized backdoor trigger, the malicious demonstrations are retrieved from the poisoned memory or knowledge base with high probability. In the meantime, benign instructions without the trigger will still maintain normal performance. Unlike conventional backdoor attacks, AgentPoison requires no additional model training or fine-tuning, and the optimized backdoor trigger exhibits superior transferability, in-context coherence, and stealthiness. Extensive experiments demonstrate AgentPoison's effectiveness in attacking three types of real-world LLM agents: RAG-based autonomous driving agent, knowledge-intensive QA agent, and healthcare EHRAgent. On each agent, AgentPoison achieves an average attack success rate higher than 80% with minimal impact on benign performance (less than 1%) with a poison rate less than 0.1%.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-17",
    "authors": [
      {
        "authorId": "1641681688",
        "name": "Zhaorun Chen"
      },
      {
        "authorId": "2261738344",
        "name": "Zhen Xiang"
      },
      {
        "authorId": "2312102403",
        "name": "Chaowei Xiao"
      },
      {
        "authorId": "2293597685",
        "name": "Dawn Song"
      },
      {
        "authorId": "2309303056",
        "name": "Bo Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.49820016084324
  },
  {
    "paperId": "90f293e3df04f8d2eda999d7fb7a5f13714f31f1",
    "url": "https://www.semanticscholar.org/paper/90f293e3df04f8d2eda999d7fb7a5f13714f31f1",
    "title": "Evaluating the accuracy of a state-of-the-art large language model for prediction of admissions from the emergency room",
    "abstract": "BACKGROUND\nArtificial intelligence (AI) and large language models (LLMs) can play a critical role in emergency room operations by augmenting decision-making about patient admission. However, there are no studies for LLMs using real-world data and scenarios, in comparison to and being informed by traditional supervised machine learning (ML) models. We evaluated the performance of GPT-4 for predicting patient admissions from emergency department (ED) visits. We compared performance to traditional ML models both naively and when informed by few-shot examples and/or numerical probabilities.\n\n\nMETHODS\nWe conducted a retrospective study using electronic health records across 7 NYC hospitals. We trained Bio-Clinical-BERT and XGBoost (XGB) models on unstructured and structured data, respectively, and created an ensemble model reflecting ML performance. We then assessed GPT-4 capabilities in many scenarios: through Zero-shot, Few-shot with and without retrieval-augmented generation (RAG), and with and without ML numerical probabilities.\n\n\nRESULTS\nThe Ensemble ML model achieved an area under the receiver operating characteristic curve (AUC) of 0.88, an area under the precision-recall curve (AUPRC) of 0.72 and an accuracy of 82.9%. The na√Øve GPT-4's performance (0.79 AUC, 0.48 AUPRC, and 77.5% accuracy) showed substantial improvement when given limited, relevant data to learn from (ie, RAG) and underlying ML probabilities (0.87 AUC, 0.71 AUPRC, and 83.1% accuracy). Interestingly, RAG alone boosted performance to near peak levels (0.82 AUC, 0.56 AUPRC, and 81.3% accuracy).\n\n\nCONCLUSIONS\nThe na√Øve LLM had limited performance but showed significant improvement in predicting ED admissions when supplemented with real-world examples to learn from, particularly through RAG, and/or numerical probabilities from traditional ML models. Its peak performance, although slightly lower than the pure ML model, is noteworthy given its potential for providing reasoning behind predictions. Further refinement of LLMs with real-world data is necessary for successful integration as decision-support tools in care settings.",
    "venue": "J. Am. Medical Informatics Assoc.",
    "year": 2024,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-21",
    "authors": [
      {
        "authorId": "116603382",
        "name": "Benjamin S. Glicksberg"
      },
      {
        "authorId": "2019858",
        "name": "P. Timsina"
      },
      {
        "authorId": "2228854157",
        "name": "Dhavalkumar Patel"
      },
      {
        "authorId": "2151688831",
        "name": "Ashwin S. Sawant"
      },
      {
        "authorId": "98181691",
        "name": "A. Vaid"
      },
      {
        "authorId": "2302387524",
        "name": "Ganesh Raut"
      },
      {
        "authorId": "2196347145",
        "name": "Alexander W. Charney"
      },
      {
        "authorId": "2278220781",
        "name": "Donald Apakama"
      },
      {
        "authorId": "2278218539",
        "name": "Brendan G Carr"
      },
      {
        "authorId": "2284458676",
        "name": "Robert Freeman"
      },
      {
        "authorId": "2292250182",
        "name": "Girish N. Nadkarni"
      },
      {
        "authorId": "2299137153",
        "name": "Eyal Klang"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.49820016084324
  },
  {
    "paperId": "d8e74989b39055faccab06d69aad12a89d499939",
    "url": "https://www.semanticscholar.org/paper/d8e74989b39055faccab06d69aad12a89d499939",
    "title": "How well do LLMs cite relevant medical references? An evaluation framework and analyses",
    "abstract": "Large language models (LLMs) are currently being used to answer medical questions across a variety of clinical domains. Recent top-performing commercial LLMs, in particular, are also capable of citing sources to support their responses. In this paper, we ask: do the sources that LLMs generate actually support the claims that they make? To answer this, we propose three contributions. First, as expert medical annotations are an expensive and time-consuming bottleneck for scalable evaluation, we demonstrate that GPT-4 is highly accurate in validating source relevance, agreeing 88% of the time with a panel of medical doctors. Second, we develop an end-to-end, automated pipeline called \\textit{SourceCheckup} and use it to evaluate five top-performing LLMs on a dataset of 1200 generated questions, totaling over 40K pairs of statements and sources. Interestingly, we find that between ~50% to 90% of LLM responses are not fully supported by the sources they provide. We also evaluate GPT-4 with retrieval augmented generation (RAG) and find that, even still, around 30\\% of individual statements are unsupported, while nearly half of its responses are not fully supported. Third, we open-source our curated dataset of medical questions and expert annotations for future evaluations. Given the rapid pace of LLM development and the potential harms of incorrect or outdated medical information, it is crucial to also understand and quantify their capability to produce relevant, trustworthy medical references.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 15,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-03",
    "authors": [
      {
        "authorId": "2266355762",
        "name": "Kevin Wu"
      },
      {
        "authorId": "2253396752",
        "name": "Eric Wu"
      },
      {
        "authorId": "2282541900",
        "name": "Ally Cassasola"
      },
      {
        "authorId": "2282623003",
        "name": "Angela Zhang"
      },
      {
        "authorId": "2282542576",
        "name": "Kevin Wei"
      },
      {
        "authorId": "2282902327",
        "name": "Teresa Nguyen"
      },
      {
        "authorId": "2282541731",
        "name": "Sith Riantawan"
      },
      {
        "authorId": "2282541471",
        "name": "Patricia Shi Riantawan"
      },
      {
        "authorId": "2282542159",
        "name": "Daniel E. Ho"
      },
      {
        "authorId": "2283249107",
        "name": "James Zou"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.58883083359672
  },
  {
    "paperId": "3d5f0e67b65136d8963ca7884fb33ffa8c86476e",
    "url": "https://www.semanticscholar.org/paper/3d5f0e67b65136d8963ca7884fb33ffa8c86476e",
    "title": "Code Search Is All You Need? Improving Code Suggestions with Code Search",
    "abstract": "Modern integrated development environments (IDEs) provide various automated code suggestion techniques (e.g., code completion and code generation) to help developers improve their efficiency. Such techniques may retrieve similar code snippets from the code base or leverage deep learning models to provide code suggestions. However, how to effectively enhance the code suggestions using code retrieval has not been systematically investigated. In this paper, we study and explore a retrieval-augmented framework for code suggestions. Specifically, our framework leverages different retrieval approaches and search strategies to search similar code snippets. Then the retrieved code is used to further enhance the performance of language models on code suggestions. We conduct experiments by integrating different language models into our framework and compare the results with their original models. We find that our framework noticeably improves the performance of both code completion and code generation by up to 53.8% and 130.8% in terms of BLEU-4, respectively. Our study highlights that integrating the retrieval process into code suggestions can improve the performance of code suggestions by a large margin.",
    "venue": "International Conference on Software Engineering",
    "year": 2024,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2024-04-12",
    "authors": [
      {
        "authorId": "2293350098",
        "name": "Junkai Chen"
      },
      {
        "authorId": "2265809330",
        "name": "Xing Hu"
      },
      {
        "authorId": "2293350478",
        "name": "Zhenhao Li"
      },
      {
        "authorId": "2296784064",
        "name": "Cuiyun Gao"
      },
      {
        "authorId": "2265241871",
        "name": "Xin Xia"
      },
      {
        "authorId": "2266943614",
        "name": "David Lo"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.49820016084324
  },
  {
    "paperId": "fc3fa1f54b1c7c7aa3b24d80fb7842e5adc858b1",
    "url": "https://www.semanticscholar.org/paper/fc3fa1f54b1c7c7aa3b24d80fb7842e5adc858b1",
    "title": "IMSS: A Novel Approach to Design of Adaptive Search System Using Second Generation Big Data Analytics",
    "abstract": null,
    "venue": "",
    "year": 2017,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "47216812",
        "name": "Dheeraj Malhotra"
      },
      {
        "authorId": "70389688",
        "name": "O. Rishi"
      }
    ],
    "source": "semantic_scholar",
    "score": 83.47424036192305
  },
  {
    "paperId": "2f6de8291c9a803faa7f7a33c74f4a2a3debd83b",
    "url": "https://www.semanticscholar.org/paper/2f6de8291c9a803faa7f7a33c74f4a2a3debd83b",
    "title": "Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks",
    "abstract": "We introduce a new family of prompt injection attacks, termed Neural Exec. Unlike known attacks that rely on handcrafted strings (e.g.,\"Ignore previous instructions and...\"), we show that it is possible to conceptualize the creation of execution triggers as a differentiable search problem and use learning-based methods to autonomously generate them. Our results demonstrate that a motivated adversary can forge triggers that are not only drastically more effective than current handcrafted ones but also exhibit inherent flexibility in shape, properties, and functionality. In this direction, we show that an attacker can design and generate Neural Execs capable of persisting through multi-stage preprocessing pipelines, such as in the case of Retrieval-Augmented Generation (RAG)-based applications. More critically, our findings show that attackers can produce triggers that deviate markedly in form and shape from any known attack, sidestepping existing blacklist-based detection and sanitation approaches.",
    "venue": "AISec@CCS",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-06",
    "authors": [
      {
        "authorId": "2290076053",
        "name": "Dario Pasquini"
      },
      {
        "authorId": "2290072234",
        "name": "Martin Strohmeier"
      },
      {
        "authorId": "2290072200",
        "name": "Carmela Troncoso"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "4308208fac24626e0c927ee728038aadc4e87266",
    "url": "https://www.semanticscholar.org/paper/4308208fac24626e0c927ee728038aadc4e87266",
    "title": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models",
    "abstract": "In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting. Despite the impressive accomplishments, large language models (LLMs), even with retrieval-augmented generation (RAG), still struggle to efficiently and effectively integrate a large amount of new experiences after pre-training. In this work, we introduce HippoRAG, a novel retrieval framework inspired by the hippocampal indexing theory of human long-term memory to enable deeper and more efficient knowledge integration over new experiences. HippoRAG synergistically orchestrates LLMs, knowledge graphs, and the Personalized PageRank algorithm to mimic the different roles of neocortex and hippocampus in human memory. We compare HippoRAG with existing RAG methods on multi-hop question answering and show that our method outperforms the state-of-the-art methods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves comparable or better performance than iterative retrieval like IRCoT while being 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into IRCoT brings further substantial gains. Finally, we show that our method can tackle new types of scenarios that are out of reach of existing methods. Code and data are available at https://github.com/OSU-NLP-Group/HippoRAG.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-23",
    "authors": [
      {
        "authorId": "1666169546",
        "name": "Bernal Jimenez Gutierrez"
      },
      {
        "authorId": "1406331721",
        "name": "Yiheng Shu"
      },
      {
        "authorId": "2022231256",
        "name": "Yu Gu"
      },
      {
        "authorId": "19168196",
        "name": "Michihiro Yasunaga"
      },
      {
        "authorId": "1758652",
        "name": "Yu Su"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "75bae34f9361cd99d8f64bb8311feb01c7c75107",
    "url": "https://www.semanticscholar.org/paper/75bae34f9361cd99d8f64bb8311feb01c7c75107",
    "title": "HDLdebugger: Streamlining HDL debugging with Large Language Models",
    "abstract": "In the domain of chip design, Hardware Description Languages (HDLs) play a pivotal role. However, due to the complex syntax of HDLs and the limited availability of online resources, debugging HDL codes remains a difficult and time-intensive task, even for seasoned engineers. Consequently, there is a pressing need to develop automated HDL code debugging models, which can alleviate the burden on hardware engineers. Despite the strong capabilities of Large Language Models (LLMs) in generating, completing, and debugging software code, their utilization in the specialized field of HDL debugging has been limited and, to date, has not yielded satisfactory results. In this paper, we propose an LLM-assisted HDL debugging framework, namely HDLdebugger, which consists of HDL debugging data generation via a reverse engineering approach, a search engine for retrieval-augmented generation, and a retrieval-augmented LLM fine-tuning approach. Through the integration of these components, HDLdebugger can automate and streamline HDL debugging for chip design. Our comprehensive experiments, conducted on an HDL code dataset sourced from Huawei, reveal that HDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional effectiveness in HDL code debugging.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 14,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-18",
    "authors": [
      {
        "authorId": "2244166898",
        "name": "Xufeng Yao"
      },
      {
        "authorId": "2145537904",
        "name": "Haoyang Li"
      },
      {
        "authorId": "2290271874",
        "name": "T. H. Chan"
      },
      {
        "authorId": "2290127205",
        "name": "Wenyi Xiao"
      },
      {
        "authorId": "2290331022",
        "name": "Mingxuan Yuan"
      },
      {
        "authorId": "2282991476",
        "name": "Yu Huang"
      },
      {
        "authorId": "2290932917",
        "name": "Lei Chen"
      },
      {
        "authorId": "2283428383",
        "name": "Bei Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "65dcebe57c9a0b68850c0d6e625965b27522a95d",
    "url": "https://www.semanticscholar.org/paper/65dcebe57c9a0b68850c0d6e625965b27522a95d",
    "title": "Fine Tuning LLM for Enterprise: Practical Guidelines and Recommendations",
    "abstract": "There is a compelling necessity from enterprises for fine tuning LLMs (Large Language Models) o get them trained on proprietary domain knowledge. The challenge is to imbibe the LLMs with domain specific knowledge using the most optimial resource and cost and in the best possible time. Many enterprises rely on RAG (Retrieval Augmented Generation) which does not need LLMs to be ine-tuned but they are limited by the quality of vector databases and their retrieval capabilities rather than the intrinsic capabilities of the LLMs themselves. In our current work we focus on fine tuning LLaMA, an open source LLM using proprietary documents and code from an enterprise repository and use the fine tuned models to evaluate the quality of responses. As part of this work, we aim to guide beginners on how to start with fine tuning an LLM for documentation and code by making educated guesses on size of GPU required and options that are available for formatting the data. We also propose pre processing recipes for both documentation and code to prepare dataset in different formats. The proposed methods of data preparation for document datasets are forming paragraph chunks, forming question and answer pairs and forming keyword and paragraph chunk pairs. For code dataset we propose forming summary and function pairs. Further, we qualitatively evaluate the results of the models for domain specific queries. Finally, we also propose practical guidelines and recommendations for fine tuning LLMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 14,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-23",
    "authors": [
      {
        "authorId": "2296993504",
        "name": "J. MathavRaj"
      },
      {
        "authorId": "2296993204",
        "name": "VM Kushala"
      },
      {
        "authorId": "9319209",
        "name": "Harikrishna Warrier"
      },
      {
        "authorId": "2144102277",
        "name": "Yogesh Gupta"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "6837fb40385740828fac2d46a77da13c5db7d8db",
    "url": "https://www.semanticscholar.org/paper/6837fb40385740828fac2d46a77da13c5db7d8db",
    "title": "Cross-Data Knowledge Graph Construction for LLM-enabled Educational Question-Answering System: A Case Study at HCMUT",
    "abstract": "In today's rapidly evolving landscape of Artificial Intelligence, large language models (LLMs) have emerged as a vibrant research topic. LLMs find applications in various fields and contribute significantly. Despite their powerful language capabilities, similar to pre-trained language models (PLMs), LLMs still face challenges in remembering events, incorporating new information, and addressing domain-specific issues or hallucinations. To overcome these limitations, researchers have proposed Retrieval-Augmented Generation (RAG) techniques, some others have proposed the integration of LLMs with Knowledge Graphs (KGs) to provide factual context, thereby improving performance and delivering more accurate feedback to user queries. Education plays a crucial role in human development and progress. With the technology transformation, traditional education is being replaced by digital or blended education. Therefore, educational data in the digital environment is increasing day by day. Data in higher education institutions are diverse, comprising various sources such as unstructured/structured text, relational databases, web/app-based API access, etc. Constructing a Knowledge Graph from these cross-data sources is not a simple task. This article proposes a method for automatically constructing a Knowledge Graph from multiple data sources and discusses some initial applications (experimental trials) of KG in conjunction with LLMs for question-answering tasks.",
    "venue": "AIQAM@ICMR",
    "year": 2024,
    "citationCount": 14,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2404.09296",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle"
    ],
    "publicationDate": "2024-04-14",
    "authors": [
      {
        "authorId": "2305832269",
        "name": "Tuan Bui"
      },
      {
        "authorId": "2296720317",
        "name": "Oanh Tran"
      },
      {
        "authorId": "2296785804",
        "name": "Phuong Nguyen"
      },
      {
        "authorId": "2296718318",
        "name": "Bao Ho"
      },
      {
        "authorId": "2296770724",
        "name": "Long Nguyen"
      },
      {
        "authorId": "2305832263",
        "name": "Thang Bui"
      },
      {
        "authorId": "2296717570",
        "name": "Tho Quan"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.62075301653314
  },
  {
    "paperId": "4688eafc5f77d72bb1b04b382cb3a1ca2b089963",
    "url": "https://www.semanticscholar.org/paper/4688eafc5f77d72bb1b04b382cb3a1ca2b089963",
    "title": "NV-Retriever: Improving text embedding models with effective hard-negative mining",
    "abstract": "Text embedding models have been popular for information retrieval applications such as semantic search and Question-Answering systems based on Retrieval-Augmented Generation (RAG). Those models are typically Transformer models that are fine-tuned with contrastive learning objectives. Many papers introduced new embedding model architectures and training approaches, however, one of the key ingredients, the process of mining negative passages, remains poorly explored or described. One of the challenging aspects of fine-tuning embedding models is the selection of high quality hard-negative passages for contrastive learning. In this paper we propose a family of positive-aware mining methods that leverage the positive relevance score for more effective false negatives removal. We also provide a comprehensive ablation study on hard-negative mining methods over their configurations, exploring different teacher and base models. We demonstrate the efficacy of our proposed methods by introducing the NV-Retriever-v1 model, which scores 60.9 on MTEB Retrieval (BEIR) benchmark and 0.65 points higher than previous methods. The model placed 1st when it was published to MTEB Retrieval on July 07, 2024.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-22",
    "authors": [
      {
        "authorId": "2240556137",
        "name": "G. D. S. P. Moreira"
      },
      {
        "authorId": "2312327686",
        "name": "Radek Osmulski"
      },
      {
        "authorId": "2312332664",
        "name": "Mengyao Xu"
      },
      {
        "authorId": "50309003",
        "name": "Ronay Ak"
      },
      {
        "authorId": "2080538157",
        "name": "Benedikt D. Schifferer"
      },
      {
        "authorId": "79722739",
        "name": "Even Oldridge"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "9e4e3132dd293b12b18fb1d7a82b28509d1cd0e6",
    "url": "https://www.semanticscholar.org/paper/9e4e3132dd293b12b18fb1d7a82b28509d1cd0e6",
    "title": "RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models",
    "abstract": "The recent emergence of Medical Large Vision Language Models (Med-LVLMs) has enhanced medical diagnosis. However, current Med-LVLMs frequently encounter factual issues, often generating responses that do not align with established medical facts. Retrieval-Augmented Generation (RAG), which utilizes external knowledge, can improve the factual accuracy of these models but introduces two major challenges. First, limited retrieved contexts might not cover all necessary information, while excessive retrieval can introduce irrelevant and inaccurate references, interfering with the model‚Äôs generation. Second, in cases where the model originally responds correctly, applying RAG can lead to an over-reliance on retrieved contexts, resulting in incorrect answers. To address these issues, we propose RULE, which consists of two components. First, we introduce a provably effective strategy for controlling factuality risk through the calibrated selection of the number of retrieved contexts. Second, based on samples where over-reliance on retrieved contexts led to errors, we curate a preference dataset to fine-tune the model, balancing its dependence on inherent knowledge and retrieved contexts for generation. We demonstrate the effectiveness of RAFE on three medical VQA datasets, achieving an average improvement of 20.8% in factual accuracy.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-06",
    "authors": [
      {
        "authorId": "2261083308",
        "name": "Peng Xia"
      },
      {
        "authorId": "2307274105",
        "name": "Kangyu Zhu"
      },
      {
        "authorId": "2322263754",
        "name": "Haoran Li"
      },
      {
        "authorId": "2283767917",
        "name": "Hongtu Zhu"
      },
      {
        "authorId": "2302843836",
        "name": "Yun Li"
      },
      {
        "authorId": "2305606293",
        "name": "Gang Li"
      },
      {
        "authorId": "2249882382",
        "name": "Linjun Zhang"
      },
      {
        "authorId": "2284173253",
        "name": "Huaxiu Yao"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "450ce5f335d2b1224ef36e680e94ee00d2f12a6c",
    "url": "https://www.semanticscholar.org/paper/450ce5f335d2b1224ef36e680e94ee00d2f12a6c",
    "title": "Reinforcement Learning for Optimizing RAG for Domain Chatbots",
    "abstract": "With the advent of Large Language Models (LLM), conversational assistants have become prevalent for domain use cases. LLMs acquire the ability to contextual question answering through training, and Retrieval Augmented Generation (RAG) further enables the bot to answer domain-specific questions. This paper describes a RAG-based approach for building a chatbot that answers user's queries using Frequently Asked Questions (FAQ) data. We train an in-house retrieval embedding model using infoNCE loss, and experimental results demonstrate that the in-house model works significantly better than the well-known general-purpose public embedding model, both in terms of retrieval accuracy and Out-of-Domain (OOD) query detection. As an LLM, we use an open API-based paid ChatGPT model. We noticed that a previously retrieved-context could be used to generate an answer for specific patterns/sequences of queries (e.g., follow-up queries). Hence, there is a scope to optimize the number of LLM tokens and cost. Assuming a fixed retrieval model and an LLM, we optimize the number of LLM tokens using Reinforcement Learning (RL). Specifically, we propose a policy-based model external to the RAG, which interacts with the RAG pipeline through policy actions and updates the policy to optimize the cost. The policy model can perform two actions: to fetch FAQ context or skip retrieval. We use the open API-based GPT-4 as the reward model. We then train a policy model using policy gradient on multiple training chat sessions. As a policy model, we experimented with a public gpt-2 model and an in-house BERT model. With the proposed RL-based optimization combined with similarity threshold, we are able to achieve significant cost savings while getting a slightly improved accuracy. Though we demonstrate results for the FAQ chatbot, the proposed RL approach is generic and can be experimented with any existing RAG pipeline.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-10",
    "authors": [
      {
        "authorId": "2279546910",
        "name": "Mandar Kulkarni"
      },
      {
        "authorId": "2279546832",
        "name": "Praveen Tangarajan"
      },
      {
        "authorId": "2279584491",
        "name": "Kyung Kim"
      },
      {
        "authorId": "35108744",
        "name": "Anusua Trivedi"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "038cf7f6b8fc781d3b74fe2cfe6db2bf35908a37",
    "url": "https://www.semanticscholar.org/paper/038cf7f6b8fc781d3b74fe2cfe6db2bf35908a37",
    "title": "Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations",
    "abstract": "The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1) vulnerability to noisy documents through low-level perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG (\\textit{GARAG}), which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our \\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-22",
    "authors": [
      {
        "authorId": "2158892171",
        "name": "Sukmin Cho"
      },
      {
        "authorId": "8599185",
        "name": "Soyeong Jeong"
      },
      {
        "authorId": "2148402840",
        "name": "Jeongyeon Seo"
      },
      {
        "authorId": "2297768281",
        "name": "Taeho Hwang"
      },
      {
        "authorId": "2109285560",
        "name": "Jong C. Park"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "9f3c17e20dff7321ddc849f8bf5194ba94370c46",
    "url": "https://www.semanticscholar.org/paper/9f3c17e20dff7321ddc849f8bf5194ba94370c46",
    "title": "Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization",
    "abstract": "Large language models (LLMs), even when specifically trained to process long input contexts, struggle to capture relevant information located in the middle of their input. This phenomenon has been known as the lost-in-the-middle problem. In this work, we make three contributions. First, we set out to understand the factors that cause this phenomenon. In doing so, we establish a connection between lost-in-the-middle to LLMs' intrinsic attention bias: LLMs exhibit a U-shaped attention bias where the tokens at the beginning and at the end of its input receive higher attention, regardless of their relevance. Second, we mitigate this positional bias through a calibration mechanism, found-in-the-middle, that allows the model to attend to contexts faithfully according to their relevance, even though when they are in the middle. Third, we show found-in-the-middle not only achieves better performance in locating relevant information within a long context, but also eventually leads to improved retrieval-augmented generation (RAG) performance across various tasks, outperforming existing methods by up to 15 percentage points. These findings open up future directions in understanding LLM attention bias and its potential consequences.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-23",
    "authors": [
      {
        "authorId": "2650640",
        "name": "Cheng-Yu Hsieh"
      },
      {
        "authorId": "2475831",
        "name": "Yung-Sung Chuang"
      },
      {
        "authorId": "2305626187",
        "name": "Chun-Liang Li"
      },
      {
        "authorId": "2278799290",
        "name": "Zifeng Wang"
      },
      {
        "authorId": "2253355959",
        "name": "Long T. Le"
      },
      {
        "authorId": "2308067790",
        "name": "Abhishek Kumar"
      },
      {
        "authorId": "2288494477",
        "name": "James Glass"
      },
      {
        "authorId": "2308039282",
        "name": "Alexander Ratner"
      },
      {
        "authorId": "2278969944",
        "name": "Chen-Yu Lee"
      },
      {
        "authorId": "2308040151",
        "name": "Ranjay Krishna"
      },
      {
        "authorId": "2305619900",
        "name": "Tomas Pfister"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "450ce5f335d2b1224ef36e680e94ee00d2f12a6c",
    "url": "https://www.semanticscholar.org/paper/450ce5f335d2b1224ef36e680e94ee00d2f12a6c",
    "title": "Reinforcement Learning for Optimizing RAG for Domain Chatbots",
    "abstract": "With the advent of Large Language Models (LLM), conversational assistants have become prevalent for domain use cases. LLMs acquire the ability to contextual question answering through training, and Retrieval Augmented Generation (RAG) further enables the bot to answer domain-specific questions. This paper describes a RAG-based approach for building a chatbot that answers user's queries using Frequently Asked Questions (FAQ) data. We train an in-house retrieval embedding model using infoNCE loss, and experimental results demonstrate that the in-house model works significantly better than the well-known general-purpose public embedding model, both in terms of retrieval accuracy and Out-of-Domain (OOD) query detection. As an LLM, we use an open API-based paid ChatGPT model. We noticed that a previously retrieved-context could be used to generate an answer for specific patterns/sequences of queries (e.g., follow-up queries). Hence, there is a scope to optimize the number of LLM tokens and cost. Assuming a fixed retrieval model and an LLM, we optimize the number of LLM tokens using Reinforcement Learning (RL). Specifically, we propose a policy-based model external to the RAG, which interacts with the RAG pipeline through policy actions and updates the policy to optimize the cost. The policy model can perform two actions: to fetch FAQ context or skip retrieval. We use the open API-based GPT-4 as the reward model. We then train a policy model using policy gradient on multiple training chat sessions. As a policy model, we experimented with a public gpt-2 model and an in-house BERT model. With the proposed RL-based optimization combined with similarity threshold, we are able to achieve significant cost savings while getting a slightly improved accuracy. Though we demonstrate results for the FAQ chatbot, the proposed RL approach is generic and can be experimented with any existing RAG pipeline.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-10",
    "authors": [
      {
        "authorId": "2279546910",
        "name": "Mandar Kulkarni"
      },
      {
        "authorId": "2279546832",
        "name": "Praveen Tangarajan"
      },
      {
        "authorId": "2279584491",
        "name": "Kyung Kim"
      },
      {
        "authorId": "35108744",
        "name": "Anusua Trivedi"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.58585994422887
  },
  {
    "paperId": "176793264b4344a598d301dcfb8d43e13f323ef0",
    "url": "https://www.semanticscholar.org/paper/176793264b4344a598d301dcfb8d43e13f323ef0",
    "title": "Vul-RAG: Enhancing LLM-based Vulnerability Detection via Knowledge-level RAG",
    "abstract": "Vulnerability detection is essential for software quality assurance. In recent years, deep learning models (especially large language models) have shown promise in vulnerability detection. In this work, we propose a novel LLM-based vulnerability detection technique Vul-RAG, which leverages knowledge-level retrieval-augmented generation (RAG) framework to detect vulnerability for the given code in three phases. First, Vul-RAG constructs a vulnerability knowledge base by extracting multi-dimension knowledge via LLMs from existing CVE instances; second, for a given code snippet, Vul-RAG} retrieves the relevant vulnerability knowledge from the constructed knowledge base based on functional semantics; third, Vul-RAG leverages LLMs to check the vulnerability of the given code snippet by reasoning the presence of vulnerability causes and fixing solutions of the retrieved vulnerability knowledge. Our evaluation of Vul-RAG on our constructed benchmark PairVul shows that Vul-RAG substantially outperforms all baselines by 12.96\\%/110\\% relative improvement in accuracy/pairwise-accuracy. In addition, our user study shows that the vulnerability knowledge generated by Vul-RAG can serve as high-quality explanations which can improve the manual detection accuracy from 0.60 to 0.77.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-17",
    "authors": [
      {
        "authorId": "2133930029",
        "name": "Xueying Du"
      },
      {
        "authorId": "2307070440",
        "name": "Geng Zheng"
      },
      {
        "authorId": "2296494155",
        "name": "Kaixin Wang"
      },
      {
        "authorId": "2296601233",
        "name": "Jiayi Feng"
      },
      {
        "authorId": "2307018188",
        "name": "Wentai Deng"
      },
      {
        "authorId": "2007711208",
        "name": "Mingwei Liu"
      },
      {
        "authorId": "144943089",
        "name": "Bihuan Chen"
      },
      {
        "authorId": "2216694669",
        "name": "Xin Peng"
      },
      {
        "authorId": "2308628801",
        "name": "Tao Ma"
      },
      {
        "authorId": "2265400203",
        "name": "Yiling Lou"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "7a57a52e1a273799bed7c882bc12177ca89609ab",
    "url": "https://www.semanticscholar.org/paper/7a57a52e1a273799bed7c882bc12177ca89609ab",
    "title": "Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems",
    "abstract": "Large language models (LLMs) and foundation models have been recently touted as a game-changer for 6 G systems. However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language models that were designed for natural language processing (NLP) applications. To address this challenge and create wireless-centric foundation models, this paper presents a comprehensive vision on how to design universal foundation models that are tailored towards the unique needs of next-generation wireless systems, thereby paving the way towards the deployment of artificial intelligence (AI)-native networks. Diverging from NLP-based foundation models, the proposed framework promotes the design of large multi-modal models (LMMs) fostered by three key capabilities: 1) processing of multi-modal sensing data, 2) grounding of physical symbol representations in real-world wireless systems using causal reasoning and retrieval-augmented generation (RAG), and 3) enabling instructibility from the wireless environment feedback to facilitate dynamic network adaptation thanks to logical and mathematical reasoning facilitated by neuro-symbolic AI. In essence, these properties enable the proposed LMM framework to build universal capabilities that cater to various cross-layer networking tasks and alignment of intents across different domains. Preliminary results from experimental evaluation demonstrate the efficacy of grounding using RAG in LMMs, and showcase the alignment of LMMs with wireless system designs. Furthermore, the enhanced rationale exhibited in the responses to mathematical questions by LMMs, compared to vanilla LLMs, demonstrates the logical and mathematical reasoning capabilities inherent in LMMs. Building on those results, we present a sequel of open questions and challenges for LMMs. We then conclude with a set of recommendations that ignite the path towards LMM-empowered AI-native systems.",
    "venue": "IEEE Network",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2402.01748",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-30",
    "authors": [
      {
        "authorId": "134770854",
        "name": "Shengzhe Xu"
      },
      {
        "authorId": "2275603455",
        "name": "Christo Kurisummoottil Thomas"
      },
      {
        "authorId": "2119144208",
        "name": "Omar Hashash"
      },
      {
        "authorId": "50027530",
        "name": "N. Muralidhar"
      },
      {
        "authorId": "2269149200",
        "name": "Walid Saad"
      },
      {
        "authorId": "2263149583",
        "name": "Naren Ramakrishnan"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.47424036192305
  },
  {
    "paperId": "c8a951262fc797e7f883b3ad4c68d2463f9cf9e4",
    "url": "https://www.semanticscholar.org/paper/c8a951262fc797e7f883b3ad4c68d2463f9cf9e4",
    "title": "MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge Discovery",
    "abstract": "Retrieval-Augmented Generation (RAG) leverages retrieval tools to access external databases, thereby enhancing the generation quality of large language models (LLMs) through optimized context. However, the existing retrieval methods are constrained inherently, as they can only perform relevance matching between explicitly stated queries and well-formed knowledge, but unable to handle tasks involving ambiguous information needs or unstructured knowledge. Consequently, existing RAG systems are primarily effective for straightforward question-answering tasks. In this work, we propose MemoRAG, a novel retrieval-augmented generation paradigm empowered by long-term memory. MemoRAG adopts a dual-system architecture. On the one hand, it employs a light but long-range LLM to form the global memory of database. Once a task is presented, it generates draft answers, cluing the retrieval tools to locate useful information within the database. On the other hand, it leverages an expensive but expressive LLM, which generates the ultimate answer based on the retrieved information. Building on this general framework, we further optimize MemoRAG's performance by enhancing its cluing mechanism and memorization capacity. In our experiment, MemoRAG achieves superior performance across a variety of evaluation tasks, including both complex ones where conventional RAG fails and straightforward ones where RAG is commonly applied.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-09",
    "authors": [
      {
        "authorId": "1972030827",
        "name": "Hongjin Qian"
      },
      {
        "authorId": "2153419738",
        "name": "Peitian Zhang"
      },
      {
        "authorId": "2284309569",
        "name": "Zheng Liu"
      },
      {
        "authorId": "1580228663",
        "name": "Kelong Mao"
      },
      {
        "authorId": "2257039188",
        "name": "Zhicheng Dou"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "ed74f1cd4de2cd0519a97bfcfba21325b04c10e2",
    "url": "https://www.semanticscholar.org/paper/ed74f1cd4de2cd0519a97bfcfba21325b04c10e2",
    "title": "WirelessLLM: Empowering Large Language Models Towards Wireless Intelligence",
    "abstract": "The rapid evolution of wireless technologies and the growing complexity of network infrastructures necessitate a paradigm shift in how communication networks are designed, configured, and managed. Recent advancements in Large Language Models (LLMs) have sparked interest in their potential to revolutionize wireless communication systems. However, existing studies on LLMs for wireless systems are limited to a direct application for telecom language understanding. To empower LLMs with knowledge and expertise in the wireless domain, this paper proposes WirelessLLM, a comprehensive framework for adapting and enhancing LLMs to address the unique challenges and requirements of wireless communication networks. We first identify three foundational principles that underpin WirelessLLM: knowledge alignment, knowledge fusion, and knowledge evolution. Then, we investigate the enabling technologies to build WirelessLLM, including prompt engineering, retrieval augmented generation, tool usage, multi-modal pre-training, and domain-specific fine-tuning. Moreover, we present three case studies to demonstrate the practical applicability and benefits of WirelessLLM for solving typical problems in wireless networks. Finally, we conclude this paper by highlighting key challenges and outlining potential avenues for future research.",
    "venue": "J. Commun. Inf. Networks",
    "year": 2024,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-27",
    "authors": [
      {
        "authorId": "2111877589",
        "name": "Jiawei Shao"
      },
      {
        "authorId": "2302323728",
        "name": "Jingwen Tong"
      },
      {
        "authorId": "2303414429",
        "name": "Qiong Wu"
      },
      {
        "authorId": "2293669306",
        "name": "Wei Guo"
      },
      {
        "authorId": "2293659726",
        "name": "Zijian Li"
      },
      {
        "authorId": "2112335060",
        "name": "Zehong Lin"
      },
      {
        "authorId": "2294227891",
        "name": "Jun Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "fd30d3189b3bc3295ddad05ac1f683ce41f5e9cb",
    "url": "https://www.semanticscholar.org/paper/fd30d3189b3bc3295ddad05ac1f683ce41f5e9cb",
    "title": "LitLLM: A Toolkit for Scientific Literature Review",
    "abstract": "Conducting literature reviews for scientific papers is essential for understanding research, its limitations, and building on existing work. It is a tedious task which makes an automatic literature review generator appealing. Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-actual information-and ignore the latest research they have not been trained on. To address these limitations, we propose a toolkit that operates on Retrieval Augmented Generation (RAG) principles, specialized prompting and instructing techniques with the help of LLMs. Our system first initiates a web search to retrieve relevant papers by summarizing user-provided abstracts into keywords using an off-the-shelf LLM. Authors can enhance the search by supplementing it with relevant papers or keywords, contributing to a tailored retrieval process. Second, the system re-ranks the retrieved papers based on the user-provided abstract. Finally, the related work section is generated based on the re-ranked results and the abstract. There is a substantial reduction in time and effort for literature review compared to traditional methods, establishing our toolkit as an efficient alternative. Our open-source toolkit is accessible at https://github.com/shubhamagarwal92/LitLLM and Huggingface space (https://huggingface.co/spaces/shubhamagarwal92/LitLLM) with the video demo at https://youtu.be/E2ggOZBAFw0.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-02-02",
    "authors": [
      {
        "authorId": "2275258816",
        "name": "Shubham Agarwal"
      },
      {
        "authorId": "3266173",
        "name": "I. Laradji"
      },
      {
        "authorId": "1778839",
        "name": "Laurent Charlin"
      },
      {
        "authorId": "2275240361",
        "name": "Christopher Pal"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "531abcf7be2d58b45e57f3099728b9e2b02ea937",
    "url": "https://www.semanticscholar.org/paper/531abcf7be2d58b45e57f3099728b9e2b02ea937",
    "title": "REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models",
    "abstract": "The integration of multimodal Electronic Health Records (EHR) data has significantly improved clinical predictive capabilities. Leveraging clinical notes and multivariate time-series EHR, existing models often lack the medical context relevent to clinical tasks, prompting the incorporation of external knowledge, particularly from the knowledge graph (KG). Previous approaches with KG knowledge have primarily focused on structured knowledge extraction, neglecting unstructured data modalities and semantic high dimensional medical knowledge. In response, we propose REALM, a Retrieval-Augmented Generation (RAG) driven framework to enhance multimodal EHR representations that address these limitations. Firstly, we apply Large Language Model (LLM) to encode long context clinical notes and GRU model to encode time-series EHR data. Secondly, we prompt LLM to extract task-relevant medical entities and match entities in professionally labeled external knowledge graph (PrimeKG) with corresponding medical knowledge. By matching and aligning with clinical standards, our framework eliminates hallucinations and ensures consistency. Lastly, we propose an adaptive multimodal fusion network to integrate extracted knowledge with multimodal EHR data. Our extensive experiments on MIMIC-III mortality and readmission tasks showcase the superior performance of our REALM framework over baselines, emphasizing the effectiveness of each module. REALM framework contributes to refining the use of multimodal EHR data in healthcare and bridging the gap with nuanced medical context essential for informed clinical predictions.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-10",
    "authors": [
      {
        "authorId": "2098545",
        "name": "Yinghao Zhu"
      },
      {
        "authorId": "2275114809",
        "name": "Changyu Ren"
      },
      {
        "authorId": "2239069575",
        "name": "Shiyun Xie"
      },
      {
        "authorId": "2283879816",
        "name": "Shukai Liu"
      },
      {
        "authorId": "2283994746",
        "name": "Hangyuan Ji"
      },
      {
        "authorId": "2239053540",
        "name": "Zixiang Wang"
      },
      {
        "authorId": "2284180939",
        "name": "Tao Sun"
      },
      {
        "authorId": "2238715250",
        "name": "Long He"
      },
      {
        "authorId": "2258837278",
        "name": "Zhoujun Li"
      },
      {
        "authorId": "2283876739",
        "name": "Xi Zhu"
      },
      {
        "authorId": "2238637284",
        "name": "Chengwei Pan"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "df403ff1331117ec316a741d691bd2b2c5030cc4",
    "url": "https://www.semanticscholar.org/paper/df403ff1331117ec316a741d691bd2b2c5030cc4",
    "title": "Studying Large Language Model Behaviors Under Realistic Knowledge Conflicts",
    "abstract": "Retrieval-augmented generation (RAG) mitigates many problems of fully parametric language models, such as temporal degradation, hallucinations, and lack of grounding. In RAG, the model‚Äôs knowledge can be updated from documents provided in context. This leads to cases of conflict between the model‚Äôs parametric knowledge and the contextual information, where the model may not always update its knowledge. Previous work studied knowledge conflicts by creating synthetic documents that contradict the model‚Äôs correct parametric answers. We present a framework for studying knowledge conflicts in a realistic setup. We update incorrect parametric knowledge using real conflicting documents. This reflects how knowledge conflicts arise in practice. In this realistic scenario, we find that knowledge updates fail less often than previously reported. In cases where the models still fail to update their answers, we find a parametric bias: the incorrect parametric answer appearing in context makes the knowledge update likelier to fail. These results suggest that the factual parametric knowledge of LLMs can negatively influence their reading abilities and behaviors. Our code is available at: https://github.com/kortukov/realistic_knowledge_conflicts/ .",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2264447799",
        "name": "Evgenii Kortukov"
      },
      {
        "authorId": "2253465295",
        "name": "Alexander Rubinstein"
      },
      {
        "authorId": "2257366367",
        "name": "Elisa Nguyen"
      },
      {
        "authorId": "2390510",
        "name": "Seong Joon Oh"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "7ae8f3fea5c8db1283bae48b5537a8ed7e8bec7b",
    "url": "https://www.semanticscholar.org/paper/7ae8f3fea5c8db1283bae48b5537a8ed7e8bec7b",
    "title": "ActiveRAG: Revealing the Treasures of Knowledge via Active Learning",
    "abstract": "Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks. However, current RAG models position LLMs as passive knowledge receptors, thereby restricting their capacity for learning and comprehending ex-ternal knowledge. In this paper, we present A CTIVE RAG, an innovative RAG framework that shifts from passive knowledge acquisition to an active learning mechanism. This approach utilizes the Knowledge Construction mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism to incorporate the outcomes from both chains of thought and knowledge construction, thereby calibrating the intrinsic cognition of LLMs. Our experimental results demonstrate that A CTIVE RAG surpasses previous RAG models, achieving a 5% improvement on question-answering datasets. All data and codes are available at https://github.com/ OpenMatch/ActiveRAG .",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2284986197",
        "name": "Zhipeng Xu"
      },
      {
        "authorId": "49047064",
        "name": "Zhenghao Liu"
      },
      {
        "authorId": "2284945375",
        "name": "Yibin Liu"
      },
      {
        "authorId": "2139787803",
        "name": "Chenyan Xiong"
      },
      {
        "authorId": "2277242040",
        "name": "Yukun Yan"
      },
      {
        "authorId": "2267033597",
        "name": "Shuo Wang"
      },
      {
        "authorId": "2286160362",
        "name": "Shi Yu"
      },
      {
        "authorId": "2266886975",
        "name": "Zhiyuan Liu"
      },
      {
        "authorId": "2204644192",
        "name": "Ge Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "31be6dba573a704940617e12d3d2dfbe9178c04f",
    "url": "https://www.semanticscholar.org/paper/31be6dba573a704940617e12d3d2dfbe9178c04f",
    "title": "BioRAG: A RAG-LLM Framework for Biological Question Reasoning",
    "abstract": "The question-answering system for Life science research, which is characterized by the rapid pace of discovery, evolving insights, and complex interactions among knowledge entities, presents unique challenges in maintaining a comprehensive knowledge warehouse and accurate information retrieval. To address these issues, we introduce BioRAG, a novel Retrieval-Augmented Generation (RAG) with the Large Language Models (LLMs) framework. Our approach starts with parsing, indexing, and segmenting an extensive collection of 22 million scientific papers as the basic knowledge, followed by training a specialized embedding model tailored to this domain. Additionally, we enhance the vector retrieval process by incorporating a domain-specific knowledge hierarchy, which aids in modeling the intricate interrelationships among each query and context. For queries requiring the most current information, BioRAG deconstructs the question and employs an iterative retrieval process incorporated with the search engine for step-by-step reasoning. Rigorous experiments have demonstrated that our model outperforms fine-tuned LLM, LLM with search engines, and other scientific RAG frameworks across multiple life science question-answering tasks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-02",
    "authors": [
      {
        "authorId": "2290751442",
        "name": "Chengrui Wang"
      },
      {
        "authorId": "2253519659",
        "name": "Qingqing Long"
      },
      {
        "authorId": "2311703192",
        "name": "Meng Xiao"
      },
      {
        "authorId": "2257128565",
        "name": "Xunxin Cai"
      },
      {
        "authorId": "2305155503",
        "name": "Chengjun Wu"
      },
      {
        "authorId": "2287843869",
        "name": "Zhen Meng"
      },
      {
        "authorId": "2287878697",
        "name": "Xuezhi Wang"
      },
      {
        "authorId": "2259918839",
        "name": "Yuanchun Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "2eff8cbb4b226575a2f6aa93c40804c163e463ac",
    "url": "https://www.semanticscholar.org/paper/2eff8cbb4b226575a2f6aa93c40804c163e463ac",
    "title": "From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process",
    "abstract": "Regulatory compliance in the pharmaceutical industry entails navigating through complex and voluminous guidelines, often requiring significant human resources. To address these challenges, our study introduces a chatbot model that utilizes generative AI and the Retrieval Augmented Generation (RAG) method. This chatbot is designed to search for guideline documents relevant to the user inquiries and provide answers based on the retrieved guidelines. Recognizing the inherent need for high reliability in this domain, we propose the Question and Answer Retrieval Augmented Generation (QA-RAG) model. In comparative experiments, the QA-RAG model demonstrated a significant improvement in accuracy, outperforming all other baselines including conventional RAG methods. This paper details QA-RAG's structure and performance evaluation, emphasizing its potential for the regulatory compliance domain in the pharmaceutical industry and beyond. We have made our work publicly available for further research and development.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-26",
    "authors": [
      {
        "authorId": "2282545265",
        "name": "Jaewoong Kim"
      },
      {
        "authorId": "2282539913",
        "name": "Moohong Min"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "8b264b811dd14dc0ed0d396e82c4f159c671cda5",
    "url": "https://www.semanticscholar.org/paper/8b264b811dd14dc0ed0d396e82c4f159c671cda5",
    "title": "Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge",
    "abstract": "Large language models (LLMs) are transforming the way information is retrieved with vast amounts of knowledge being summarized and presented via natural language conversations. Yet, LLMs are prone to highlight the most frequently seen pieces of information from the training set and to neglect the rare ones. In the field of biomedical research, latest discoveries are key to academic and industrial actors and are obscured by the abundance of an ever-increasing literature corpus (the information overload problem). Surfacing new associations between biomedical entities, e.g., drugs, genes, diseases, with LLMs becomes a challenge of capturing the long-tail knowledge of the biomedical scientific production. To overcome this challenge, Retrieval Augmented Generation (RAG) has been proposed to alleviate some of the shortcomings of LLMs by augmenting the prompts with context retrieved from external datasets. RAG methods typically select the context via maximum similarity search over text embeddings. In this study, we show that RAG methods leave out a significant proportion of relevant information due to clusters of over-represented concepts in the biomedical literature. We introduce a novel information-retrieval method that leverages a knowledge graph to downsample these clusters and mitigate the information overload problem. Its retrieval performance is about twice better than embedding similarity alternatives on both precision and recall. Finally, we demonstrate that both embedding similarity and knowledge graph retrieval methods can be advantageously combined into a hybrid model that outperforms both, enabling potential improvements to biomedical question-answering models.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-19",
    "authors": [
      {
        "authorId": "2284691128",
        "name": "Julien Delile"
      },
      {
        "authorId": "1782727",
        "name": "Srayanta Mukherjee"
      },
      {
        "authorId": "97479431",
        "name": "Anton Van Pamel"
      },
      {
        "authorId": "2284689952",
        "name": "Leonid Zhukov"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "eb95c327260725498404eb43ec370d419b8d92c7",
    "url": "https://www.semanticscholar.org/paper/eb95c327260725498404eb43ec370d419b8d92c7",
    "title": "SGLang: Efficient Execution of Structured Language Model Programs",
    "abstract": "Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs. However, efficient systems are lacking for programming and executing these applications. We introduce SGLang, a system for efficient execution of complex language model programs. SGLang consists of a frontend language and a runtime. The frontend simplifies programming with primitives for generation and parallelism control. The runtime accelerates execution with novel optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding. Experiments show that SGLang achieves up to 6.4x higher throughput compared to state-of-the-art inference systems on various large language and multi-modal models on tasks including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, and multi-turn chat. The code is publicly available at https://github.com/sgl-project/sglang",
    "venue": "",
    "year": 2023,
    "citationCount": 43,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-12-12",
    "authors": [
      {
        "authorId": "2149970173",
        "name": "Lianmin Zheng"
      },
      {
        "authorId": "2273793944",
        "name": "Liangsheng Yin"
      },
      {
        "authorId": "2273723676",
        "name": "Zhiqiang Xie"
      },
      {
        "authorId": "2262463672",
        "name": "Chuyue Sun"
      },
      {
        "authorId": "2273914430",
        "name": "Jeff Huang"
      },
      {
        "authorId": "2239168517",
        "name": "Cody Hao Yu"
      },
      {
        "authorId": "2265490669",
        "name": "Shiyi Cao"
      },
      {
        "authorId": "2279756528",
        "name": "Christos Kozyrakis"
      },
      {
        "authorId": "2055174324",
        "name": "Ion Stoica"
      },
      {
        "authorId": "2254681613",
        "name": "Joseph E. Gonzalez"
      },
      {
        "authorId": "2262214357",
        "name": "Clark W. Barrett"
      },
      {
        "authorId": "2262215507",
        "name": "Ying Sheng"
      }
    ],
    "source": "semantic_scholar",
    "score": 126.76284450877392
  },
  {
    "paperId": "9de9fa60a786ca23f924f5521326b2a264c22228",
    "url": "https://www.semanticscholar.org/paper/9de9fa60a786ca23f924f5521326b2a264c22228",
    "title": "Lynx: An Open Source Hallucination Evaluation Model",
    "abstract": "Retrieval Augmented Generation (RAG) techniques aim to mitigate hallucinations in Large Language Models (LLMs). However, LLMs can still produce information that is unsupported or contradictory to the retrieved contexts. We introduce LYNX, a SOTA hallucination detection LLM that is capable of advanced reasoning on challenging real-world hallucination scenarios. To evaluate LYNX, we present HaluBench, a comprehensive hallucination evaluation benchmark, consisting of 15k samples sourced from various real-world domains. Our experiment results show that LYNX outperforms GPT-4o, Claude-3-Sonnet, and closed and open-source LLM-as-a-judge models on HaluBench. We release LYNX, HaluBench and our evaluation code for public access.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-11",
    "authors": [
      {
        "authorId": "1749231011",
        "name": "Selvan Sunitha Ravi"
      },
      {
        "authorId": "2300406",
        "name": "B. Mielczarek"
      },
      {
        "authorId": "2266468273",
        "name": "Anand Kannappan"
      },
      {
        "authorId": "2111313627",
        "name": "Douwe Kiela"
      },
      {
        "authorId": "2266468744",
        "name": "Rebecca Qian"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.53877639491068
  },
  {
    "paperId": "ec1bec009e68a4df478aaf11e3615e5587768990",
    "url": "https://www.semanticscholar.org/paper/ec1bec009e68a4df478aaf11e3615e5587768990",
    "title": "CRAG - Comprehensive RAG Benchmark",
    "abstract": "Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution to alleviate Large Language Model (LLM)'s deficiency in lack of knowledge. Existing RAG datasets, however, do not adequately represent the diverse and dynamic nature of real-world Question Answering (QA) tasks. To bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual question answering benchmark of 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a diverse array of questions across five domains and eight question categories, reflecting varied entity popularity from popular to long-tail, and temporal dynamisms ranging from years to seconds. Our evaluation of this benchmark highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve<=34% accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63% of questions without any hallucination. CRAG also reveals much lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity, suggesting future research directions. The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge and attracted thousands of participants and submissions. We commit to maintaining CRAG to serve research communities in advancing RAG solutions and general QA solutions. CRAG is available at https://github.com/facebookresearch/CRAG/.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-07",
    "authors": [
      {
        "authorId": "2305569900",
        "name": "Xiao Yang"
      },
      {
        "authorId": "49871029",
        "name": "Kai Sun"
      },
      {
        "authorId": "2305482447",
        "name": "Hao Xin"
      },
      {
        "authorId": "2214733444",
        "name": "Yushi Sun"
      },
      {
        "authorId": "2305481658",
        "name": "Nikita Bhalla"
      },
      {
        "authorId": "2306139906",
        "name": "Xiangsen Chen"
      },
      {
        "authorId": "2305482313",
        "name": "Sajal Choudhary"
      },
      {
        "authorId": "2305481573",
        "name": "Rongze Daniel Gui"
      },
      {
        "authorId": "2305570216",
        "name": "Ziran Will Jiang"
      },
      {
        "authorId": "2269119211",
        "name": "Ziyu Jiang"
      },
      {
        "authorId": "2305625674",
        "name": "Lingkun Kong"
      },
      {
        "authorId": "2305481275",
        "name": "Brian Moran"
      },
      {
        "authorId": "2305531076",
        "name": "Jiaqi Wang"
      },
      {
        "authorId": "2290243809",
        "name": "Y. Xu"
      },
      {
        "authorId": "2305481525",
        "name": "An Yan"
      },
      {
        "authorId": "2218987196",
        "name": "Chenyu Yang"
      },
      {
        "authorId": "2305481568",
        "name": "Eting Yuan"
      },
      {
        "authorId": "47291370",
        "name": "Hanwen Zha"
      },
      {
        "authorId": "2305482335",
        "name": "Nan Tang"
      },
      {
        "authorId": "2282352887",
        "name": "Lei Chen"
      },
      {
        "authorId": "2305481622",
        "name": "Nicolas Scheffer"
      },
      {
        "authorId": "2282384339",
        "name": "Yue Liu"
      },
      {
        "authorId": "2305649695",
        "name": "Nirav Shah"
      },
      {
        "authorId": "2305481558",
        "name": "Rakesh Wanga"
      },
      {
        "authorId": "2247977368",
        "name": "Anuj Kumar"
      },
      {
        "authorId": "2072801764",
        "name": "Wen-tau Yih"
      },
      {
        "authorId": "2283964663",
        "name": "Xin Dong"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "4dc4644508a7868ad14f6c2c06a34056c6c333f7",
    "url": "https://www.semanticscholar.org/paper/4dc4644508a7868ad14f6c2c06a34056c6c333f7",
    "title": "KRAGEN: a knowledge graph-enhanced RAG framework for biomedical problem solving using large language models",
    "abstract": "Abstract Motivation Answering and solving complex problems using a large language model (LLM) given a certain domain such as biomedicine is a challenging task that requires both factual consistency and logic, and LLMs often suffer from some major limitations, such as hallucinating false or irrelevant information, or being influenced by noisy data. These issues can compromise the trustworthiness, accuracy, and compliance of LLM-generated text and insights. Results Knowledge Retrieval Augmented Generation ENgine (KRAGEN) is a new tool that combines knowledge graphs, Retrieval Augmented Generation (RAG), and advanced prompting techniques to solve complex problems with natural language. KRAGEN converts knowledge graphs into a vector database and uses RAG to retrieve relevant facts from it. KRAGEN uses advanced prompting techniques: namely graph-of-thoughts (GoT), to dynamically break down a complex problem into smaller subproblems, and proceeds to solve each subproblem by using the relevant knowledge through the RAG framework, which limits the hallucinations, and finally, consolidates the subproblems and provides a solution. KRAGEN‚Äôs graph visualization allows the user to interact with and evaluate the quality of the solution‚Äôs GoT structure and logic. Availability and implementation KRAGEN is deployed by running its custom Docker containers. KRAGEN is available as open-source from GitHub at: https://github.com/EpistasisLab/KRAGEN.",
    "venue": "Bioinformatics",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": {
      "url": "https://academic.oup.com/bioinformatics/advance-article-pdf/doi/10.1093/bioinformatics/btae353/58064527/btae353.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-01",
    "authors": [
      {
        "authorId": "2203911993",
        "name": "Nicholas Matsumoto"
      },
      {
        "authorId": "2254426097",
        "name": "Jay Moran"
      },
      {
        "authorId": "2255540908",
        "name": "Hyunjun Choi"
      },
      {
        "authorId": "2256062196",
        "name": "Miguel E. Hernandez"
      },
      {
        "authorId": "2265706743",
        "name": "Mythreye Venkatesan"
      },
      {
        "authorId": "2304611993",
        "name": "Paul Wang"
      },
      {
        "authorId": "2256771969",
        "name": "Jason H. Moore"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "0e877d47b71f9fb68ea70bde2c0518459e2837a6",
    "url": "https://www.semanticscholar.org/paper/0e877d47b71f9fb68ea70bde2c0518459e2837a6",
    "title": "A real-time 3D end-to-end augmented reality system (and its representation transformations)",
    "abstract": "The new generation of HMDs coming to the market is expected to enable many new applications that allow free viewpoint experiences with captured video objects. Current applications usually rely on 3D content that is manually created or captured in an offline manner. In contrast, this paper focuses on augmented reality applications that use live captured 3D objects while maintaining free viewpoint interaction. We present a system that allows live dynamic 3D objects (e.g. a person who is talking) to be captured in real-time. Real-time performance is achieved by traversing a number of representation formats and exploiting their specific benefits. For instance, depth images are maintained for fast neighborhood retrieval and occlusion determination, while implicit surfaces are used to facilitate multi-source aggregation for both geometry and texture. The result is a 3D reconstruction system that outputs multi-textured triangle meshes at real-time rates. An end-to-end system is presented that captures and reconstructs live 3D data and allows for this data to be used on a networked (AR) device. For allocating the different functional blocks onto the available physical devices, a number of alternatives are proposed considering the available computational power and bandwidth for each of the components. As we will show, the representation format can play an important role in this functional allocation and allows for a flexible system that can support a highly heterogeneous infrastructure.",
    "venue": "Optical Engineering + Applications",
    "year": 2016,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2016-09-27",
    "authors": [
      {
        "authorId": "40311674",
        "name": "D. Tytgat"
      },
      {
        "authorId": "153049828",
        "name": "M. Aerts"
      },
      {
        "authorId": "47631844",
        "name": "J. De Busser"
      },
      {
        "authorId": "37157476",
        "name": "S. Lievens"
      },
      {
        "authorId": "134600323",
        "name": "Patrice Rondao Alface"
      },
      {
        "authorId": "2961743",
        "name": "J. Macq"
      }
    ],
    "source": "semantic_scholar",
    "score": 85.79441541679836
  },
  {
    "paperId": "c36153a0dd02e31b39ff5780ccf56ed1d0294e14",
    "url": "https://www.semanticscholar.org/paper/c36153a0dd02e31b39ff5780ccf56ed1d0294e14",
    "title": "Optimizing large language models in digestive disease: strategies and challenges to improve clinical outcomes.",
    "abstract": "Large Language Models (LLMs) are transformer-based neural networks with billions of parameters trained on very large text corpora from diverse sources. LLMs have the potential to improve healthcare due to their capability to parse complex concepts and generate context-based responses. The interest in LLMs has not spared digestive disease academics, who have mainly investigated foundational LLM accuracy, which ranges from 25% to 90% and is influenced by the lack of standardized rules to report methodologies and results for LLM-oriented research. In addition, a critical issue is the absence of a universally accepted definition of accuracy, varying from binary to scalar interpretations, often tied to grader expertise without reference to clinical guidelines. We address strategies and challenges to increase accuracy. In particular, LLMs can be infused with domain knowledge using Retrieval Augmented Generation (RAG) or Supervised Fine-Tuning (SFT) with reinforcement learning from human feedback (RLHF). RAG faces challenges with in-context window limits and accurate information retrieval from the provided context. SFT, a deeper adaptation method, is computationally demanding and requires specialized knowledge. LLMs may increase patient quality of care across the field of digestive diseases,¬†where physicians are often engaged in screening, treatment and surveillance for a broad range of pathologies for which in-context learning or SFT with RLHF could improve clinical decision-making and patient outcomes. However, despite their potential, the safe deployment of LLMs in healthcare still needs to overcome hurdles in accuracy, suggesting a need for strategies that integrate human feedback with advanced model training.",
    "venue": "Liver international (Print)",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": {
      "url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/liv.15974",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "Review",
      "JournalArticle"
    ],
    "publicationDate": "2024-05-31",
    "authors": [
      {
        "authorId": "2299973744",
        "name": "Mauro Giuffre"
      },
      {
        "authorId": "2299973097",
        "name": "Simone Kresevic"
      },
      {
        "authorId": "2294847690",
        "name": "Nicola Pugliese"
      },
      {
        "authorId": "2260731556",
        "name": "Kisung You"
      },
      {
        "authorId": "2256766771",
        "name": "Dennis L. Shung"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "3c2130e219528df234658b94810de33fe7b077dc",
    "url": "https://www.semanticscholar.org/paper/3c2130e219528df234658b94810de33fe7b077dc",
    "title": "Are Large Language Models Good at Utility Judgments?",
    "abstract": "Retrieval-augmented generation (RAG) is considered to be a promising approach to alleviate the hallucination issue of large language models (LLMs), and it has received widespread attention from researchers recently. Due to the limitation in the semantic understanding of retrieval models, the success of RAG heavily lies on the ability of LLMs to identify passages with utility. Recent efforts have explored the ability of LLMs to assess the relevance of passages in retrieval, but there has been limited work on evaluating the utility of passages in supporting question answering. In this work, we conduct a comprehensive study about the capabilities of LLMs in utility evaluation for open-domain QA. Specifically, we introduce a benchmarking procedure and collection of candidate passages with different characteristics, facilitating a series of experiments with five representative LLMs. Our experiments reveal that: (i) well-instructed LLMs can distinguish between relevance and utility, and that LLMs are highly receptive to newly generated counterfactual passages. Moreover, (ii) we scrutinize key factors that affect utility judgments in the instruction design. And finally, (iii) to verify the efficacy of utility judgments in practical retrieval augmentation applications, we delve into LLMs' QA capabilities using the evidence judged with utility and direct dense retrieval results. (iv) We propose a k-sampling, listwise approach to reduce the dependency of LLMs on the sequence of input passages, thereby facilitating subsequent answer generation. We believe that the way we formalize and study the problem along with our findings contributes to a critical assessment of retrieval-augmented LLMs. Our code and benchmark can be found at \\url{https://github.com/ict-bigdatalab/utility_judgments}.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-28",
    "authors": [
      {
        "authorId": "2260353683",
        "name": "Hengran Zhang"
      },
      {
        "authorId": "2109960367",
        "name": "Ruqing Zhang"
      },
      {
        "authorId": "1777025",
        "name": "J. Guo"
      },
      {
        "authorId": "2265490493",
        "name": "M. D. Rijke"
      },
      {
        "authorId": "7888704",
        "name": "Yixing Fan"
      },
      {
        "authorId": "2244825947",
        "name": "Xueqi Cheng"
      }
    ],
    "source": "semantic_scholar",
    "score": 110.53877639491068
  },
  {
    "paperId": "c48dbe89ac73b0f7701dc655526be2cb286e11f6",
    "url": "https://www.semanticscholar.org/paper/c48dbe89ac73b0f7701dc655526be2cb286e11f6",
    "title": "How to improve ChatGPT performance for nephrologists: a technique guide.",
    "abstract": null,
    "venue": "JN. Journal of Nephrology (Milano. 1992)",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-21",
    "authors": [
      {
        "authorId": "2260930470",
        "name": "Jing Miao"
      },
      {
        "authorId": "3464205",
        "name": "C. Thongprayoon"
      },
      {
        "authorId": "4114489",
        "name": "Iasmina M. Craici"
      },
      {
        "authorId": "4112984",
        "name": "W. Cheungpasitporn"
      }
    ],
    "source": "semantic_scholar",
    "score": 84.53877639491068
  },
  {
    "paperId": "03cfdde24c6b9837ad8933cb535a2c4a7c0fd971",
    "url": "https://www.semanticscholar.org/paper/03cfdde24c6b9837ad8933cb535a2c4a7c0fd971",
    "title": "HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild",
    "abstract": "Hallucinations pose a significant challenge to the reliability of large language models (LLMs) in critical domains. Recent benchmarks designed to assess LLM hallucinations within conventional NLP tasks, such as knowledge-intensive question answering (QA) and summarization, are insufficient for capturing the complexities of user-LLM interactions in dynamic, real-world settings. To address this gap, we introduce HaluEval-Wild, the first benchmark specifically designed to evaluate LLM hallucinations in the wild. We meticulously collect challenging (adversarially filtered by Alpaca) user queries from ShareGPT, an existing real-world user-LLM interaction datasets, to evaluate the hallucination rates of various LLMs. Upon analyzing the collected queries, we categorize them into five distinct types, which enables a fine-grained analysis of the types of hallucinations LLMs exhibit, and synthesize the reference answers with the powerful GPT-4 model and retrieval-augmented generation (RAG). Our benchmark offers a novel approach towards enhancing our comprehension of and improving LLM reliability in scenarios reflective of real-world interactions. Our benchmark is available at https://github.com/HaluEval-Wild/HaluEval-Wild.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-07",
    "authors": [
      {
        "authorId": "2290238327",
        "name": "Zhiying Zhu"
      },
      {
        "authorId": "48064856",
        "name": "Zhiqing Sun"
      },
      {
        "authorId": "2257099254",
        "name": "Yiming Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "c78d1cc1f9cba23235c8e67bac5c896f8e4708b5",
    "url": "https://www.semanticscholar.org/paper/c78d1cc1f9cba23235c8e67bac5c896f8e4708b5",
    "title": "LLMs in Biomedicine: A study on clinical Named Entity Recognition",
    "abstract": "Large Language Models (LLMs) demonstrate remarkable versatility in various NLP tasks but encounter distinct challenges in biomedical due to the complexities of language and data scarcity. This paper investigates LLMs application in the biomedical domain by exploring strategies to enhance their performance for the NER task. Our study reveals the importance of meticulously designed prompts in the biomedical. Strategic selection of in-context examples yields a marked improvement, offering ~15-20\\% increase in F1 score across all benchmark datasets for biomedical few-shot NER. Additionally, our results indicate that integrating external biomedical knowledge via prompting strategies can enhance the proficiency of general-purpose LLMs to meet the specialized needs of biomedical NER. Leveraging a medical knowledge base, our proposed method, DiRAG, inspired by Retrieval-Augmented Generation (RAG), can boost the zero-shot F1 score of LLMs for biomedical NER. Code is released at \\url{https://github.com/masoud-monajati/LLM_Bio_NER}",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-10",
    "authors": [
      {
        "authorId": "2051713098",
        "name": "Masoud Monajatipoor"
      },
      {
        "authorId": "2296218135",
        "name": "Jiaxin Yang"
      },
      {
        "authorId": "1689543587",
        "name": "Joel Stremmel"
      },
      {
        "authorId": "2295989114",
        "name": "Melika Emami"
      },
      {
        "authorId": "70437778",
        "name": "Fazlolah Mohaghegh"
      },
      {
        "authorId": "1491749242",
        "name": "Mozhdeh Rouhsedaghat"
      },
      {
        "authorId": "2296002884",
        "name": "Kai-Wei Chang"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "b28ad67a90bea98eafefe6259b888c1d75b2ccbb",
    "url": "https://www.semanticscholar.org/paper/b28ad67a90bea98eafefe6259b888c1d75b2ccbb",
    "title": "T-RAG: Lessons from the LLM Trenches",
    "abstract": "Large Language Models (LLM) have shown remarkable language capabilities fueling attempts to integrate them into applications across a wide range of domains. An important application area is question answering over private enterprise documents where the main considerations are data security, which necessitates applications that can be deployed on-prem, limited computational resources and the need for a robust application that correctly responds to queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent framework for building LLM-based applications. While building a RAG is relatively straightforward, making it robust and a reliable application requires extensive customization and relatively deep knowledge of the application domain. We share our experiences building and deploying an LLM application for question answering over private organizational documents. Our application combines the use of RAG with a finetuned open-source LLM. Additionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure to represent entity hierarchies within the organization. This is used to generate a textual description to augment the context when responding to user queries pertaining to entities within the organization's hierarchy. Our evaluations, including a Needle in a Haystack test, show that this combination performs better than a simple RAG or finetuning implementation. Finally, we share some lessons learned based on our experiences building an LLM application for real-world use.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-12",
    "authors": [
      {
        "authorId": "72339731",
        "name": "M. Fatehkia"
      },
      {
        "authorId": "2081346",
        "name": "J. Lucas"
      },
      {
        "authorId": "2283846603",
        "name": "Sanjay Chawla"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "7d2f92bfcded0fe63cb3926155d769263b9580c9",
    "url": "https://www.semanticscholar.org/paper/7d2f92bfcded0fe63cb3926155d769263b9580c9",
    "title": "RLCoder: Reinforcement Learning for Repository-Level Code Completion",
    "abstract": "Repository-level code completion aims to generate code for unfinished code snippets within the context of a specified repository. Existing approaches mainly rely on retrieval-augmented generation strategies due to limitations in input sequence length. However, traditional lexical-based retrieval methods like BM25 struggle to capture code semantics, while model-based retrieval methods face challenges due to the lack of labeled data for training. Therefore, we propose RLCoder, a novel reinforcement learning framework, which can enable the retriever to learn to retrieve useful content for code completion without the need for labeled data. Specifically, we iteratively evaluate the usefulness of retrieved content based on the perplexity of the target code when provided with the retrieved content as additional context, and provide feedback to update the retriever parameters. This iterative process enables the retriever to learn from its successes and failures, gradually improving its ability to retrieve relevant and high-quality content. Considering that not all situations require information beyond code files and not all retrieved context is helpful for generation, we also introduce a stop signal mechanism, allowing the retriever to decide when to retrieve and which candidates to retain autonomously. Extensive experimental results demonstrate that RLCoder consistently outperforms state-of-the-art methods on CrossCodeEval and RepoEval, achieving 12.2% EM improvement over previous methods. Moreover, experiments show that our framework can generalize across different programming languages and further improve previous methods like RepoCoder. We provide the code and data at https://github.com/DeepSoftwareAnalytics/RLCoder.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-28",
    "authors": [
      {
        "authorId": "2239164852",
        "name": "Yanlin Wang"
      },
      {
        "authorId": "2314374754",
        "name": "Daya Guo"
      },
      {
        "authorId": "2254800142",
        "name": "Jiachi Chen"
      },
      {
        "authorId": "2305869100",
        "name": "Ruikai Zhang"
      },
      {
        "authorId": "2305694096",
        "name": "Yuchi Ma"
      },
      {
        "authorId": "2267902535",
        "name": "Zibin Zheng"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "35a8d6d9188e60c515aa3184e7125307e57c61f4",
    "url": "https://www.semanticscholar.org/paper/35a8d6d9188e60c515aa3184e7125307e57c61f4",
    "title": "A Comparison of Methods for Evaluating Generative IR",
    "abstract": "Information retrieval systems increasingly incorporate generative components. For example, in a retrieval augmented generation (RAG) system, a retrieval component might provide a source of ground truth, while a generative component summarizes and augments its responses. In other systems, a large language model (LLM) might directly generate responses without consulting a retrieval component. While there are multiple definitions of generative information retrieval (Gen-IR) systems, in this paper we focus on those systems where the system's response is not drawn from a fixed collection of documents or passages. The response to a query may be entirely new text. Since traditional IR evaluation methods break down under this model, we explore various methods that extend traditional offline evaluation approaches to the Gen-IR context. Offline IR evaluation traditionally employs paid human assessors, but increasingly LLMs are replacing human assessment, demonstrating capabilities similar or superior to crowdsourced labels. Given that Gen-IR systems do not generate responses from a fixed set, we assume that methods for Gen-IR evaluation must largely depend on LLM-generated labels. Along with methods based on binary and graded relevance, we explore methods based on explicit subtopics, pairwise preferences, and embeddings. We first validate these methods against human assessments on several TREC Deep Learning Track tasks; we then apply these methods to evaluate the output of several purely generative systems. For each method we consider both its ability to act autonomously, without the need for human labels or other input, and its ability to support human auditing. To trust these methods, we must be assured that their results align with human assessments. In order to do so, evaluation criteria must be transparent, so that outcomes can be audited by human assessors.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-05",
    "authors": [
      {
        "authorId": "81447039",
        "name": "Negar Arabzadeh"
      },
      {
        "authorId": "2278827569",
        "name": "Charles L. A. Clarke"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "c3a989448a106db8005341400a22b75fae01f4b4",
    "url": "https://www.semanticscholar.org/paper/c3a989448a106db8005341400a22b75fae01f4b4",
    "title": "TSpec-LLM: An Open-source Dataset for LLM Understanding of 3GPP Specifications",
    "abstract": "Understanding telecom standards involves sorting through numerous technical documents, such as those produced by the 3rd Generation Partnership Project (3GPP), which is time-consuming and labor-intensive. While large language models (LLMs) can assist with the extensive 3GPP knowledge base, an inclusive dataset is crucial for their effective pre-training and fine-tuning. In this paper, we introduce \\textit{TSpec-LLM}, an open-source comprehensive dataset covering all 3GPP documents from Release 8 to Release 19 (1999--2023). To evaluate its efficacy, we first select a representative sample of 3GPP documents, create corresponding technical questions, and assess the baseline performance of various LLMs. We then incorporate a retrieval-augmented generation (RAG) framework to enhance LLM capabilities by retrieving relevant context from the \\textit{TSpec-LLM} dataset. Our evaluation shows that using a naive-RAG framework on \\textit{TSpec-LLM} improves the accuracy of GPT-3.5, Gemini 1.0 Pro, and GPT-4 from 44\\%, 46\\%, and 51\\% to 71\\%, 75\\%, and 72\\%, respectively.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-03",
    "authors": [
      {
        "authorId": "73775548",
        "name": "Rasoul Nikbakht"
      },
      {
        "authorId": "2005320281",
        "name": "Mohamed Benzaghta"
      },
      {
        "authorId": "2267728718",
        "name": "Giovanni Geraci"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "93a13307f5d56992b2c116019073f1c94b96e6bd",
    "url": "https://www.semanticscholar.org/paper/93a13307f5d56992b2c116019073f1c94b96e6bd",
    "title": "MCQGen: A Large Language Model-Driven MCQ Generator for Personalized Learning",
    "abstract": "In the dynamic landscape of contemporary education, the evolution of teaching strategies such as blended learning and flipped classrooms has highlighted the need for efficient and effective generation of multiple-choice questions (MCQs). To address this, we introduce MCQGen, a novel generative artificial intelligence framework designed for the automated creation of MCQs. MCQGen uniquely integrates a large language model (LLM) with retrieval-augmented generation and advanced prompt engineering techniques, drawing from an extensive external knowledge base. This integration significantly enhances the ability of the LLM to produce educationally relevant questions that align with both the goals of educators and the diverse learning needs of students. The framework employs innovative prompt engineering, combining chain-of-thought and self-refine prompting techniques, to enhance the performance of the LLM. This process leads to the generation of questions that are not only contextually relevant and challenging but also reflective of common student misconceptions, contributing effectively to personalized learning experiences and enhancing student engagement and understanding. Our extensive evaluations showcase the effectiveness of MCQGen in producing high-quality MCQs for various educational needs and learning styles. The framework demonstrates its potential to significantly reduce the time and expertise required for MCQ creation, marking its practical utility in modern education. In essence, MCQGen offers an innovative and robust solution for the automated generation of MCQs, enhancing personalized learning in the digital era.",
    "venue": "IEEE Access",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "52205613",
        "name": "C. Hang"
      },
      {
        "authorId": "2313939528",
        "name": "Chee Wei Tan"
      },
      {
        "authorId": "3395394",
        "name": "Pei-Duo Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.95836866004329
  },
  {
    "paperId": "a8c86a10951e21814606bddb68c18d1980f3f481",
    "url": "https://www.semanticscholar.org/paper/a8c86a10951e21814606bddb68c18d1980f3f481",
    "title": "Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey",
    "abstract": "With the rapid development of artificial intelligence, large language models (LLMs) have made remarkable advancements in natural language processing. These models are trained on vast datasets to exhibit powerful language understanding and generation capabilities across various applications, including machine translation, chatbots, and agents. However, LLMs have revealed a variety of privacy and security issues throughout their life cycle, drawing significant academic and industrial attention. Moreover, the risks faced by LLMs differ significantly from those encountered by traditional language models. Given that current surveys lack a clear taxonomy of unique threat models across diverse scenarios, we emphasize the unique privacy and security threats associated with five specific scenarios: pre-training, fine-tuning, retrieval-augmented generation systems, deployment, and LLM-based agents. Addressing the characteristics of each risk, this survey outlines potential threats and countermeasures. Research on attack and defense situations can offer feasible research directions, enabling more areas to benefit from LLMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-06-12",
    "authors": [
      {
        "authorId": "2306843705",
        "name": "Shang Wang"
      },
      {
        "authorId": "2185053609",
        "name": "Tianqing Zhu"
      },
      {
        "authorId": "2156640111",
        "name": "Bo Liu"
      },
      {
        "authorId": "2294002570",
        "name": "Ming Ding"
      },
      {
        "authorId": "2306096381",
        "name": "Xu Guo"
      },
      {
        "authorId": "2261513740",
        "name": "Dayong Ye"
      },
      {
        "authorId": "2134555583",
        "name": "Wanlei Zhou"
      },
      {
        "authorId": "2305619559",
        "name": "Philip S. Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "93b4dbb13e7a87f10d33e084ef3725b3df305d3e",
    "url": "https://www.semanticscholar.org/paper/93b4dbb13e7a87f10d33e084ef3725b3df305d3e",
    "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
    "abstract": "Large Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions. To address these challenges, a fine-tuning process is employed, involving feedback and examples to refine models. The objective is to enhance AI models through continuous feedback loops, utilizing metrics such as cosine similarity, LLM evaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like GPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on financial datasets, including the FinanceBench and RAG Instruct Benchmark Tester Dataset, illustrating the necessity of fine-tuning. The results showcase the capability of fine-tuned models to surpass the accuracy of zero-shot LLMs, providing superior question and answering capabilities. Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-27",
    "authors": [
      {
        "authorId": "2279813822",
        "name": "Liang Zhang"
      },
      {
        "authorId": "2279831793",
        "name": "Katherine Jijo"
      },
      {
        "authorId": "2282528163",
        "name": "Spurthi Setty"
      },
      {
        "authorId": "2279830841",
        "name": "Eden Chung"
      },
      {
        "authorId": "2282539958",
        "name": "Fatima Javid"
      },
      {
        "authorId": "2279830757",
        "name": "Natan Vidra"
      },
      {
        "authorId": "2279838243",
        "name": "Thomas Clifford"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "ee6839535cf6b864d8b1110eea304f89d4bb3f4a",
    "url": "https://www.semanticscholar.org/paper/ee6839535cf6b864d8b1110eea304f89d4bb3f4a",
    "title": "In Defense of RAG in the Era of Long-Context Language Models",
    "abstract": "Overcoming the limited context limitations in early-generation LLMs, retrieval-augmented generation (RAG) has been a reliable solution for context-based answer generation in the past. Recently, the emergence of long-context LLMs allows the models to incorporate much longer text sequences, making RAG less attractive. Recent studies show that long-context LLMs significantly outperform RAG in long-context applications. Unlike the existing works favoring the long-context LLM over RAG, we argue that the extremely long context in LLMs suffers from a diminished focus on relevant information and leads to potential degradation in answer quality. This paper revisits the RAG in long-context answer generation. We propose an order-preserve retrieval-augmented generation (OP-RAG) mechanism, which significantly improves the performance of RAG for long-context question-answer applications. With OP-RAG, as the number of retrieved chunks increases, the answer quality initially rises, and then declines, forming an inverted U-shaped curve. There exist sweet points where OP-RAG could achieve higher answer quality with much less tokens than long-context LLM taking the whole context as input. Extensive experiments on public benchmark demonstrate the superiority of our OP-RAG.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-03",
    "authors": [
      {
        "authorId": "2310671061",
        "name": "Tan Yu"
      },
      {
        "authorId": "2305675294",
        "name": "Anbang Xu"
      },
      {
        "authorId": "2305730480",
        "name": "Rama Akkiraju"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "fb1931e9069cf8bfe11a1b8a1055ace7b526db1d",
    "url": "https://www.semanticscholar.org/paper/fb1931e9069cf8bfe11a1b8a1055ace7b526db1d",
    "title": "Don't Forget to Connect! Improving RAG with Graph-based Reranking",
    "abstract": "Retrieval Augmented Generation (RAG) has greatly improved the performance of Large Language Model (LLM) responses by grounding generation with context from existing documents. These systems work well when documents are clearly relevant to a question context. But what about when a document has partial information, or less obvious connections to the context? And how should we reason about connections between documents? In this work, we seek to answer these two core questions about RAG generation. We introduce G-RAG, a reranker based on graph neural networks (GNNs) between the retriever and reader in RAG. Our method combines both connections between documents and semantic information (via Abstract Meaning Representation graphs) to provide a context-informed ranker for RAG. G-RAG outperforms state-of-the-art approaches while having smaller computational footprint. Additionally, we assess the performance of PaLM 2 as a reranker and find it to significantly underperform G-RAG. This result emphasizes the importance of reranking for RAG even when using Large Language Models.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-28",
    "authors": [
      {
        "authorId": "2304174753",
        "name": "Jialin Dong"
      },
      {
        "authorId": "3422551",
        "name": "Bahare Fatemi"
      },
      {
        "authorId": "2271808",
        "name": "Bryan Perozzi"
      },
      {
        "authorId": "2303470421",
        "name": "Lin F. Yang"
      },
      {
        "authorId": "40900939",
        "name": "Anton Tsitsulin"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "62390c0002c6de5e9252e12e2eec2e78ebc1c3d4",
    "url": "https://www.semanticscholar.org/paper/62390c0002c6de5e9252e12e2eec2e78ebc1c3d4",
    "title": "CLAPnq: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems",
    "abstract": "Abstract Retrieval Augmented Generation (RAG) has become a popular application for large language models. It is preferable that successful RAG systems provide accurate answers that are supported by being grounded in a passage without any hallucinations. While considerable work is required for building a full RAG pipeline, being able to benchmark performance is also necessary. We present CLAPnq, a benchmark Long-form Question Answering dataset for the full RAG pipeline. CLAPnq includes long answers with grounded gold passages from Natural Questions (NQ) and a corpus to perform either retrieval, generation, or the full RAG pipeline. The CLAPnq answers are concise, 3x smaller than the full passage, and cohesive, meaning that the answer is composed fluently, often by integrating multiple pieces of the passage that are not contiguous. RAG models must adapt to these properties to be successful at CLAPnq. We present baseline experiments and analysis for CLAPnq that highlight areas where there is still significant room for improvement in grounded RAG. CLAPnq is publicly available at https://github.com/primeqa/clapnq.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-07",
    "authors": [
      {
        "authorId": "144063596",
        "name": "Sara Rosenthal"
      },
      {
        "authorId": "2707234",
        "name": "Avirup Sil"
      },
      {
        "authorId": "2261287685",
        "name": "Radu Florian"
      },
      {
        "authorId": "1781292",
        "name": "S. Roukos"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "b1b93a793e648d0f2ee896f1d7ed511463347487",
    "url": "https://www.semanticscholar.org/paper/b1b93a793e648d0f2ee896f1d7ed511463347487",
    "title": "Differential Privacy of Cross-Attention with Provable Guarantee",
    "abstract": "Cross-attention has become a fundamental module nowadays in many important artificial intelligence applications, e.g., retrieval-augmented generation (RAG), system prompt, guided stable diffusion, and many more. Ensuring cross-attention privacy is crucial and urgently needed because its key and value matrices may contain sensitive information about model providers and their users. In this work, we design a novel differential privacy (DP) data structure to address the privacy security of cross-attention with a theoretical guarantee. In detail, let $n$ be the input token length of system prompt/RAG data, $d$ be the feature dimension, $0<\\alpha \\le 1$ be the relative error parameter, $R$ be the maximum value of the query and key matrices, $R_w$ be the maximum value of the value matrix, and $r,s,\\epsilon_s$ be parameters of polynomial kernel methods. Then, our data structure requires $\\widetilde{O}(ndr^2)$ memory consumption with $\\widetilde{O}(nr^2)$ initialization time complexity and $\\widetilde{O}(\\alpha^{-1} r^2)$ query time complexity for a single token query. In addition, our data structure can guarantee that the process of answering user query satisfies $(\\epsilon, \\delta)$-DP with $\\widetilde{O}(n^{-1} \\epsilon^{-1} \\alpha^{-1/2} R^{2s} R_w r^2)$ additive error and $n^{-1} (\\alpha + \\epsilon_s)$ relative error between our output and the true answer. Furthermore, our result is robust to adaptive queries in which users can intentionally attack the cross-attention system. To our knowledge, this is the first work to provide DP for cross-attention and is promising to inspire more privacy algorithm design in large generative models (LGMs).",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-20",
    "authors": [
      {
        "authorId": "2260056944",
        "name": "Jiuxiang Gu"
      },
      {
        "authorId": "2260827689",
        "name": "Yingyu Liang"
      },
      {
        "authorId": "113515522",
        "name": "Zhenmei Shi"
      },
      {
        "authorId": "2284489474",
        "name": "Zhao Song"
      },
      {
        "authorId": "2303410611",
        "name": "Yufa Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "ddea71bb0d42a0ec0c490cabf7c9e48850b7d610",
    "url": "https://www.semanticscholar.org/paper/ddea71bb0d42a0ec0c490cabf7c9e48850b7d610",
    "title": "Compressing Long Context for Enhancing RAG with AMR-based Concept Distillation",
    "abstract": "Large Language Models (LLMs) have made significant strides in information acquisition. However, their overreliance on potentially flawed parametric knowledge leads to hallucinations and inaccuracies, particularly when handling long-tail, domain-specific queries. Retrieval Augmented Generation (RAG) addresses this limitation by incorporating external, non-parametric knowledge. Nevertheless, the retrieved long-context documents often contain noisy, irrelevant information alongside vital knowledge, negatively diluting LLMs' attention. Inspired by the supportive role of essential concepts in individuals' reading comprehension, we propose a novel concept-based RAG framework with the Abstract Meaning Representation (AMR)-based concept distillation algorithm. The proposed algorithm compresses the cluttered raw retrieved documents into a compact set of crucial concepts distilled from the informative nodes of AMR by referring to reliable linguistic features. The concepts explicitly constrain LLMs to focus solely on vital information in the inference process. We conduct extensive experiments on open-domain question-answering datasets to empirically evaluate the proposed method's effectiveness. The results indicate that the concept-based RAG framework outperforms other baseline methods, particularly as the number of supporting documents increases, while also exhibiting robustness across various backbone LLMs. This emphasizes the distilled concepts are informative for augmenting the RAG process by filtering out interference information. To the best of our knowledge, this is the first work introducing AMR to enhance the RAG, presenting a potential solution to augment inference performance with semantic-based context compression.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-06",
    "authors": [
      {
        "authorId": "2266763119",
        "name": "Kaize Shi"
      },
      {
        "authorId": "2151433946",
        "name": "Xueyao Sun"
      },
      {
        "authorId": "2300093395",
        "name": "Qing Li"
      },
      {
        "authorId": "2261934340",
        "name": "Guandong Xu"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "25bfd05485c1cb1a004b1adb77741629f3b3ee35",
    "url": "https://www.semanticscholar.org/paper/25bfd05485c1cb1a004b1adb77741629f3b3ee35",
    "title": "RTLFixer: Automatically Fixing RTL Syntax Errors with Large Language Models",
    "abstract": "This paper presents RTLFixer, a novel framework enabling automatic syntax errors fixing for Verilog code with Large Language Models (LLMs). Despite LLM's promising capabilities, our analysis indicates that approximately 55% of errors in LLM-generated Verilog are syntax-related, leading to compilation failures. To tackle this issue, we introduce a novel debugging framework that employs Retrieval-Augmented Generation (RAG) and ReAct prompting, enabling LLMs to act as autonomous agents in interactively debugging the code with feedback. This framework demonstrates exceptional proficiency in resolving syntax errors, successfully correcting about 98.5% of compilation errors in our debugging dataset, comprising 212 erroneous implementations derived from the VerilogEval benchmark. Our method leads to 32.3% and 10.1% increase in pass@1 success rates in the VerilogEval-Machine and VerilogEval-Human benchmarks, respectively.",
    "venue": "Design Automation Conference",
    "year": 2023,
    "citationCount": 36,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-11-28",
    "authors": [
      {
        "authorId": "3328096",
        "name": "Yun-Da Tsai"
      },
      {
        "authorId": "2264029479",
        "name": "Mingjie Liu"
      },
      {
        "authorId": "2268825069",
        "name": "Haoxing Ren"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.16376868966336
  },
  {
    "paperId": "8db921900955a447d389582143912eee3046fd3e",
    "url": "https://www.semanticscholar.org/paper/8db921900955a447d389582143912eee3046fd3e",
    "title": "BioinspiredLLM: Conversational Large Language Model for the Mechanics of Biological and Bio‚ÄêInspired Materials",
    "abstract": "The study of biological materials and bio‚Äêinspired materials science is well established; however, surprisingly little knowledge is systematically translated to engineering solutions. To accelerate discovery and guide insights, an open‚Äêsource autoregressive transformer large language model (LLM), BioinspiredLLM, is reported. The model is finetuned with a corpus of over a thousand peer‚Äêreviewed articles in the field of structural biological and bio‚Äêinspired materials and can be prompted to recall information, assist with research tasks, and function as an engine for creativity. The model has proven that it is able to accurately recall information about biological materials and is further strengthened with enhanced reasoning ability, as well as with Retrieval‚ÄêAugmented Generation (RAG) to incorporate new data during generation that can also help to traceback sources, update the knowledge base, and connect knowledge domains. BioinspiredLLM also has shown to develop sound hypotheses regarding biological materials design and remarkably so for materials that have never been explicitly studied before. Lastly, the model shows impressive promise in collaborating with other generative artificial intelligence models in a workflow that can reshape the traditional materials design process. This collaborative generative artificial intelligence method can stimulate and enhance bio‚Äêinspired materials design workflows. Biological materials are at a critical intersection of multiple scientific fields and models like BioinspiredLLM help to connect knowledge domains.",
    "venue": "Advancement of science",
    "year": 2023,
    "citationCount": 37,
    "openAccessPdf": {
      "url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/advs.202306724",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Medicine",
      "Computer Science",
      "Physics"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-09-15",
    "authors": [
      {
        "authorId": "2182256086",
        "name": "Rachel K. Luu"
      },
      {
        "authorId": "2273480",
        "name": "M. Buehler"
      }
    ],
    "source": "semantic_scholar",
    "score": 134.5637923958958
  },
  {
    "paperId": "069bceb8a9f88991c68cf04fecdecf79937db766",
    "url": "https://www.semanticscholar.org/paper/069bceb8a9f88991c68cf04fecdecf79937db766",
    "title": "Locally-Adaptive Quantization for Streaming Vector Search",
    "abstract": "Retrieving the most similar vector embeddings to a given query among a massive collection of vectors has long been a key component of countless real-world applications. The recently introduced Retrieval-Augmented Generation is one of the most prominent examples. For many of these applications, the database evolves over time by inserting new data and removing outdated data. In these cases, the retrieval problem is known as streaming similarity search. While Locally-Adaptive Vector Quantization (LVQ), a highly efficient vector compression method, yields state-of-the-art search performance for non-evolving databases, its usefulness in the streaming setting has not been yet established. In this work, we study LVQ in streaming similarity search. In support of our evaluation, we introduce two improvements of LVQ: Turbo LVQ and multi-means LVQ that boost its search performance by up to 28% and 27%, respectively. Our studies show that LVQ and its new variants enable blazing fast vector search, outperforming its closest competitor by up to 9.4x for identically distributed data and by up to 8.8x under the challenging scenario of data distribution shifts (i.e., where the statistical distribution of the data changes over time). We release our contributions as part of Scalable Vector Search, an open-source library for high-performance similarity search.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-03",
    "authors": [
      {
        "authorId": "2276606286",
        "name": "Cecilia Aguerrebere"
      },
      {
        "authorId": "2059111676",
        "name": "Mark Hildebrand"
      },
      {
        "authorId": "3286327",
        "name": "I. Bhati"
      },
      {
        "authorId": "2276606593",
        "name": "Ted Willke"
      },
      {
        "authorId": "2276606934",
        "name": "Mariano Tepper"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "814f0b1658c49c79bc32f3d2b89045de007871c6",
    "url": "https://www.semanticscholar.org/paper/814f0b1658c49c79bc32f3d2b89045de007871c6",
    "title": "DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple Experts Fine-tuning",
    "abstract": "We propose Multiple Experts Fine-tuning Framework to build a financial large language model (LLM), DISC-FinLLM. Our methodology improves general LLMs by endowing them with multi-turn question answering abilities, domain text processing capabilities, mathematical computation skills, and retrieval-enhanced generation capabilities. We build a financial instruction-tuning dataset named DISC-FIN-SFT, including instruction samples of four categories (consulting, NLP tasks, computing and retrieval-augmented generation). Evaluations conducted on multiple benchmarks demonstrate that our model performs better than baseline models in various financial scenarios. Further resources can be found at https://github.com/FudanDISC/DISC-FinLLM.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 34,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-23",
    "authors": [
      {
        "authorId": "2256716476",
        "name": "Wei Chen"
      },
      {
        "authorId": "2261394047",
        "name": "Qiushi Wang"
      },
      {
        "authorId": "2261388076",
        "name": "Zefei Long"
      },
      {
        "authorId": "2261393689",
        "name": "Xianyin Zhang"
      },
      {
        "authorId": "2261475779",
        "name": "Zhongtian Lu"
      },
      {
        "authorId": "2244122886",
        "name": "Bingxuan Li"
      },
      {
        "authorId": "2116420560",
        "name": "Siyuan Wang"
      },
      {
        "authorId": "2260830982",
        "name": "Jiarong Xu"
      },
      {
        "authorId": "2261455485",
        "name": "Xiang Bai"
      },
      {
        "authorId": "2253852592",
        "name": "Xuanjing Huang"
      },
      {
        "authorId": "2118602528",
        "name": "Zhongyu Wei"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.3302209223412
  },
  {
    "paperId": "1c5bc4f10b95a90d0283d0aacc94332aae508169",
    "url": "https://www.semanticscholar.org/paper/1c5bc4f10b95a90d0283d0aacc94332aae508169",
    "title": "Huatuo-26M, a Large-scale Chinese Medical QA Dataset",
    "abstract": "In this paper, we release a largest ever medical Question Answering (QA) dataset with 26 million QA pairs. We benchmark many existing approaches in our dataset in terms of both retrieval and generation. Experimental results show that the existing models perform far lower than expected and the released dataset is still challenging in the pre-trained language model era. Moreover, we also experimentally show the benefit of the proposed dataset in many aspects: (i) trained models for other QA datasets in a zero-shot fashion; and (ii) as external knowledge for retrieval-augmented generation (RAG); and (iii) improving existing pre-trained language models by using the QA pairs as a pre-training corpus in continued training manner. We believe that this dataset will not only contribute to medical research but also facilitate both the patients and clinical doctors. See \\url{https://github.com/FreedomIntelligence/Huatuo-26M}.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 33,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.01526",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-02",
    "authors": [
      {
        "authorId": "2130169642",
        "name": "Jianquan Li"
      },
      {
        "authorId": "2107999481",
        "name": "Xidong Wang"
      },
      {
        "authorId": "2108406947",
        "name": "Xiangbo Wu"
      },
      {
        "authorId": "2214794954",
        "name": "Zhiyi Zhang"
      },
      {
        "authorId": "2115192068",
        "name": "Xiaolong Xu"
      },
      {
        "authorId": "2215497308",
        "name": "Jie Fu"
      },
      {
        "authorId": "2086549",
        "name": "P. Tiwari"
      },
      {
        "authorId": "2101317304",
        "name": "Xiang Wan"
      },
      {
        "authorId": "2894465",
        "name": "Benyou Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 122.89540786924243
  },
  {
    "paperId": "ac3390c58ef059277466c51a4b642ada8c4b3205",
    "url": "https://www.semanticscholar.org/paper/ac3390c58ef059277466c51a4b642ada8c4b3205",
    "title": "Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking",
    "abstract": "Standard Full-Data classifiers in NLP demand thousands of labeled examples, which is impractical in data-limited domains. Few-shot methods offer an alternative, utilizing contrastive learning techniques that can be effective with as little as 20 examples per class. Similarly, Large Language Models (LLMs) like GPT-4 can perform effectively with just 1-5 examples per class. However, the performance-cost trade-offs of these methods remain underexplored, a critical concern for budget-limited organizations. Our work addresses this gap by studying the aforementioned approaches over the Banking77 financial intent detection dataset, including the evaluation of cutting-edge LLMs by OpenAI, Cohere, and Anthropic in a comprehensive set of few-shot scenarios. We complete the picture with two additional methods: first, a cost-effective querying method for LLMs based on retrieval-augmented generation (RAG), able to reduce operational costs multiple times compared to classic few-shot approaches, and second, a data augmentation method using GPT-4, able to improve performance in data-limited scenarios. Finally, to inspire future research, we provide a human expert‚Äôs curated subset of Banking77, along with extensive error analysis.",
    "venue": "International Conference on AI in Finance",
    "year": 2023,
    "citationCount": 35,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2311.06102",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2023-11-10",
    "authors": [
      {
        "authorId": "2130142972",
        "name": "L. Loukas"
      },
      {
        "authorId": "2234394970",
        "name": "Ilias Stogiannidis"
      },
      {
        "authorId": "2266239572",
        "name": "Odysseas Diamantopoulos"
      },
      {
        "authorId": "1950133",
        "name": "Prodromos Malakasiotis"
      },
      {
        "authorId": "2253607907",
        "name": "Stavros Vassos"
      }
    ],
    "source": "semantic_scholar",
    "score": 123.75278407684165
  },
  {
    "paperId": "88406852aa9664ce104a5c6b6de673feba60f514",
    "url": "https://www.semanticscholar.org/paper/88406852aa9664ce104a5c6b6de673feba60f514",
    "title": "SciSpace Copilot: Empowering Researchers through Intelligent Reading Assistance",
    "abstract": "We introduce SciSpace Copilot, an AI research assistant that helps in understanding and reading research papers faster by providing a plethora of features. Answering questions from a document has recently become popular using the Retrieval Augmented Generation (RAG) approach. Our tool uses an advanced question-answering pipeline to get accurate answers and also provide exact citations for the same. We provide many more valuable features on scientific text, including generating explanations, generating summaries, adding notes and highlights, and finding related papers from our 200 million corpus. Our tool supports 100+ languages, making research more accessible across language barriers. Thousands of users use SciSpace Copilot on a daily basis by uploading their articles to understand research faster and better. Our tool can be accessed at this link: https://typeset.io.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://ojs.aaai.org/index.php/AAAI/article/download/30578/32740",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-24",
    "authors": [
      {
        "authorId": "2159735730",
        "name": "Trinita Roy"
      },
      {
        "authorId": "2293611588",
        "name": "Asheesh Kumar"
      },
      {
        "authorId": "2293467619",
        "name": "Daksh Raghuvanshi"
      },
      {
        "authorId": "2294162354",
        "name": "Siddhant Jain"
      },
      {
        "authorId": "2293463472",
        "name": "Goutham Vignesh"
      },
      {
        "authorId": "2293460144",
        "name": "Kartik Shinde"
      },
      {
        "authorId": "30556571",
        "name": "Rohan Tondulkar"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "697775b02833f4e48c47161948f2b5a53fae60ef",
    "url": "https://www.semanticscholar.org/paper/697775b02833f4e48c47161948f2b5a53fae60ef",
    "title": "STALL+: Boosting LLM-based Repository-level Code Completion with Static Analysis",
    "abstract": "Repository-level code completion is challenging as it involves complicated contexts from multiple files in the repository. To date, researchers have proposed two technical categories to enhance LLM-based repository-level code completion, i.e., retrieval-augmented generation (RAG) and static analysis integration. This work performs the first study on the static analysis integration in LLM-based repository-level code completion by investigating both the effectiveness and efficiency of static analysis integration strategies across different phases of code completion. We first implement a framework STALL+, which supports an extendable and customizable integration of multiple static analysis strategies into the complete pipeline of LLM-based repository-level code completion; and based on STALL+, we perform extensive experiments by including different code LLMs on the latest repository-level code completion benchmark CrossCodeEval. Our findings show that integrating file-level dependencies in prompting phase performs the best while the integration in post-processing phase performs the worse. Additionally, we observe different improvements from static analysis between dynamic languages and static languages, i.e., the best combination is prompting-phase with decoding-phase integration for Java while the best combination is prompting-phase with post-processing-phase integration for Python given the limitations of statically analyzing dynamic languages. Additionally, we find the complementarity between RAG and static analysis integration as well as their cost-effectiveness after combination.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-14",
    "authors": [
      {
        "authorId": "2226464443",
        "name": "Junwei Liu"
      },
      {
        "authorId": "2116664540",
        "name": "Yixuan Chen"
      },
      {
        "authorId": "2007711208",
        "name": "Mingwei Liu"
      },
      {
        "authorId": "2216694669",
        "name": "Xin Peng"
      },
      {
        "authorId": "2265400203",
        "name": "Yiling Lou"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "269d38423e4d83f1403b6d977d5a9fc60d6f72ca",
    "url": "https://www.semanticscholar.org/paper/269d38423e4d83f1403b6d977d5a9fc60d6f72ca",
    "title": "ECG Semantic Integrator (ESI): A Foundation ECG Model Pretrained with LLM-Enhanced Cardiological Text",
    "abstract": "The utilization of deep learning on electrocardiogram (ECG) analysis has brought the advanced accuracy and efficiency of cardiac healthcare diagnostics. By leveraging the capabilities of deep learning in semantic understanding, especially in feature extraction and representation learning, this study introduces a new multimodal contrastive pretaining framework that aims to improve the quality and robustness of learned representations of 12-lead ECG signals. Our framework comprises two key components, including Cardio Query Assistant (CQA) and ECG Semantics Integrator(ESI). CQA integrates a retrieval-augmented generation (RAG) pipeline to leverage large language models (LLMs) and external medical knowledge to generate detailed textual descriptions of ECGs. The generated text is enriched with information about demographics and waveform patterns. ESI integrates both contrastive and captioning loss to pretrain ECG encoders for enhanced representations. We validate our approach through various downstream tasks, including arrhythmia detection and ECG-based subject identification. Our experimental results demonstrate substantial improvements over strong baselines in these tasks. These baselines encompass supervised and self-supervised learning methods, as well as prior multimodal pretraining approaches.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-26",
    "authors": [
      {
        "authorId": "2110983770",
        "name": "Han Yu"
      },
      {
        "authorId": "2242796946",
        "name": "Peikun Guo"
      },
      {
        "authorId": "2296599151",
        "name": "Akane Sano"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "6d533b0f318fd22d664356b56b68023560d3c60f",
    "url": "https://www.semanticscholar.org/paper/6d533b0f318fd22d664356b56b68023560d3c60f",
    "title": "Generative AI Agents With Large Language Model for Satellite Networks via a Mixture of Experts Transmission",
    "abstract": "In response to the needs of 6G global communications, satellite communication networks have emerged as a key solution. However, the large-scale development of satellite communication networks is constrained by complex system models, whose modeling is challenging for massive users. Moreover, transmission interference between satellites and users seriously affects communication performance. To solve these problems, this paper develops generative artificial intelligence (AI) agents for model formulation and then applies a mixture of experts (MoE) approach to design transmission strategies. Specifically, we leverage large language models (LLMs) to build an interactive modeling paradigm and utilize retrieval-augmented generation (RAG) to extract satellite expert knowledge that supports mathematical modeling. Afterward, by integrating the expertise of multiple specialized components, we propose an MoE-proximal policy optimization (PPO) approach to solve the formulated problem. Each expert can optimize the optimization variables at which it excels through specialized training through its own network and then aggregate them through the gating network to perform joint optimization. The simulation results validate the accuracy and effectiveness of employing a generative agent for problem formulation. Furthermore, the superiority of the proposed MoE-ppo approach over other benchmarks is confirmed in solving the formulated problem. The adaptability of MoE-PPO to various customized modeling problems has also been demonstrated.",
    "venue": "IEEE Journal on Selected Areas in Communications",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2404.09134",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-14",
    "authors": [
      {
        "authorId": "2266463634",
        "name": "Ruichen Zhang"
      },
      {
        "authorId": "2175043468",
        "name": "Hongyang Du"
      },
      {
        "authorId": "2237948862",
        "name": "Yinqiu Liu"
      },
      {
        "authorId": "2266084696",
        "name": "Dusist Niyato"
      },
      {
        "authorId": "2261731446",
        "name": "Jiawen Kang"
      },
      {
        "authorId": "2943819",
        "name": "Zehui Xiong"
      },
      {
        "authorId": "2237828911",
        "name": "Abbas Jamalipour"
      },
      {
        "authorId": "1470983736",
        "name": "Dong In Kim"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.1886522358297
  },
  {
    "paperId": "878032e7d2728c88f304db1f04d7e8a203c966f8",
    "url": "https://www.semanticscholar.org/paper/878032e7d2728c88f304db1f04d7e8a203c966f8",
    "title": "CorpusLM: Towards a Unified Language Model on Corpus for Knowledge-Intensive Tasks",
    "abstract": "Large language models (LLMs) have gained significant attention in various fields but prone to hallucination, especially in knowledge-intensive (KI) tasks. To address this, retrieval-augmented generation (RAG) has emerged as a popular solution to enhance factual accuracy. However, traditional retrieval modules often rely on large document index and disconnect with generative tasks. With the advent of generative retrieval (GR), language models can retrieve by directly generating document identifiers (DocIDs), offering superior performance in retrieval tasks. However, the potential relationship between GR and downstream tasks remains unexplored. In this paper, we propose \\textbf{CorpusLM}, a unified language model that leverages external corpus to tackle various knowledge-intensive tasks by integrating generative retrieval, closed-book generation, and RAG through a unified greedy decoding process. We design the following mechanisms to facilitate effective retrieval and generation, and improve the end-to-end effectiveness of KI tasks: (1) We develop a ranking-oriented DocID list generation strategy, which refines GR by directly learning from a DocID ranking list, to improve retrieval quality. (2) We design a continuous DocIDs-References-Answer generation strategy, which facilitates effective and efficient RAG. (3) We employ well-designed unsupervised DocID understanding tasks, to comprehend DocID semantics and their relevance to downstream tasks. We evaluate our approach on the widely used KILT benchmark with two variants of backbone models, i.e., T5 and Llama2. Experimental results demonstrate the superior performance of our models in both retrieval and downstream tasks.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-02",
    "authors": [
      {
        "authorId": "2144456832",
        "name": "Xiaoxi Li"
      },
      {
        "authorId": "2257039188",
        "name": "Zhicheng Dou"
      },
      {
        "authorId": "2118788278",
        "name": "Yujia Zhou"
      },
      {
        "authorId": "2282524575",
        "name": "Fangchao Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.1886522358297
  },
  {
    "paperId": "282d9048c524eb3d87f73a3fe5ef49bc7297e8b4",
    "url": "https://www.semanticscholar.org/paper/282d9048c524eb3d87f73a3fe5ef49bc7297e8b4",
    "title": "Robust Multi Model RAG Pipeline For Documents Containing Text, Table & Images",
    "abstract": "RAG (Retrieval Augmented Generation) is generally used for generating results from the existing knowledge-base. RAG refers to finding references (R), Adding references (A) and improving generation(i.e, answers to the question) (G). MultiModel-RAGs are used for generation of results over the documents which contain images and texts. There exists multiple different Multimodel-RAGs but these are not still efficient in generation of the results from the documents which contain relationships between images and texts. This study has proposed the solution to enable effective retrieval and generation of results, which includes the relationship between images and texts. The comparison of proposed Multimodal RAG with four different datasets (i.e., Short-form-type-QA, Long-form-type-QA, MCQ-type-QA, True-False-type-QA) shows the proposed solution improves the effectiveness of the existing Multimodal RAGs. Testing of proposed Multimodal RAG over two different other multimodal LLM i.e, Open-AI & Gemini helps in deciding whether the proposed solution fits best with LLM in different cases.",
    "venue": "2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2024-06-05",
    "authors": [
      {
        "authorId": "2309509809",
        "name": "Pankaj Joshi"
      },
      {
        "authorId": "2309555295",
        "name": "Aditya Gupta"
      },
      {
        "authorId": "2309823955",
        "name": "Pankaj Kumar"
      },
      {
        "authorId": "2309529808",
        "name": "Manas Sisodia"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "555401ddacee23b8d26ea9754bd71cfb331bddf7",
    "url": "https://www.semanticscholar.org/paper/555401ddacee23b8d26ea9754bd71cfb331bddf7",
    "title": "Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together",
    "abstract": "Natural Language Processing (NLP) systems are increasingly taking the form of sophisticated modular pipelines, e.g., Retrieval Augmented Generation (RAG), where each module may involve a distinct Language Model (LM) and an associated prompt template. These compound systems often lack intermediate labels or gradient flow to optimize each module, making their end-to-end optimization challenging. Here we seek strategies to optimize both the module-level LM weights and the associated prompt templates of such systems to maximize a downstream task metric. We propose for the first time combining the weight and prompt optimization strategies to optimize a modular LM pipeline by alternating between the two to get the same LM to teach itself. In experiments with multi-hop QA, mathematical reasoning, and feature-based classification using mistral-7b, llama-2-7b, and llama-3-8b, these BetterTogether strategies optimizing the weights and prompts of a pipeline together outperform directly optimizing weights alone and prompts alone by up to 60% and 6%, respectively, on average across LMs and tasks. Our BetterTogether optimizer is released in DSPy at [http://dspy.ai](http://dspy.ai).",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-15",
    "authors": [
      {
        "authorId": "1914569491",
        "name": "Dilara Soylu"
      },
      {
        "authorId": "2254255092",
        "name": "Christopher Potts"
      },
      {
        "authorId": "144112155",
        "name": "O. Khattab"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "98cb07d3e50f0718ff17eca4e898f451e0f3381d",
    "url": "https://www.semanticscholar.org/paper/98cb07d3e50f0718ff17eca4e898f451e0f3381d",
    "title": "MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models",
    "abstract": "Artificial Intelligence (AI) has demonstrated significant potential in healthcare, particularly in disease diagnosis and treatment planning. Recent progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new possibilities for interactive diagnostic tools. However, these models often suffer from factual hallucination, which can lead to incorrect diagnoses. Fine-tuning and retrieval-augmented generation (RAG) have emerged as methods to address these issues. However, the amount of high-quality data and distribution shifts between training data and deployment data limit the application of fine-tuning methods. Although RAG is lightweight and effective, existing RAG-based approaches are not sufficiently general to different medical domains and can potentially cause misalignment issues, both between modalities and between the model and the ground truth. In this paper, we propose a versatile multimodal RAG system, MMed-RAG, designed to enhance the factuality of Med-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an adaptive retrieved contexts selection method, and a provable RAG-based preference fine-tuning strategy. These innovations make the RAG process sufficiently general and reliable, significantly improving alignment when introducing retrieved contexts. Experimental results across five medical datasets (involving radiology, ophthalmology, pathology) on medical VQA and report generation demonstrate that MMed-RAG can achieve an average improvement of 43.8% in the factual accuracy of Med-LVLMs. Our data and code are available in https://github.com/richard-peng-xia/MMed-RAG.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-16",
    "authors": [
      {
        "authorId": "2261083308",
        "name": "Peng Xia"
      },
      {
        "authorId": "2326298368",
        "name": "Peng Xia"
      },
      {
        "authorId": "2307274105",
        "name": "Kangyu Zhu"
      },
      {
        "authorId": "2322263754",
        "name": "Haoran Li"
      },
      {
        "authorId": "2322263754",
        "name": "Haoran Li"
      },
      {
        "authorId": "2328347288",
        "name": "Weijia Shi"
      },
      {
        "authorId": "2326295765",
        "name": "Sheng Wang"
      },
      {
        "authorId": "2249882382",
        "name": "Linjun Zhang"
      },
      {
        "authorId": "2278917478",
        "name": "James Zou"
      },
      {
        "authorId": "2249857007",
        "name": "Huaxiu Yao"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "59b5223861e7f5e62a1fad746b74a26a21992f04",
    "url": "https://www.semanticscholar.org/paper/59b5223861e7f5e62a1fad746b74a26a21992f04",
    "title": "Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge",
    "abstract": "In recent years, multimodal large language models (MLLMs) have made significant strides by training on vast high-quality image-text datasets, enabling them to generally understand images well. However, the inherent difficulty in explicitly conveying fine-grained or spatially dense information in text, such as masks, poses a challenge for MLLMs, limiting their ability to answer questions requiring an understanding of detailed or localized visual elements. Drawing inspiration from the Retrieval-Augmented Generation (RAG) concept, this paper proposes a new visual prompt approach to integrate fine-grained external knowledge, gleaned from specialized vision models (e.g., instance segmentation/OCR models), into MLLMs. This is a promising yet underexplored direction for enhancing MLLMs' performance. Our approach diverges from concurrent works, which transform external knowledge into additional text prompts, necessitating the model to indirectly learn the correspondence between visual content and text coordinates. Instead, we propose embedding fine-grained knowledge information directly into a spatial embedding map as a visual prompt. This design can be effortlessly incorporated into various MLLMs, such as LLaVA and Mipha, considerably improving their visual understanding performance. Through rigorous experiments, we demonstrate that our method can enhance MLLM performance across nine benchmarks, amplifying their fine-grained context-aware capabilities.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-05",
    "authors": [
      {
        "authorId": "2293551673",
        "name": "Yuanze Lin"
      },
      {
        "authorId": "2268606250",
        "name": "Yunsheng Li"
      },
      {
        "authorId": "2310288341",
        "name": "Dongdong Chen"
      },
      {
        "authorId": "2266438865",
        "name": "Weijian Xu"
      },
      {
        "authorId": "2293447682",
        "name": "Ronald Clark"
      },
      {
        "authorId": "2291064871",
        "name": "Philip Torr"
      },
      {
        "authorId": "2268671310",
        "name": "Lu Yuan"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "5aad568d4f02b09af3c282b1f4c20ee0993bc2e6",
    "url": "https://www.semanticscholar.org/paper/5aad568d4f02b09af3c282b1f4c20ee0993bc2e6",
    "title": "Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding",
    "abstract": "The digital landscape is rapidly evolving with an ever-increasing volume of online news, emphasizing the need for swift and precise analysis of complex events. We refer to the complex events composed of many news articles over an extended period as Temporal Complex Event (TCE). This paper proposes a novel approach using Large Language Models (LLMs) to systematically extract and analyze the event chain within TCE, characterized by their key points and timestamps. We establish a benchmark, named TCELongBench, to evaluate the proficiency of LLMs in handling temporal dynamics and understanding extensive text. This benchmark encompasses three distinct tasks - reading comprehension, temporal sequencing, and future event forecasting. In the experiment, we leverage retrieval-augmented generation (RAG) method and LLMs with long context window to deal with lengthy news articles of TCE. Our findings indicate that models with suitable retrievers exhibit comparable performance with those utilizing long context window.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-04",
    "authors": [
      {
        "authorId": "2304613417",
        "name": "Zhihan Zhang"
      },
      {
        "authorId": "2258806194",
        "name": "Yixin Cao"
      },
      {
        "authorId": "2269465569",
        "name": "Chenchen Ye"
      },
      {
        "authorId": "51487414",
        "name": "Yunshan Ma"
      },
      {
        "authorId": "32781973",
        "name": "Lizi Liao"
      },
      {
        "authorId": "2270722858",
        "name": "Tat-Seng Chua"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "aa59536123b29599115cb28027d9ddb67fc1c613",
    "url": "https://www.semanticscholar.org/paper/aa59536123b29599115cb28027d9ddb67fc1c613",
    "title": "Telecom Language Models: Must They Be Large?",
    "abstract": "The increasing interest in Large Language Models (LLMs) within the telecommunications sector underscores their potential to revolutionize operational efficiency. However, the deployment of these sophisticated models is often hampered by their substantial size and computational demands, raising concerns about their viability in resource-constrained environments. Addressing this challenge, recent advancements have seen the emergence of small language models that surprisingly exhibit performance comparable to their larger counterparts in many tasks, such as coding and common-sense reasoning. Phi-2, a compact yet powerful model, exemplifies this new wave of efficient small language models. This paper conducts a comprehensive evaluation of Phi-2‚Äôs intrinsic understanding of the telecommunications domain. Recognizing the scale-related limitations, we enhance Phi-2‚Äôs capabilities through a Retrieval-Augmented Generation approach, meticulously integrating an extensive knowledge base specifically curated with telecom standard specifications. The enhanced Phi-2 model demonstrates a profound improvement in accuracy, answering questions about telecom standards with a precision that closely rivals the more resource-intensive GPT-3.5. The paper further explores the refined capabilities of Phi-2 in addressing problem-solving scenarios within the telecom sector, highlighting its potentials and limitations.",
    "venue": "IEEE International Symposium on Personal, Indoor and Mobile Radio Communications",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-07",
    "authors": [
      {
        "authorId": "3393923",
        "name": "Nicola Piovesan"
      },
      {
        "authorId": "2261362548",
        "name": "Antonio De Domenico"
      },
      {
        "authorId": "70486867",
        "name": "Fadhel Ayed"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.1886522358297
  },
  {
    "paperId": "21620a67bbef3a4c607bf17be07d42514163dfaf",
    "url": "https://www.semanticscholar.org/paper/21620a67bbef3a4c607bf17be07d42514163dfaf",
    "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
    "abstract": "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The increasing demands of application scenarios have driven the evolution of RAG, leading to the integration of advanced retrievers, LLMs and other complementary technologies, which in turn has amplified the intricacy of RAG systems. However, the rapid advancements are outpacing the foundational RAG paradigm, with many methods struggling to be unified under the process of\"retrieve-then-generate\". In this context, this paper examines the limitations of the existing RAG paradigm and introduces the modular RAG framework. By decomposing complex RAG systems into independent modules and specialized operators, it facilitates a highly reconfigurable framework. Modular RAG transcends the traditional linear architecture, embracing a more advanced design that integrates routing, scheduling, and fusion mechanisms. Drawing on extensive research, this paper further identifies prevalent RAG patterns-linear, conditional, branching, and looping-and offers a comprehensive analysis of their respective implementation nuances. Modular RAG presents innovative opportunities for the conceptualization and deployment of RAG systems. Finally, the paper explores the potential emergence of new operators and paradigms, establishing a solid theoretical foundation and a practical roadmap for the continued evolution and practical deployment of RAG technologies.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-26",
    "authors": [
      {
        "authorId": "2280046531",
        "name": "Yunfan Gao"
      },
      {
        "authorId": "2275320371",
        "name": "Yun Xiong"
      },
      {
        "authorId": "2291409458",
        "name": "Meng Wang"
      },
      {
        "authorId": "2256769434",
        "name": "Haofen Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "58549ff432aa1c176eec8a2c81e75af676cb4b10",
    "url": "https://www.semanticscholar.org/paper/58549ff432aa1c176eec8a2c81e75af676cb4b10",
    "title": "Evaluation of RAG Metrics for Question Answering in the Telecom Domain",
    "abstract": "Retrieval Augmented Generation (RAG) is widely used to enable Large Language Models (LLMs) perform Question Answering (QA) tasks in various domains. However, RAG based on open-source LLM for specialized domains has challenges of evaluating generated responses. A popular framework in the literature is the RAG Assessment (RAGAS), a publicly available library which uses LLMs for evaluation. One disadvantage of RAGAS is the lack of details of derivation of numerical value of the evaluation metrics. One of the outcomes of this work is a modified version of this package for few metrics (faithfulness, context relevance, answer relevance, answer correctness, answer similarity and factual correctness) through which we provide the intermediate outputs of the prompts by using any LLMs. Next, we analyse the expert evaluations of the output of the modified RAGAS package and observe the challenges of using it in the telecom domain. We also study the effect of the metrics under correct vs. wrong retrieval and observe that few of the metrics have higher values for correct retrieval. We also study for differences in metrics between base embeddings and those domain adapted via pre-training and fine-tuning. Finally, we comment on the suitability and challenges of using these metrics for in-the-wild telecom QA task.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-15",
    "authors": [
      {
        "authorId": "2284351918",
        "name": "Sujoy Roychowdhury"
      },
      {
        "authorId": "3336244",
        "name": "Sumit Soman"
      },
      {
        "authorId": "145229246",
        "name": "H. G. Ranjani"
      },
      {
        "authorId": "2307071525",
        "name": "Neeraj Gunda"
      },
      {
        "authorId": "2307072193",
        "name": "Vansh Chhabra"
      },
      {
        "authorId": "2307071756",
        "name": "Sai Krishna Bala"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "4e32860d8ba978be8fbcb3c0f9a13a9b899541cf",
    "url": "https://www.semanticscholar.org/paper/4e32860d8ba978be8fbcb3c0f9a13a9b899541cf",
    "title": "Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability",
    "abstract": "This paper presents an analysis of open-source large language models (LLMs) and their application in Retrieval-Augmented Generation (RAG) tasks, specific for enterprise-specific data sets scraped from their websites. With the increasing reliance on LLMs in natural language processing, it is crucial to evaluate their performance, accessibility, and integration within specific organizational contexts. This study examines various open-source LLMs, explores their integration into RAG frameworks using enterprise-specific data, and assesses the performance of different open-source embeddings in enhancing the retrieval and generation process. Our findings indicate that open-source LLMs, combined with effective embedding techniques, can significantly improve the accuracy and efficiency of RAG systems, offering a viable alternative to proprietary solutions for enterprises.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-17",
    "authors": [
      {
        "authorId": "2307008038",
        "name": "B. Gautam"
      },
      {
        "authorId": "33856997",
        "name": "A. Purwar"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "8cc4cd653c157e4f96dfd963def73dbefa19dc9e",
    "url": "https://www.semanticscholar.org/paper/8cc4cd653c157e4f96dfd963def73dbefa19dc9e",
    "title": "Improving Medical Multi-modal Contrastive Learning with Expert Annotations",
    "abstract": "We introduce eCLIP, an enhanced version of the CLIP model that integrates expert annotations in the form of radiologist eye-gaze heatmaps. It tackles key challenges in contrastive multi-modal medical imaging analysis, notably data scarcity and the\"modality gap\"-- a significant disparity between image and text embeddings that diminishes the quality of representations and hampers cross-modal interoperability. eCLIP integrates a heatmap processor and leverages mixup augmentation to efficiently utilize the scarce expert annotations, thus boosting the model's learning effectiveness. eCLIP is designed to be generally applicable to any variant of CLIP without requiring any modifications of the core architecture. Through detailed evaluations across several tasks, including zero-shot inference, linear probing, cross-modal retrieval, and Retrieval Augmented Generation (RAG) of radiology reports using a frozen Large Language Model, eCLIP showcases consistent improvements in embedding quality. The outcomes reveal enhanced alignment and uniformity, affirming eCLIP's capability to harness high-quality annotations for enriched multi-modal analysis in the medical imaging domain.",
    "venue": "European Conference on Computer Vision",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-15",
    "authors": [
      {
        "authorId": "2151858739",
        "name": "Yogesh Kumar"
      },
      {
        "authorId": "2265492561",
        "name": "Pekka Marttinen"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "1b0fa09f097591d697162300cc6ecb3ee425fd8d",
    "url": "https://www.semanticscholar.org/paper/1b0fa09f097591d697162300cc6ecb3ee425fd8d",
    "title": "Chain of Agents: Large Language Models Collaborating on Long-Context Tasks",
    "abstract": "Addressing the challenge of effectively processing long contexts has become a critical issue for Large Language Models (LLMs). Two common strategies have emerged: 1) reducing the input length, such as retrieving relevant chunks by Retrieval-Augmented Generation (RAG), and 2) expanding the context window limit of LLMs. However, both strategies have drawbacks: input reduction has no guarantee of covering the part with needed information, while window extension struggles with focusing on the pertinent information for solving the task. To mitigate these limitations, we propose Chain-of-Agents (CoA), a novel framework that harnesses multi-agent collaboration through natural language to enable information aggregation and context reasoning across various LLMs over long-context tasks. CoA consists of multiple worker agents who sequentially communicate to handle different segmented portions of the text, followed by a manager agent who synthesizes these contributions into a coherent final output. CoA processes the entire input by interleaving reading and reasoning, and it mitigates long context focus issues by assigning each agent a short context. We perform comprehensive evaluation of CoA on a wide range of long-context tasks in question answering, summarization, and code completion, demonstrating significant improvements by up to 10% over strong baselines of RAG, Full-Context, and multi-agent LLMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-04",
    "authors": [
      {
        "authorId": "2145039557",
        "name": "Yusen Zhang"
      },
      {
        "authorId": "2068169921",
        "name": "Ruoxi Sun"
      },
      {
        "authorId": "2258603407",
        "name": "Yanfei Chen"
      },
      {
        "authorId": "2264567300",
        "name": "Tomas Pfister"
      },
      {
        "authorId": "2305152311",
        "name": "Rui Zhang"
      },
      {
        "authorId": "2676352",
        "name": "Sercan √ñ. Arik"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "7ff441587afcd88860971210450cc1df7c772ff6",
    "url": "https://www.semanticscholar.org/paper/7ff441587afcd88860971210450cc1df7c772ff6",
    "title": "Grounding Language Models for Visual Entity Recognition",
    "abstract": "We introduce AutoVER, an Autoregressive model for Visual Entity Recognition. Our model extends an autoregressive Multi-modal Large Language Model by employing retrieval augmented constrained generation. It mitigates low performance on out-of-domain entities while excelling in queries that require visually-situated reasoning. Our method learns to distinguish similar entities within a vast label space by contrastively training on hard negative pairs in parallel with a sequence-to-sequence objective without an external retriever. During inference, a list of retrieved candidate answers explicitly guides language generation by removing invalid decoding paths. The proposed method achieves significant improvements across different dataset splits in the recently proposed Oven-Wiki benchmark. Accuracy on the Entity seen split rises from 32.7% to 61.5%. It also demonstrates superior performance on the unseen and query splits by a substantial double-digit margin.",
    "venue": "European Conference on Computer Vision",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-28",
    "authors": [
      {
        "authorId": "2186608582",
        "name": "Zilin Xiao"
      },
      {
        "authorId": "2288257904",
        "name": "Ming Gong"
      },
      {
        "authorId": "1399431057",
        "name": "Paola Cascante-Bonilla"
      },
      {
        "authorId": "2265515206",
        "name": "Xingyao Zhang"
      },
      {
        "authorId": "2265516958",
        "name": "Jie Wu"
      },
      {
        "authorId": "2273090626",
        "name": "Vicente Ordonez"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "9a4e4ab77c3d836bab35e0578de68e8ce79af1e8",
    "url": "https://www.semanticscholar.org/paper/9a4e4ab77c3d836bab35e0578de68e8ce79af1e8",
    "title": "Graph Neural Prompting with Large Language Models",
    "abstract": "Large language models (LLMs) have shown remarkable generalization capability with exceptional performance in various language modeling tasks. However, they still exhibit inherent limitations in precisely capturing and returning grounded knowledge. While existing work has explored utilizing knowledge graphs (KGs) to enhance language modeling via joint training and customized model architectures, applying this to LLMs is problematic owing to their large number of parameters and high computational cost. Therefore, how to enhance pre-trained LLMs using grounded knowledge, e.g., retrieval-augmented generation, remains an open question. In this work, we propose Graph Neural Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs. GNP encompasses various designs, including a standard graph neural network encoder, a cross-modality pooling module, a domain projector, and a self-supervised link prediction objective. Extensive experiments on multiple datasets demonstrate the superiority of GNP on both commonsense and biomedical reasoning tasks across different LLM sizes and settings. Code is available at https://github.com/meettyj/GNP.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2023,
    "citationCount": 32,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.15427",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-09-27",
    "authors": [
      {
        "authorId": "46879986",
        "name": "Yijun Tian"
      },
      {
        "authorId": "2248096816",
        "name": "Huan Song"
      },
      {
        "authorId": "2249432235",
        "name": "Zichen Wang"
      },
      {
        "authorId": "2256768980",
        "name": "Haozhu Wang"
      },
      {
        "authorId": "2248753090",
        "name": "Ziqing Hu"
      },
      {
        "authorId": "2262512203",
        "name": "Fang Wang"
      },
      {
        "authorId": "144539424",
        "name": "N. Chawla"
      },
      {
        "authorId": "2248954229",
        "name": "Panpan Xu"
      }
    ],
    "source": "semantic_scholar",
    "score": 122.4476134219972
  },
  {
    "paperId": "ff01d3dab60dd4b7426c884b009dda83540c0c1e",
    "url": "https://www.semanticscholar.org/paper/ff01d3dab60dd4b7426c884b009dda83540c0c1e",
    "title": "Attention Sorting Combats Recency Bias In Long Context Language Models",
    "abstract": "Current language models often fail to incorporate long contexts efficiently during generation. We show that a major contributor to this issue are attention priors that are likely learned during pre-training: relevant information located earlier in context is attended to less on average. Yet even when models fail to use the information from a relevant document in their response, they still pay preferential attention to that document compared to an irrelevant document at the same position. We leverage this fact to introduce ``attention sorting'': perform one step of decoding, sort documents by the attention they receive (highest attention going last), repeat the process, generate the answer with the newly sorted context. We find that attention sorting improves performance of long context models. Our findings highlight some challenges in using off-the-shelf language models for retrieval augmented generation.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 31,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.01427",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-28",
    "authors": [
      {
        "authorId": "1970752",
        "name": "A. Peysakhovich"
      },
      {
        "authorId": "1977806",
        "name": "Adam Lerer"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.9860385419959
  },
  {
    "paperId": "dcf2e723ee9c3270c98ff768b139cca75d29242e",
    "url": "https://www.semanticscholar.org/paper/dcf2e723ee9c3270c98ff768b139cca75d29242e",
    "title": "A Study on the Implementation of Generative AI Services Using an Enterprise Data-Based LLM Application Architecture",
    "abstract": "This study presents a method for implementing generative AI services by utilizing the Large Language Models (LLM) application architecture. With recent advancements in generative AI technology, LLMs have gained prominence across various domains. In this context, the research addresses the challenge of information scarcity and proposes specific remedies by harnessing LLM capabilities. The investigation delves into strategies for mitigating the issue of inadequate data, offering tailored solutions. The study delves into the efficacy of employing fine-tuning techniques and direct document integration to alleviate data insufficiency. A significant contribution of this work is the development of a Retrieval-Augmented Generation (RAG) model, which tackles the aforementioned challenges. The RAG model is carefully designed to enhance information storage and retrieval processes, ensuring improved content generation. The research elucidates the key phases of the information storage and retrieval methodology underpinned by the RAG model. A comprehensive analysis of these steps is undertaken, emphasizing their significance in addressing the scarcity of data. The study highlights the efficacy of the proposed method, showcasing its applicability through illustrative instances. By implementing the RAG model for information storage and retrieval, the research not only contributes to a deeper comprehension of generative AI technology but also facilitates its practical usability within enterprises utilizing LLMs. This work holds substantial value in advancing the field of generative AI, offering insights into enhancing data-driven content generation and fostering active utilization of LLM-based services within corporate settings.",
    "venue": "Advances in Artificial Intelligence and Machine Learning",
    "year": 2023,
    "citationCount": 31,
    "openAccessPdf": {
      "url": "https://doi.org/10.54364/aaiml.2023.1191",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-03",
    "authors": [
      {
        "authorId": "98037302",
        "name": "CheonSu Jeong"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.9860385419959
  },
  {
    "paperId": "24548471ba5dde1a737011edf3c51417a1d834e5",
    "url": "https://www.semanticscholar.org/paper/24548471ba5dde1a737011edf3c51417a1d834e5",
    "title": "Banishing LLM Hallucinations Requires Rethinking Generalization",
    "abstract": "Despite their powerful chat, coding, and reasoning abilities, Large Language Models (LLMs) frequently hallucinate. Conventional wisdom suggests that hallucinations are a consequence of a balance between creativity and factuality, which can be mitigated, but not eliminated, by grounding the LLM in external knowledge sources. Through extensive systematic experiments, we show that these traditional approaches fail to explain why LLMs hallucinate in practice. Specifically, we show that LLMs augmented with a massive Mixture of Memory Experts (MoME) can easily memorize large datasets of random numbers. We corroborate these experimental findings with a theoretical construction showing that simple neural networks trained to predict the next token hallucinate when the training loss is above a threshold as it usually does in practice when training on internet scale data. We interpret our findings by comparing against traditional retrieval methods for mitigating hallucinations. We use our findings to design a first generation model for removing hallucinations -- Lamini-1 -- that stores facts in a massive mixture of millions of memory experts that are retrieved dynamically.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-25",
    "authors": [
      {
        "authorId": "2308225152",
        "name": "Johnny Li"
      },
      {
        "authorId": "2047545222",
        "name": "Saksham Consul"
      },
      {
        "authorId": "153844015",
        "name": "Eda Zhou"
      },
      {
        "authorId": "2308233725",
        "name": "James Wong"
      },
      {
        "authorId": "2308101899",
        "name": "Naila Farooqui"
      },
      {
        "authorId": "2309901274",
        "name": "Yuxin Ye"
      },
      {
        "authorId": "2308101429",
        "name": "Nithyashree Manohar"
      },
      {
        "authorId": "2308234606",
        "name": "Zhuxiaona Wei"
      },
      {
        "authorId": "2308408446",
        "name": "Tian Wu"
      },
      {
        "authorId": "2308099889",
        "name": "Ben Echols"
      },
      {
        "authorId": "2308231752",
        "name": "Sharon Zhou"
      },
      {
        "authorId": "2308101012",
        "name": "Gregory Diamos"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "b10482ab3dd1d340c3c926d92c3e617c24ee3949",
    "url": "https://www.semanticscholar.org/paper/b10482ab3dd1d340c3c926d92c3e617c24ee3949",
    "title": "Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content. This issue is common in both non-retrieval-based generation and retrieval-augmented generation approaches, and existing post-hoc rectification methods may not address the accumulated hallucination errors that may be caused by the\"snowballing\"issue, especially in reasoning tasks. To tackle these challenges, we introduce a novel approach called Real-time Verification and Rectification (Ever). Instead of waiting until the end of the generation process to rectify hallucinations, Ever employs a real-time, step-wise generation and hallucination rectification strategy. The primary objective is to detect and rectify hallucinations as they occur during the text generation process. When compared to both retrieval-based and non-retrieval-based baselines, Ever demonstrates a significant improvement in generating trustworthy and factually accurate text across a diverse range of tasks, including short-form QA, biography generation, and multi-hop reasoning.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 27,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-15",
    "authors": [
      {
        "authorId": "2266813792",
        "name": "Haoqiang Kang"
      },
      {
        "authorId": "2266755863",
        "name": "Juntong Ni"
      },
      {
        "authorId": "2266811937",
        "name": "Huaxiu Yao"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.98306765262805
  },
  {
    "paperId": "1a0480c43ef4e193d427eec261745b0b2e07ec18",
    "url": "https://www.semanticscholar.org/paper/1a0480c43ef4e193d427eec261745b0b2e07ec18",
    "title": "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs",
    "abstract": "Retrieval Augmented Generation (RAG) enhances the abilities of Large Language Models (LLMs) by enabling the retrieval of documents into the LLM context to provide more accurate and relevant responses. Existing RAG solutions do not focus on queries that may require fetching multiple documents with substantially different contents. Such queries occur frequently, but are challenging because the embeddings of these documents may be distant in the embedding space, making it hard to retrieve them all. This paper introduces Multi-Head RAG (MRAG), a novel scheme designed to address this gap with a simple yet powerful idea: leveraging activations of Transformer's multi-head attention layer, instead of the decoder layer, as keys for fetching multi-aspect documents. The driving motivation is that different attention heads can learn to capture different data aspects. Harnessing the corresponding activations results in embeddings that represent various facets of data items and queries, improving the retrieval accuracy for complex queries. We provide an evaluation methodology and metrics, multi-aspect datasets that we release online, and real-world use cases to demonstrate MRAG's effectiveness, showing improvements of up to 20% in relevance over standard RAG baselines. MRAG can be seamlessly integrated with existing RAG frameworks and benchmarking tools like RAGAS as well as different classes of data stores.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-07",
    "authors": [
      {
        "authorId": "2919642",
        "name": "Maciej Besta"
      },
      {
        "authorId": "100980110",
        "name": "Ale≈° Kub√≠ƒçek"
      },
      {
        "authorId": "2305484323",
        "name": "Roman Niggli"
      },
      {
        "authorId": "3308719",
        "name": "Robert Gerstenberger"
      },
      {
        "authorId": "2305484706",
        "name": "Lucas Weitzendorf"
      },
      {
        "authorId": "2305486931",
        "name": "Mingyuan Chi"
      },
      {
        "authorId": "2180167644",
        "name": "Patrick Iff"
      },
      {
        "authorId": "2232599373",
        "name": "Joanna Gajda"
      },
      {
        "authorId": "2268757436",
        "name": "Piotr Nyczyk"
      },
      {
        "authorId": "2257345219",
        "name": "J√ºrgen M√ºller"
      },
      {
        "authorId": "50544096",
        "name": "H. Niewiadomski"
      },
      {
        "authorId": "2238206038",
        "name": "Marcin Chrapek"
      },
      {
        "authorId": "18356904",
        "name": "Michal Podstawski"
      },
      {
        "authorId": "2238205863",
        "name": "Torsten Hoefler"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "2c4f702a4bbb733e1abfb722c5c74fa15aa85ee5",
    "url": "https://www.semanticscholar.org/paper/2c4f702a4bbb733e1abfb722c5c74fa15aa85ee5",
    "title": "Memory3: Language Modeling with Explicit Memory",
    "abstract": "The training and inference of large language models (LLMs) are together a costly process that transports knowledge from raw data to meaningful computation. Inspired by the memory hierarchy of the human brain, we reduce this cost by equipping LLMs with explicit memory, a memory format cheaper than model parameters and text retrieval-augmented generation (RAG). Conceptually, with most of its knowledge externalized to explicit memories, the LLM can enjoy a smaller parameter size, training cost, and inference cost, all proportional to the amount of remaining\"abstract knowledge\". As a preliminary proof of concept, we train from scratch a 2.4B LLM, which achieves better performance than much larger LLMs as well as RAG models, and maintains higher decoding speed than RAG. The model is named $\\text{Memory}^3$, since explicit memory is the third form of memory in LLMs after implicit memory (model parameters) and working memory (context key-values). We introduce a memory circuitry theory to support the externalization of knowledge, and present novel techniques including a memory sparsification mechanism that makes storage tractable and a two-stage pretraining scheme that facilitates memory formation.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-01",
    "authors": [
      {
        "authorId": "2309755519",
        "name": "Hongkang Yang"
      },
      {
        "authorId": "2146389150",
        "name": "Zehao Lin"
      },
      {
        "authorId": "2309212122",
        "name": "Wenjin Wang"
      },
      {
        "authorId": "2282083454",
        "name": "Hao Wu"
      },
      {
        "authorId": "2268429641",
        "name": "Zhiyu Li"
      },
      {
        "authorId": "2268400606",
        "name": "Bo Tang"
      },
      {
        "authorId": "2290242475",
        "name": "Wenqiang Wei"
      },
      {
        "authorId": "2304359240",
        "name": "Jinbo Wang"
      },
      {
        "authorId": "2309903772",
        "name": "Zeyun Tang"
      },
      {
        "authorId": "2268434524",
        "name": "Shichao Song"
      },
      {
        "authorId": "2303402664",
        "name": "Chenyang Xi"
      },
      {
        "authorId": "2308360157",
        "name": "Yu Yu"
      },
      {
        "authorId": "2309428633",
        "name": "Kai Chen"
      },
      {
        "authorId": "2268399953",
        "name": "Feiyu Xiong"
      },
      {
        "authorId": "2309791857",
        "name": "Linpeng Tang"
      },
      {
        "authorId": "2282137531",
        "name": "E. Weinan"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "babd098744d3ab051781323c1062b8d6395ca2e8",
    "url": "https://www.semanticscholar.org/paper/babd098744d3ab051781323c1062b8d6395ca2e8",
    "title": "Evaluating RAG-Fusion with RAGElo: an Automated Elo-based Framework",
    "abstract": "Challenges in the automated evaluation of Retrieval-Augmented Generation (RAG) Question-Answering (QA) systems include hallucination problems in domain-specific knowledge and the lack of gold standard benchmarks for company internal tasks. This results in difficulties in evaluating RAG variations, like RAG-Fusion (RAGF), in the context of a product QA task at Infineon Technologies. To solve these problems, we propose a comprehensive evaluation framework, which leverages Large Language Models (LLMs) to generate large datasets of synthetic queries based on real user queries and in-domain documents, uses LLM-as-a-judge to rate retrieved documents and answers, evaluates the quality of answers, and ranks different variants of Retrieval-Augmented Generation (RAG) agents with RAGElo's automated Elo-based competition. LLM-as-a-judge rating of a random sample of synthetic queries shows a moderate, positive correlation with domain expert scoring in relevance, accuracy, completeness, and precision. While RAGF outperformed RAG in Elo score, a significance analysis against expert annotations also shows that RAGF significantly outperforms RAG in completeness, but underperforms in precision. In addition, Infineon's RAGF assistant demonstrated slightly higher performance in document relevance based on MRR@5 scores. We find that RAGElo positively aligns with the preferences of human annotators, though due caution is still required. Finally, RAGF's approach leads to more complete answers based on expert annotations and better answers overall based on RAGElo's evaluation criteria.",
    "venue": "LLM4Eval@SIGIR",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-20",
    "authors": [
      {
        "authorId": "2282961608",
        "name": "Zackary Rackauckas"
      },
      {
        "authorId": "2307916869",
        "name": "Arthur Camara"
      },
      {
        "authorId": "2307917279",
        "name": "Jakub Zavrel"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "63b331998b17fdb879073873cf06004a6df18b44",
    "url": "https://www.semanticscholar.org/paper/63b331998b17fdb879073873cf06004a6df18b44",
    "title": "A fine-tuning enhanced RAG system with quantized influence measure as AI judge",
    "abstract": null,
    "venue": "Scientific Reports",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "https://www.nature.com/articles/s41598-024-79110-x.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-26",
    "authors": [
      {
        "authorId": "1845889141",
        "name": "K. Rangan"
      },
      {
        "authorId": "2287852165",
        "name": "Yiqiao Yin"
      }
    ],
    "source": "semantic_scholar",
    "score": 76.87639203842082
  },
  {
    "paperId": "a1853500aa8ff46e440a6f2e76bfbb2a06b25378",
    "url": "https://www.semanticscholar.org/paper/a1853500aa8ff46e440a6f2e76bfbb2a06b25378",
    "title": "TrumorGPT: Query Optimization and Semantic Reasoning over Networks for Automated Fact-Checking",
    "abstract": "In the age of social media, the rapid spread of misinformation and rumors has led to the emergence of infodemics, where false information poses a significant threat to society. To combat this issue, we introduce TrumorGPT, a novel generative artificial intelligence solution designed for automated fact-checking. TrumorGPT aims to distinguish \"trumors\", which are rumors that turn out to be true, providing a crucial tool in differentiating between mere speculation and verified facts. This framework merges machine learning with natural language processing techniques, leveraging a large language model (LLM) with few-shot learning for knowledge graph construction and semantic reasoning. TrumorGPT addresses the \"hallucination\" issue common in LLMs and the limitations of static training data by incorporating retrieval-augmented generation. This approach involves accessing and utilizing information from regularly updated knowledge graphs that consist of the latest news and information, ensuring that fact-checking of TrumorGPT is based on the most recent data. Accessing updated knowledge graphs greatly enhances the proficiency of TrumorGPT in delivering accurate and reliable information promptly. Evaluating with extensive datasets, TrumorGPT demonstrates superior performance in automated fact-checking. Its ability to effectively conduct automated fact-checking across various platforms marks a critical step forward in the fight against misinformation, enhancing trust and accuracy in the digital information age.",
    "venue": "Annual Conference on Information Sciences and Systems",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-13",
    "authors": [
      {
        "authorId": "52205613",
        "name": "C. Hang"
      },
      {
        "authorId": "3395394",
        "name": "Pei-Duo Yu"
      },
      {
        "authorId": "2149491635",
        "name": "C. Tan"
      }
    ],
    "source": "semantic_scholar",
    "score": 106.87639203842082
  },
  {
    "paperId": "dae6286d86bc1412de4a524e8311b102109a1538",
    "url": "https://www.semanticscholar.org/paper/dae6286d86bc1412de4a524e8311b102109a1538",
    "title": "ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities",
    "abstract": "In this work, we introduce ChatQA 2, an Llama 3.0-based model with a 128K context window, designed to bridge the gap between open-source LLMs and leading proprietary models (e.g., GPT-4-Turbo) in long-context understanding and retrieval-augmented generation (RAG) capabilities. These two capabilities are essential for LLMs to process large volumes of information that cannot fit into a single prompt and are complementary to each other, depending on the downstream tasks and computational budgets. We present a detailed continued training recipe to extend the context window of Llama3-70B-base from 8K to 128K tokens, along with a three-stage instruction tuning process to enhance the model's instruction-following, RAG performance, and long-context understanding capabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model outperforms most existing state-of-the-art models, including GPT-4-Turbo-2024-04-09, Qwen2-72B-Instruct, and Llama3.1-70B-Instruct, on ultra-long tasks beyond 100K tokens, as well as on the RAG benchmark using only a 4K context window, showing the strong long context capability across varying sequence lengths. We further provide extensive comparisons between direct long-context and RAG solutions using the same state-of-the-art long-context LLMs. Interestingly, we find that the performance of strong long-context LLMs using RAG improves when retrieving a larger number of chunks. With a large set of top-k chunks, RAG consistently outperforms direct long-context solution using the same state-of-the-art long-context models (e.g., Llama3-ChatQA-2-70B and Qwen2-72B-Instruct) on both 32K benchmarks and real-world 128K tasks. To advance research in this field, we open-sourced the model weights, training data, and the evaluation setup for the for the community: https://chatqa2-project.github.io/",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-19",
    "authors": [
      {
        "authorId": "2254989105",
        "name": "Peng Xu"
      },
      {
        "authorId": "2253664013",
        "name": "Wei Ping"
      },
      {
        "authorId": "2253618149",
        "name": "Xianchao Wu"
      },
      {
        "authorId": "2256582287",
        "name": "Zihan Liu"
      },
      {
        "authorId": "1911755",
        "name": "Mohammad Shoeybi"
      },
      {
        "authorId": "2264406909",
        "name": "Bryan Catanzaro"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "cd5537cd7691b7a9f6d5a5b8d473219e6b074da6",
    "url": "https://www.semanticscholar.org/paper/cd5537cd7691b7a9f6d5a5b8d473219e6b074da6",
    "title": "Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines",
    "abstract": "In the domain of digital information dissemination, search engines act as pivotal conduits linking information seekers with providers. The advent of chat-based search engines utilizing Large Language Models (LLMs) and Retrieval Augmented Generation (RAG), exemplified by Bing Chat, marks an evolutionary leap in the search ecosystem. They demonstrate metacognitive abilities in interpreting web information and crafting responses with human-like understanding and creativity. Nonetheless, the intricate nature of LLMs renders their\"cognitive\"processes opaque, challenging even their designers' understanding. This research aims to dissect the mechanisms through which an LLM-powered chat-based search engine, specifically Bing Chat, selects information sources for its responses. To this end, an extensive dataset has been compiled through engagements with New Bing, documenting the websites it cites alongside those listed by the conventional search engine. Employing natural language processing (NLP) techniques, the research reveals that Bing Chat exhibits a preference for content that is not only readable and formally structured, but also demonstrates lower perplexity levels, indicating a unique inclination towards text that is predictable by the underlying LLM. Further enriching our analysis, we procure an additional dataset through interactions with the GPT-4 based knowledge retrieval API, unveiling a congruent text preference between the RAG API and Bing Chat. This consensus suggests that these text preferences intrinsically emerge from the underlying language models, rather than being explicitly crafted by Bing Chat's developers. Moreover, our investigation documents a greater similarity among websites cited by RAG technologies compared to those ranked highest by conventional search engines.",
    "venue": "Social Science Research Network",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Economics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-29",
    "authors": [
      {
        "authorId": "2288802958",
        "name": "Lijia Ma"
      },
      {
        "authorId": "2152778054",
        "name": "Xingchen Xu"
      },
      {
        "authorId": "2289800070",
        "name": "Yong Tan"
      }
    ],
    "source": "semantic_scholar",
    "score": 106.87639203842082
  },
  {
    "paperId": "91e011a952de940e5aea485fed5e49140924a8ca",
    "url": "https://www.semanticscholar.org/paper/91e011a952de940e5aea485fed5e49140924a8ca",
    "title": "FIT-RAG: Black-Box RAG with Factual Information and Token Reduction",
    "abstract": "Due to the extraordinarily large number of parameters, fine-tuning Large Language Models (LLMs) to update long-tail or out-of-date knowledge is impractical in lots of applications. To avoid fine-tuning, we can alternatively treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG. Recently, black-box RAG has achieved success in knowledge-intensive tasks and has gained much attention. Existing black-box RAG methods typically fine-tune the retriever to cater to LLMs‚Äô preferences and concatenate all the retrieved documents as the input, which suffers from two issues: (1) Ignorance of Factual Information. The LLM preferred documents may not contain the factual information for the given question, which can mislead the retriever and hurt the effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating all the retrieved documents brings large amounts of unnecessary tokens for LLMs, which degenerates the efficiency of black-box RAG. To address these issues, this paper proposes a novel black-box RAG framework which utilizes the factual information in the retrieval and reduces the number of tokens for augmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by constructing a bi-label document scorer which takes the factual information and LLMs‚Äô preferences as labels respectively. Besides, it reduces the tokens by introducing a self-knowledge recognizer and a sub-document-level token reducer, which enables FIT-RAG to avoid unnecessary augmentation and reduce augmentation tokens as much as possible. FIT-RAG achieves both superior effectiveness and efficiency, which is validated by extensive experiments across three open-domain question-answering datasets: TriviaQA, NQ and PopQA. FIT-RAG can improve the answering accuracy of Llama2-13B-Chat by 14.3% on TriviaQA, 19.9% on NQ and 27.5% on PopQA, respectively. Furthermore, it can save approximately half of the tokens on average across the three datasets.",
    "venue": "ACM Transactions on Information Systems",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-21",
    "authors": [
      {
        "authorId": "2293238106",
        "name": "Yuren Mao"
      },
      {
        "authorId": "2223421628",
        "name": "Xuemei Dong"
      },
      {
        "authorId": "2292411692",
        "name": "Wenyi Xu"
      },
      {
        "authorId": "2292513358",
        "name": "Yunjun Gao"
      },
      {
        "authorId": "2292401852",
        "name": "Bin Wei"
      },
      {
        "authorId": "2292448869",
        "name": "Ying Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.87639203842082
  },
  {
    "paperId": "f696ffb1f0408e06ab4d91985f3e3f837c370c77",
    "url": "https://www.semanticscholar.org/paper/f696ffb1f0408e06ab4d91985f3e3f837c370c77",
    "title": "RATT: A Thought Structure for Coherent and Correct LLM Reasoning",
    "abstract": "Large Language Models (LLMs) gain substantial reasoning and decision-making capabilities from thought structures. However, existing methods such as Tree of Thought and Retrieval Augmented Thoughts often fall short in complex tasks due to the limitations of insufficient local retrieval of factual knowledge and inadequate global selection of strategies. These limitations make it challenging for these methods to balance factual accuracy and comprehensive logical optimization effectively. To address these limitations, we introduce the Retrieval Augmented Thought Tree (RATT), a novel thought structure that considers both overall logical soundness and factual correctness at each step of the thinking process. Specifically, at every point of a thought branch, RATT performs planning and lookahead to explore and evaluate multiple potential reasoning steps, and integrate the fact-checking ability of Retrieval-Augmented Generation (RAG) with LLM's ability to assess overall strategy. Through this combination of factual knowledge and strategic feasibility, the RATT adjusts and integrates the thought tree structure to search for the most promising branches within the search space. This thought structure significantly enhances the model's coherence in logical inference and efficiency in decision-making, and thus increases the limit of the capacity of LLM to generate reliable inferences and decisions based on thought structures. A broad range of experiments on different types of tasks showcases that the RATT structure significantly outperforms existing methods in factual correctness and logical coherence.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-04",
    "authors": [
      {
        "authorId": "2304897819",
        "name": "Jinghan Zhang"
      },
      {
        "authorId": "2304896765",
        "name": "Xiting Wang"
      },
      {
        "authorId": "22500310",
        "name": "Weijieying Ren"
      },
      {
        "authorId": "2304822381",
        "name": "Lu Jiang"
      },
      {
        "authorId": "1669829502",
        "name": "Dongjie Wang"
      },
      {
        "authorId": "2238129790",
        "name": "Kunpeng Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "89d06465e506314dcf4723ee9667be23d87e0ebe",
    "url": "https://www.semanticscholar.org/paper/89d06465e506314dcf4723ee9667be23d87e0ebe",
    "title": "CompAct: Compressing Retrieved Documents Actively for Question Answering",
    "abstract": "Retrieval-augmented generation supports language models to strengthen their factual groundings by providing external contexts. However, language models often face challenges when given extensive information, diminishing their effectiveness in solving questions. Context compression tackles this issue by filtering out irrelevant information, but current methods still struggle in realistic scenarios where crucial information cannot be captured with a single-step approach. To overcome this limitation, we introduce CompAct, a novel framework that employs an active strategy to condense extensive documents without losing key information. Our experiments demonstrate that CompAct brings significant improvements in both performance and compression rate on multi-hop question-answering benchmarks. CompAct flexibly operates as a cost-efficient plug-in module with various off-the-shelf retrievers or readers, achieving exceptionally high compression rates (47x).",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-12",
    "authors": [
      {
        "authorId": "2284687462",
        "name": "Chanwoong Yoon"
      },
      {
        "authorId": "2294868533",
        "name": "Taewhoo Lee"
      },
      {
        "authorId": "2261405895",
        "name": "Hyeon Hwang"
      },
      {
        "authorId": "2281744951",
        "name": "Minbyul Jeong"
      },
      {
        "authorId": "2281792202",
        "name": "Jaewoo Kang"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "fb15b5be005ed93263e2968bc741c47f3a663e8a",
    "url": "https://www.semanticscholar.org/paper/fb15b5be005ed93263e2968bc741c47f3a663e8a",
    "title": "From Questions to Insightful Answers: Building an Informed Chatbot for University Resources",
    "abstract": "This paper presents BARKPLUG V.2, a Large Language Model (LLM)-based chatbot system built using Retrieval Augmented Generation (RAG) pipelines to enhance the user experience and access to information within academic settings.The objective of BARKPLUG V.2 is to provide information to users about various campus resources, including academic departments, programs, campus facilities, and student resources at a university setting in an interactive fashion. Our system leverages university data as an external data corpus and ingests it into our RAG pipelines for domain-specific question-answering tasks. We evaluate the effectiveness of our system in generating accurate and pertinent responses for Mississippi State University, as a case study, using quantitative measures, employing frameworks such as Retrieval Augmented Generation Assessment(RAGAS). Furthermore, we evaluate the usability of this system via subjective satisfaction surveys using the System Usability Scale (SUS). Our system demonstrates impressive quantitative performance, with a mean RAGAS score of 0.96, and experience, as validated by usability assessments.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-05-13",
    "authors": [
      {
        "authorId": "2175998538",
        "name": "Subash Neupane"
      },
      {
        "authorId": "2301204757",
        "name": "Elias Hossain"
      },
      {
        "authorId": "2301203940",
        "name": "Jason Keith"
      },
      {
        "authorId": "2301203928",
        "name": "Himanshu Tripathi"
      },
      {
        "authorId": "2301204752",
        "name": "Farbod Ghiasi"
      },
      {
        "authorId": "40080937",
        "name": "Noorbakhsh Amiri Golilarz"
      },
      {
        "authorId": "2284767873",
        "name": "Amin Amirlatifi"
      },
      {
        "authorId": "2284767922",
        "name": "Sudip Mittal"
      },
      {
        "authorId": "2066329536",
        "name": "Shahram Rahimi"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "bc4b9a9170b45dbbea278e67a8334c4965fa8c06",
    "url": "https://www.semanticscholar.org/paper/bc4b9a9170b45dbbea278e67a8334c4965fa8c06",
    "title": "Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for Dialogue",
    "abstract": "We study the limitations of Large Language Models (LLMs) for the task of response generation in human-machine dialogue. Several techniques have been proposed in the literature for different dialogue types (e.g., Open-Domain). However, the evaluations of these techniques have been limited in terms of base LLMs, dialogue types and evaluation metrics. In this work, we extensively analyze different LLM adaptation techniques when applied to different dialogue types. We have selected two base LLMs, Llama-2 and Mistral, and four dialogue types Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering. We evaluate the performance of in-context learning and fine-tuning techniques across datasets selected for each dialogue type. We assess the impact of incorporating external knowledge to ground the generation in both scenarios of Retrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistent evaluation and explainability criteria for automatic metrics and human evaluation protocols. Our analysis shows that there is no universal best-technique for adapting large language models as the efficacy of each technique depends on both the base LLM and the specific type of dialogue. Last but not least, the assessment of the best adaptation technique should include human evaluation to avoid false expectations and outcomes derived from automatic metrics.",
    "venue": "International Conference on Natural Language Generation",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-10",
    "authors": [
      {
        "authorId": "2277741736",
        "name": "Simone Alghisi"
      },
      {
        "authorId": "2277741847",
        "name": "Massimo Rizzoli"
      },
      {
        "authorId": "2038117205",
        "name": "G. Roccabruna"
      },
      {
        "authorId": "2101322593",
        "name": "Seyed Mahed Mousavi"
      },
      {
        "authorId": "2200033404",
        "name": "Giuseppe Riccardi"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "4effe670a1f3c74eafe0e898d8cb596e2e46f041",
    "url": "https://www.semanticscholar.org/paper/4effe670a1f3c74eafe0e898d8cb596e2e46f041",
    "title": "Design of an Autonomous Cyber Defence Agent using Hybrid AI models",
    "abstract": "This paper extends the design of an autonomous cyber defence (ACD) agent to monitor and actuate within a protected core network segment. The goal is to take advantage of recent developments in AI models to define a hybrid architecture that combines deep reinforcement learning (DRL), large language models (LLMs), and rule-based models. The motivation comes from the fact that modern network segments within colored clouds are using software-defined controllers with the means to host ACD agents and other cybersecurity tools implementing hybrid AI models. For example, our ACD agent uses a DRL model and the chatbot uses an LLM to create an interface with human cybersecurity experts. The ACD agent was evaluated against two red agent strategies in a gym environment using a set of actions to defend services in the network (monitor, analyse, decoy, remove, and restore). Our chatbot was developed using retrieval augmented generation and a prompting agent to augment a pre-trained LLM with data from cybersecurity knowledge graphs. We performed a comparative analysis between a baseline implementation and our chatbot using generation/retrieval metrics. The results suggest that both ACD agent and chatbot can potentially enhance the defence of critical networks connected to untrusted infrastructure.",
    "venue": "2024 International Conference on Military Communication and Information Systems (ICMCIS)",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-23",
    "authors": [
      {
        "authorId": "118260411",
        "name": "Johannes F. Loevenich"
      },
      {
        "authorId": "2304789725",
        "name": "Erik Adler"
      },
      {
        "authorId": "2304790016",
        "name": "R√©mi Mercier"
      },
      {
        "authorId": "2242956242",
        "name": "Alexander Velazquez"
      },
      {
        "authorId": "2242948655",
        "name": "R. R. F. Lopes"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "3cb337f26b504d3d76374ae5011584f155bf28e9",
    "url": "https://www.semanticscholar.org/paper/3cb337f26b504d3d76374ae5011584f155bf28e9",
    "title": "EXPRESS: AI-Human Hybrids for Marketing Research: Leveraging LLMs as Collaborators",
    "abstract": "The authors‚Äô central premise is that a human-LLM hybrid approach leads to efficiency and effectiveness gains in the marketing research process. In qualitative research, they show that LLMs can assist in both data generation and analysis; LLMs effectively create sample characteristics, generate synthetic respondents, and conduct and moderate in-depth interviews. The AI-human hybrid generates information-rich, coherent data that surpasses human-only data in depth and insightfulness and matches human performance in data analysis tasks of generating themes and summaries. Evidence from expert judges shows that humans and LLMs possess complementary skills; the human-LLM hybrid outperforms its human-only or LLM-only counterpart. For quantitative research, the LLM correctly picks the answer direction and valence, with the quality of synthetic data significantly improving through few-shot learning and retrieval-augmented generation. The authors demonstrate the value of the AI-human hybrid by collaborating with a Fortune 500 food company and replicating a 2019 qualitative and quantitative study using GPT-4. For their empirical investigation, the authors design the system architecture and prompts to create personas, ask questions, and obtain responses from synthetic respondents. They provide roadmaps for integrating LLMs into qualitative and quantitative marketing research and conclude that LLMs serve as valuable collaborators in the insight generation process.",
    "venue": "Journal of Marketing",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-09",
    "authors": [
      {
        "authorId": "2280576064",
        "name": "Neeraj Arora"
      },
      {
        "authorId": "2280541186",
        "name": "Ishita Chakraborty"
      },
      {
        "authorId": "2280603015",
        "name": "Yohei Nishimura"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "63f2c402f5ddcdc65145b6cde29ad724e1923329",
    "url": "https://www.semanticscholar.org/paper/63f2c402f5ddcdc65145b6cde29ad724e1923329",
    "title": "Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG",
    "abstract": "Retrieval-augmented generation (RAG) empowers large language models (LLMs) to utilize external knowledge sources. The increasing capacity of LLMs to process longer input sequences opens up avenues for providing more retrieved information, to potentially enhance the quality of generated outputs. It is plausible to assume that a larger retrieval set would contain more relevant information (higher recall), that might result in improved performance. However, our empirical findings demonstrate that for many long-context LLMs, the quality of generated output initially improves first, but then subsequently declines as the number of retrieved passages increases. This paper investigates this phenomenon, identifying the detrimental impact of retrieved\"hard negatives\"as a key contributor. To mitigate this and enhance the robustness of long-context LLM-based RAG, we propose both training-free and training-based approaches. We first showcase the effectiveness of retrieval reordering as a simple yet powerful training-free optimization. Furthermore, we explore training-based methods, specifically RAG-specific implicit LLM fine-tuning and RAG-oriented fine-tuning with intermediate reasoning, demonstrating their capacity for substantial performance gains. Finally, we conduct a systematic analysis of design choices for these training-based methods, including data distribution, retriever selection, and training context length.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-08",
    "authors": [
      {
        "authorId": "2057050247",
        "name": "Bowen Jin"
      },
      {
        "authorId": "2256335437",
        "name": "Jinsung Yoon"
      },
      {
        "authorId": "2257136881",
        "name": "Jiawei Han"
      },
      {
        "authorId": "2676352",
        "name": "Sercan √ñ. Arik"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "45a92d8483c80ccfc0ae497ce308d882a11a368e",
    "url": "https://www.semanticscholar.org/paper/45a92d8483c80ccfc0ae497ce308d882a11a368e",
    "title": "WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge Conflicts from Wikipedia",
    "abstract": "Retrieval-augmented generation (RAG) has emerged as a promising solution to mitigate the limitations of large language models (LLMs), such as hallucinations and outdated information. However, it remains unclear how LLMs handle knowledge conflicts arising from different augmented retrieved passages, especially when these passages originate from the same source and have equal trustworthiness. In this work, we conduct a comprehensive evaluation of LLM-generated answers to questions that have varying answers based on contradictory passages from Wikipedia, a dataset widely regarded as a high-quality pre-training resource for most LLMs. Specifically, we introduce WikiContradict, a benchmark consisting of 253 high-quality, human-annotated instances designed to assess LLM performance when augmented with retrieved passages containing real-world knowledge conflicts. We benchmark a diverse range of both closed and open-source LLMs under different QA scenarios, including RAG with a single passage, and RAG with 2 contradictory passages. Through rigorous human evaluations on a subset of WikiContradict instances involving 5 LLMs and over 3,500 judgements, we shed light on the behaviour and limitations of these models. For instance, when provided with two passages containing contradictory facts, all models struggle to generate answers that accurately reflect the conflicting nature of the context, especially for implicit conflicts requiring reasoning. Since human evaluation is costly, we also introduce an automated model that estimates LLM performance using a strong open-source language model, achieving an F-score of 0.8. Using this automated metric, we evaluate more than 1,500 answers from seven LLMs across all WikiContradict instances. To facilitate future work, we release WikiContradict on: https://ibm.biz/wikicontradict.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-19",
    "authors": [
      {
        "authorId": "2307897753",
        "name": "Yufang Hou"
      },
      {
        "authorId": "2307463172",
        "name": "Alessandra Pascale"
      },
      {
        "authorId": "2307467605",
        "name": "Javier Carnerero-Cano"
      },
      {
        "authorId": "3235883",
        "name": "T. Tchrakian"
      },
      {
        "authorId": "2808565",
        "name": "Radu Marinescu"
      },
      {
        "authorId": "2307465399",
        "name": "Elizabeth Daly"
      },
      {
        "authorId": "8350409",
        "name": "Inkit Padhi"
      },
      {
        "authorId": "1706272",
        "name": "P. Sattigeri"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "c2d0602e1a0d2508f2be44b6791501290a48873b",
    "url": "https://www.semanticscholar.org/paper/c2d0602e1a0d2508f2be44b6791501290a48873b",
    "title": "A RAG Chatbot for Precision Medicine of Multiple Myeloma",
    "abstract": "The advent of precision medicine has revolutionized cancer treatment by integrating individual genetic, lifestyle, and environmental factors to tailor patient care (Huang et al., 2020; Ginsburg and Phillips, 2018). However, the complexity and heterogeneity of diseases like Multiple Myeloma (MM) pose significant challenges in leveraging the vast amounts of genomic data and biomedical literature available for personalized treatment planning (Rajkumar, 2014; Rollig et al., 2015). To address this, we present an innovative Retrieval-Augmented Generation (RAG) based chatbot framework that harnesses the power of Natural Language Processing (NLP) and state-of-the-art language models to curate and analyze MM-specific literature and provide personalized treatment recommendations based on patient-specific genomic data (Lewis et al., 2020). Our framework integrates the BioMed-RoBERTa-base model for embedding generation (Gururangan et al., 2020) and the Mistral-7B language model for question answering (Anthropic, 2023), enabling effective understanding and response to complex clinical queries. The retrieval component is enhanced by Amazon OpenSearch Service, ensuring fast and accurate access to relevant information. A comprehensive data analysis pipeline, including exploratory data analysis, semantic search, clustering, and topic modeling, provides valuable insights into the MM research landscape, informing the chatbot's knowledge base and uncovering potential research directions (Blei et al., 2003; Mikolov et al., 2013). Deployed using Amazon Kendra, our RAG chatbot offers a user-friendly and scalable platform for accessing MM information, incorporating features such as user authentication, customizable web interface, and continuous improvement based on user feedback. The framework aims to democratize access to precision medicine by providing clinicians with a sophisticated tool for interpreting complex genomic data in the context of MM, streamlining clinical workflows, and facilitating the development of personalized treatment plans (Patel et al., 2015). This paper presents the conceptualization, development, and potential impact of our RAG-based chatbot framework on the landscape of MM treatment and precision medicine. We argue that the synergistic integration of AI, NLP, and domain-specific knowledge marks a new era of healthcare, characterized by highly personalized, data-driven, and effective treatment modalities (Thong et al., 2021). Our framework not only advances the field of precision medicine in MM but also serves as a blueprint for the development of similar systems in other complex diseases, ultimately improving patient outcomes and quality of life.",
    "venue": "medRxiv",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "https://www.medrxiv.org/content/medrxiv/early/2024/03/18/2024.03.14.24304293.full.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-03-18",
    "authors": [
      {
        "authorId": "2291966216",
        "name": "M. A. Quidwai"
      },
      {
        "authorId": "2291966465",
        "name": "A. Lagana"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "4a9d0384089638d54936acdc9ab71b9a86ec1f58",
    "url": "https://www.semanticscholar.org/paper/4a9d0384089638d54936acdc9ab71b9a86ec1f58",
    "title": "Utilizing Large Language Models to Identify Evidence of Suicidality Risk through Analysis of Emotionally Charged Posts",
    "abstract": "This paper presents our contribution to the CLPsych 2024 shared task, focusing on the use of open-source large language models (LLMs) for suicide risk assessment through the analysis of social media posts. We achieved first place (out of 15 participating teams) in the task of providing summarized evidence of a user‚Äôs suicide risk. Our approach is based on Retrieval Augmented Generation (RAG), where we retrieve the top-k (k=5) posts with the highest emotional charge and provide the level of three different negative emotions (sadness, fear, anger) for each post during the generation phase.",
    "venue": "Workshop on Computational Linguistics and Clinical Psychology",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "9451998",
        "name": "Ahmet Uluslu"
      },
      {
        "authorId": "2316056982",
        "name": "Andrianos Michail"
      },
      {
        "authorId": "2291379872",
        "name": "Simon Clematide"
      }
    ],
    "source": "semantic_scholar",
    "score": 89.1415686865115
  },
  {
    "paperId": "769b2b4a4ca1e917e5ceb2b4d38016e3785ec5c7",
    "url": "https://www.semanticscholar.org/paper/769b2b4a4ca1e917e5ceb2b4d38016e3785ec5c7",
    "title": "Examining Long-Context Large Language Models for Environmental Review Document Comprehension",
    "abstract": "As LLMs become increasingly ubiquitous, researchers have tried various techniques to augment the knowledge provided to these models. Long context and retrieval-augmented generation (RAG) are two such methods that have recently gained popularity. In this work, we examine the benefits of both of these techniques by utilizing question answering (QA) task in a niche domain. While the effectiveness of LLM-based QA systems has already been established at an acceptable level in popular domains such as trivia and literature, it has not often been established in niche domains that traditionally require specialized expertise. We construct the NEPAQuAD1.0 benchmark to evaluate the performance of five long-context LLMs -- Claude Sonnet, Gemini, GPT-4, Llama 3.1, and Mistral -- when answering questions originating from Environmental Impact Statements prepared by U.S. federal government agencies in accordance with the National Environmental Environmental Act (NEPA). We specifically measure the ability of LLMs to understand the nuances of legal, technical, and compliance-related information present in NEPA documents in different contextual scenarios. We test the LLMs' internal prior NEPA knowledge by providing questions without any context, as well as assess how LLMs synthesize the contextual information present in long NEPA documents to facilitate the question/answering task. We compare the performance of the models in handling different types of questions (e.g., problem-solving, divergent, etc.). Our results suggest that RAG powered models significantly outperform those provided with only the PDF context in terms of answer accuracy, regardless of the choice of the LLM. Our further analysis reveals that many models perform better answering closed type questions (Yes/No) than divergent and problem-solving questions.",
    "venue": "",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": "2024-07-10",
    "authors": [
      {
        "authorId": "2282946151",
        "name": "Hung Phan"
      },
      {
        "authorId": "145536102",
        "name": "Anurag Acharya"
      },
      {
        "authorId": "2310611783",
        "name": "Sarthak Chaturvedi"
      },
      {
        "authorId": "2310735323",
        "name": "Shivam Sharma"
      },
      {
        "authorId": "2310613288",
        "name": "Mike J. Parker"
      },
      {
        "authorId": "2310608478",
        "name": "Dan Nally"
      },
      {
        "authorId": "2266204425",
        "name": "Ali Jannesari"
      },
      {
        "authorId": "2265756467",
        "name": "Karl Pazdernik"
      },
      {
        "authorId": "3285377",
        "name": "M. Halappanavar"
      },
      {
        "authorId": "2258957941",
        "name": "Sai Munikoti"
      },
      {
        "authorId": "24029613",
        "name": "Sameera Horawalavithana"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "03ae366d33290066edc41287f90fc67476d70bbc",
    "url": "https://www.semanticscholar.org/paper/03ae366d33290066edc41287f90fc67476d70bbc",
    "title": "How Much Can RAG Help the Reasoning of LLM?",
    "abstract": "Retrieval-Augmented Generation (RAG) has gained significant popularity in modern Large Language Models (LLMs) due to its effectiveness in introducing new knowledge and reducing hallucinations. However, the deep understanding of RAG remains limited, how does RAG help the reasoning process and can RAG help improve the reasoning capability remains question. While external documents are typically considered as a method to incorporate domain-specific information, they also contain intermediate reasoning results related to the query, this suggests that documents could enhance the reasoning capability of LLMs, which has not been previously explored. In this paper, we investigate this issue in depth and find that while RAG can assist with reasoning, the help is limited. If we conceptualize the reasoning process as a tree with fixed depth, then RAG struggles to assist LLMs in performing deeper reasoning. Additionally, the information in the documents requires preprocessing to filter out noise. We demonstrate that this preprocessing is difficult to achieve simply fine-tuning of the LLM, it often necessitates numerous additional transformer layers to solve the problem. To simplify the problem, we propose DPrompt tuning, which effectively resolves the issue within just limited transformer layers, leading to improved performance.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-03",
    "authors": [
      {
        "authorId": "2257093587",
        "name": "Jingyu Liu"
      },
      {
        "authorId": "2324132222",
        "name": "Jiaen Lin"
      },
      {
        "authorId": "2324249867",
        "name": "Yong Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "27711fa3c56c9189c29ec559c86ff6ae1a1b0c60",
    "url": "https://www.semanticscholar.org/paper/27711fa3c56c9189c29ec559c86ff6ae1a1b0c60",
    "title": "RAG-based Explainable Prediction of Road Users Behaviors for Automated Driving using Knowledge Graphs and Large Language Models",
    "abstract": "Prediction of road users' behaviors in the context of autonomous driving has gained considerable attention by the scientific community in the last years. Most works focus on predicting behaviors based on kinematic information alone, a simplification of the reality since road users are humans, and as such they are highly influenced by their surrounding context. In addition, a large plethora of research works rely on powerful Deep Learning techniques, which exhibit high performance metrics in prediction tasks but may lack the ability to fully understand and exploit the contextual semantic information contained in the road scene, not to mention their inability to provide explainable predictions that can be understood by humans. In this work, we propose an explainable road users' behavior prediction system that integrates the reasoning abilities of Knowledge Graphs (KG) and the expressiveness capabilities of Large Language Models (LLM) by using Retrieval Augmented Generation (RAG) techniques. For that purpose, Knowledge Graph Embeddings (KGE) and Bayesian inference are combined to allow the deployment of a fully inductive reasoning system that enables the issuing of predictions that rely on legacy information contained in the graph as well as on current evidence gathered in real time by onboard sensors. Two use cases have been implemented following the proposed approach: 1) Prediction of pedestrians' crossing actions; 2) Prediction of lane change maneuvers. In both cases, the performance attained surpasses the current state of the art in terms of anticipation and F1-score, showing a promising avenue for future research in this field.",
    "venue": "Expert systems with applications",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-01",
    "authors": [
      {
        "authorId": "2299153447",
        "name": "Mohamed Manzour Hussien"
      },
      {
        "authorId": "2269736704",
        "name": "Angie Nataly Melo"
      },
      {
        "authorId": "2696367",
        "name": "Augusto Luis Ballardini"
      },
      {
        "authorId": "2060066469",
        "name": "Carlota Salinas Maldonado"
      },
      {
        "authorId": "40247865",
        "name": "R. Izquierdo"
      },
      {
        "authorId": "2273926606",
        "name": "Miguel 'Angel Sotelo"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "6a6aed60f421169c9ae192beaaf3c423827dcd52",
    "url": "https://www.semanticscholar.org/paper/6a6aed60f421169c9ae192beaaf3c423827dcd52",
    "title": "G3: An Effective and Adaptive Framework for Worldwide Geolocalization Using Large Multi-Modality Models",
    "abstract": "Worldwide geolocalization aims to locate the precise location at the coordinate level of photos taken anywhere on the Earth. It is very challenging due to 1) the difficulty of capturing subtle location-aware visual semantics, and 2) the heterogeneous geographical distribution of image data. As a result, existing studies have clear limitations when scaled to a worldwide context. They may easily confuse distant images with similar visual contents, or cannot adapt to various locations worldwide with different amounts of relevant data. To resolve these limitations, we propose G3, a novel framework based on Retrieval-Augmented Generation (RAG). In particular, G3 consists of three steps, i.e., Geo-alignment, Geo-diversification, and Geo-verification to optimize both retrieval and generation phases of worldwide geolocalization. During Geo-alignment, our solution jointly learns expressive multi-modal representations for images, GPS and textual descriptions, which allows us to capture location-aware semantics for retrieving nearby images for a given query. During Geo-diversification, we leverage a prompt ensembling method that is robust to inconsistent retrieval performance for different image queries. Finally, we combine both retrieved and generated GPS candidates in Geo-verification for location prediction. Experiments on two well-established datasets IM2GPS3k and YFCC4k verify the superiority of G3 compared to other state-of-the-art methods. Our code and data are available online for reproduction.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-23",
    "authors": [
      {
        "authorId": "2264224432",
        "name": "Pengyue Jia"
      },
      {
        "authorId": "2249554788",
        "name": "Yiding Liu"
      },
      {
        "authorId": "2263797116",
        "name": "Xiaopeng Li"
      },
      {
        "authorId": "2238104000",
        "name": "Xiangyu Zhao"
      },
      {
        "authorId": "2223878432",
        "name": "Yuhao Wang"
      },
      {
        "authorId": "2187889118",
        "name": "Yantong Du"
      },
      {
        "authorId": "2309188625",
        "name": "Xiao Han"
      },
      {
        "authorId": "2298206411",
        "name": "Xuetao Wei"
      },
      {
        "authorId": "2237948548",
        "name": "Shuaiqiang Wang"
      },
      {
        "authorId": "2243455567",
        "name": "Dawei Yin"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "abf5eafba0dece6d5e593f3632f3509c2c10567a",
    "url": "https://www.semanticscholar.org/paper/abf5eafba0dece6d5e593f3632f3509c2c10567a",
    "title": "GraphInsight: Unlocking Insights in Large Language Models for Graph Structure Understanding",
    "abstract": "Although Large Language Models (LLMs) have demonstrated potential in processing graphs, they struggle with comprehending graphical structure information through prompts of graph description sequences, especially as the graph size increases. We attribute this challenge to the uneven memory performance of LLMs across different positions in graph description sequences, known as ''positional biases''. To address this, we propose GraphInsight, a novel framework aimed at improving LLMs' comprehension of both macro- and micro-level graphical information. GraphInsight is grounded in two key strategies: 1) placing critical graphical information in positions where LLMs exhibit stronger memory performance, and 2) investigating a lightweight external knowledge base for regions with weaker memory performance, inspired by retrieval-augmented generation (RAG). Moreover, GraphInsight explores integrating these two strategies into LLM agent processes for composite graph tasks that require multi-step reasoning. Extensive empirical studies on benchmarks with a wide range of evaluation tasks show that GraphInsight significantly outperforms all other graph description methods (e.g., prompting techniques and reordering strategies) in understanding graph structures of varying sizes.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-05",
    "authors": [
      {
        "authorId": "2112823208",
        "name": "Yukun Cao"
      },
      {
        "authorId": "2319869433",
        "name": "Shuo Han"
      },
      {
        "authorId": "2319809105",
        "name": "Zengyi Gao"
      },
      {
        "authorId": "2287966974",
        "name": "Zezhong Ding"
      },
      {
        "authorId": "2279249222",
        "name": "Xike Xie"
      },
      {
        "authorId": "2288042256",
        "name": "S. K. Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "d797cbd4bbdf60d12cca5c9521111dcf2336958a",
    "url": "https://www.semanticscholar.org/paper/d797cbd4bbdf60d12cca5c9521111dcf2336958a",
    "title": "Evaluating Students' Open-ended Written Responses with LLMs: Using the RAG Framework for GPT-3.5, GPT-4, Claude-3, and Mistral-Large",
    "abstract": "Evaluating open-ended written examination responses from students is an essential yet timeintensive task for educators, requiring a high degree of effort, consistency, and precision. Recent developments in Large Language Models (LLMs) present a promising opportunity to balance the need for thorough evaluation with efficient use of educators‚Äô time. We explore LLMs‚ÄîGPT-3.5, GPT-4, Claude-3, and Mistral-Large‚Äîin assessing university students‚Äô open-ended responses to questions about reference material they have studied. Each model was instructed to evaluate 54 responses repeatedly under two conditions: 10 times (10-shot) with a temperature setting of 0.0 and 10 times with a temperature of 0.5, expecting a total of 1,080 evaluations per model and 4,320 evaluations across all models. The RAG (Retrieval Augmented Generation) framework was used to make the LLMs to process the evaluation. Notable variations existed in studied LLMs consistency and the grading outcomes. There is a need to comprehend strengths and weaknesses of using LLMs for educational assessments.",
    "venue": "Advances in Artificial Intelligence and Machine Learning",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-08",
    "authors": [
      {
        "authorId": "2244404876",
        "name": "Jussi S. Jauhiainen"
      },
      {
        "authorId": "2244312558",
        "name": "Agust√≠n Garagorry Guerra"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "b3011f474c8a1b81bb742b9341e64568eb9d8bb0",
    "url": "https://www.semanticscholar.org/paper/b3011f474c8a1b81bb742b9341e64568eb9d8bb0",
    "title": "The Rise and Design of Enterprise Large Language Models",
    "abstract": "This article investigates a new phenomenon of enterprise large language models (ELLMs) focusing on what they are, why they are being developed, and what are some key capabilities. In addition, the article drills down on issues associated with integrating retrieval augmented generation approaches into ELLMs, including emerging research issues.",
    "venue": "IEEE Intelligent Systems",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-01",
    "authors": [
      {
        "authorId": "2248468437",
        "name": "Daniel E. O‚ÄôLeary"
      },
      {
        "authorId": "2248468437",
        "name": "Daniel E. O‚ÄôLeary"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.1415686865115
  },
  {
    "paperId": "1dcae23c009a115c488f83b91007705660222ea4",
    "url": "https://www.semanticscholar.org/paper/1dcae23c009a115c488f83b91007705660222ea4",
    "title": "Large Language Models (LLMs): Deployment, Tokenomics and Sustainability",
    "abstract": "The rapid advancement of Large Language Models (LLMs) has significantly impacted human-computer interaction, epitomized by the release of GPT-4o, which introduced comprehensive multi-modality capabilities. In this paper, we first explored the deployment strategies, economic considerations, and sustainability challenges associated with the state-of-the-art LLMs. More specifically, we discussed the deployment debate between Retrieval-Augmented Generation (RAG) and fine-tuning, highlighting their respective advantages and limitations. After that, we quantitatively analyzed the requirement of xPUs in training and inference. Additionally, for the tokenomics of LLM services, we examined the balance between performance and cost from the quality of experience (QoE)'s perspective of end users. Lastly, we envisioned the future hybrid architecture of LLM processing and its corresponding sustainability concerns, particularly in the environmental carbon footprint impact. Through these discussions, we provided a comprehensive overview of the operational and strategic considerations essential for the responsible development and deployment of LLMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-05-27",
    "authors": [
      {
        "authorId": "2303436566",
        "name": "Haiwei Dong"
      },
      {
        "authorId": "2303424071",
        "name": "Shuang Xie"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "82de481ebb5cf401b215178c20b058c7c1a4f9cd",
    "url": "https://www.semanticscholar.org/paper/82de481ebb5cf401b215178c20b058c7c1a4f9cd",
    "title": "Recent advances in text embedding: A Comprehensive Review of Top-Performing Methods on the MTEB Benchmark",
    "abstract": "Text embedding methods have become increasingly popular in both industrial and academic fields due to their critical role in a variety of natural language processing tasks. The significance of universal text embeddings has been further highlighted with the rise of Large Language Models (LLMs) applications such as Retrieval-Augmented Systems (RAGs). While previous models have attempted to be general-purpose, they often struggle to generalize across tasks and domains. However, recent advancements in training data quantity, quality and diversity; synthetic data generation from LLMs as well as using LLMs as backbones encourage great improvements in pursuing universal text embeddings. In this paper, we provide an overview of the recent advances in universal text embedding models with a focus on the top performing text embeddings on Massive Text Embedding Benchmark (MTEB). Through detailed comparison and analysis, we highlight the key contributions and limitations in this area, and propose potentially inspiring future research directions.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-05-27",
    "authors": [
      {
        "authorId": "2304744152",
        "name": "Hongliu Cao"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "94f5f59e98c1ef87db6eace3e6b1ed916377c7ba",
    "url": "https://www.semanticscholar.org/paper/94f5f59e98c1ef87db6eace3e6b1ed916377c7ba",
    "title": "Automated C/C++ Program Repair for High-Level Synthesis via Large Language Models",
    "abstract": "In High-Level Synthesis (HLS), converting a regular C/C++ program into its HLS-compatible counterpart (HLS-C) still requires tremendous manual effort. Various program scripts have been introduced to automate this process. But the resulting codes usually contain many issues that should be manually repaired by developers. Since Large Language Models (LLMs) have the ability to automate code generation, they can also be used for automated program repair in HLS. However, due to the limited training of LLMs considering hardware and software simultaneously, hallucinations may occur during program repair using LLMs, leading to compilation failures. Besides, using LLMs for iterative repair also incurs a high cost. To address these challenges, we propose an LLM-driven program repair framework that takes regular $\\mathrm{C} / \\mathrm{C}++$ code as input and automatically generates its corresponding HLS-C code for synthesis while minimizing human repair effort. To mitigate the hallucinations in LLMs and enhance the prompt quality, a Retrieval-Augmented Generation (RAG) paradigm is introduced to guide the LLMs toward correct repair. In addition, we use LLMs to create a static bit width optimization program to identify the optimized bit widths for variables. Moreover, LLM-driven HLS optimization strategies are introduced to add/tune pragmas in HLS-C programs for circuit optimization. Experimental results demonstrate that the proposed LLM-driven automated framework can achieve much higher repair pass rates in 24 real-world applications compared with the traditional scripts and the direct application of LLMs for program repair. The codes are open-sourced at this link: https://github.com/code-source1/catapult.",
    "venue": "Workshop on Machine Learning for CAD",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2407.03889",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-04",
    "authors": [
      {
        "authorId": "2243950298",
        "name": "Kangwei Xu"
      },
      {
        "authorId": "31602842",
        "name": "Grace Li Zhang"
      },
      {
        "authorId": "35875297",
        "name": "Xunzhao Yin"
      },
      {
        "authorId": "2248805727",
        "name": "Cheng Zhuo"
      },
      {
        "authorId": "2071332998",
        "name": "Ulf Schlichtmann"
      },
      {
        "authorId": "2269058160",
        "name": "Bing Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "eee3bf09bcc997a3586e29f7d3c3d37a42aff87d",
    "url": "https://www.semanticscholar.org/paper/eee3bf09bcc997a3586e29f7d3c3d37a42aff87d",
    "title": "FaithEval: Can Your Language Model Stay Faithful to Context, Even If \"The Moon is Made of Marshmallows\"",
    "abstract": "Ensuring faithfulness to context in large language models (LLMs) and retrieval-augmented generation (RAG) systems is crucial for reliable deployment in real-world applications, as incorrect or unsupported information can erode user trust. Despite advancements on standard benchmarks, faithfulness hallucination-where models generate responses misaligned with the provided context-remains a significant challenge. In this work, we introduce FaithEval, a novel and comprehensive benchmark tailored to evaluate the faithfulness of LLMs in contextual scenarios across three diverse tasks: unanswerable, inconsistent, and counterfactual contexts. These tasks simulate real-world challenges where retrieval mechanisms may surface incomplete, contradictory, or fabricated information. FaithEval comprises 4.9K high-quality problems in total, validated through a rigorous four-stage context construction and validation framework, employing both LLM-based auto-evaluation and human validation. Our extensive study across a wide range of open-source and proprietary models reveals that even state-of-the-art models often struggle to remain faithful to the given context, and that larger models do not necessarily exhibit improved faithfulness.Project is available at: \\url{https://github.com/SalesforceAIResearch/FaithEval}.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-30",
    "authors": [
      {
        "authorId": "2321601631",
        "name": "Yifei Ming"
      },
      {
        "authorId": "3234247",
        "name": "Senthil Purushwalkam"
      },
      {
        "authorId": "1824294087",
        "name": "Shrey Pandit"
      },
      {
        "authorId": "2321405223",
        "name": "Zixuan Ke"
      },
      {
        "authorId": "1399659909",
        "name": "Xuan-Phi Nguyen"
      },
      {
        "authorId": "2267728986",
        "name": "Caiming Xiong"
      },
      {
        "authorId": "2313536726",
        "name": "Shafiq Joty"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "0b8a12065f4f5da0165c82e2a21261c1793b65c3",
    "url": "https://www.semanticscholar.org/paper/0b8a12065f4f5da0165c82e2a21261c1793b65c3",
    "title": "Text2SQL is Not Enough: Unifying AI and Databases with TAG",
    "abstract": "AI systems that serve natural language questions over databases promise to unlock tremendous value. Such systems would allow users to leverage the powerful reasoning and knowledge capabilities of language models (LMs) alongside the scalable computational power of data management systems. These combined capabilities would empower users to ask arbitrary natural language questions over custom data sources. However, existing methods and benchmarks insufficiently explore this setting. Text2SQL methods focus solely on natural language questions that can be expressed in relational algebra, representing a small subset of the questions real users wish to ask. Likewise, Retrieval-Augmented Generation (RAG) considers the limited subset of queries that can be answered with point lookups to one or a few data records within the database. We propose Table-Augmented Generation (TAG), a unified and general-purpose paradigm for answering natural language questions over databases. The TAG model represents a wide range of interactions between the LM and database that have been previously unexplored and creates exciting research opportunities for leveraging the world knowledge and reasoning capabilities of LMs over data. We systematically develop benchmarks to study the TAG problem and find that standard methods answer no more than 20% of queries correctly, confirming the need for further research in this area. We release code for the benchmark at https://github.com/TAG-Research/TAG-Bench.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-27",
    "authors": [
      {
        "authorId": "2220693332",
        "name": "Asim Biswal"
      },
      {
        "authorId": "2290487661",
        "name": "Liana Patel"
      },
      {
        "authorId": "2317009471",
        "name": "Siddarth Jha"
      },
      {
        "authorId": "46215704",
        "name": "Amog Kamsetty"
      },
      {
        "authorId": "2290853114",
        "name": "Shu Liu"
      },
      {
        "authorId": "2317118075",
        "name": "Joseph E. Gonzalez"
      },
      {
        "authorId": "1412355294",
        "name": "Carlos Guestrin"
      },
      {
        "authorId": "2253469020",
        "name": "Matei Zaharia"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "e6a72ac42c4785ea187e29be8ffcf53efe0ad86a",
    "url": "https://www.semanticscholar.org/paper/e6a72ac42c4785ea187e29be8ffcf53efe0ad86a",
    "title": "Overcoming LLM Challenges using RAG-Driven Precision in Coffee Leaf Disease Remediation",
    "abstract": "This research introduces an innovative AI-driven precision agriculture system, leveraging YOLOv8 for disease identification and Retrieval Augmented Generation (RAG) for context-aware diagnosis. Focused on addressing the challenges of diseases affecting the coffee production sector in Karnataka, The system integrates sophisticated object detection techniques with language models to address the inherent constraints associated with Large Language Models (LLMs). Our methodology not only tackles the issue of hallucinations in LLMs, but also introduces dynamic disease identification and remediation strategies. Real-time monitoring, collaborative dataset expansion, and organizational involvement ensure the system‚Äôs adaptability in diverse agricultural settings. The effect of the suggested system extends beyond automation, aiming to secure food supplies, protect livelihoods, and promote eco-friendly farming practices. By facilitating precise disease identification, the system contributes to sustainable and environmentally conscious agriculture, reducing reliance on pesticides. Looking to the future, the project envisions continuous development in RAG-integrated object detection systems, emphasizing scalability, reliability, and usability. This research strives to be a beacon for positive change in agriculture, aligning with global efforts toward sustainable and technologically enhanced food production.",
    "venue": "2024 International Conference on Emerging Technologies in Computer Science for Interdisciplinary Applications (ICETCS)",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2405.01310",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-22",
    "authors": [
      {
        "authorId": "2279102441",
        "name": "Dr. Selva Kumar"
      },
      {
        "authorId": "2268491277",
        "name": "Imadh Ajaz Banday"
      },
      {
        "authorId": "2299334017",
        "name": "Vibha Venkatesh"
      },
      {
        "authorId": "2268647274",
        "name": "Afifah Khan"
      },
      {
        "authorId": "2279080179",
        "name": "Mohammed Ajmal Khan"
      },
      {
        "authorId": "2299325763",
        "name": "Manikantha Gada"
      }
    ],
    "source": "semantic_scholar",
    "score": 100.79441541679836
  },
  {
    "paperId": "ffc6dba31ba734e6eef98645364ea241c470f63e",
    "url": "https://www.semanticscholar.org/paper/ffc6dba31ba734e6eef98645364ea241c470f63e",
    "title": "From RAGs to rich parameters: Probing how language models utilize external knowledge over parametric information for factual queries",
    "abstract": "Retrieval Augmented Generation (RAG) enriches the ability of language models to reason using external context to augment responses for a given user prompt. This approach has risen in popularity due to practical applications in various applications of language models in search, question/answering, and chat-bots. However, the exact nature of how this approach works isn't clearly understood. In this paper, we mechanistically examine the RAG pipeline to highlight that language models take shortcut and have a strong bias towards utilizing only the context information to answer the question, while relying minimally on their parametric memory. We probe this mechanistic behavior in language models with: (i) Causal Mediation Analysis to show that the parametric memory is minimally utilized when answering a question and (ii) Attention Contributions and Knockouts to show that the last token residual stream do not get enriched from the subject token in the question, but gets enriched from other informative tokens in the context. We find this pronounced shortcut behaviour true across both LLaMa and Phi family of models.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-18",
    "authors": [
      {
        "authorId": "123351917",
        "name": "Hitesh Wadhwa"
      },
      {
        "authorId": "2307071990",
        "name": "Rahul Seetharaman"
      },
      {
        "authorId": "2307073959",
        "name": "Somyaa Aggarwal"
      },
      {
        "authorId": "2261733601",
        "name": "Reshmi Ghosh"
      },
      {
        "authorId": "2114710333",
        "name": "Samyadeep Basu"
      },
      {
        "authorId": "2261733701",
        "name": "Soundararajan Srinivasan"
      },
      {
        "authorId": "2307415582",
        "name": "Wenlong Zhao"
      },
      {
        "authorId": "2307073525",
        "name": "Shreyas Chaudhari"
      },
      {
        "authorId": "2126496227",
        "name": "Ehsan Aghazadeh"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "4971b33efce8be02c0d3a42d519b272937f0aa58",
    "url": "https://www.semanticscholar.org/paper/4971b33efce8be02c0d3a42d519b272937f0aa58",
    "title": "Exploring the Potential of Large Language Models for Automation in Technical Customer Service",
    "abstract": "Purpose: The purpose of this study is to investigate the potential of Large Language Models (LLMs) in transforming technical customer service (TCS) through the automation of cognitive tasks. Design/Methodology/Approach: Using a prototyping approach, the research assesses the feasibility of automating cognitive tasks in TCS with LLMs, employing real-world technical incident data from a Swiss telecommunications operator. Findings: Lower-level cognitive tasks such as translation, summarization, and content generation can be effectively automated with LLMs like GPT-4, while higher-level tasks such as reasoning require more advanced technological approaches such as Retrieval-Augmented Generation (RAG) or finetuning ; furthermore, the study underscores the significance of data ecosystems in enabling more complex cognitive tasks by fostering data sharing among various actors involved. Originality/Value: This study contributes to the emerging theory on LLM potential and technical feasibility in service management, providing concrete insights for operators of TCS units and highlighting the need for further research to address limitations and validate the applicability of LLMs across different domains.",
    "venue": "",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Economics"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-05-15",
    "authors": [
      {
        "authorId": "2265751810",
        "name": "Jochen Wulf"
      },
      {
        "authorId": "2265751747",
        "name": "Juerg Meierhofer"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "0afc8bcf4c487fd3383713fb579a666bef99d1f2",
    "url": "https://www.semanticscholar.org/paper/0afc8bcf4c487fd3383713fb579a666bef99d1f2",
    "title": "LProtector: An LLM-driven Vulnerability Detection System",
    "abstract": "This paper presents LProtector, an automated vulnerability detection system for C/C++ codebases driven by the large language model (LLM) GPT-4o and Retrieval-Augmented Generation (RAG). As software complexity grows, traditional methods face challenges in detecting vulnerabilities effectively. LProtector leverages GPT-4o's powerful code comprehension and generation capabilities to perform binary classification and identify vulnerabilities within target codebases. We conducted experiments on the Big-Vul dataset, showing that LProtector outperforms two state-of-the-art baselines in terms of F1 score, demonstrating the potential of integrating LLMs with vulnerability detection.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-11-10",
    "authors": [
      {
        "authorId": "2330189593",
        "name": "Ze Sheng"
      },
      {
        "authorId": "2330219955",
        "name": "Fenghua Wu"
      },
      {
        "authorId": "2330175824",
        "name": "Xiangwu Zuo"
      },
      {
        "authorId": "2331030314",
        "name": "Chao Li"
      },
      {
        "authorId": "2331448272",
        "name": "Yuxin Qiao"
      },
      {
        "authorId": "2330588015",
        "name": "Lei Hang"
      }
    ],
    "source": "semantic_scholar",
    "score": 85.79441541679836
  },
  {
    "paperId": "51fb89610710d7c4d25ce02de9f563cca4a5fecc",
    "url": "https://www.semanticscholar.org/paper/51fb89610710d7c4d25ce02de9f563cca4a5fecc",
    "title": "TC-RAG:Turing-Complete RAG's Case study on Medical LLM Systems",
    "abstract": "In the pursuit of enhancing domain-specific Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) emerges as a promising solution to mitigate issues such as hallucinations, outdated knowledge, and limited expertise in highly specialized queries. However, existing approaches to RAG fall short by neglecting system state variables, which are crucial for ensuring adaptive control, retrieval halting, and system convergence. In this paper, we introduce the TC-RAG through rigorous proof, a novel framework that addresses these challenges by incorporating a Turing Complete System to manage state variables, thereby enabling more efficient and accurate knowledge retrieval. By leveraging a memory stack system with adaptive retrieval, reasoning, and planning capabilities, TC-RAG not only ensures the controlled halting of retrieval processes but also mitigates the accumulation of erroneous knowledge via Push and Pop actions. In the case study of the medical domain, our extensive experiments on real-world healthcare datasets demonstrate the superiority of TC-RAG over existing methods in accuracy by over 7.20\\%. Our dataset and code have been available at https://https://github.com/Artessay/SAMA.git.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-17",
    "authors": [
      {
        "authorId": "2181297535",
        "name": "Xinke Jiang"
      },
      {
        "authorId": "2276489740",
        "name": "Yue Fang"
      },
      {
        "authorId": "2276425552",
        "name": "Rihong Qiu"
      },
      {
        "authorId": "2309505219",
        "name": "Haoyu Zhang"
      },
      {
        "authorId": "2215472732",
        "name": "Yongxin Xu"
      },
      {
        "authorId": "2282496720",
        "name": "Hao Chen"
      },
      {
        "authorId": "2282082431",
        "name": "Wentao Zhang"
      },
      {
        "authorId": "2276704515",
        "name": "Ruizhe Zhang"
      },
      {
        "authorId": "2279829353",
        "name": "Yuchen Fang"
      },
      {
        "authorId": "2276425819",
        "name": "Xu Chu"
      },
      {
        "authorId": "2145804723",
        "name": "Junfeng Zhao"
      },
      {
        "authorId": "2253831765",
        "name": "Yasha Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "13241422a6fef4e41dbb4d0f3e40e39363e07826",
    "url": "https://www.semanticscholar.org/paper/13241422a6fef4e41dbb4d0f3e40e39363e07826",
    "title": "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information",
    "abstract": "Large Vision-Language Models (LVLMs) have become pivotal at the intersection of computer vision and natural language processing. However, the full potential of LVLMs‚Äô Retrieval-Augmented Generation (RAG) capabilities remains underutilized. Existing works either focus solely on the text modality or are limited to specific tasks. Moreover, most LVLMs struggle to selectively utilize retrieved information and are sensitive to irrelevant or misleading references. To address these challenges, we propose a self-refinement framework designed to teach LVLMs to Selectively Utilize Retrieved Information (SURf). Specifically, when given questions that are incorrectly answered by the LVLM backbone, we obtain references that help correct the answers (positive references) and those that do not (negative references). We then fine-tune the LVLM backbone using a combination of these positive and negative references. Our experiments across three tasks and seven datasets demonstrate that our framework significantly enhances LVLMs‚Äô ability to effectively utilize retrieved multimodal references and improves their robustness against irrelevant or misleading information. The source code is available at https://anonymous.4open.science/r/SURf-6433.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-09-21",
    "authors": [
      {
        "authorId": "2202454665",
        "name": "Jiashuo Sun"
      },
      {
        "authorId": "2284727955",
        "name": "Jihai Zhang"
      },
      {
        "authorId": "2110348767",
        "name": "Yucheng Zhou"
      },
      {
        "authorId": "2720303",
        "name": "Zhao-yu Su"
      },
      {
        "authorId": "2262446609",
        "name": "Xiaoye Qu"
      },
      {
        "authorId": "2307325455",
        "name": "Yu Cheng"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "bd4d4dfe8e48a82768c5cd3f6d27530d6d355740",
    "url": "https://www.semanticscholar.org/paper/bd4d4dfe8e48a82768c5cd3f6d27530d6d355740",
    "title": "ARAGOG: Advanced RAG Output Grading",
    "abstract": "Retrieval-Augmented Generation (RAG) is essential for integrating external knowledge into Large Language Model (LLM) outputs. While the literature on RAG is growing, it primarily focuses on systematic reviews and comparisons of new state-of-the-art (SoTA) techniques against their predecessors, with a gap in extensive experimental comparisons. This study begins to address this gap by assessing various RAG methods' impacts on retrieval precision and answer similarity. We found that Hypothetical Document Embedding (HyDE) and LLM reranking significantly enhance retrieval precision. However, Maximal Marginal Relevance (MMR) and Cohere rerank did not exhibit notable advantages over a baseline Naive RAG system, and Multi-query approaches underperformed. Sentence Window Retrieval emerged as the most effective for retrieval precision, despite its variable performance on answer similarity. The study confirms the potential of the Document Summary Index as a competent retrieval approach. All resources related to this research are publicly accessible for further investigation through our GitHub repository ARAGOG (https://github.com/predlico/ARAGOG). We welcome the community to further this exploratory study in RAG systems.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-04-01",
    "authors": [
      {
        "authorId": "2294361167",
        "name": "Matouvs Eibich"
      },
      {
        "authorId": "2294361283",
        "name": "Shivay Nagpal"
      },
      {
        "authorId": "2294362877",
        "name": "Alexander Fred-Ojala"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "7b56f03e742352c9aeca6714fdac30092ca7ddb5",
    "url": "https://www.semanticscholar.org/paper/7b56f03e742352c9aeca6714fdac30092ca7ddb5",
    "title": "Beyond Words: On Large Language Models Actionability in Mission-Critical Risk Analysis",
    "abstract": "Context. Risk analysis assesses potential risks in specific scenarios. Risk analysis principles are context-less; the same methodology can be applied to a risk connected to health and information technology security. Risk analysis requires a vast knowledge of national and international regulations and standards and is time and effort-intensive. A large language model can quickly summarize information in less time than a human and can be fine-tuned to specific tasks. Aim. Our empirical study aims to investigate the effectiveness of Retrieval-Augmented Generation and fine-tuned LLM in risk analysis. To our knowledge, no prior study has explored its capabilities in risk analysis. Method. We manually curated 193 unique scenarios leading to 1283 representative samples from over 50 mission-critical analyses archived by the industrial context team in the last five years. We compared the base GPT-3.5 and GPT-4 models versus their Retrieval-Augmented Generation and fine-tuned counterparts. We employ two human experts as competitors of the models and three other human experts to review the models and the former human experts' analysis. The reviewers analyzed 5,000 scenario analyses. Results and Conclusions. Human experts demonstrated higher accuracy, but LLMs are quicker and more actionable. Moreover, our findings show that RAG-assisted LLMs have the lowest hallucination rates, effectively uncovering hidden risks and complementing human expertise. Thus, the choice of model depends on specific needs, with FTMs for accuracy, RAG for hidden risks discovery, and base models for comprehensiveness and actionability. Therefore, experts can leverage LLMs as an effective complementing companion in risk analysis within a condensed timeframe. They can also save costs by averting unnecessary expenses associated with implementing unwarranted countermeasures.",
    "venue": "International Symposium on Empirical Software Engineering and Measurement",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-06-11",
    "authors": [
      {
        "authorId": "2180646859",
        "name": "Matteo Esposito"
      },
      {
        "authorId": "2293318055",
        "name": "Francesco Palagiano"
      },
      {
        "authorId": "2266468602",
        "name": "Valentina Lenarduzzi"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "4b0c63c86e8cdcc7c0f45246c92c3e98d77855d7",
    "url": "https://www.semanticscholar.org/paper/4b0c63c86e8cdcc7c0f45246c92c3e98d77855d7",
    "title": "RAG based Question-Answering for Contextual Response Prediction System",
    "abstract": "Large Language Models (LLMs) have shown versatility in various Natural Language Processing (NLP) tasks, including their potential as effective question-answering systems. However, to provide precise and relevant information in response to specific customer queries in industry settings, LLMs require access to a comprehensive knowledge base to avoid hallucinations. Retrieval Augmented Generation (RAG) emerges as a promising technique to address this challenge. Yet, developing an accurate question-answering framework for real-world applications using RAG entails several challenges: 1) data availability issues, 2) evaluating the quality of generated content, and 3) the costly nature of human evaluation. In this paper, we introduce an end-to-end framework that employs LLMs with RAG capabilities for industry use cases. Given a customer query, the proposed system retrieves relevant knowledge documents and leverages them, along with previous chat history, to generate response suggestions for customer service agents in the contact centers of a major retail company. Through comprehensive automated and human evaluations, we show that this solution outperforms the current BERT-based algorithms in accuracy and relevance. Our findings suggest that RAG-based LLMs can be an excellent support to human customer service representatives by lightening their workload.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-05",
    "authors": [
      {
        "authorId": "2319823357",
        "name": "Sriram Veturi"
      },
      {
        "authorId": "1491319666",
        "name": "Saurabh Vaichal"
      },
      {
        "authorId": "2319830658",
        "name": "Reshma Lal Jagadheesh"
      },
      {
        "authorId": "66674465",
        "name": "Nafis Irtiza Tripto"
      },
      {
        "authorId": "2319771117",
        "name": "Nian Yan"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "839b4ea7a9a295652dc8eb79d2031f16e4f84a3c",
    "url": "https://www.semanticscholar.org/paper/839b4ea7a9a295652dc8eb79d2031f16e4f84a3c",
    "title": "FaaF: Facts as a Function for the evaluation of generated text",
    "abstract": "The demand for accurate and efficient verification of information in texts generated by large language models (LMs) is at an all-time high, but remains unresolved. Recent efforts have focused on extracting and verifying atomic facts from these texts via prompting LM evaluators. However, we demonstrate that this method of prompting is unreliable when faced with incomplete or inaccurate reference information. We introduce Facts as a Function (FaaF), a new approach to the fact verification task that leverages the function-calling capabilities of LMs. FaaF significantly enhances the ability of LMs to identify unsupported facts in texts, while also improving efficiency and significantly lowering costs compared to prompt-based methods. Additionally, we propose a framework for evaluating factual recall in Retrieval Augmented Generation (RAG) systems, which we employ to compare prompt-based and FaaF methods using various LMs under challenging conditions.",
    "venue": "",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2024-03-06",
    "authors": [
      {
        "authorId": "116318302",
        "name": "Vasileios Katranidis"
      },
      {
        "authorId": "2290079689",
        "name": "Gabor Barany"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "d3e916564f8d009266df3eeef0ad75d3f83b6813",
    "url": "https://www.semanticscholar.org/paper/d3e916564f8d009266df3eeef0ad75d3f83b6813",
    "title": "Benchmarking Cognitive Domains for LLMS: Insights from Taiwanese Hakka Culture",
    "abstract": "This study introduces a comprehensive benchmark designed to evaluate the performance of large language models (LLMs) in understanding and processing cultural knowledge, with a specific focus on Hakka culture as a case study. Leveraging Bloom's Taxonomy, the study develops a multi-dimensional framework that systematically assesses LLMs across six cog-nitive domains: Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating. This benchmark ex-tends beyond traditional single-dimensional evaluations by providing a deeper analysis of LLMs' abilities to handle cul-turally specific content, ranging from basic recall of facts to higher-order cognitive tasks such as creative synthesis. Ad-ditionally, the study integrates Retrieval-Augmented Generation (RAG) technology to address the challenges of minority cultural knowledge representation in LLMs, demonstrating how RAG enhances the models' performance by dynamically incorporating relevant external information. The results high-light the effectiveness of RAG in improving accuracy across all cognitive domains, particularly in tasks requiring precise retrieval and application of cultural know ledge. However, the findings also reveal the limitations of RAG in creative tasks, underscoring the need for further optimization. This benchmark provides a robust tool for evaluating and com-paring LLMs in culturally diverse contexts, offering valuable insights for future research and development in AI -driven cultural knowledge preservation and dissemination.",
    "venue": "Oriental COCOSDA International Conference on Speech Database and Assessments",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2409.01556",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-09-03",
    "authors": [
      {
        "authorId": "2319451926",
        "name": "Chen-Chi Chang"
      },
      {
        "authorId": "2319404247",
        "name": "Ching-Yuan Chen"
      },
      {
        "authorId": "2319390299",
        "name": "Hung-Shin Lee"
      },
      {
        "authorId": "2319878416",
        "name": "Chih-Cheng Lee"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "26e494fba108bac068c00dd867eb16b834cf3453",
    "url": "https://www.semanticscholar.org/paper/26e494fba108bac068c00dd867eb16b834cf3453",
    "title": "HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA",
    "abstract": "As language model agents leveraging external tools rapidly evolve, significant progress has been made in question-answering(QA) methodologies utilizing supplementary documents and the Retrieval-Augmented Generation (RAG) approach. This advancement has improved the response quality of language models and alleviates the appearance of hallucination. However, these methods exhibit limited retrieval accuracy when faced with massive indistinguishable documents, presenting notable challenges in their practical application. In response to these emerging challenges, we present HiQA, an advanced framework for multi-document question-answering (MDQA) that integrates cascading metadata into content as well as a multi-route retrieval mechanism. We also release a benchmark called MasQA to evaluate and research in MDQA. Finally, HiQA demonstrates the state-of-the-art performance in multi-document environments.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2282547167",
        "name": "Xinyue Chen"
      },
      {
        "authorId": "2282543521",
        "name": "Pengyu Gao"
      },
      {
        "authorId": "2282672210",
        "name": "Jiangjiang Song"
      },
      {
        "authorId": "2282920674",
        "name": "Xiaoyang Tan"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "bd0b0cd5b01b3a9b62aacda5fb1b8f357e30e533",
    "url": "https://www.semanticscholar.org/paper/bd0b0cd5b01b3a9b62aacda5fb1b8f357e30e533",
    "title": "Evaluating and Enhancing Large Language Models‚Äô Performance in Domain-Specific Medicine: Development and Usability Study With DocOA",
    "abstract": "Background The efficacy of large language models (LLMs) in domain-specific medicine, particularly for managing complex diseases such as osteoarthritis (OA), remains largely unexplored. Objective This study focused on evaluating and enhancing the clinical capabilities and explainability of LLMs in specific domains, using OA management as a case study. Methods A domain-specific benchmark framework was developed to evaluate LLMs across a spectrum from domain-specific knowledge to clinical applications in real-world clinical scenarios. DocOA, a specialized LLM designed for OA management integrating retrieval-augmented generation and instructional prompts, was developed. It can identify the clinical evidence upon which its answers are based through retrieval-augmented generation, thereby demonstrating the explainability of those answers. The study compared the performance of GPT-3.5, GPT-4, and a specialized assistant, DocOA, using objective and human evaluations. Results Results showed that general LLMs such as GPT-3.5 and GPT-4 were less effective in the specialized domain of OA management, particularly in providing personalized treatment recommendations. However, DocOA showed significant improvements. Conclusions This study introduces a novel benchmark framework that assesses the domain-specific abilities of LLMs in multiple aspects, highlights the limitations of generalized LLMs in clinical contexts, and demonstrates the potential of tailored approaches for developing domain-specific medical LLMs.",
    "venue": "Journal of Medical Internet Research",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://s3.ca-central-1.amazonaws.com/assets.jmir.org/assets/preprints/preprint-58158-accepted.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-07",
    "authors": [
      {
        "authorId": "2242332878",
        "name": "Xi Chen"
      },
      {
        "authorId": "2272458208",
        "name": "Li Wang"
      },
      {
        "authorId": "82207389",
        "name": "M. You"
      },
      {
        "authorId": "2280884151",
        "name": "Weizhi Liu"
      },
      {
        "authorId": "2274354702",
        "name": "Yu Fu"
      },
      {
        "authorId": "2280904043",
        "name": "Jie Xu"
      },
      {
        "authorId": "2304579494",
        "name": "Shaiting Zhang"
      },
      {
        "authorId": "2249082125",
        "name": "Gang Chen"
      },
      {
        "authorId": "2275102924",
        "name": "Kang Li"
      },
      {
        "authorId": "2268769761",
        "name": "Jian Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "e970d243e9fb56100a0371733466ad887a038938",
    "url": "https://www.semanticscholar.org/paper/e970d243e9fb56100a0371733466ad887a038938",
    "title": "Chat2Data: An Interactive Data Analysis System with RAG, Vector Databases and LLMs",
    "abstract": "\n Traditional data analysis methods require users to write programming codes or issue SQL queries to analyze the data, which are inconvenient for ordinary users. Large language models (LLMs) can alleviate these limitations by enabling users to interact with the data with natural language (NL), e.g., result retrieval and summarization for unstructured data and transforming the NL text to SQL queries or codes for structured data. However, existing LLMs have three limitations: hallucination (due to lacking domain knowledge for vertical domains), high cost for LLM reasoning, and low accuracy for complicated tasks. To address these problems, we propose a prototype, Chat2Data, to interactively analyze the data with natural language. Chat2Data adopts a three-layer method, where the first layer uses Retrieval-Augmented Generation (RAG) to embed domain knowledge in order to address the hallucination problem, the second layer utilizes vector databases to reduce the number of interactions with LLMs so as to improve the performance, and the third layer designs a pipeline agent to decompose a complex task to multiple subtasks and use multiple round reasoning to generate the results in order to improve the accuracy of LLMs. We demonstrate Chat2Data with two real scenarios, unstructured data retrieval and summarization, and natural language-based structured data analysis. The online demo is available at\n http://vdemo.dbmind.cn.\n",
    "venue": "Proceedings of the VLDB Endowment",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-01",
    "authors": [
      {
        "authorId": "2123787551",
        "name": "Xinyang Zhao"
      },
      {
        "authorId": "50177381",
        "name": "Xuanhe Zhou"
      },
      {
        "authorId": "2108491595",
        "name": "Guoliang Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "7efb7421c5c51009630e6858ec70fbebda8e5f03",
    "url": "https://www.semanticscholar.org/paper/7efb7421c5c51009630e6858ec70fbebda8e5f03",
    "title": "Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata",
    "abstract": "The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-19",
    "authors": [
      {
        "authorId": "2307469414",
        "name": "Mykhailo Poliakov"
      },
      {
        "authorId": "29407704",
        "name": "N. Shvai"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "eee3bf09bcc997a3586e29f7d3c3d37a42aff87d",
    "url": "https://www.semanticscholar.org/paper/eee3bf09bcc997a3586e29f7d3c3d37a42aff87d",
    "title": "FaithEval: Can Your Language Model Stay Faithful to Context, Even If \"The Moon is Made of Marshmallows\"",
    "abstract": "Ensuring faithfulness to context in large language models (LLMs) and retrieval-augmented generation (RAG) systems is crucial for reliable deployment in real-world applications, as incorrect or unsupported information can erode user trust. Despite advancements on standard benchmarks, faithfulness hallucination-where models generate responses misaligned with the provided context-remains a significant challenge. In this work, we introduce FaithEval, a novel and comprehensive benchmark tailored to evaluate the faithfulness of LLMs in contextual scenarios across three diverse tasks: unanswerable, inconsistent, and counterfactual contexts. These tasks simulate real-world challenges where retrieval mechanisms may surface incomplete, contradictory, or fabricated information. FaithEval comprises 4.9K high-quality problems in total, validated through a rigorous four-stage context construction and validation framework, employing both LLM-based auto-evaluation and human validation. Our extensive study across a wide range of open-source and proprietary models reveals that even state-of-the-art models often struggle to remain faithful to the given context, and that larger models do not necessarily exhibit improved faithfulness.Project is available at: \\url{https://github.com/SalesforceAIResearch/FaithEval}.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-30",
    "authors": [
      {
        "authorId": "2321601631",
        "name": "Yifei Ming"
      },
      {
        "authorId": "3234247",
        "name": "Senthil Purushwalkam"
      },
      {
        "authorId": "1824294087",
        "name": "Shrey Pandit"
      },
      {
        "authorId": "2321405223",
        "name": "Zixuan Ke"
      },
      {
        "authorId": "1399659909",
        "name": "Xuan-Phi Nguyen"
      },
      {
        "authorId": "2267728986",
        "name": "Caiming Xiong"
      },
      {
        "authorId": "2313536726",
        "name": "Shafiq Joty"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "c7294ac75adfaa120a39d4461c2772e6f3c756e6",
    "url": "https://www.semanticscholar.org/paper/c7294ac75adfaa120a39d4461c2772e6f3c756e6",
    "title": "Multi-Agent RAG Chatbot Architecture for Decision Support in Net-Zero Emission Energy Systems",
    "abstract": "Modern energy platforms are increasingly leveraging Artificial Intelligence (AI) for effective decision-making and efficient operations. This has led to the development of expansive data spaces that comprise both structured and unstructured energy data in various modalities. Conversational agents with the most recent advancements in Large Language Models (LLM) are primed to facilitate the efficient retrieval of this diverse information for decision support. In this paper, we propose a multi-agent chatbot architecture for decision support in net-zero emissions energy systems, leveraging LLMs and Retrieval-Augmented Generation (RAG). This architecture consists of a Chatbot User Interface (UI), an advanced Natural Language Understanding (NLU) module for precise entity and intent recognition, a robust Chatbot Core with four specialized agents: Observer, Knowledge Retriever, Behavior Analyzer, and Visualizer and Response Construction Module. These components work together to address diverse decision support needs in energy environments, specifically for net zero carbon emissions initiatives that need to consider diverse parameters and large volumes of data. We showcase the chatbot's successful integration and evaluation for decision support in the net-zero emissions energy system of a large tertiary education institution.",
    "venue": "International Conference on Industrial Technology",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-25",
    "authors": [
      {
        "authorId": "102727201",
        "name": "Gihan Gamage"
      },
      {
        "authorId": "47528865",
        "name": "Nishan Mills"
      },
      {
        "authorId": "144286739",
        "name": "Daswin De Silva"
      },
      {
        "authorId": "2185595070",
        "name": "Milos Manic"
      },
      {
        "authorId": "2027038075",
        "name": "Harsha Moraliyage"
      },
      {
        "authorId": "2123238050",
        "name": "Andrew Jennings"
      },
      {
        "authorId": "143775049",
        "name": "D. Alahakoon"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "1b691a13d38a53733e4c9c9aebda4ea102368660",
    "url": "https://www.semanticscholar.org/paper/1b691a13d38a53733e4c9c9aebda4ea102368660",
    "title": "CrAM: Credibility-Aware Attention Modification in LLMs for Combating Misinformation in RAG",
    "abstract": "Retrieval-Augmented Generation (RAG) can alleviate hallucinations of Large Language Models (LLMs) by referencing external documents. However, the misinformation in external documents may mislead LLMs' generation. To address this issue, we explore the task of\"credibility-aware RAG\", in which LLMs automatically adjust the influence of retrieved documents based on their credibility scores to counteract misinformation. To this end, we introduce a plug-and-play method named $\\textbf{Cr}$edibility-aware $\\textbf{A}$ttention $\\textbf{M}$odification (CrAM). CrAM identifies influential attention heads in LLMs and adjusts their attention weights based on the credibility of the documents, thereby reducing the impact of low-credibility documents. Experiments on Natual Questions and TriviaQA using Llama2-13B, Llama3-8B, and Qwen1.5-7B show that CrAM improves the RAG performance of LLMs against misinformation pollution by over 20%, even surpassing supervised fine-tuning methods.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-17",
    "authors": [
      {
        "authorId": "2260342358",
        "name": "Boyi Deng"
      },
      {
        "authorId": "2117833732",
        "name": "Wenjie Wang"
      },
      {
        "authorId": "31734386",
        "name": "Fengbin Zhu"
      },
      {
        "authorId": "2260433198",
        "name": "Qifan Wang"
      },
      {
        "authorId": "2280911299",
        "name": "Fuli Feng"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "650428d6b779f00bdb692d4e146b7aac06577aeb",
    "url": "https://www.semanticscholar.org/paper/650428d6b779f00bdb692d4e146b7aac06577aeb",
    "title": "Dynamic Contexts for Generating Suggestion Questions in RAG Based Conversational Systems",
    "abstract": "When interacting with Retrieval-Augmented Generation (RAG)-based conversational agents, the users must carefully craft their queries to be understood correctly. Yet, understanding the system's capabilities can be challenging for the users, leading to ambiguous questions that necessitate further clarification. This work aims to bridge the gap by developing a suggestion question generator. To generate suggestion questions, our approach involves utilizing dynamic context, which includes both dynamic few-shot examples and dynamically retrieved contexts. Through experiments, we show that the dynamic contexts approach can generate better suggestion questions as compared to other prompting approaches.",
    "venue": "The Web Conference",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3589335.3651905",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-18",
    "authors": [
      {
        "authorId": "41051323",
        "name": "Anuja Tayal"
      },
      {
        "authorId": "2292029192",
        "name": "Aman Tyagi"
      }
    ],
    "source": "semantic_scholar",
    "score": 85.79441541679836
  },
  {
    "paperId": "ffc6dba31ba734e6eef98645364ea241c470f63e",
    "url": "https://www.semanticscholar.org/paper/ffc6dba31ba734e6eef98645364ea241c470f63e",
    "title": "From RAGs to rich parameters: Probing how language models utilize external knowledge over parametric information for factual queries",
    "abstract": "Retrieval Augmented Generation (RAG) enriches the ability of language models to reason using external context to augment responses for a given user prompt. This approach has risen in popularity due to practical applications in various applications of language models in search, question/answering, and chat-bots. However, the exact nature of how this approach works isn't clearly understood. In this paper, we mechanistically examine the RAG pipeline to highlight that language models take shortcut and have a strong bias towards utilizing only the context information to answer the question, while relying minimally on their parametric memory. We probe this mechanistic behavior in language models with: (i) Causal Mediation Analysis to show that the parametric memory is minimally utilized when answering a question and (ii) Attention Contributions and Knockouts to show that the last token residual stream do not get enriched from the subject token in the question, but gets enriched from other informative tokens in the context. We find this pronounced shortcut behaviour true across both LLaMa and Phi family of models.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-18",
    "authors": [
      {
        "authorId": "123351917",
        "name": "Hitesh Wadhwa"
      },
      {
        "authorId": "2307071990",
        "name": "Rahul Seetharaman"
      },
      {
        "authorId": "2307073959",
        "name": "Somyaa Aggarwal"
      },
      {
        "authorId": "2261733601",
        "name": "Reshmi Ghosh"
      },
      {
        "authorId": "2114710333",
        "name": "Samyadeep Basu"
      },
      {
        "authorId": "2261733701",
        "name": "Soundararajan Srinivasan"
      },
      {
        "authorId": "2307415582",
        "name": "Wenlong Zhao"
      },
      {
        "authorId": "2307073525",
        "name": "Shreyas Chaudhari"
      },
      {
        "authorId": "2126496227",
        "name": "Ehsan Aghazadeh"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "f321cce0932f2a257849a195c0cac1194a8b11d7",
    "url": "https://www.semanticscholar.org/paper/f321cce0932f2a257849a195c0cac1194a8b11d7",
    "title": "SFR-RAG: Towards Contextually Faithful LLMs",
    "abstract": "Retrieval Augmented Generation (RAG), a paradigm that integrates external contextual information with large language models (LLMs) to enhance factual accuracy and relevance, has emerged as a pivotal area in generative AI. The LLMs used in RAG applications are required to faithfully and completely comprehend the provided context and users' questions, avoid hallucination, handle unanswerable, counterfactual or otherwise low-quality and irrelevant contexts, perform complex multi-hop reasoning and produce reliable citations. In this paper, we introduce SFR-RAG, a small LLM that is instruction-tuned with an emphasis on context-grounded generation and hallucination minimization. We also present ContextualBench, a new evaluation framework compiling multiple popular and diverse RAG benchmarks, such as HotpotQA and TriviaQA, with consistent RAG settings to ensure reproducibility and consistency in model assessments. Experimental results demonstrate that our SFR-RAG-9B model outperforms leading baselines such as Command-R+ (104B) and GPT-4o, achieving state-of-the-art results in 3 out of 7 benchmarks in ContextualBench with significantly fewer parameters. The model is also shown to be resilient to alteration in the contextual information and behave appropriately when relevant context is removed. Additionally, the SFR-RAG model maintains competitive performance in general instruction-following tasks and function-calling capabilities.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-16",
    "authors": [
      {
        "authorId": "1399659909",
        "name": "Xuan-Phi Nguyen"
      },
      {
        "authorId": "1824294087",
        "name": "Shrey Pandit"
      },
      {
        "authorId": "3234247",
        "name": "Senthil Purushwalkam"
      },
      {
        "authorId": "2064635172",
        "name": "Austin Xu"
      },
      {
        "authorId": "2258571998",
        "name": "Hailin Chen"
      },
      {
        "authorId": "2321601631",
        "name": "Yifei Ming"
      },
      {
        "authorId": "2321405223",
        "name": "Zixuan Ke"
      },
      {
        "authorId": "2238207181",
        "name": "Silvio Savarese"
      },
      {
        "authorId": "2321408694",
        "name": "Caiming Xong"
      },
      {
        "authorId": "2313536726",
        "name": "Shafiq Joty"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "8dfb4349a11c503aa0228791575ee1929f808c14",
    "url": "https://www.semanticscholar.org/paper/8dfb4349a11c503aa0228791575ee1929f808c14",
    "title": "Conversing with business process-aware large language models: the BPLLM framework",
    "abstract": null,
    "venue": "Journal of Intelligence and Information Systems",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-04",
    "authors": [
      {
        "authorId": "1689571",
        "name": "M. Bernardi"
      },
      {
        "authorId": "2299549691",
        "name": "Angelo Casciani"
      },
      {
        "authorId": "1706324",
        "name": "Marta Cimitile"
      },
      {
        "authorId": "2299549225",
        "name": "Andrea Marrella"
      }
    ],
    "source": "semantic_scholar",
    "score": 70.79441541679836
  },
  {
    "paperId": "fbabfcb083183dd48b3fdd7c7401637dbb066b2c",
    "url": "https://www.semanticscholar.org/paper/fbabfcb083183dd48b3fdd7c7401637dbb066b2c",
    "title": "A + B: A General Generator-Reader Framework for Optimizing LLMs to Unleash Synergy Potential",
    "abstract": "Retrieval-Augmented Generation (RAG) is an effective solution to supplement necessary knowledge to large language models (LLMs). Targeting its bottleneck of retriever performance,\"generate-then-read\"pipeline is proposed to replace the retrieval stage with generation from the LLM itself. Although promising, this research direction is underexplored and still cannot work in the scenario when source knowledge is given. In this paper, we formalize a general\"A + B\"framework with varying combinations of foundation models and types for systematic investigation. We explore the efficacy of the base and chat versions of LLMs and found their different functionalities suitable for generator A and reader B, respectively. Their combinations consistently outperform single models, especially in complex scenarios. Furthermore, we extend the application of the\"A + B\"framework to scenarios involving source documents through continuous learning, enabling the direct integration of external knowledge into LLMs. This approach not only facilitates effective acquisition of new knowledge but also addresses the challenges of safety and helpfulness post-adaptation. The paper underscores the versatility of the\"A + B\"framework, demonstrating its potential to enhance the practical application of LLMs across various domains.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-06",
    "authors": [
      {
        "authorId": "2116724274",
        "name": "Wei Tang"
      },
      {
        "authorId": "2275189609",
        "name": "Yixin Cao"
      },
      {
        "authorId": "2249532957",
        "name": "Jiahao Ying"
      },
      {
        "authorId": "2217713470",
        "name": "Bo Wang"
      },
      {
        "authorId": "2109814746",
        "name": "Yuyue Zhao"
      },
      {
        "authorId": "2220696660",
        "name": "Yong Liao"
      },
      {
        "authorId": "2241500995",
        "name": "Pengyuan Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "13241422a6fef4e41dbb4d0f3e40e39363e07826",
    "url": "https://www.semanticscholar.org/paper/13241422a6fef4e41dbb4d0f3e40e39363e07826",
    "title": "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information",
    "abstract": "Large Vision-Language Models (LVLMs) have become pivotal at the intersection of computer vision and natural language processing. However, the full potential of LVLMs‚Äô Retrieval-Augmented Generation (RAG) capabilities remains underutilized. Existing works either focus solely on the text modality or are limited to specific tasks. Moreover, most LVLMs struggle to selectively utilize retrieved information and are sensitive to irrelevant or misleading references. To address these challenges, we propose a self-refinement framework designed to teach LVLMs to Selectively Utilize Retrieved Information (SURf). Specifically, when given questions that are incorrectly answered by the LVLM backbone, we obtain references that help correct the answers (positive references) and those that do not (negative references). We then fine-tune the LVLM backbone using a combination of these positive and negative references. Our experiments across three tasks and seven datasets demonstrate that our framework significantly enhances LVLMs‚Äô ability to effectively utilize retrieved multimodal references and improves their robustness against irrelevant or misleading information. The source code is available at https://anonymous.4open.science/r/SURf-6433.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-09-21",
    "authors": [
      {
        "authorId": "2202454665",
        "name": "Jiashuo Sun"
      },
      {
        "authorId": "2284727955",
        "name": "Jihai Zhang"
      },
      {
        "authorId": "2110348767",
        "name": "Yucheng Zhou"
      },
      {
        "authorId": "2720303",
        "name": "Zhao-yu Su"
      },
      {
        "authorId": "2262446609",
        "name": "Xiaoye Qu"
      },
      {
        "authorId": "2307325455",
        "name": "Yu Cheng"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "494211ae8f2f19f67478df4624a3e9107754aa7c",
    "url": "https://www.semanticscholar.org/paper/494211ae8f2f19f67478df4624a3e9107754aa7c",
    "title": "RAG Does Not Work for Enterprises",
    "abstract": "Retrieval-Augmented Generation (RAG) improves the accuracy and relevance of large language model outputs by incorporating knowledge retrieval. However, implementing RAG in enterprises poses challenges around data security, accuracy, scalability, and integration. This paper explores the unique requirements for enterprise RAG, surveys current approaches and limitations, and discusses potential advances in semantic search, hybrid queries, and optimized retrieval. It proposes an evaluation framework to validate enterprise RAG solutions, including quantitative testing, qualitative analysis, ablation studies, and industry case studies. This framework aims to help demonstrate the ability of purpose-built RAG architectures to deliver accuracy and relevance improvements with enterprise-grade security, compliance and integration. The paper concludes with implications for enterprise deployments, limitations, and future research directions. Close collaboration between researchers and industry partners may accelerate progress in developing and deploying retrieval-augmented generation technology.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-05-31",
    "authors": [
      {
        "authorId": "8325245",
        "name": "T. Bruckhaus"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "9279c19ebeaf8673412eefcafb8032b1a71b920f",
    "url": "https://www.semanticscholar.org/paper/9279c19ebeaf8673412eefcafb8032b1a71b920f",
    "title": "Pencils Down! Automatic Rubric-based Evaluation of Retrieve/Generate Systems",
    "abstract": "Current IR evaluation paradigms are challenged by large language models (LLMs) and retrieval-augmented generation (RAG) meth-ods. Furthermore, evaluation either resorts to expensive human judgments or lead to an over-reliance on LLMs. To remedy this situation, we introduce the RUBRIC metric, which puts information retrieval systems to the proverbial test. This metric leverages a bank of query-related test questions to quantify relevant information content that is contained in the systems‚Äô responses. The process involves (1) decomposing the query into detailed questions, and (2) checking each for answerability using passages in the system response. Using three TREC benchmarks, we demonstrate that our LLM-based RUBRIC approach works successfully. Unlike previous LLM-based evaluation measures, our paradigm lends itself for incorporating a human-in-the-loop while avoiding some pitfalls of over-reliance on AI or resorting to expensive manual passage-level judgments. Moreover, our evaluation is repeatable and extensible and can be scored with existing evaluation tools. 1",
    "venue": "International Conference on the Theory of Information Retrieval",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3664190.3672511",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-02",
    "authors": [
      {
        "authorId": "2282137842",
        "name": "Naghmeh Farzi"
      },
      {
        "authorId": "2302798511",
        "name": "Laura Dietz"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "c4aeec57b9ad4fa36cbd9bdad05dbbbd340183df",
    "url": "https://www.semanticscholar.org/paper/c4aeec57b9ad4fa36cbd9bdad05dbbbd340183df",
    "title": "ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling",
    "abstract": "Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks. However, existing retrievers are often misaligned with LLMs due to their separate training processes and the black-box nature of LLMs. To address this challenge, we propose ARL2, a retriever learning technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and score relevant evidence, enabling learning the retriever from robust LLM supervision. Furthermore, ARL2 uses an adaptive self-training strategy for curating high-quality and diverse relevance data, which can effectively reduce the annotation cost. Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared to the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer learning capabilities and strong zero-shot generalization abilities. Our code will be published at \\url{https://github.com/zhanglingxi-cs/ARL2}.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-21",
    "authors": [
      {
        "authorId": "2284884352",
        "name": "Lingxi Zhang"
      },
      {
        "authorId": "2259265562",
        "name": "Yue Yu"
      },
      {
        "authorId": "2284955273",
        "name": "Kuan Wang"
      },
      {
        "authorId": "145657504",
        "name": "Chao Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "4aa34e1298d46a990ac0286eb8d25ca2e5b44e98",
    "url": "https://www.semanticscholar.org/paper/4aa34e1298d46a990ac0286eb8d25ca2e5b44e98",
    "title": "Answering real-world clinical questions using large language model based systems",
    "abstract": "Evidence to guide healthcare decisions is often limited by a lack of relevant and trustworthy literature as well as difficulty in contextualizing existing research for a specific patient. Large language models (LLMs) could potentially address both challenges by either summarizing published literature or generating new studies based on real-world data (RWD). We evaluated the ability of five LLM-based systems in answering 50 clinical questions and had nine independent physicians review the responses for relevance, reliability, and actionability. As it stands, general-purpose LLMs (ChatGPT-4, Claude 3 Opus, Gemini Pro 1.5) rarely produced answers that were deemed relevant and evidence-based (2% - 10%). In contrast, retrieval augmented generation (RAG)-based and agentic LLM systems produced relevant and evidence-based answers for 24% (OpenEvidence) to 58% (ChatRWD) of questions. Only the agentic ChatRWD was able to answer novel questions compared to other LLMs (65% vs. 0-9%). These results suggest that while general-purpose LLMs should not be used as-is, a purpose-built system for evidence summarization based on RAG and one for generating novel evidence working synergistically would improve availability of pertinent evidence for patient care.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-06-29",
    "authors": [
      {
        "authorId": "2281891862",
        "name": "Y. Low"
      },
      {
        "authorId": "2309176736",
        "name": "Michael L. Jackson"
      },
      {
        "authorId": "2309172833",
        "name": "Rebecca J. Hyde"
      },
      {
        "authorId": "2309217348",
        "name": "Robert E. Brown"
      },
      {
        "authorId": "47832570",
        "name": "Neil M. Sanghavi"
      },
      {
        "authorId": "2309172019",
        "name": "Julian D. Baldwin"
      },
      {
        "authorId": "2309172444",
        "name": "C. W. Pike"
      },
      {
        "authorId": "2261167578",
        "name": "Jananee Muralidharan"
      },
      {
        "authorId": "2309173047",
        "name": "Gavin Hui"
      },
      {
        "authorId": "2309172725",
        "name": "Natasha Alexander"
      },
      {
        "authorId": "2292714687",
        "name": "Hadeel Hassan"
      },
      {
        "authorId": "6343521",
        "name": "R. Nene"
      },
      {
        "authorId": "2309175312",
        "name": "Morgan Pike"
      },
      {
        "authorId": "2255564743",
        "name": "Courtney J. Pokrzywa"
      },
      {
        "authorId": "46926217",
        "name": "Shivam Vedak"
      },
      {
        "authorId": "2309175575",
        "name": "Adam Paul Yan"
      },
      {
        "authorId": "2308086997",
        "name": "Dong-han Yao"
      },
      {
        "authorId": "1816765342",
        "name": "A. Zipursky"
      },
      {
        "authorId": "2309171331",
        "name": "Christina Dinh"
      },
      {
        "authorId": "2309173916",
        "name": "Philip Ballentine"
      },
      {
        "authorId": "2282501753",
        "name": "D. Derieg"
      },
      {
        "authorId": "1394589076",
        "name": "Vladimir Polony"
      },
      {
        "authorId": "11354336",
        "name": "Rehan N. Chawdry"
      },
      {
        "authorId": "2309178572",
        "name": "Jordan Davies"
      },
      {
        "authorId": "39930868",
        "name": "Brigham B. Hyde"
      },
      {
        "authorId": "2262093570",
        "name": "Nigam H. Shah"
      },
      {
        "authorId": "2282503109",
        "name": "S. Gombar"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "9c4a05000c0d63cf8e157172b2ca64c66f936165",
    "url": "https://www.semanticscholar.org/paper/9c4a05000c0d63cf8e157172b2ca64c66f936165",
    "title": "Poisoned LangChain: Jailbreak LLMs by LangChain",
    "abstract": "With the development of natural language processing (NLP), large language models (LLMs) are becoming increasingly popular. LLMs are integrating more into everyday life, raising public concerns about their security vulnerabilities. Consequently, the security of large language models is becoming critically important. Currently, the techniques for attacking and defending against LLMs are continuously evolving. One significant method type of attack is the jailbreak attack, which designed to evade model safety mechanisms and induce the generation of inappropriate content. Existing jailbreak attacks primarily rely on crafting inducement prompts for direct jailbreaks, which are less effective against large models with robust filtering and high comprehension abilities. Given the increasing demand for real-time capabilities in large language models, real-time updates and iterations of new knowledge have become essential. Retrieval-Augmented Generation (RAG), an advanced technique to compensate for the model's lack of new knowledge, is gradually becoming mainstream. As RAG enables the model to utilize external knowledge bases, it provides a new avenue for jailbreak attacks. In this paper, we conduct the first work to propose the concept of indirect jailbreak and achieve Retrieval-Augmented Generation via LangChain. Building on this, we further design a novel method of indirect jailbreak attack, termed Poisoned-LangChain (PLC), which leverages a poisoned external knowledge base to interact with large language models, thereby causing the large models to generate malicious non-compliant dialogues.We tested this method on six different large language models across three major categories of jailbreak issues. The experiments demonstrate that PLC successfully implemented indirect jailbreak attacks under three different scenarios, achieving success rates of 88.56%, 79.04%, and 82.69% respectively.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-26",
    "authors": [
      {
        "authorId": "2308349277",
        "name": "Ziqiu Wang"
      },
      {
        "authorId": "2272273477",
        "name": "Jun Liu"
      },
      {
        "authorId": "2308668247",
        "name": "Shengkai Zhang"
      },
      {
        "authorId": "2272010117",
        "name": "Yang Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "e6a72ac42c4785ea187e29be8ffcf53efe0ad86a",
    "url": "https://www.semanticscholar.org/paper/e6a72ac42c4785ea187e29be8ffcf53efe0ad86a",
    "title": "Overcoming LLM Challenges using RAG-Driven Precision in Coffee Leaf Disease Remediation",
    "abstract": "This research introduces an innovative AI-driven precision agriculture system, leveraging YOLOv8 for disease identification and Retrieval Augmented Generation (RAG) for context-aware diagnosis. Focused on addressing the challenges of diseases affecting the coffee production sector in Karnataka, The system integrates sophisticated object detection techniques with language models to address the inherent constraints associated with Large Language Models (LLMs). Our methodology not only tackles the issue of hallucinations in LLMs, but also introduces dynamic disease identification and remediation strategies. Real-time monitoring, collaborative dataset expansion, and organizational involvement ensure the system‚Äôs adaptability in diverse agricultural settings. The effect of the suggested system extends beyond automation, aiming to secure food supplies, protect livelihoods, and promote eco-friendly farming practices. By facilitating precise disease identification, the system contributes to sustainable and environmentally conscious agriculture, reducing reliance on pesticides. Looking to the future, the project envisions continuous development in RAG-integrated object detection systems, emphasizing scalability, reliability, and usability. This research strives to be a beacon for positive change in agriculture, aligning with global efforts toward sustainable and technologically enhanced food production.",
    "venue": "2024 International Conference on Emerging Technologies in Computer Science for Interdisciplinary Applications (ICETCS)",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2405.01310",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-22",
    "authors": [
      {
        "authorId": "2279102441",
        "name": "Dr. Selva Kumar"
      },
      {
        "authorId": "2268491277",
        "name": "Imadh Ajaz Banday"
      },
      {
        "authorId": "2299334017",
        "name": "Vibha Venkatesh"
      },
      {
        "authorId": "2268647274",
        "name": "Afifah Khan"
      },
      {
        "authorId": "2279080179",
        "name": "Mohammed Ajmal Khan"
      },
      {
        "authorId": "2299325763",
        "name": "Manikantha Gada"
      }
    ],
    "source": "semantic_scholar",
    "score": 100.79441541679836
  },
  {
    "paperId": "547ce91f3b7390c8887f91df8a7f97217fff7bf2",
    "url": "https://www.semanticscholar.org/paper/547ce91f3b7390c8887f91df8a7f97217fff7bf2",
    "title": "EyeGPT: Ophthalmic Assistant with Large Language Models",
    "abstract": "Artificial intelligence (AI) has gained significant attention in healthcare consultation due to its potential to improve clinical workflow and enhance medical communication. However, owing to the complex nature of medical information, large language models (LLM) trained with general world knowledge might not possess the capability to tackle medical-related tasks at an expert level. Here, we introduce EyeGPT, a specialized LLM designed specifically for ophthalmology, using three optimization strategies including role-playing, finetuning, and retrieval-augmented generation. In particular, we proposed a comprehensive evaluation framework that encompasses a diverse dataset, covering various subspecialties of ophthalmology, different users, and diverse inquiry intents. Moreover, we considered multiple evaluation metrics, including accuracy, understandability, trustworthiness, empathy, and the proportion of hallucinations. By assessing the performance of different EyeGPT variants, we identify the most effective one, which exhibits comparable levels of understandability, trustworthiness, and empathy to human ophthalmologists (all Ps>0.05). Overall, ur study provides valuable insights for future research, facilitating comprehensive comparisons and evaluations of different strategies for developing specialized LLMs in ophthalmology. The potential benefits include enhancing the patient experience in eye care and optimizing ophthalmologists' services.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-29",
    "authors": [
      {
        "authorId": "2289905784",
        "name": "Xiaolan Chen"
      },
      {
        "authorId": "2289901547",
        "name": "Ziwei Zhao"
      },
      {
        "authorId": "2243248038",
        "name": "Weiyi Zhang"
      },
      {
        "authorId": "2248954243",
        "name": "Pusheng Xu"
      },
      {
        "authorId": "2289901878",
        "name": "Le Gao"
      },
      {
        "authorId": "2290016908",
        "name": "Mingpu Xu"
      },
      {
        "authorId": "2289869805",
        "name": "Yue Wu"
      },
      {
        "authorId": "2289870904",
        "name": "Yinwen Li"
      },
      {
        "authorId": "2064971829",
        "name": "Danli Shi"
      },
      {
        "authorId": "2243857011",
        "name": "M. He"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "5815c1b86b7b818dfd77d96120fa3d33f9a4b803",
    "url": "https://www.semanticscholar.org/paper/5815c1b86b7b818dfd77d96120fa3d33f9a4b803",
    "title": "Evaluating and Enhancing Large Language Models Performance in Domain-specific Medicine: Osteoarthritis Management with DocOA",
    "abstract": "The efficacy of large language models (LLMs) in domain-specific medicine, particularly for managing complex diseases such as osteoarthritis (OA), remains largely unexplored. This study focused on evaluating and enhancing the clinical capabilities of LLMs in specific domains, using osteoarthritis (OA) management as a case study. A domain specific benchmark framework was developed, which evaluate LLMs across a spectrum from domain-specific knowledge to clinical applications in real-world clinical scenarios. DocOA, a specialized LLM tailored for OA management that integrates retrieval-augmented generation (RAG) and instruction prompts, was developed. The study compared the performance of GPT-3.5, GPT-4, and a specialized assistant, DocOA, using objective and human evaluations. Results showed that general LLMs like GPT-3.5 and GPT-4 were less effective in the specialized domain of OA management, particularly in providing personalized treatment recommendations. However, DocOA showed significant improvements. This study introduces a novel benchmark framework which assesses the domain-specific abilities of LLMs in multiple aspects, highlights the limitations of generalized LLMs in clinical contexts, and demonstrates the potential of tailored approaches for developing domain-specific medical LLMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-20",
    "authors": [
      {
        "authorId": "2242332878",
        "name": "Xi Chen"
      },
      {
        "authorId": "82207389",
        "name": "M. You"
      },
      {
        "authorId": "2272458208",
        "name": "Li Wang"
      },
      {
        "authorId": "2280884151",
        "name": "Weizhi Liu"
      },
      {
        "authorId": "2281005825",
        "name": "Yu Fu"
      },
      {
        "authorId": "2280904043",
        "name": "Jie Xu"
      },
      {
        "authorId": "2267720309",
        "name": "Shaoting Zhang"
      },
      {
        "authorId": "2249082125",
        "name": "Gang Chen"
      },
      {
        "authorId": "2253855199",
        "name": "Kang Li"
      },
      {
        "authorId": "2268769761",
        "name": "Jian Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "44d0c9a483b0af2f3952ae9acdde3d091472bc69",
    "url": "https://www.semanticscholar.org/paper/44d0c9a483b0af2f3952ae9acdde3d091472bc69",
    "title": "Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions",
    "abstract": "Generative search engines have the potential to transform how people seek information online, but generated responses from existing large language models (LLMs)-backed generative search engines may not always be accurate. Nonetheless, retrieval-augmented generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable part of a claim. To this end, we propose evaluating the robustness of generative search engines in the realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning incorrect responses. Through a comprehensive human evaluation of various generative search engines, such as Bing Chat, PerplexityAI, and YouChat across diverse queries, we demonstrate the effectiveness of adversarial factual questions in inducing incorrect responses. Moreover, retrieval-augmented generation exhibits a higher susceptibility to factual errors compared to LLMs without retrieval. These findings highlight the potential security risks of these systems and emphasize the need for rigorous evaluation before deployment.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-25",
    "authors": [
      {
        "authorId": "2109906988",
        "name": "Xuming Hu"
      },
      {
        "authorId": "2257084387",
        "name": "Xiaochuan Li"
      },
      {
        "authorId": "2223828460",
        "name": "Junzhe Chen"
      },
      {
        "authorId": "2295785468",
        "name": "Yinghui Li"
      },
      {
        "authorId": "2258649570",
        "name": "Yangning Li"
      },
      {
        "authorId": "2273814061",
        "name": "Xiaoguang Li"
      },
      {
        "authorId": "2136912252",
        "name": "Yasheng Wang"
      },
      {
        "authorId": "2275118289",
        "name": "Qun Liu"
      },
      {
        "authorId": "2114092431",
        "name": "Lijie Wen"
      },
      {
        "authorId": "2284755458",
        "name": "Philip S. Yu"
      },
      {
        "authorId": "2681038",
        "name": "Zhijiang Guo"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "5bb0a5b94fecf50533798d99a922a4ac49c1c482",
    "url": "https://www.semanticscholar.org/paper/5bb0a5b94fecf50533798d99a922a4ac49c1c482",
    "title": "EquinorQA: Large Language Models for Question Answering Over Proprietary Data",
    "abstract": ". Large Language Models (LLMs) have become the state-of-the-art technology in a variety of language understanding tasks. Accordingly, many commercial organizations have been increasingly trying to integrate LLMs in multiple areas of their production and analytics. A typical scenario is the need for answering questions over a domain-specific, private collection of documents, such that the answer is supported by evidence clearly referenced from those documents. The Retrieval-Augmented Generation (RAG) framework has been recently used by many applications for this kind of scenarios, as it intuitively bridges dedicated data collections and state-of-the-art generative models. Yet, LLMs are known to present data contamination, a phenomenon in which their performance on evaluation data relevant to a task is influenced by said data being already incorporated to the LLM during training phase. In this paper, we assess the performance of LLMs within the domain of Equinor, the largest energy company in Norway. Specifically, we address question answering with a RAG-based approach over a novel data collection not available for well-established LLMs during training, in order to study the effect of data contamination for this task. Beyond shedding light on LLM performance for a highly-demanded, realistic industrial scenario, we also analyze its potential impact for an ensemble of personas in Equinor with particular information needs and contexts.",
    "venue": "European Conference on Artificial Intelligence",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA241049",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2280332848",
        "name": "Dar√≠o Garigliotti"
      },
      {
        "authorId": "2327548913",
        "name": "Bjarte Johansen"
      },
      {
        "authorId": "2327548586",
        "name": "Jakob Vigerust Kallestad"
      },
      {
        "authorId": "2327705187",
        "name": "Seong-Eun Cho"
      },
      {
        "authorId": "2327548923",
        "name": "C√©sar Ferri"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "659df3de2c4f21b17811b70e45a04702adbf8bfc",
    "url": "https://www.semanticscholar.org/paper/659df3de2c4f21b17811b70e45a04702adbf8bfc",
    "title": "Ask, Assess, and Refine: Rectifying Factual Consistency and Hallucination in LLMs with Metric-Guided Feedback Learning",
    "abstract": "Recent advancements in Large Language Models (LLMs) have heralded unprecedented capabilities in information-seeking and text generation, as evidenced by applications like Bing Chat and perplexity.ai. Despite these strides, challenges on hallucination and factual inconsistency continue to impede their wider real-world adoption. Contemporary methods, including retrieval-augmented LLMs and feedback-based learning, serve as alternatives to mitigate these challenges. However, challenges remain, particularly regarding referencing erroneous evidence (citation errors) and generating information not present in the evidence (hallucination). In this paper, we introduce the \\mathsf{A}^2\\mathsf{R} framework: Ask, Assess, and Refine. Our approach utilizes an explicit evaluation paradigm, incorporating metrics specifically tailored to assess citation errors and hallucination, aiming to address these prevalent challenges robustly. Capitalizing on these evaluations, we devise a strategy to formulate actionable natural language feedback, enabling iterative refinements that yield improved factual consistency and reduced hallucinations in responses. Our experiments on ASQA, ELI5, and QAMPARI datasets demonstrate our method‚Äôs superiority in enhancing correctness, fluency, and citation quality.",
    "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2115276420",
        "name": "Dongyub Lee"
      },
      {
        "authorId": "2291481255",
        "name": "Eunhwan Park"
      },
      {
        "authorId": "2291442764",
        "name": "Hodong Lee"
      },
      {
        "authorId": "2239151880",
        "name": "Heuiseok Lim"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "7779957a61f877839891acefcef0299e4ba8968b",
    "url": "https://www.semanticscholar.org/paper/7779957a61f877839891acefcef0299e4ba8968b",
    "title": "Generative AI for Low-Carbon Artificial Intelligence of Things with Large Language Models",
    "abstract": "By integrating Artificial Intelligence (AI) with the Internet of Things (IoT), Artificial Intelligence of Things (AIoT) has revolutionized many fields. However, AIoT is facing the challenges of energy consumption and carbon emissions due to the continuous advancement of mobile technology. Fortunately, Generative AI (GAI) holds immense potential to reduce carbon emissions of AIoT due to its excellent reasoning and generation capabilities. In this article, we explore the potential of GAI for carbon emissions reduction and propose a novel GAI-enabled solution for low-carbon AIoT. Specifically, we first study the main impacts that cause carbon emissions in AIoT, and then introduce GAI techniques and their relations to carbon emissions. We then explore the application prospects of GAI in low-carbon AIoT, focusing on how GAI can reduce carbon emissions of network components. Subsequently, we propose a Large Language Model (LLM)-enabled carbon emission optimization framework, in which we design pluggable LLM and Retrieval Augmented Generation (RAG) modules to generate more accurate and reliable optimization problems. Furthermore, we utilize Generative Diffusion Models (GDMs) to identify optimal strategies for carbon emission reduction. Numerical results demonstrate the effectiveness of the proposed framework. Finally, we insightfully provide open research directions for low-carbon AIoT.",
    "venue": "IEEE Internet of Things Magazine",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2404.18077",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-28",
    "authors": [
      {
        "authorId": "2212103887",
        "name": "Jinbo Wen"
      },
      {
        "authorId": "2266463634",
        "name": "Ruichen Zhang"
      },
      {
        "authorId": "1713586",
        "name": "D. Niyato"
      },
      {
        "authorId": "2237507634",
        "name": "Jiawen Kang"
      },
      {
        "authorId": "2175043468",
        "name": "Hongyang Du"
      },
      {
        "authorId": "2238332424",
        "name": "Yang Zhang"
      },
      {
        "authorId": "2264568786",
        "name": "Zhu Han"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.79441541679836
  },
  {
    "paperId": "9716eff749e27faf90abb1b7c88728e23ed35032",
    "url": "https://www.semanticscholar.org/paper/9716eff749e27faf90abb1b7c88728e23ed35032",
    "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
    "abstract": "Retrieval augmented generation (RAG) is a process where a large language model (LLM) retrieves useful information from a database and then generates the responses. It is becoming popular in enterprise settings for daily business operations. For example, Copilot for Microsoft 365 has accumulated millions of businesses. However, the security implications of adopting such RAG-based systems are unclear. In this paper, we introduce ConfusedPilot, a class of security vulnerabilities of RAG systems that confuse Copilot and cause integrity and confidentiality violations in its responses. First, we investigate a vulnerability that embeds malicious text in the modified prompt in RAG, corrupting the responses generated by the LLM. Second, we demonstrate a vulnerability that leaks secret data, which leverages the caching mechanism during retrieval. Third, we investigate how both vulnerabilities can be exploited to propagate misinformation within the enterprise and ultimately impact its operations, such as sales and manufacturing. We also discuss the root cause of these attacks by investigating the architecture of a RAG-based system. This study highlights the security vulnerabilities in today's RAG-based systems and proposes design guidelines to secure future RAG-based systems.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-09",
    "authors": [
      {
        "authorId": "2315811335",
        "name": "Ayush RoyChowdhury"
      },
      {
        "authorId": "2315997079",
        "name": "Mulong Luo"
      },
      {
        "authorId": "2052173230",
        "name": "Prateek Sahu"
      },
      {
        "authorId": "2110879164",
        "name": "Sarbartha Banerjee"
      },
      {
        "authorId": "2268772577",
        "name": "Mohit Tiwari"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "8953e68f5f5223b4b1c2029baa6283c3a15c667d",
    "url": "https://www.semanticscholar.org/paper/8953e68f5f5223b4b1c2029baa6283c3a15c667d",
    "title": "RAG-Based LLM Chatbot Using Llama-2",
    "abstract": "Chatbots, otherwise known as autonomous conversational agents, are a rising utilitarian application of Natural Language Processing. They enable the streamlining of information searches and improve user productivity and experience. This study focuses on building a chatbot that is aimed at assisting victims of sexual harassment, using a Large Language Model (LLM). While ML-based chatbots are a notable prospect, LLM-powered chatbots offer more human-like conversations and can surpass humans in empathy. This project evaluated the performance of the LLM Llama-2 model in generating accurate and empathetic answers to create a supportive, sensitive, and informative chatbot for the victims of sexual harassment. The model leverages Retrieval Augmented generation to achieve a commendable accuracy of above 95%, providing information in an understanding and helpful tone. The model is also capable of providing helpful advice without judgement or preconceived notions about the victim, one of the reasons victims do not report their harassers.",
    "venue": "IEEE International Conference on Distributed Computing Systems",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-23",
    "authors": [
      {
        "authorId": "2308304853",
        "name": "Sonia Vakayil"
      },
      {
        "authorId": "2308307646",
        "name": "D. S. Juliet"
      },
      {
        "authorId": "2246817974",
        "name": "A. J"
      },
      {
        "authorId": "2308304855",
        "name": "Sunil Vakayil"
      }
    ],
    "source": "semantic_scholar",
    "score": 92.47918433002164
  },
  {
    "paperId": "d29eff3b2ee10760bf5bf3cc97372aa6e196f3fc",
    "url": "https://www.semanticscholar.org/paper/d29eff3b2ee10760bf5bf3cc97372aa6e196f3fc",
    "title": "Towards Unlocking Insights from Logbooks Using AI",
    "abstract": "Electronic logbooks contain valuable information about activities and events concerning their associated particle accelerator facilities. However, the highly technical nature of logbook entries can hinder their usability and automation. As natural language processing (NLP) continues advancing, it offers opportunities to address various challenges that logbooks present. This work explores jointly testing a tailored Retrieval Augmented Generation (RAG) model for enhancing the usability of particle accelerator logbooks at institutes like DESY, BESSY, Fermilab, BNL, SLAC, LBNL, and CERN. The RAG model uses a corpus built on logbook contributions and aims to unlock insights from these logbooks by leveraging retrieval over facility datasets, including discussion about potential multimodal sources. Our goals are to increase the FAIR-ness (findability, accessibility, interoperability, and reusability) of logbooks by exploiting their information content to streamline everyday use, enable macro-analysis for root cause analysis, and facilitate problem-solving automation.",
    "venue": "Towards Unlocking Insights from Logbooks Using AI",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2406.12881",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Physics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-25",
    "authors": [
      {
        "authorId": "2643868",
        "name": "Antonin Sulc"
      },
      {
        "authorId": "2307464122",
        "name": "Alex Bien"
      },
      {
        "authorId": "2258550441",
        "name": "A. Eichler"
      },
      {
        "authorId": "2307463684",
        "name": "Daniel Ratner"
      },
      {
        "authorId": "2307459657",
        "name": "Florian Rehm"
      },
      {
        "authorId": "2299328155",
        "name": "F. Mayet"
      },
      {
        "authorId": "2307465024",
        "name": "Gregor Hartmann"
      },
      {
        "authorId": "122560220",
        "name": "Hayden Hoschouer"
      },
      {
        "authorId": "2307462230",
        "name": "Henrik Tuennermann"
      },
      {
        "authorId": "2307461675",
        "name": "Jan Kaiser"
      },
      {
        "authorId": "2273367399",
        "name": "Jason St. John"
      },
      {
        "authorId": "2307464656",
        "name": "Jennefer Maldonado"
      },
      {
        "authorId": "2307463982",
        "name": "Kyle Hazelwood"
      },
      {
        "authorId": "2264286637",
        "name": "Raimund Kammering"
      },
      {
        "authorId": "2307462277",
        "name": "Thorsten Hellert"
      },
      {
        "authorId": "6148137",
        "name": "T. Wilksen"
      },
      {
        "authorId": "2257002083",
        "name": "Verena Kain"
      },
      {
        "authorId": "2307490240",
        "name": "Wan-Lin Hu"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "a2181d42180c11f5bb56f8d500a0f75bc73d1614",
    "url": "https://www.semanticscholar.org/paper/a2181d42180c11f5bb56f8d500a0f75bc73d1614",
    "title": "RAGs to Style: Personalizing LLMs with Style Embeddings",
    "abstract": "This paper studies the use of style embeddings to enhance author profiling for the goal of personalization of Large Language Models (LLMs). Using a style-based Retrieval-Augmented Generation (RAG) approach, we meticulously study the efficacy of style embeddings in capturing distinctive authorial nuances. The proposed method leverages this acquired knowledge to enhance the personalization capabilities of LLMs. In the assessment of this approach, we have employed the LaMP benchmark, specifically tailored for evaluating language models across diverse dimensions of personalization. The empirical observations from our investigation reveal that, in comparison to term matching or context matching, style proves to be marginally superior in the development of personalized LLMs.",
    "venue": "PERSONALIZE",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2181717922",
        "name": "Abhiman Neelakanteswara"
      },
      {
        "authorId": "2291365473",
        "name": "Shreyas Chaudhari"
      },
      {
        "authorId": "2291364823",
        "name": "Hamed Zamani"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "b493140e61d58c5e6d92c83e63a156c82288b513",
    "url": "https://www.semanticscholar.org/paper/b493140e61d58c5e6d92c83e63a156c82288b513",
    "title": "How do you know that? Teaching Generative Language Models to Reference Answers to Biomedical Questions",
    "abstract": "Large language models (LLMs) have recently become the leading source of answers for users‚Äô questions online. Despite their ability to offer eloquent answers, their accuracy and reliability can pose a significant challenge. This is especially true for sensitive domains such as biomedicine, where there is a higher need for factually correct answers. This paper introduces a biomedical retrieval-augmented generation (RAG) system designed to enhance the reliability of generated responses. The system is based on a fine-tuned LLM for the referenced question-answering, where retrieved relevant abstracts from PubMed are passed to LLM‚Äôs context as input through a prompt. Its output is an answer based on PubMed abstracts, where each statement is referenced accordingly, allowing the users to verify the answer. Our retrieval system achieves an absolute improvement of 23% compared to the PubMed search engine. Based on the manual evaluation on a small sample, our fine-tuned LLM component achieves comparable results to GPT-4 Turbo in referencing relevant abstracts. We make the dataset used to fine-tune the models and the fine-tuned models based on Mistral-7B-instruct-v0.1 and v0.2 publicly available.",
    "venue": "Workshop on Biomedical Natural Language Processing",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-06",
    "authors": [
      {
        "authorId": "2186574263",
        "name": "Bojana Ba≈°aragin"
      },
      {
        "authorId": "2310335182",
        "name": "Adela Ljajic"
      },
      {
        "authorId": "2082038831",
        "name": "Darija Medvecki"
      },
      {
        "authorId": "2310336122",
        "name": "Lorenzo Cassano"
      },
      {
        "authorId": "118640249",
        "name": "Milos Kosprdic"
      },
      {
        "authorId": "2277252154",
        "name": "Nikola Milosevic"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "1cbf29e761ef9a847ef9c70e6cd3c3fd5378d2c3",
    "url": "https://www.semanticscholar.org/paper/1cbf29e761ef9a847ef9c70e6cd3c3fd5378d2c3",
    "title": "Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research",
    "abstract": "Qualitative data collection and analysis approaches, such as those employing interviews and focus groups, provide rich insights into customer attitudes, sentiment, and behavior. However, manually analyzing qualitative data requires extensive time and effort to identify relevant topics and thematic insights. This study proposes a novel approach to address this challenge by leveraging Retrieval Augmented Generation (RAG) based Large Language Models (LLMs) for analyzing interview transcripts. The novelty of this work lies in strategizing the research inquiry as one that is augmented by an LLM that serves as a novice research assistant. This research explores the mental model of LLMs to serve as novice qualitative research assistants for researchers in the talent management space. A RAG-based LLM approach is extended to enable topic modeling of semi-structured interview data, showcasing the versatility of these models beyond their traditional use in information retrieval and search. Our findings demonstrate that the LLM-augmented RAG approach can successfully extract topics of interest, with significant coverage compared to manually generated topics from the same dataset. This establishes the viability of employing LLMs as novice qualitative research assistants. Additionally, the study recommends that researchers leveraging such models lean heavily on quality criteria used in traditional qualitative research to ensure rigor and trustworthiness of their approach. Finally, the paper presents key recommendations for industry practitioners seeking to reconcile the use of LLMs with established qualitative research paradigms, providing a roadmap for the effective integration of these powerful, albeit novice, AI tools in the analysis of qualitative datasets within talent",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-20",
    "authors": [
      {
        "authorId": "28967297",
        "name": "S. Bhaduri"
      },
      {
        "authorId": "2316486657",
        "name": "Satya Kapoor"
      },
      {
        "authorId": "2316487583",
        "name": "Alex Gil"
      },
      {
        "authorId": "2310717331",
        "name": "Anshul Mittal"
      },
      {
        "authorId": "1403856055",
        "name": "Rutu Mulkar-Mehta"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "4cb8e7ef158a3f896ae8ae8a340f980249305746",
    "url": "https://www.semanticscholar.org/paper/4cb8e7ef158a3f896ae8ae8a340f980249305746",
    "title": "AI-Press: A Multi-Agent News Generating and Feedback Simulation System Powered by Large Language Models",
    "abstract": "The rise of various social platforms has transformed journalism. The growing demand for news content has led to the increased use of large language models (LLMs) in news production due to their speed and cost-effectiveness. However, LLMs still encounter limitations in professionalism and ethical judgment in news generation. Additionally, predicting public feedback is usually difficult before news is released. To tackle these challenges, we introduce AI-Press, an automated news drafting and polishing system based on multi-agent collaboration and Retrieval-Augmented Generation. We develop a feedback simulation system that generates public feedback considering demographic distributions. Through extensive quantitative and qualitative evaluations, our system shows significant improvements in news-generating capabilities and verifies the effectiveness of public feedback simulation.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-10",
    "authors": [
      {
        "authorId": "2325199497",
        "name": "Xiawei Liu"
      },
      {
        "authorId": "2325204830",
        "name": "Shiyue Yang"
      },
      {
        "authorId": "2273571619",
        "name": "Xinnong Zhang"
      },
      {
        "authorId": "2273677947",
        "name": "Haoyu Kuang"
      },
      {
        "authorId": "2301759530",
        "name": "Libo Sun"
      },
      {
        "authorId": "2325207867",
        "name": "Yihang Yang"
      },
      {
        "authorId": "2286049042",
        "name": "Siming Chen"
      },
      {
        "authorId": "2253852592",
        "name": "Xuanjing Huang"
      },
      {
        "authorId": "2266438567",
        "name": "Zhongyu Wei"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "60fbb18bc8d9f3a928f35726f95f511ce7fa51ee",
    "url": "https://www.semanticscholar.org/paper/60fbb18bc8d9f3a928f35726f95f511ce7fa51ee",
    "title": "Performance of Publicly Available Large Language Models on Internal Medicine Board-style Questions",
    "abstract": "Ongoing research attempts to benchmark large language models (LLM) against physicians‚Äô fund of knowledge by assessing LLM performance on medical examinations. No prior study has assessed LLM performance on internal medicine (IM) board examination questions. Limited data exists on how knowledge supplied to the models, derived from medical texts improves LLM performance. The performance of GPT-3.5, GPT-4.0, LaMDA and Llama 2, with and without additional model input augmentation, was assessed on 240 randomly selected IM board-style questions. Questions were sourced from the Medical Knowledge Self-Assessment Program released by the American College of Physicians with each question serving as part of the LLM prompt. When available, LLMs were accessed both through their application programming interface (API) and their corresponding chatbot. Mode inputs were augmented with Harrison‚Äôs Principles of Internal Medicine using the method of Retrieval Augmented Generation. LLM-generated explanations to 25 correctly answered questions were presented in a blinded fashion alongside the MKSAP explanation to an IM board-certified physician tasked with selecting the human generated response. GPT-4.0, accessed either through Bing Chat or its API, scored 77.5‚Äì80.7% outperforming GPT-3.5, human respondents, LaMDA and Llama 2 in that order. GPT-4.0 outperformed human MKSAP users on every tested IM subject with its highest and lowest percentile scores in Infectious Disease (80th) and Rheumatology (99.7th), respectively. There is a 3.2‚Äì5.3% decrease in performance of both GPT-3.5 and GPT-4.0 when accessing the LLM through its API instead of its online chatbot. There is 4.5‚Äì7.5% increase in performance of both GPT-3.5 and GPT-4.0 accessed through their APIs after additional input augmentation. The blinded reviewer correctly identified the human generated MKSAP response in 72% of the 25-question sample set. GPT-4.0 performed best on IM board-style questions outperforming human respondents. Augmenting with domain-specific information improved performance rendering Retrieval Augmented Generation a possible technique for improving accuracy in medical examination LLM responses.",
    "venue": "PLOS Digital Health",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-09-01",
    "authors": [
      {
        "authorId": "2264961040",
        "name": "Constantine Tarabanis"
      },
      {
        "authorId": "2321496315",
        "name": "Sohail Zahid"
      },
      {
        "authorId": "2224224766",
        "name": "M. Mamalis"
      },
      {
        "authorId": "2321507627",
        "name": "Kevin Zhang"
      },
      {
        "authorId": "3297060",
        "name": "E. Kalampokis"
      },
      {
        "authorId": "51919564",
        "name": "Lior Jankelson"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "96f5ce59e3dd54fa508e13a499de7fd7b633b022",
    "url": "https://www.semanticscholar.org/paper/96f5ce59e3dd54fa508e13a499de7fd7b633b022",
    "title": "Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization",
    "abstract": "In many modern LLM applications, such as retrieval augmented generation, prompts have become programs themselves. In these settings, prompt programs are repeatedly called with different user queries or data instances. A big practical challenge is optimizing such prompt programs. Recent work has mostly focused on either simple prompt programs or assumed that the general structure of a prompt program is fixed. We introduce SAMMO, a framework to perform symbolic prompt program search for compile-time optimizations of prompt programs. SAMMO represents prompt programs on a symbolic level which allows for a rich set of transformations that can be searched over during optimization. We show that SAMMO generalizes previous methods and improves the performance of complex prompts on (1) instruction tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several different LLMs. We make all code available open-source at https://github.com/microsoft/sammo .",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-02",
    "authors": [
      {
        "authorId": "2294720265",
        "name": "Tobias Schnabel"
      },
      {
        "authorId": "2294720743",
        "name": "Jennifer Neville"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "68bd565e94ea98e43a82c99cc31b5823bdd2bf10",
    "url": "https://www.semanticscholar.org/paper/68bd565e94ea98e43a82c99cc31b5823bdd2bf10",
    "title": "Observations on Building RAG Systems for Technical Documents",
    "abstract": "Retrieval augmented generation (RAG) for technical documents creates challenges as embeddings do not often capture domain information. We review prior art for important factors affecting RAG and perform experiments to highlight best practices and potential challenges to build RAG systems for technical documents.",
    "venue": "Tiny Papers @ ICLR",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-03-31",
    "authors": [
      {
        "authorId": "3336244",
        "name": "Sumit Soman"
      },
      {
        "authorId": "2284351918",
        "name": "Sujoy Roychowdhury"
      }
    ],
    "source": "semantic_scholar",
    "score": 83.47918433002164
  },
  {
    "paperId": "c8defe1745206ed14949182739e6e970852c8027",
    "url": "https://www.semanticscholar.org/paper/c8defe1745206ed14949182739e6e970852c8027",
    "title": "IITK at SemEval-2024 Task 2: Exploring the Capabilities of LLMs for Safe Biomedical Natural Language Inference for Clinical Trials",
    "abstract": "Large Language models (LLMs) have demonstrated state-of-the-art performance in various natural language processing (NLP) tasks across multiple domains, yet they are prone to shortcut learning and factual inconsistencies. This research investigates LLMs‚Äô robustness, consistency, and faithful reasoning when performing Natural Language Inference (NLI) on breast cancer Clinical Trial Reports (CTRs) in the context of SemEval 2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials. We examine the reasoning capabilities of LLMs and their adeptness at logical problem-solving. A comparative analysis is conducted on pre-trained language models (PLMs), GPT-3.5, and Gemini Pro under zero-shot settings using Retrieval-Augmented Generation (RAG) framework, integrating various reasoning chains. The evaluation yields an F1 score of 0.69, consistency of 0.71, and a faithfulness score of 0.90 on the test dataset.",
    "venue": "International Workshop on Semantic Evaluation",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-06",
    "authors": [
      {
        "authorId": "2295670128",
        "name": "Shreyasi Mandal"
      },
      {
        "authorId": "2477939",
        "name": "Ashutosh Modi"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "ddfce70a9fad94e03a610afb7f4b5d2c0883b91a",
    "url": "https://www.semanticscholar.org/paper/ddfce70a9fad94e03a610afb7f4b5d2c0883b91a",
    "title": "LLMs for Explainable Few-shot Deception Detection",
    "abstract": "This study investigates the effectiveness of Large Language Models (LLMs) in detecting deception using a Retrieval Augmented Generation (RAG) framework for few-shot learning in domain-agnostic settings. Our approach combines the sophisticated reasoning capabilities and extensive knowledge base of LLMs to identify deceptive statements across various contexts, with a focus on the explainability of the detection process. This emphasis on explainability enables a detailed analysis of the model's methodologies in distinguishing between truthful and deceptive statements. Additionally, we examine the impact of different definitions of deception, from overt falsehoods to subtle misrepresentations, on the model's accuracy. Our main contributions include providing initial insights into the adaptability of LLMs for deception detection and highlighting the challenges faced in this endeavor, thereby encouraging further exploration in this area.",
    "venue": "IWSPA@CODASPY",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2024-06-19",
    "authors": [
      {
        "authorId": "2102167",
        "name": "Dainis Boumber"
      },
      {
        "authorId": "2269458097",
        "name": "Bryan Edward Tuck"
      },
      {
        "authorId": "2282528633",
        "name": "Rakesh M. Verma"
      },
      {
        "authorId": "2162541301",
        "name": "Fatima Zahra Qachfar"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "9368325ba67c0cac3902273deabebd02f7a1f139",
    "url": "https://www.semanticscholar.org/paper/9368325ba67c0cac3902273deabebd02f7a1f139",
    "title": "Towards Reliable Medical Question Answering: Techniques and Challenges in Mitigating Hallucinations in Language Models",
    "abstract": "The rapid advancement of large language models (LLMs) has significantly impacted various domains, including healthcare and biomedicine. However, the phenomenon of hallucination, where LLMs generate outputs that deviate from factual accuracy or context, poses a critical challenge, especially in high-stakes domains. This paper conducts a scoping study of existing techniques for mitigating hallucinations in knowledge-based task in general and especially for medical domains. Key methods covered in the paper include Retrieval-Augmented Generation (RAG)-based techniques, iterative feedback loops, supervised fine-tuning, and prompt engineering. These techniques, while promising in general contexts, require further adaptation and optimization for the medical domain due to its unique demands for up-to-date, specialized knowledge and strict adherence to medical guidelines. Addressing these challenges is crucial for developing trustworthy AI systems that enhance clinical decision-making and patient safety as well as accuracy of biomedical scientific research.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-25",
    "authors": [
      {
        "authorId": "2316949896",
        "name": "Duy Khoa Pham"
      },
      {
        "authorId": "2155028",
        "name": "Quoc Bao Vo"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "57d288508838a433a122e0fccf99fc540c73fff2",
    "url": "https://www.semanticscholar.org/paper/57d288508838a433a122e0fccf99fc540c73fff2",
    "title": "Multi-Level Querying using A Knowledge Pyramid",
    "abstract": "This paper addresses the need for improved precision in existing Retrieval-Augmented Generation (RAG) methods that primarily focus on enhancing recall. We propose a multi-layer knowledge pyramid approach within the RAG framework to achieve a better balance between precision and recall. The knowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs), and chunk-based raw text. We employ cross-layer augmentation techniques for comprehensive knowledge coverage and dynamic updates of the Ontology schema and instances. To ensure compactness, we utilize cross-layer filtering methods for knowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall model for retrieval, starting from the top of the pyramid and progressing down until a confident answer is obtained. We introduce two benchmarks for domain-specific knowledge retrieval, one in the academic domain and the other in the financial domain. The effectiveness of the methods has been validated through comprehensive experiments by outperforming 19 SOTA methods. An encouraging observation is that the proposed method has augmented the GPT-4, providing 395\\% F1 gain by improving its performance from 0.1636 to 0.8109.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-31",
    "authors": [
      {
        "authorId": "2314169880",
        "name": "Rubing Chen"
      },
      {
        "authorId": "2108162240",
        "name": "Xu-Lu Zhang"
      },
      {
        "authorId": "2313746412",
        "name": "Jiaxin Wu"
      },
      {
        "authorId": "2291324376",
        "name": "Wenqi Fan"
      },
      {
        "authorId": "2115493866",
        "name": "Xiao Wei"
      },
      {
        "authorId": "2293397899",
        "name": "Qing Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "ce52b934c49f4c61d07d3e9bb668c5b19e3638ae",
    "url": "https://www.semanticscholar.org/paper/ce52b934c49f4c61d07d3e9bb668c5b19e3638ae",
    "title": "Large Language Models in Modern Forensic Investigations: Harnessing the Power of Generative Artificial Intelligence in Crime Resolution and Suspect Identification",
    "abstract": "Large Language Models (LLMs) have recently captured the attention of the scientific c ommunity. S ince t he global launch of LLM-based chatbots in late 2022, the field h as witnessed a rapid increase in interest from researchers, technology providers and citizens alike. With its wide-ranging applicability, Generative Artificial I ntelligence (GenAI) h as t he p otential to impact various aspects of society, from improving communication and accessibility to transforming industries such as healthcare, education and security. More specifically, in the field of Forensic Science, LLMs could offer significant b enefits as sisting Law Enforcement Agencies (LEAs) and Forensic Practitioners in crime investigations. This paper proposes the implementation of a Retrieval Augmented Generation (RAG) LLM, trained with criminology data, to provide swift and actionable insights into specific incidents, thereby enhancing Forensic Data Analysis and facilitating the daily operations of LEAs.",
    "venue": "2024 5th International Conference in Electronic Engineering, Information Technology & Education (EEITE)",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2024-05-29",
    "authors": [
      {
        "authorId": "2319931235",
        "name": "Anastasios Nikolakopoulos"
      },
      {
        "authorId": "2280448795",
        "name": "Spyridon Evangelatos"
      },
      {
        "authorId": "2319929808",
        "name": "Eleni Veroni"
      },
      {
        "authorId": "2319922119",
        "name": "Konstantinos Chasapas"
      },
      {
        "authorId": "2280611463",
        "name": "Nikolaos Gousetis"
      },
      {
        "authorId": "2685335",
        "name": "A. Apostolaras"
      },
      {
        "authorId": "2280611046",
        "name": "Christos Nikolopoulos"
      },
      {
        "authorId": "144358916",
        "name": "T. Korakis"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "4999b609520483098c835d13fba18df828706fe8",
    "url": "https://www.semanticscholar.org/paper/4999b609520483098c835d13fba18df828706fe8",
    "title": "GAIA: A General AI Assistant for Intelligent Accelerator Operations",
    "abstract": "Large-scale machines like particle accelerators are usually run by a team of experienced operators. In case of a particle accelerator, these operators possess suitable background knowledge on both accelerator physics and the technology comprising the machine. Due to the complexity of the machine, particular subsystems of the machine are taken care of by experts, who the operators can turn to. In this work the reasoning and action (ReAct) prompting paradigm is used to couple an open-weights large language model (LLM) with a high-level machine control system framework and other tools, e.g. the electronic logbook or machine design documentation. By doing so, a multi-expert retrieval augmented generation (RAG) system is implemented, which assists operators in knowledge retrieval tasks, interacts with the machine directly if needed, or writes high level control system scripts. This consolidation of expert knowledge and machine interaction can simplify and speed up machine operation tasks for both new and experienced human operators.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Physics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-02",
    "authors": [
      {
        "authorId": "2299328155",
        "name": "F. Mayet"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "8fb8bc1dbcada0612c9faa8704424a27327048ae",
    "url": "https://www.semanticscholar.org/paper/8fb8bc1dbcada0612c9faa8704424a27327048ae",
    "title": "iRAG: Advancing RAG for Videos with an Incremental Approach",
    "abstract": "Retrieval-augmented generation (RAG) systems combine the strengths of language generation and information retrieval to power many real-world applications like chatbots. Use of RAG for understanding of videos is appealing but there are two critical limitations. One-time, upfront conversion of all content in large corpus of videos into text descriptions entails high processing times. Also, not all information in the rich video data is typically captured in the text descriptions. Since user queries are not known apriori, developing a system for video to text conversion and interactive querying of video data is challenging. To address these limitations, we propose an incremental RAG system called iRAG, which augments RAG with a novel incremental workflow to enable interactive querying of a large corpus of videos. Unlike traditional RAG, iRAG quickly indexes large repositories of videos, and in the incremental workflow, it uses the index to opportunistically extract more details from select portions of the videos to retrieve context relevant to an interactive user query. Such an incremental workflow avoids long video to text conversion times, and overcomes information loss issues due to conversion of video to text, by doing on-demand query-specific extraction of details in video data. This ensures high quality of responses to interactive user queries that are often not known apriori. To the best of our knowledge, iRAG is the first system to augment RAG with an incremental workflow to support efficient interactive querying of a large corpus of videos. Experimental results on real-world datasets demonstrate 23x to 25x faster video to text ingestion, while ensuring that latency and quality of responses to interactive user queries is comparable to responses from a traditional RAG where all video data is converted to text upfront before any user querying.",
    "venue": "International Conference on Information and Knowledge Management",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-18",
    "authors": [
      {
        "authorId": "1388023083",
        "name": "Md. Adnan Arefeen"
      },
      {
        "authorId": "2781191",
        "name": "Biplob K. Debnath"
      },
      {
        "authorId": "153376331",
        "name": "M. Y. S. Uddin"
      },
      {
        "authorId": "1752242",
        "name": "S. Chakradhar"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "c0875f62cf69911fc9599c5d0f96df0972d489d9",
    "url": "https://www.semanticscholar.org/paper/c0875f62cf69911fc9599c5d0f96df0972d489d9",
    "title": "DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning",
    "abstract": "Text-to-image diffusion models have been shown to suffer from sample-level memorization, possibly reproducing near-perfect replica of images that they are trained on, which may be undesirable. To remedy this issue, we develop the first differentially private (DP) retrieval-augmented generation algorithm that is capable of generating high-quality image samples while providing provable privacy guarantees. Specifically, we assume access to a text-to-image diffusion model trained on a small amount of public data, and design a DP retrieval mechanism to augment the text prompt with samples retrieved from a private retrieval dataset. Our \\emph{differentially private retrieval-augmented diffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to adapt to another domain, and can use state-of-the-art generative models to generate high-quality image samples while satisfying rigorous DP guarantees. For instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a privacy budget of $\\epsilon=10$, while providing a $3.5$ point improvement in FID compared to public-only retrieval for up to $10,000$ queries.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-21",
    "authors": [
      {
        "authorId": "2292404036",
        "name": "Jonathan Lebensold"
      },
      {
        "authorId": "2095979",
        "name": "Maziar Sanjabi"
      },
      {
        "authorId": "2274101827",
        "name": "Pietro Astolfi"
      },
      {
        "authorId": "1456285042",
        "name": "Adriana Romero-Soriano"
      },
      {
        "authorId": "2256991985",
        "name": "Kamalika Chaudhuri"
      },
      {
        "authorId": "2298274880",
        "name": "Michael Rabbat"
      },
      {
        "authorId": "2290241478",
        "name": "Chuan Guo"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "a49596befc6fee2e528c98733ec517811510515a",
    "url": "https://www.semanticscholar.org/paper/a49596befc6fee2e528c98733ec517811510515a",
    "title": "DebateQA: Evaluating Question Answering on Debatable Knowledge",
    "abstract": "The rise of large language models (LLMs) has enabled us to seek answers to inherently debatable questions on LLM chatbots, necessitating a reliable way to evaluate their ability. However, traditional QA benchmarks assume fixed answers are inadequate for this purpose. To address this, we introduce DebateQA, a dataset of 2,941 debatable questions, each accompanied by multiple human-annotated partial answers that capture a variety of perspectives. We develop two metrics: Perspective Diversity, which evaluates the comprehensiveness of perspectives, and Dispute Awareness, which assesses if the LLM acknowledges the question's debatable nature. Experiments demonstrate that both metrics align with human preferences and are stable across different underlying models. Using DebateQA with two metrics, we assess 12 popular LLMs and retrieval-augmented generation methods. Our findings reveal that while LLMs generally excel at recognizing debatable issues, their ability to provide comprehensive answers encompassing diverse perspectives varies considerably.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-02",
    "authors": [
      {
        "authorId": "2158524037",
        "name": "Rongwu Xu"
      },
      {
        "authorId": "2314701155",
        "name": "Xuan Qi"
      },
      {
        "authorId": "2257044451",
        "name": "Zehan Qi"
      },
      {
        "authorId": "2304324554",
        "name": "Wei Xu"
      },
      {
        "authorId": "2681038",
        "name": "Zhijiang Guo"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "26aafb853a1d34dba90d0b6246355b8102844f17",
    "url": "https://www.semanticscholar.org/paper/26aafb853a1d34dba90d0b6246355b8102844f17",
    "title": "From RAGs to riches: Using large language models to write documents for clinical trials",
    "abstract": "Clinical trials require numerous documents to be written -- protocols, consent forms, clinical study reports and others. Large language models (LLMs) offer the potential to rapidly generate first versions of these documents, however there are concerns about the quality of their output Here we report an evaluation of LLMs in generating parts of one such document, clinical trial protocols. We find that an offthe-shelf LLM delivers reasonable results, especially when assessing content relevance and the correct use of terminology. However, deficiencies remain: specifically clinical thinking and logic, and appropriate use of references. To improve performance, we used retrieval-augmented generation (RAG) to prompt an LLM with accurate up-to-date information. As a result of using RAG, the writing quality of the LLM improves substantially, which has implications for the practical useability of LLMs in clinical trial-related writing.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-26",
    "authors": [
      {
        "authorId": "2283902403",
        "name": "Nigel Markey"
      },
      {
        "authorId": "2283902153",
        "name": "Ilyass El-Mansouri"
      },
      {
        "authorId": "3422955",
        "name": "Ga√´tan Rensonnet"
      },
      {
        "authorId": "2287837069",
        "name": "Casper van Langen"
      },
      {
        "authorId": "2283902581",
        "name": "Christoph Meier"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "6dcf4cb034f1630ef1c52ffb2c3a5450d2ca9936",
    "url": "https://www.semanticscholar.org/paper/6dcf4cb034f1630ef1c52ffb2c3a5450d2ca9936",
    "title": "Investigating Context-Faithfulness in Large Language Models: The Roles of Memory Strength and Evidence Style",
    "abstract": "Retrieval-augmented generation (RAG) improves Large Language Models (LLMs) by incorporating external information into the response generation process. However, how context-faithful LLMs are and what factors influence LLMs' context-faithfulness remain largely unexplored. In this study, we investigate the impact of memory strength and evidence presentation on LLMs' receptiveness to external evidence. We introduce a method to quantify the memory strength of LLMs by measuring the divergence in LLMs' responses to different paraphrases of the same question, which is not considered by previous works. We also generate evidence in various styles to evaluate the effects of evidence in different styles. Two datasets are used for evaluation: Natural Questions (NQ) with popular questions and popQA featuring long-tail questions. Our results show that for questions with high memory strength, LLMs are more likely to rely on internal memory, particularly for larger LLMs such as GPT-4. On the other hand, presenting paraphrased evidence significantly increases LLMs' receptiveness compared to simple repetition or adding details.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-17",
    "authors": [
      {
        "authorId": "10733723",
        "name": "Yuepei Li"
      },
      {
        "authorId": "2269146567",
        "name": "Kang Zhou"
      },
      {
        "authorId": "2179882123",
        "name": "Qiao Qiao"
      },
      {
        "authorId": "2321456601",
        "name": "Bach Nguyen"
      },
      {
        "authorId": "2269553302",
        "name": "Qing Wang"
      },
      {
        "authorId": "2269413564",
        "name": "Qi Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "96611889cf8afd332792dfd3f2d611ebabc28f83",
    "url": "https://www.semanticscholar.org/paper/96611889cf8afd332792dfd3f2d611ebabc28f83",
    "title": "ERATTA: Extreme RAG for Table To Answers with Large Language Models",
    "abstract": "Large language models (LLMs) with retrieval augmented-generation (RAG) have been the optimal choice for scalable generative AI solutions in the recent past. Although RAG implemented with AI agents (agentic-RAG) has been recently popularized, its suffers from unstable cost and unreliable performances for Enterprise-level data-practices. Most existing use-cases that incorporate RAG with LLMs have been either generic or extremely domain specific, thereby questioning the scalability and generalizability of RAG-LLM approaches. In this work, we propose a unique LLM-based system where multiple LLMs can be invoked to enable data authentication, user-query routing, data-retrieval and custom prompting for question-answering capabilities from Enterprise-data tables. The source tables here are highly fluctuating and large in size and the proposed framework enables structured responses in under 10 seconds per query. Additionally, we propose a five metric scoring module that detects and reports hallucinations in the LLM responses. Our proposed system and scoring metrics achieve>90% confidence scores across hundreds of user queries in the sustainability, financial health and social media domains. Extensions to the proposed extreme RAG architectures can enable heterogeneous source querying using LLMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-07",
    "authors": [
      {
        "authorId": "2971095",
        "name": "Sohini Roychowdhury"
      },
      {
        "authorId": "2266467988",
        "name": "Marko Krema"
      },
      {
        "authorId": "2300173088",
        "name": "Anvar Mahammad"
      },
      {
        "authorId": "2266468695",
        "name": "Brian Moore"
      },
      {
        "authorId": "2266469117",
        "name": "Arijit Mukherjee"
      },
      {
        "authorId": "2300172713",
        "name": "Punit Prakashchandra"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "87c4a9910293346a02c758fb46560367df2b103a",
    "url": "https://www.semanticscholar.org/paper/87c4a9910293346a02c758fb46560367df2b103a",
    "title": "Unlocking Telecom Domain Knowledge Using LLMs",
    "abstract": "Conversational assistants have become increasingly popular as they use Large Language Models (LLMs) and Retrieval Augmented Generation (RAG) for domain context. In this work, we present an end-to-end solution that leverages RAG for telecom domain Question Answering (QA) on standards documents. We highlight that retrieval quality is important, along with an efficient indexing mechanism for the document embeddings. We also index images and tables for QA on standards documents. Our Telecom Knowledge Assistant is useful for handling specific queries from telecom domain experts, as well as for novice learners. The developed approach and solution are amenable to adapt for other domains as well.",
    "venue": "International Conference on Communication Systems and Networks",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-01-03",
    "authors": [
      {
        "authorId": "2284351918",
        "name": "Sujoy Roychowdhury"
      },
      {
        "authorId": "2284348858",
        "name": "Nishkarsh Jain"
      },
      {
        "authorId": "3336244",
        "name": "Sumit Soman"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "7cfd2426ca908c8c5a81bd7c7ca01f914a972de4",
    "url": "https://www.semanticscholar.org/paper/7cfd2426ca908c8c5a81bd7c7ca01f914a972de4",
    "title": "HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems",
    "abstract": "Retrieval-Augmented Generation (RAG) has been shown to improve knowledge capabilities and alleviate the hallucination problem of LLMs. The Web is a major source of external knowledge used in RAG systems, and many commercial systems such as ChatGPT and Perplexity have used Web search engines as their major retrieval systems. Typically, such RAG systems retrieve search results, download HTML sources of the results, and then extract plain texts from the HTML sources. Plain text documents or chunks are fed into the LLMs to augment the generation. However, much of the structural and semantic information inherent in HTML, such as headings and table structures, is lost during this plain-text-based RAG process. To alleviate this problem, we propose HtmlRAG, which uses HTML instead of plain text as the format of retrieved knowledge in RAG. We believe HTML is better than plain text in modeling knowledge in external documents, and most LLMs possess robust capacities to understand HTML. However, utilizing HTML presents new challenges. HTML contains additional content such as tags, JavaScript, and CSS specifications, which bring extra input tokens and noise to the RAG system. To address this issue, we propose HTML cleaning, compression, and pruning strategies, to shorten the HTML while minimizing the loss of information. Specifically, we design a two-step block-tree-based pruning method that prunes useless HTML blocks and keeps only the relevant part of the HTML. Experiments on six QA datasets confirm the superiority of using HTML in RAG systems.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-11-05",
    "authors": [
      {
        "authorId": "2157217564",
        "name": "Jiejun Tan"
      },
      {
        "authorId": "1897235",
        "name": "Zhicheng Dou"
      },
      {
        "authorId": "2329315304",
        "name": "Wen Wang"
      },
      {
        "authorId": "2242171637",
        "name": "Mang Wang"
      },
      {
        "authorId": "2329314315",
        "name": "Weipeng Chen"
      },
      {
        "authorId": "2186578511",
        "name": "Ji-Rong Wen"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "b534c9333f678489c108eb9b7d4946531e45e467",
    "url": "https://www.semanticscholar.org/paper/b534c9333f678489c108eb9b7d4946531e45e467",
    "title": "EfficientRAG: Efficient Retriever for Multi-Hop Question Answering",
    "abstract": "Retrieval-augmented generation (RAG) methods encounter difficulties when addressing complex questions like multi-hop queries.While iterative retrieval methods improve performance by gathering additional information, current approaches often rely on multiple calls of large language models (LLMs).In this paper, we introduce EfficientRAG, an efficient retriever for multi-hop question answering.EfficientRAG iteratively generates new queries without the need for LLM calls at each iteration and filters out irrelevant information.Experimental results demonstrate that EfficientRAG surpasses existing RAG methods on three open-domain multi-hop question-answering datasets.The code is available in [aka.ms/efficientrag](https://github.com/NIL-zhuang/EfficientRAG-official).",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-08-08",
    "authors": [
      {
        "authorId": "2296707191",
        "name": "Ziyuan Zhuang"
      },
      {
        "authorId": "2315286979",
        "name": "Zhiyang Zhang"
      },
      {
        "authorId": "2220695955",
        "name": "Sitao Cheng"
      },
      {
        "authorId": "2261394438",
        "name": "Fangkai Yang"
      },
      {
        "authorId": "2315462674",
        "name": "Jia Liu"
      },
      {
        "authorId": "2315522390",
        "name": "Shujian Huang"
      },
      {
        "authorId": "2303522901",
        "name": "Qingwei Lin"
      },
      {
        "authorId": "148121358",
        "name": "S. Rajmohan"
      },
      {
        "authorId": "2307618254",
        "name": "Dongmei Zhang"
      },
      {
        "authorId": "2279711828",
        "name": "Qi Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 81.47918433002164
  },
  {
    "paperId": "ffac965686a42acc99e5e6628485cdf2bfb41e1b",
    "url": "https://www.semanticscholar.org/paper/ffac965686a42acc99e5e6628485cdf2bfb41e1b",
    "title": "FinTextQA: A Dataset for Long-form Financial Question Answering",
    "abstract": "Accurate evaluation of financial question answering (QA) systems necessitates a comprehensive dataset encompassing diverse question types and contexts. However, current financial QA datasets lack scope diversity and question complexity. This work introduces FinTextQA, a novel dataset for long-form question answering (LFQA) in finance. FinTextQA comprises 1,262 high-quality, source-attributed QA pairs extracted and selected from finance textbooks and government agency websites.Moreover, we developed a Retrieval-Augmented Generation (RAG)-based LFQA system, comprising an embedder, retriever, reranker, and generator. A multi-faceted evaluation approach, including human ranking, automatic metrics, and GPT-4 scoring, was employed to benchmark the performance of different LFQA system configurations under heightened noisy conditions. The results indicate that: (1) Among all compared generators, Baichuan2-7B competes closely with GPT-3.5-turbo in accuracy score; (2) The most effective system configuration on our dataset involved setting the embedder, retriever, reranker, and generator as Ada2, Automated Merged Retrieval, Bge-Reranker-Base, and Baichuan2-7B, respectively; (3) models are less susceptible to noise after the length of contexts reaching a specific threshold.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-16",
    "authors": [
      {
        "authorId": "2301490231",
        "name": "Jian Chen"
      },
      {
        "authorId": "1800462890",
        "name": "Peilin Zhou"
      },
      {
        "authorId": "2147311343",
        "name": "Y. Hua"
      },
      {
        "authorId": "2301456174",
        "name": "Yingxin Loh"
      },
      {
        "authorId": "2301592651",
        "name": "Kehui Chen"
      },
      {
        "authorId": "2224621614",
        "name": "Ziyuan Li"
      },
      {
        "authorId": "2301770063",
        "name": "Bing Zhu"
      },
      {
        "authorId": "2301541118",
        "name": "Junwei Liang"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "2b9989e505a4d6fdad23f12d894483426e5d106f",
    "url": "https://www.semanticscholar.org/paper/2b9989e505a4d6fdad23f12d894483426e5d106f",
    "title": "Evaluating Robustness of Generative Search Engine on Adversarial Factoid Questions",
    "abstract": "Generative search engines have the potential to transform how people seek information on-line, but generated responses from existing large language models (LLMs)-backed generative search engines may not always be accurate. Nonetheless, retrieval-augmented generation exacerbates safety concerns, since adversaries may successfully evade the entire sys-tem by subtly manipulating the most vulnerable part of a claim. To this end, we pro-pose evaluating the robustness of generative search engines in the realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning incorrect responses. Through a comprehensive human evaluation of various generative search engines, such as Bing Chat, PerplexityAI, and YouChat across diverse queries, we demonstrate the effectiveness of adversarial factual questions in inducing incorrect responses. Moreover, retrieval-augmented generation exhibits a higher susceptibility to factual errors compared to LLMs without retrieval. These findings highlight the potential security risks of these systems and emphasize the need for rigorous evaluation before deployment. Our constructed dataset and codes are available at: https://github.com/HKUSTGZ-NLP/Adversarial-Attack.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2109906988",
        "name": "Xuming Hu"
      },
      {
        "authorId": "2257084387",
        "name": "Xiaochuan Li"
      },
      {
        "authorId": "2223828460",
        "name": "Junzhe Chen"
      },
      {
        "authorId": "2295785468",
        "name": "Yinghui Li"
      },
      {
        "authorId": "2258649570",
        "name": "Yangning Li"
      },
      {
        "authorId": "2273814061",
        "name": "Xiaoguang Li"
      },
      {
        "authorId": "2136912252",
        "name": "Yasheng Wang"
      },
      {
        "authorId": "2275118289",
        "name": "Qun Liu"
      },
      {
        "authorId": "2114092431",
        "name": "Lijie Wen"
      },
      {
        "authorId": "2284755458",
        "name": "Philip S. Yu"
      },
      {
        "authorId": "2313696805",
        "name": "Zhijiang Guo"
      },
      {
        "authorId": "2316576465",
        "name": "Hkustgz"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "85aa0091198be31b3cbf6d03dcabfe0af0988583",
    "url": "https://www.semanticscholar.org/paper/85aa0091198be31b3cbf6d03dcabfe0af0988583",
    "title": "Dimensionality Reduction in Sentence Transformer Vector Databases with Fast Fourier Transform",
    "abstract": "Dimensionality reduction in vector databases is pivotal for streamlining AI data management, enabling efficient storage, faster computation, and improved model performance. This paper explores the benefits of reducing vector database dimensions, with a focus on computational efficiency and overcoming the curse of dimensionality. We introduce a novel application of Fast Fourier Transform (FFT) to dimensionality reduction, a method previously underexploited in this context. By demonstrating its utility across various AI domains, including Retrieval-Augmented Generation (RAG) models and image processing, this FFT-based approach promises to improve data retrieval processes and enhance the efficiency and scalability of AI solutions. The incorporation of FFT may not only optimize operations in real-time processing and recommendation systems but also extend to advanced image processing techniques, where dimensionality reduction can significantly improve performance and analysis efficiency. This paper advocates for the broader adoption of FFT in vector database management, marking a significant stride towards addressing the challenges of data volume and complexity in AI research and applications. Unlike many existing approaches, we directly handle the embedding vectors produced by the model after processing a test input.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-09",
    "authors": [
      {
        "authorId": "2295731621",
        "name": "Vitaly Bulgakov"
      },
      {
        "authorId": "2295731898",
        "name": "Alec Segal"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "5ea537975c23f262bed094a1d8aced7425a9efd5",
    "url": "https://www.semanticscholar.org/paper/5ea537975c23f262bed094a1d8aced7425a9efd5",
    "title": "Enhancing International Graduate Student Experience through AI-Driven Support Systems: A LLM and RAG-Based Approach",
    "abstract": "International graduate students encounter unique challenges that impede their academic and personal success. This paper introduces an AI-powered chatbot designed specifically for these students, utilizing advanced language models and Retrieval-Augmented Generation (RAG). Unlike generic solutions, our chatbot is tailored with a dataset curated from Reddit communi-ties frequented by international students, enabling it to provide highly relevant and actionable advice. The system combines GPT-3.5's generative capabilities with precise information retrieval to effectively guide students through academic procedures, cul-tural adjustments, and personal challenges. An evaluation shows that our RAG-enhanced model outperforms standard GPT-3.5, demonstrating significant improvements in response accuracy and relevance. This research not only advances AI applications in student support but also offers practical, real-time aid to enhance international students' educational experiences.",
    "venue": "2024 International Conference on Data Science and Its Applications (ICoDSA)",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2024-07-10",
    "authors": [
      {
        "authorId": "2319919899",
        "name": "Binita Saha"
      },
      {
        "authorId": "2265682571",
        "name": "Utsha Saha"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.47918433002164
  },
  {
    "paperId": "f6417c02f190016f7b1d381b0e5947816c378182",
    "url": "https://www.semanticscholar.org/paper/f6417c02f190016f7b1d381b0e5947816c378182",
    "title": "Can Github issues be solved with Tree Of Thoughts?",
    "abstract": "While there have been extensive studies in code generation by large language models (LLM), where benchmarks like HumanEval have been surpassed with an impressive 96.3% success rate, these benchmarks predominantly judge a model's performance on basic function-level code generation and lack the critical thinking and concept of scope required of real-world scenarios such as solving GitHub issues. This research introduces the application of the Tree of Thoughts (ToT) language model reasoning framework for enhancing the decision-making and problem-solving abilities of LLMs for this complex task. Compared to traditional input-output (IO) prompting and Retrieval Augmented Generation (RAG) techniques, ToT is designed to improve performance by facilitating a structured exploration of multiple reasoning trajectories and enabling self-assessment of potential solutions. We experimentally deploy ToT in tackling a Github issue contained within an instance of the SWE-bench. However, our results reveal that the ToT framework alone is not enough to give LLMs the critical reasoning capabilities to outperform existing methods. In this paper we analyze the potential causes of these shortcomings and identify key areas for improvement such as deepening the thought process and introducing agentic capabilities. The insights of this research are aimed at informing future directions for refining the application of ToT and better harnessing the potential of LLMs in real-world problem-solving scenarios.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-05-20",
    "authors": [
      {
        "authorId": "2302823078",
        "name": "Ricardo La Rosa"
      },
      {
        "authorId": "2302797688",
        "name": "Corey Hulse"
      },
      {
        "authorId": "2302871455",
        "name": "Bangdi Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "98027ba0387ce614332b894de8f244b7d159d374",
    "url": "https://www.semanticscholar.org/paper/98027ba0387ce614332b894de8f244b7d159d374",
    "title": "Bridging Human and AI Decision-Making with LLMs: The RAGADA Approach",
    "abstract": ": The Retrieval Augmented Generation Algorithmic Decision Alignment (RAGADA) architecture is an advancement in AI-augmented decision-making for corporate environments. This paper discusses RAGADA‚Äôs innovative architecture that merges RAG and Multi-Agent System (MAS) with sophisticated business algorithms and dynamic interfaces, enhancing natural language interaction between AI systems and users. This fusion extends AI‚Äôs reach, facilitating adaptable decision-making tools for leaders, in line with evolving business strategies and ethical standards. Experimental validation of RAGADA within the banking sector, involving diverse stakeholder groups ranging from customers to business and ethical managers, confirms its effectiveness. The system adeptly translates natural language inquiries into actionable insights, thereby improving the user experience and decision-making transparency. This validation underscores RAGADA‚Äôs potential to transform stakeholder engagement and demonstrates a leap in utilizing AI for strategic and ethical business management.",
    "venue": "International Conference on Enterprise Information Systems",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "3017827",
        "name": "Tapio Pitk√§ranta"
      },
      {
        "authorId": "2299658327",
        "name": "Leena Pitk√§ranta"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "9aac23b61414400493c2dcf95d7dc7a5a4e3b068",
    "url": "https://www.semanticscholar.org/paper/9aac23b61414400493c2dcf95d7dc7a5a4e3b068",
    "title": "InspectorRAGet: An Introspection Platform for RAG Evaluation",
    "abstract": "Large Language Models (LLM) have become a popular approach for implementing Retrieval Augmented Generation (RAG) systems, and a significant amount of effort has been spent on building good models and metrics. In spite of increased recognition of the need for rigorous evaluation of RAG systems, few tools exist that go beyond the creation of model output and automatic calculation. We present InspectorRAGet, an introspection platform for RAG evaluation. InspectorRAGet allows the user to analyze aggregate and instance-level performance of RAG systems, using both human and algorithmic metrics as well as annotator quality. InspectorRAGet is suitable for multiple use cases and is available publicly to the community. The demo video is available at https://youtu.be/MJhe8QIXcEc",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-26",
    "authors": [
      {
        "authorId": "3468202",
        "name": "Kshitij P. Fadnis"
      },
      {
        "authorId": "2298938126",
        "name": "Siva Sankalp Patel"
      },
      {
        "authorId": "1997603",
        "name": "O. Boni"
      },
      {
        "authorId": "2114890014",
        "name": "Yannis Katsis"
      },
      {
        "authorId": "144063596",
        "name": "Sara Rosenthal"
      },
      {
        "authorId": "2298756560",
        "name": "Benjamin Sznajder"
      },
      {
        "authorId": "1994333",
        "name": "Marina Danilevsky"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "258ea61fdb5fba1c7f566bfc3cc366047e577260",
    "url": "https://www.semanticscholar.org/paper/258ea61fdb5fba1c7f566bfc3cc366047e577260",
    "title": "Maven at MEDIQA-CORR 2024: Leveraging RAG and Medical LLM for Error Detection and Correction in Medical Notes",
    "abstract": "Addressing the critical challenge of identifying and rectifying medical errors in clinical notes, we present a novel approach tailored for the MEDIQA-CORR task @ NAACL-ClinicalNLP 2024, which comprises three subtasks: binary classification, span identification, and natural language generation for error detection and correction. Binary classification involves detecting whether the text contains a medical error; span identification entails identifying the text span associated with any detected error; and natural language generation focuses on providing a free text correction if a medical error exists. Our proposed architecture leverages Named Entity Recognition (NER) for identifying disease-related terms, Retrieval-Augmented Generation (RAG) for contextual understanding from external datasets, and a quantized and fine-tuned Palmyra model for error correction. Our model achieved a global rank of 5 with an aggregate score of 0.73298, calculated as the mean of ROUGE-1-F, BERTScore, and BLEURT scores.",
    "venue": "Clinical Natural Language Processing Workshop",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2308481744",
        "name": "Suramya Jadhav"
      },
      {
        "authorId": "2308480949",
        "name": "Abhay Gajanan Shanbhag"
      },
      {
        "authorId": "2308986648",
        "name": "Sumedh Joshi"
      },
      {
        "authorId": "2308481093",
        "name": "Atharva Date"
      },
      {
        "authorId": "2301460755",
        "name": "Sheetal Sonawane"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "9054158bd18445925b07006afaf41716361be00c",
    "url": "https://www.semanticscholar.org/paper/9054158bd18445925b07006afaf41716361be00c",
    "title": "Luna: An Evaluation Foundation Model to Catch Language Model Hallucinations with High Accuracy and Low Cost",
    "abstract": "Retriever Augmented Generation (RAG) systems have become pivotal in enhancing the capabilities of language models by incorporating external knowledge retrieval mechanisms. However, a significant challenge in deploying these systems in industry applications is the detection and mitigation of hallucinations: instances where the model generates information that is not grounded in the retrieved context. Addressing this issue is crucial for ensuring the reliability and accuracy of responses generated by large language models (LLMs) in diverse industry settings. Current hallucination detection techniques fail to deliver accuracy, low latency, and low cost simultaneously. We introduce Luna: a DeBERTA-large (440M) encoder, finetuned for hallucination detection in RAG settings. We demonstrate that Luna outperforms GPT-3.5 and commercial evaluation frameworks on the hallucination detection task, with 97% and 91% reduction in cost and latency, respectively. Luna is lightweight and generalizes across multiple industry verticals and out-of-domain data, making it an ideal candidate for industry LLM applications.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-03",
    "authors": [
      {
        "authorId": "2298299139",
        "name": "Masha Belyi"
      },
      {
        "authorId": "2262445823",
        "name": "Robert Friel"
      },
      {
        "authorId": "2304484867",
        "name": "Shuai Shao"
      },
      {
        "authorId": "2123005528",
        "name": "Atindriyo Sanyal"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "d47b9407cf0938e9f15ce019031d7cd63b308a01",
    "url": "https://www.semanticscholar.org/paper/d47b9407cf0938e9f15ce019031d7cd63b308a01",
    "title": "Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks",
    "abstract": "Making the contents generated by Large Language Model (LLM), accurate, credible and traceable is crucial, especially in complex knowledge-intensive tasks that require multi-step reasoning and each step needs knowledge to solve. Retrieval-augmented generation is good potential to solve this problem. However, where and how to introduce Information Retrieval (IR) to LLM is a big challenge. Previous work has the problems that wrong knowledge retrieved by IR misleads the LLM and interaction between IR and LLM breaks the reasoning chain of LLM. This paper proposes a novel framework named Search-in-the-Chain (SearChain) for the interaction between LLM and IR to solve the challenges. First, LLM generates the reasoning chain named Chain-of-Query (CoQ) where each node consists of an IR-oriented query-answer pair. Second, IR verifies the answer of each node of CoQ. It corrects the answer that is not consistent with the retrieved information when IR gives high confidence, which improves the credibility. Third, LLM can indicate its missing knowledge in CoQ and rely on IR to provide this knowledge to LLM. These operations improve the accuracy in terms of reasoning and knowledge. Finally, SearChain generates the reasoning process and marks references to supporting documents for each reasoning step, which improves traceability. Interaction with IR in SearChain forms a novel reasoning path based on a tree, which enables LLM to dynamically modify the direction of reasoning. Experiments show that SearChain outperforms state-of-the-art baselines on complex knowledge-intensive tasks including multi-hop Q&A, slot filling, fact checking, and long-form Q&A.",
    "venue": "The Web Conference",
    "year": 2023,
    "citationCount": 21,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2304.14732",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-04-28",
    "authors": [
      {
        "authorId": "2202745",
        "name": "Shicheng Xu"
      },
      {
        "authorId": "2111815778",
        "name": "Liang Pang"
      },
      {
        "authorId": "2476503",
        "name": "Huawei Shen"
      },
      {
        "authorId": "2110251463",
        "name": "Xueqi Cheng"
      },
      {
        "authorId": "2281747744",
        "name": "Tat-Seng Chua"
      }
    ],
    "source": "semantic_scholar",
    "score": 116.36563680037474
  },
  {
    "paperId": "f727986a460a33b9e62ac23f7d8901748e5c10c9",
    "url": "https://www.semanticscholar.org/paper/f727986a460a33b9e62ac23f7d8901748e5c10c9",
    "title": "DB-GPT: Empowering Database Interactions with Private Large Language Models",
    "abstract": "The recent breakthroughs in large language models (LLMs) are positioned to transition many areas of software. Database technologies particularly have an important entanglement with LLMs as efficient and intuitive database interactions are paramount. In this paper, we present DB-GPT, a revolutionary and production-ready project that integrates LLMs with traditional database systems to enhance user experience and accessibility. DB-GPT is designed to understand natural language queries, provide context-aware responses, and generate complex SQL queries with high accuracy, making it an indispensable tool for users ranging from novice to expert. The core innovation in DB-GPT lies in its private LLM technology, which is fine-tuned on domain-specific corpora to maintain user privacy and ensure data security while offering the benefits of state-of-the-art LLMs. We detail the architecture of DB-GPT, which includes a novel retrieval augmented generation (RAG) knowledge system, an adaptive learning mechanism to continuously improve performance based on user feedback and a service-oriented multi-model framework (SMMF) with powerful data-driven agents. Our extensive experiments and user studies confirm that DB-GPT represents a paradigm shift in database interactions, offering a more natural, efficient, and secure way to engage with data repositories. The paper concludes with a discussion of the implications of DB-GPT framework on the future of human-database interaction and outlines potential avenues for further enhancements and applications in the field. The project code is available at https://github.com/eosphoros-ai/DB-GPT. Experience DB-GPT for yourself by installing it with the instructions https://github.com/eosphoros-ai/DB-GPT#install and view a concise 10-minute video at https://www.youtube.com/watch?v=KYs4nTDzEhk.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 18,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-29",
    "authors": [
      {
        "authorId": "2149919635",
        "name": "Siqiao Xue"
      },
      {
        "authorId": "2109256934",
        "name": "Caigao Jiang"
      },
      {
        "authorId": "2277741965",
        "name": "Wenhui Shi"
      },
      {
        "authorId": "2277599308",
        "name": "Fangyin Cheng"
      },
      {
        "authorId": "2277236474",
        "name": "Keting Chen"
      },
      {
        "authorId": "2277846942",
        "name": "Hongjun Yang"
      },
      {
        "authorId": "2277198733",
        "name": "Zhiping Zhang"
      },
      {
        "authorId": "2277744872",
        "name": "Jianshan He"
      },
      {
        "authorId": "2277403530",
        "name": "Hongyang Zhang"
      },
      {
        "authorId": "2277216932",
        "name": "Ganglin Wei"
      },
      {
        "authorId": "2277244714",
        "name": "Wang Zhao"
      },
      {
        "authorId": "2277238781",
        "name": "Fan Zhou"
      },
      {
        "authorId": "2277216737",
        "name": "Danrui Qi"
      },
      {
        "authorId": "2277198581",
        "name": "Hong Yi"
      },
      {
        "authorId": "2277245042",
        "name": "Shaodong Liu"
      },
      {
        "authorId": "2277961586",
        "name": "Faqiang Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.1665846874966
  },
  {
    "paperId": "9dfe5ff919bdbf2aba927700b828939f409547f4",
    "url": "https://www.semanticscholar.org/paper/9dfe5ff919bdbf2aba927700b828939f409547f4",
    "title": "Contrato360 2.0: A Document and Database-Driven Question-Answer System Using Large Language Models and Agents",
    "abstract": "We present a question-and-answer (Q\\&A) application designed to support the contract management process by leveraging combined information from contract documents (PDFs) and data retrieved from contract management systems (database). This data is processed by a large language model (LLM) to provide precise and relevant answers. The accuracy of these responses is further enhanced through the use of Retrieval-Augmented Generation (RAG), text-to-SQL techniques, and agents that dynamically orchestrate the workflow. These techniques eliminate the need to retrain the language model. Additionally, we employed Prompt Engineering to fine-tune the focus of responses. Our findings demonstrate that this multi-agent orchestration and combination of techniques significantly improve the relevance and accuracy of the answers, offering a promising direction for future information systems.",
    "venue": "International Conference on Knowledge Discovery and Information Retrieval",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-12-23",
    "authors": [
      {
        "authorId": "2332019572",
        "name": "Antony Seabra"
      },
      {
        "authorId": "2331993767",
        "name": "Claudio Cavalcante"
      },
      {
        "authorId": "2332029221",
        "name": "Jo√£o Nepomuceno"
      },
      {
        "authorId": "2332022413",
        "name": "Lucas Lago"
      },
      {
        "authorId": "2581009",
        "name": "Nicolaas Ruberg"
      },
      {
        "authorId": "2332022387",
        "name": "S√©rgio Lifschitz"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "2808681bd06507cb0bb49da76bb15d0997d8efb3",
    "url": "https://www.semanticscholar.org/paper/2808681bd06507cb0bb49da76bb15d0997d8efb3",
    "title": "MedInsight\n : A Multi-Source Context Augmentation Framework for Generating Patient-Centric Medical Responses using Large Language Models",
    "abstract": "\n Providing contextual and comprehensive medical information tailored to individual patients is critical for enabling effective care in the healthcare domain. However, existing approaches often struggle to deliver personalized responses due to the distributed nature of medical data across multiple sources such as patient records, medical literature, and online resources. To address this challenge, we present\n MedInsight\n , a multi-source context augmentation framework that leverages Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) techniques to augment patient-specific information from medical transcripts with trusted knowledge from textbooks and web resources to generate personalized and contextually relevant responses. Our framework consists of three phases: patient context retrieval, medical knowledge retrieval, and response generation. By augmenting patient context with relevant external knowledge,\n MedInsight\n generates contextually relevant responses, empowering patients and caregivers with actionable insights. Experiments on the MTSamples dataset validate\n MedInsight\n ‚Äôs effectiveness in generating contextually appropriate medical responses, using a comprehensive set of metrics including RAGAs, TruLens, ROUGE, and BertScore. Additionally, qualitative evaluations by Subject-Matter Experts (SMEs) further confirm the relevance and factual correctness of the generated responses.\n",
    "venue": "ACM Transactions on Computing for Healthcare",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-12-21",
    "authors": [
      {
        "authorId": "2175998538",
        "name": "Subash Neupane"
      },
      {
        "authorId": "2150699855",
        "name": "Shaswata Mitra"
      },
      {
        "authorId": "2284767922",
        "name": "Sudip Mittal"
      },
      {
        "authorId": "1491238594",
        "name": "Manas Gaur"
      },
      {
        "authorId": "40080937",
        "name": "Noorbakhsh Amiri Golilarz"
      },
      {
        "authorId": "2066329536",
        "name": "Shahram Rahimi"
      },
      {
        "authorId": "2284767873",
        "name": "Amin Amirlatifi"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.39720770839918
  },
  {
    "paperId": "ddf9c2aab6fafeeeca3dce50e2f25a3ba8c30435",
    "url": "https://www.semanticscholar.org/paper/ddf9c2aab6fafeeeca3dce50e2f25a3ba8c30435",
    "title": "SG-RAG: Multi-Hop Question Answering With Large Language Models Through Knowledge Graphs",
    "abstract": "Large Language Models (LLM) such as GPT3 and Llama tend to hallucinate, especially for domain-specific questions. To alleviate this problem, Retrieval Augmented Generation (RAG) has been proposed but LLMs still suffer in multihop question answering even with RAG. Knowledge Graphs represent domain information in a structured manner and they have been used for reasoning in AI. In this work, we propose SubGraph Retrieval Augmented Generation (SG-RAG), a novel zero-shot Graph RAG method that exploits the structured information in Knowledge Graphs in order to accurately answer multihop questions with LLMs. We form a Cypher query based on the given question to retrieve the set of relevant subgraphs that is further provided as context to the Language Model. We implemented and tested our methodology on a benchmark question-answering data set on movies domain. Experiments show that the accuracy of 2-hop and 3-hop questions issued to LLAMA 8B Instruct and GPT4-Turbo significantly increases compared to LLAMA and GPT with and without RAG.",
    "venue": "International Conference on Natural Language and Speech Processing",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2329168112",
        "name": "Ahmmad O. M. Saleh"
      },
      {
        "authorId": "2329167886",
        "name": "Gokhan Tur"
      },
      {
        "authorId": "2286307146",
        "name": "Y√ºcel Sayg√≠n"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "ad6fffaa1c3967f9a08b96b22aa66eba2797b7aa",
    "url": "https://www.semanticscholar.org/paper/ad6fffaa1c3967f9a08b96b22aa66eba2797b7aa",
    "title": "Assessing generalization capability of text ranking models in Polish",
    "abstract": "Retrieval-augmented generation (RAG) is becoming an increasingly popular technique for integrating internal knowledge bases with large language models. In a typical RAG pipeline, three models are used, responsible for the retrieval, reranking, and generation stages. In this article, we focus on the reranking problem for the Polish language, examining the performance of rerankers and comparing their results with available retrieval models. We conduct a comprehensive evaluation of existing models and those trained by us, utilizing a benchmark of 41 diverse information retrieval tasks for the Polish language. The results of our experiments show that most models struggle with out-of-domain generalization. However, a combination of effective optimization method and a large training dataset allows for building rerankers that are both compact in size and capable of generalization. The best of our models establishes a new state-of-the-art for reranking in the Polish language, outperforming existing models with up to 30 times more parameters.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-22",
    "authors": [
      {
        "authorId": "2956370",
        "name": "Slawomir Dadas"
      },
      {
        "authorId": "2284988827",
        "name": "Malgorzata Grkebowiec"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "43c7b0d812b03cf350b438395d9be0a834958dac",
    "url": "https://www.semanticscholar.org/paper/43c7b0d812b03cf350b438395d9be0a834958dac",
    "title": "UHH at AVeriTeC: RAG for Fact-Checking with Real-World Claims",
    "abstract": "This paper presents UHH‚Äôs approach developed for the AVeriTeC shared task. The goal of the challenge is to verify given real-world claims with evidences from the Web. In this shared task, we investigate a Retrieval-Augmented Generation (RAG) model, which mainly contains retrieval, generation, and augmentation components. We start with the selection of the top 10k evidences via BM25 scores, and continue with two approaches to retrieve the most similar evidences: (1) to retrieve top 10 evidences through vector similarity, generate questions for them, and rerank them or (2) to generate questions for the claim and retrieve the most similar evidence, again, through vector similarity. After retrieving the top evidences, a Large Language Model (LLM) is prompted using the claim along with either all evidences or individual evidence to predict the label. Our system submission, \\textbf{UHH}, using the first approach and individual evidence prompts, ranks 6th out of 23 systems.",
    "venue": "FEVER",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "27076821",
        "name": "√ñzge Sevgili"
      },
      {
        "authorId": "2293940599",
        "name": "Irina Nikishina"
      },
      {
        "authorId": "3084761",
        "name": "Seid Muhie Yimam"
      },
      {
        "authorId": "2142790279",
        "name": "Martin Semmann"
      },
      {
        "authorId": "2293941297",
        "name": "Christian Biemann"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "c0a72bff9414384340ccab2bcfc3813171c59015",
    "url": "https://www.semanticscholar.org/paper/c0a72bff9414384340ccab2bcfc3813171c59015",
    "title": "Introducing Super RAGs in Mistral 8x7B-v1",
    "abstract": "The relentless pursuit of enhancing Large Language Models (LLMs) has led to the advent of Super Retrieval-Augmented Generation (Super RAGs), a novel approach designed to elevate the performance of LLMs by integrating external knowledge sources with minimal structural modifications. This paper presents the integration of Super RAGs into the Mistral 8x7B v1, a state-of-the-art LLM, and examines the resultant improvements in accuracy, speed, and user satisfaction. Our methodology uses a fine-tuned instruct model setup and a cache tuning fork system, ensuring efficient and relevant data retrieval. The evaluation, conducted over several epochs, demonstrates significant enhancements across all metrics. The findings suggest that Super RAGs can effectively augment LLMs, paving the way for more sophisticated and reliable AI systems. This research contributes to the field by providing empirical evidence of the benefits of Super RAGs and offering insights into their potential applications.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-13",
    "authors": [
      {
        "authorId": "2291134842",
        "name": "Ayush Thakur"
      },
      {
        "authorId": "2297137960",
        "name": "Raghav Gupta"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "64c3f17f015bf0f2aaa36d7f331a3aa74c5e71d9",
    "url": "https://www.semanticscholar.org/paper/64c3f17f015bf0f2aaa36d7f331a3aa74c5e71d9",
    "title": "Decompose, Enrich, and Extract! Schema-aware Event Extraction using LLMs.",
    "abstract": "Large Language Models (LLMs) demonstrate significant capabilities in processing natural language data, promising efficient knowledge extraction from diverse textual sources to enhance situational awareness and support decision-making. However, concerns arise due to their susceptibility to hallucination, resulting in contextually inaccurate content. This work focuses on harnessing LLMs for automated Event Extraction, introducing a new method to address hallucination by decomposing the task into Event Detection and Event Argument Extraction. Moreover, the proposed method integrates dynamic schema-aware augmented retrieval examples into prompts tailored for each specific inquiry, thereby extending and adapting advanced prompting techniques such as Retrieval-Augmented Generation. Evaluation findings on prominent event extraction benchmarks and results from a synthesized benchmark illustrate the method‚Äôs superior performance compared to baseline approaches.",
    "venue": "Fusion",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2406.01045",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-03",
    "authors": [
      {
        "authorId": "49994056",
        "name": "Fatemeh Shiri"
      },
      {
        "authorId": "2282512109",
        "name": "Van Nguyen"
      },
      {
        "authorId": "2804001",
        "name": "Farhad Moghimifar"
      },
      {
        "authorId": "2304561647",
        "name": "John Yoo"
      },
      {
        "authorId": "2561045",
        "name": "Gholamreza Haffari"
      },
      {
        "authorId": "2256011160",
        "name": "Yuan-Fang Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "c1743c55e70965b0ffada89e73f38fef2216467f",
    "url": "https://www.semanticscholar.org/paper/c1743c55e70965b0ffada89e73f38fef2216467f",
    "title": "Language Modeling with Editable External Knowledge",
    "abstract": "When the world changes, so does the text that humans write about it. How do we build language models that can be easily updated to reflect these changes? One popular approach is retrieval-augmented generation, in which new documents are inserted into a knowledge base and retrieved during prediction for downstream tasks. Most prior work on these systems have focused on improving behavior during prediction through better retrieval or reasoning. This paper introduces ERASE, which instead improves model behavior when new documents are acquired, by incrementally deleting or rewriting other entries in the knowledge base each time a document is added. In two new benchmark datasets evaluating models' ability to answer questions about a stream of news articles or conversations, ERASE improves accuracy relative to conventional retrieval-augmented generation by 7-13% (Mixtral-8x7B) and 6-10% (Llama-3-8B) absolute. Code and data are available at https://github.com/belindal/ERASE",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-17",
    "authors": [
      {
        "authorId": "46708422",
        "name": "Belinda Z. Li"
      },
      {
        "authorId": "2266945896",
        "name": "Emmy Liu"
      },
      {
        "authorId": "2300174646",
        "name": "Alexis Ross"
      },
      {
        "authorId": "2307007362",
        "name": "Abbas Zeitoun"
      },
      {
        "authorId": "2285194103",
        "name": "Graham Neubig"
      },
      {
        "authorId": "2294871694",
        "name": "Jacob Andreas"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "e5111feb46bb0d66fb3a185855223770f9cb1d5d",
    "url": "https://www.semanticscholar.org/paper/e5111feb46bb0d66fb3a185855223770f9cb1d5d",
    "title": "Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering",
    "abstract": "Retrieval-augmented generation models augment knowledge encoded in a language model by providing additional relevant external knowledge (context) during generation. Although it has been shown that the quantity and quality of context impact the performance of retrieval-augmented generation models during inference, limited research explores how these characteristics affect model training. This paper explores how context quantity and quality during model training affect the performance of Fusion-in-Decoder (FiD), the state-of-the-art retrieval-augmented generation model, in extractive open-domain question answering tasks. Experimental results suggest that FiD models overfit to context quality during training and show suboptimal performance when evaluated on different context quality. Through the experimental results, we also reveal FiD models trained with different context quality have different cross-attention distribution patterns. Specifically, as context quality during training increases, FiD models tend to attend more uniformly to each passage in context. Finally, based on these observations, we propose a method to mitigate overfitting to specific context quality by introducing bias to the cross-attention distribution, which we demonstrate to be effective in improving the performance of FiD models on different context quality.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.findings-emnlp.784.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-21",
    "authors": [
      {
        "authorId": "2273675486",
        "name": "Kosuke Akimoto"
      },
      {
        "authorId": "72198234",
        "name": "Kunihiro Takeoka"
      },
      {
        "authorId": "37267314",
        "name": "M. Oyamada"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "4003b1bb28b3f8463accb3b5d57538708099f0ca",
    "url": "https://www.semanticscholar.org/paper/4003b1bb28b3f8463accb3b5d57538708099f0ca",
    "title": "Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models",
    "abstract": "The conventional use of the Retrieval-Augmented Generation (RAG) architecture has proven effective for retrieving information from diverse documents. However, challenges arise in handling complex table queries, especially within PDF documents containing intricate tabular structures.This research introduces an innovative approach to enhance the accuracy of complex table queries in RAG-based systems. Our methodology involves storing PDFs in the retrieval database and extracting tabular content separately. The extracted tables undergo a process of context enrichment, concatenating headers with corresponding values. To ensure a comprehensive understanding of the enriched data, we employ a fine-tuned version of the Llama-2-chat language model for summarisation within the RAG architecture. Furthermore, we augment the tabular data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt. This enriched data is then fed into the retrieval database alongside other PDFs. Our approach aims to significantly improve the precision of complex table queries, offering a promising solution to a longstanding challenge in information retrieval.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-04",
    "authors": [
      {
        "authorId": "2277736266",
        "name": "Uday Allu"
      },
      {
        "authorId": "2277738950",
        "name": "Biddwan Ahmed"
      },
      {
        "authorId": "2277733379",
        "name": "Vishesh Tripathi"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "b3d447f01e8b50a7a3b3ef16b1efc46d682cd49f",
    "url": "https://www.semanticscholar.org/paper/b3d447f01e8b50a7a3b3ef16b1efc46d682cd49f",
    "title": "Revisiting the Solution of Meta KDD Cup 2024: CRAG",
    "abstract": "This paper presents the solution of our team APEX in the Meta KDD CUP 2024: CRAG Comprehensive RAG Benchmark Challenge. The CRAG benchmark addresses the limitations of existing QA benchmarks in evaluating the diverse and dynamic challenges faced by Retrieval-Augmented Generation (RAG) systems. It provides a more comprehensive assessment of RAG performance and contributes to advancing research in this field. We propose a routing-based domain and dynamic adaptive RAG pipeline, which performs specific processing for the diverse and dynamic nature of the question in all three stages: retrieval, augmentation, and generation. Our method achieved superior performance on CRAG and ranked 2nd for Task 2&3 on the final competition leaderboard. Our implementation is available at this link: https://github.com/USTCAGI/CRAG-in-KDD-Cup2024.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-09",
    "authors": [
      {
        "authorId": "2322501286",
        "name": "Ouyang Jie"
      },
      {
        "authorId": "2208917508",
        "name": "Yucong Luo"
      },
      {
        "authorId": "1491233507",
        "name": "Mingyue Cheng"
      },
      {
        "authorId": "2322524150",
        "name": "Daoyu Wang"
      },
      {
        "authorId": "2322429208",
        "name": "Shuo Yu"
      },
      {
        "authorId": "2243408564",
        "name": "Qi Liu"
      },
      {
        "authorId": "2258714945",
        "name": "Enhong Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "b8ca7bdbf3d88b47dcea181b4890f80bf7276aa8",
    "url": "https://www.semanticscholar.org/paper/b8ca7bdbf3d88b47dcea181b4890f80bf7276aa8",
    "title": "VideoRAG: Scaling the context size and relevance for video question-answering",
    "abstract": "Recent advancements have led to the adaptation of several multimodal large language models (LLMs) for critical video-related use cases, particularly in Video Question-Answering (QA). However, most of the previous models sample only a limited number of frames from video due to the context size limit of backbone LLM. Another approach of applying temporal pooling to compress multiple frames, is also shown to saturate and does not scale well. These limitations cause videoQA on long videos to perform very poorly. To address this, we present VideoRAG, a system to utilize recently popularized Retrieval Augmented Generation (RAG) pipeline to select the top-k frames from video, relevant to the user query. We have observed a qualitative improvement in our experiments, indicating a promising direction to pursue. Additionally, our findings indicate that videoRAG demonstrates superior performance when addressing needle-in-the-haystack questions in long videos. Our extensible system allows for trying multiple strategies for indexing, ranking, and adding QA models.",
    "venue": "International Conference on Natural Language Generation",
    "year": 2024,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2151859861",
        "name": "Shivprasad Sagare"
      },
      {
        "authorId": "2312322662",
        "name": "Prashant Ullegaddi"
      },
      {
        "authorId": "2320840310",
        "name": "Nachiketh K S"
      },
      {
        "authorId": "2320840380",
        "name": "Navanith R"
      },
      {
        "authorId": "2312322874",
        "name": "Kinshuk Sarabhai"
      },
      {
        "authorId": "2320840363",
        "name": "Rajesh Kumar S A"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "0a5e2f812e0f8fc8e71dc74292193cea33c8d422",
    "url": "https://www.semanticscholar.org/paper/0a5e2f812e0f8fc8e71dc74292193cea33c8d422",
    "title": "GenTKG: Generative Forecasting on Temporal Knowledge Graph",
    "abstract": "The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional embedding-based and rule-based methods dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval-augmented generation framework named GenTKG combining a temporal logical rule-based retrieval strategy and few-shot parameter-efficient instruction tuning to solve the above challenges, respectively. Extensive experiments have shown that GenTKG outperforms conventional methods of temporal relational forecasting with low computation resources using extremely limited training data as few as 16 samples. GenTKG also highlights remarkable cross-domain generalizability with outperforming performance on unseen datasets without re-training, and in-domain generalizability regardless of time split in the same dataset. Our work reveals the huge potential of LLMs in the tKG domain and opens a new frontier for generative forecasting on tKGs. Code and data are released here: https://github.com/mayhugotong/GenTKG.",
    "venue": "NAACL-HLT",
    "year": 2023,
    "citationCount": 16,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.07793",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-11",
    "authors": [
      {
        "authorId": "2072387342",
        "name": "Ruotong Liao"
      },
      {
        "authorId": "2257432618",
        "name": "Xu Jia"
      },
      {
        "authorId": "10684484",
        "name": "Yunpu Ma"
      },
      {
        "authorId": "1742501819",
        "name": "Volker Tresp"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.49820016084324
  },
  {
    "paperId": "57065230d87dc9f703916185ef0220ac51a6b306",
    "url": "https://www.semanticscholar.org/paper/57065230d87dc9f703916185ef0220ac51a6b306",
    "title": "AI-TA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs",
    "abstract": "Responding to the thousands of student questions on online QA platforms each semester has a considerable human cost, particularly in computing courses with rapidly growing enrollments. To address the challenges of scalable and intelligent question-answering (QA), we introduce an innovative solution that leverages open-source Large Language Models (LLMs) from the LLaMA-2 family to ensure data privacy. Our approach combines augmentation techniques such as retrieval augmented generation (RAG), supervised fine-tuning (SFT), and learning from human preferences data using Direct Preference Optimization (DPO). Through extensive experimentation on a Piazza dataset from an introductory CS course, comprising 10,000 QA pairs and 1,500 pairs of preference data, we demonstrate a significant 30% improvement in the quality of answers, with RAG being a particularly impactful addition. Our contributions include the development of a novel architecture for educational QA, extensive evaluations of LLM performance utilizing both human assessments and LLM-based metrics, and insights into the challenges and future directions of educational data processing. This work paves the way for the development of AI-TA, an intelligent QA assistant customizable for courses with an online QA platform",
    "venue": "",
    "year": 2023,
    "citationCount": 15,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-11-05",
    "authors": [
      {
        "authorId": "2170071419",
        "name": "Yann Hicke"
      },
      {
        "authorId": "2265498048",
        "name": "Anmol Agarwal"
      },
      {
        "authorId": "2265585087",
        "name": "Qianou Ma"
      },
      {
        "authorId": "2265491261",
        "name": "Paul Denny"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.58883083359672
  },
  {
    "paperId": "ff2d0198820cd97421ab6a8626d7e10f07c2c4ba",
    "url": "https://www.semanticscholar.org/paper/ff2d0198820cd97421ab6a8626d7e10f07c2c4ba",
    "title": "ChaTA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs",
    "abstract": "To address the challenges of scalable and intelligent question-answering (QA), we introduce an innovative solution that leverages open-source Large Language Models (LLMs) to ensure data privacy. We use models from the LLaMA-2 family and augmentations including retrieval augmented generation (RAG), supervised fine-tuning (SFT), and an alternative to reinforcement learning with human feedback (RLHF). We perform our experiments on a Piazza dataset from an introductory CS course with 10 k QA pairs and 1 . 5 k pairs of preferences data and conduct both human evaluations and automatic LLM evaluations on a small subset. We find preliminary evidence that modeling techniques collectively enhance the quality of answers by 33%, and RAG is an impactful addition. This work paves the way for the development of C HA TA, an intelligent QA assistant customizable for courses with an online QA platform.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 15,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2170071419",
        "name": "Yann Hicke"
      },
      {
        "authorId": "2265498048",
        "name": "Anmol Agarwal"
      },
      {
        "authorId": "2265585087",
        "name": "Qianou Ma"
      },
      {
        "authorId": "2265491261",
        "name": "Paul Denny"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.58883083359672
  },
  {
    "paperId": "27ba056e1b9d0960b821cfddeb59e357ed894dff",
    "url": "https://www.semanticscholar.org/paper/27ba056e1b9d0960b821cfddeb59e357ed894dff",
    "title": "Which Neurons Matter in IR? Applying Integrated Gradients-based Methods to Understand Cross-Encoders",
    "abstract": "With the recent addition of Retrieval-Augmented Generation (RAG), the scope and importance of Information Retrieval (IR) has expanded. As a result, the importance of a deeper understanding of IR models also increases. However, interpretability in IR remains under-explored, especially when it comes to the models' inner mechanisms. In this paper, we explore the possibility of adapting Integrated Gradient-based methods in an IR context to identify the role of individual neurons within the model. In particular, we provide new insights into the role of what we call\"relevance\"neurons, as well as how they deal with unseen data. Finally, we carry out an in-depth pruning study to validate our findings.",
    "venue": "International Conference on the Theory of Information Retrieval",
    "year": 2024,
    "citationCount": 0,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3664190.3672528",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-27",
    "authors": [
      {
        "authorId": "2280134732",
        "name": "Mathias Vast"
      },
      {
        "authorId": "2280134548",
        "name": "Basile Van Cooten"
      },
      {
        "authorId": "2265489279",
        "name": "Laure Soulier"
      },
      {
        "authorId": "1703777",
        "name": "Benjamin Piwowarski"
      }
    ],
    "source": "semantic_scholar",
    "score": 70
  },
  {
    "paperId": "64852dd925065dab03322d8ca90c836a0725a547",
    "url": "https://www.semanticscholar.org/paper/64852dd925065dab03322d8ca90c836a0725a547",
    "title": "HyKGE: A Hypothesis Knowledge Graph Enhanced Framework for Accurate and Reliable Medical LLMs Responses",
    "abstract": "In this paper, we investigate the retrieval-augmented generation (RAG) based on Knowledge Graphs (KGs) to improve the accuracy and reliability of Large Language Models (LLMs). Recent approaches suffer from insufficient and repetitive knowledge retrieval, tedious and time-consuming query parsing, and monotonous knowledge utilization. To this end, we develop a Hypothesis Knowledge Graph Enhanced (HyKGE) framework, which leverages LLMs' powerful reasoning capacity to compensate for the incompleteness of user queries, optimizes the interaction process with LLMs, and provides diverse retrieved knowledge. Specifically, HyKGE explores the zero-shot capability and the rich knowledge of LLMs with Hypothesis Outputs to extend feasible exploration directions in the KGs, as well as the carefully curated prompt to enhance the density and efficiency of LLMs' responses. Furthermore, we introduce the HO Fragment Granularity-aware Rerank Module to filter out noise while ensuring the balance between diversity and relevance in retrieved knowledge. Experiments on two Chinese medical multiple-choice question datasets and one Chinese open-domain medical Q&A dataset with two LLM turbos demonstrate the superiority of HyKGE in terms of accuracy and explainability.",
    "venue": "",
    "year": 2023,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-12-26",
    "authors": [
      {
        "authorId": "2181297535",
        "name": "Xinke Jiang"
      },
      {
        "authorId": "2276704515",
        "name": "Ruizhe Zhang"
      },
      {
        "authorId": "2215472732",
        "name": "Yongxin Xu"
      },
      {
        "authorId": "2276425552",
        "name": "Rihong Qiu"
      },
      {
        "authorId": "2276489740",
        "name": "Yue Fang"
      },
      {
        "authorId": "2215476252",
        "name": "Zhiyuan Wang"
      },
      {
        "authorId": "2276510531",
        "name": "Jinyi Tang"
      },
      {
        "authorId": "2256163148",
        "name": "Hongxin Ding"
      },
      {
        "authorId": "2276425819",
        "name": "Xu Chu"
      },
      {
        "authorId": "2145804723",
        "name": "Junfeng Zhao"
      },
      {
        "authorId": "2253831765",
        "name": "Yasha Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "8c52b3bbe5897ba3f42b38c5bfc33bbd48f9a1f2",
    "url": "https://www.semanticscholar.org/paper/8c52b3bbe5897ba3f42b38c5bfc33bbd48f9a1f2",
    "title": "LLM4VV: Developing LLM-Driven Testsuite for Compiler Validation",
    "abstract": "Large language models (LLMs) are a new and powerful tool for a wide span of applications involving natural language and demonstrate impressive code generation abilities. In this paper, we explore the capabilitity of state-of-the-art LLMs, including closed-source options like OpenAI GPT-4 and open-source alternatives like Meta AI Codellama, to automatically generate tests and use these tests to validate and verify compiler implementations of a directive-based programming paradigm, OpenACC. Our approach entails exploring various prompt engineering techniques including a code template, retrieval-augmented generation (RAG) with code template, expressive prompt using RAG with code template, one-shot example, and RAG with one-shot example. This paper focusses on (a) exploring the capabilities of the latest LLMs for code generation, (b) investigating prompt and fine tuning methods, and (c) analyzing the outcome of LLMs generated tests",
    "venue": "Future generations computer systems",
    "year": 2023,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.04963",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-08",
    "authors": [
      {
        "authorId": "2203797239",
        "name": "Christian Munley"
      },
      {
        "authorId": "2203801016",
        "name": "Aaron Jarmusch"
      },
      {
        "authorId": "2257004353",
        "name": "Sunita Chandrasekaran"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "11e56b35fa81ddb00dc96329c077c5b39c8f9e75",
    "url": "https://www.semanticscholar.org/paper/11e56b35fa81ddb00dc96329c077c5b39c8f9e75",
    "title": "A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia",
    "abstract": "Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a novel dataset of counterfactual texts constructed to clash with a model's internal parametric knowledge. In this study, we introduce Fakepedia, a counterfactual dataset designed to evaluate grounding abilities when the internal parametric knowledge clashes with the contextual information. We benchmark various LLMs with Fakepedia and conduct a causal mediation analysis of LLM components when answering Fakepedia queries, based on our Masked Grouped Causal Tracing (MGCT) method. Through this analysis, we identify distinct computational patterns between grounded and ungrounded responses. We finally demonstrate that distinguishing grounded from ungrounded responses is achievable through computational analysis alone. Our results, together with existing findings about factual recall mechanisms, provide a coherent narrative of how grounding and factual recall mechanisms interact within LLMs.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-04",
    "authors": [
      {
        "authorId": "2269473832",
        "name": "Giovanni Monea"
      },
      {
        "authorId": "35512303",
        "name": "Maxime Peyrard"
      },
      {
        "authorId": "65826567",
        "name": "Martin Josifoski"
      },
      {
        "authorId": "113810201",
        "name": "Vishrav Chaudhary"
      },
      {
        "authorId": "2269472030",
        "name": "Jason Eisner"
      },
      {
        "authorId": "2264962872",
        "name": "Emre Kiciman"
      },
      {
        "authorId": "2269473744",
        "name": "Hamid Palangi"
      },
      {
        "authorId": "27419446",
        "name": "Barun Patra"
      },
      {
        "authorId": "2269473532",
        "name": "Robert West"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "0645c86da0f6329f13489654210eaaca87be2e22",
    "url": "https://www.semanticscholar.org/paper/0645c86da0f6329f13489654210eaaca87be2e22",
    "title": "You Truly Understand What I Need: Intellectual and Friendly Dialogue Agents grounding Knowledge and Persona",
    "abstract": "To build a conversational agent that interacts fluently with humans, previous studies blend knowledge or personal profile into the pre-trained language model. However, the model that considers knowledge and persona at the same time is still limited, leading to hallucination and a passive way of using personas. We propose an effective dialogue agent that grounds external knowledge and persona simultaneously. The agent selects the proper knowledge and persona to use for generating the answers with our candidate scoring implemented with a poly-encoder. Then, our model generates the utterance with lesser hallucination and more engagingness utilizing retrieval augmented generation with knowledge-persona enhanced query. We conduct experiments on the persona-knowledge chat and achieve state-of-the-art performance in grounding and generation tasks on the automatic metrics. Moreover, we validate the answers from the models regarding hallucination and engagingness through human evaluation and qualitative results. We show our retriever's effectiveness in extracting relevant documents compared to the other previous retrievers, along with the comparison of multiple candidate scoring methods. Code is available at https://github.com/dlawjddn803/INFO",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2301.02401",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-01-06",
    "authors": [
      {
        "authorId": "2109641713",
        "name": "J. Lim"
      },
      {
        "authorId": "48082816",
        "name": "Myunghoon Kang"
      },
      {
        "authorId": "98207546",
        "name": "Yuna Hur"
      },
      {
        "authorId": "2110516387",
        "name": "Seung-Ju Jung"
      },
      {
        "authorId": "2117157876",
        "name": "Jinsung Kim"
      },
      {
        "authorId": "2072583773",
        "name": "Yoonna Jang"
      },
      {
        "authorId": "2115276420",
        "name": "Dongyub Lee"
      },
      {
        "authorId": "3364034",
        "name": "Hyesung Ji"
      },
      {
        "authorId": null,
        "name": "Donghoon Shin"
      },
      {
        "authorId": "2596437",
        "name": "Seung Wook Kim"
      },
      {
        "authorId": "83056580",
        "name": "Heu-Jeoung Lim"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "f3af4c95c048fc698085901fa9a8dc48bb3768e7",
    "url": "https://www.semanticscholar.org/paper/f3af4c95c048fc698085901fa9a8dc48bb3768e7",
    "title": "Domain Adaptation for Conversational Query Production with the RAG Model Feedback",
    "abstract": "Conversational query production is an emerging fundamental task for the dialogue system, where search queries are generated to explore the vast and continually updating knowledge from a search engine. To accelerate this line of research, previous studies have released several datasets with human-annotated search queries. However, the limited annotations still can not cover conversations of various domains. To solve this challenge, we propose a novel domain adaptation framework. It is inspired by a weakly supervised learning algorithm from previous work (Wang et al., 2023b) that guides a model using reinforcement learning with BM25 scores as feedback. Though effective, it is fragile facing noisy content on webpages from a commercial search engine and variance in conversations because of ignoring deep semantic information of dialogue contexts. Thus, we improve the algorithm by taking the advance of retrieval-augmented generation (RAG) and exploring several practical techniques such as knowledge distillation for stable training. We conduct experiments in multiple settings across different languages. Guided by the RAG model feedback, our model is more robust and performs significantly better especially in a more challenging setting over strong baselines. 1",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.findings-emnlp.612.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1754106063",
        "name": "Ante Wang"
      },
      {
        "authorId": "2273672063",
        "name": "Linfeng Song"
      },
      {
        "authorId": "2273912098",
        "name": "Ge Xu"
      },
      {
        "authorId": "2274092718",
        "name": "Jinsong Su"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "ccd8dde6ae2ef1320113d1c0bf5736b3d3bbdd81",
    "url": "https://www.semanticscholar.org/paper/ccd8dde6ae2ef1320113d1c0bf5736b3d3bbdd81",
    "title": "GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models",
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding across various domains, including healthcare and finance. For some tasks, LLMs achieve similar or better performance than trained human beings, therefore it is reasonable to employ human exams (e.g., certification tests) to assess the performance of LLMs. We present a comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their ability to answer agriculture-related questions. In our evaluation, we also employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement) techniques, which combine information retrieval, generation capabilities, and prompting strategies to improve the LLMs' performance. To demonstrate the capabilities of LLMs, we selected agriculture exams and benchmark datasets from three of the largest agriculture producer countries: Brazil, India, and the USA. Our analysis highlights GPT-4's ability to achieve a passing score on exams to earn credits for renewing agronomist certifications, answering 93% of the questions correctly and outperforming earlier general-purpose models, which achieved 88% accuracy. On one of our experiments, GPT-4 obtained the highest performance when compared to human subjects. This performance suggests that GPT-4 could potentially pass on major graduate education admission tests or even earn credits for renewing agronomy certificates. We also explore the models' capacity to address general agriculture-related questions and generate crop management guidelines for Brazilian and Indian farmers, utilizing robust datasets from the Brazilian Agency of Agriculture (Embrapa) and graduate program exams from India. The results suggest that GPT-4, ER, and RAG can contribute meaningfully to agricultural education, assessment, and crop management practice, offering valuable insights to farmers and agricultural professionals.",
    "venue": "",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-10-10",
    "authors": [
      {
        "authorId": "2257019569",
        "name": "B. Silva"
      },
      {
        "authorId": "2256989583",
        "name": "Leonardo Nunes"
      },
      {
        "authorId": "2256989289",
        "name": "Roberto Estev√£o"
      },
      {
        "authorId": "2257349985",
        "name": "Vijay Aski"
      },
      {
        "authorId": "2256993742",
        "name": "Ranveer Chandra"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "1e35a2be239326bb4b99a4d299eecccf8c74ff69",
    "url": "https://www.semanticscholar.org/paper/1e35a2be239326bb4b99a4d299eecccf8c74ff69",
    "title": "RAPGen: An Approach for Fixing Code Inefficiencies in Zero-Shot",
    "abstract": "Performance bugs are non-functional bugs that can even manifest in well-tested commercial products. Fixing these performance bugs is an important yet challenging problem. In this work, we address this challenge and present a new approach called Retrieval-Augmented Prompt Generation (RAPGen). Given a code snippet with a performance issue, RAPGen first retrieves a prompt instruction from a pre-constructed knowledge-base of previous performance bug fixes and then generates a prompt using the retrieved instruction. It then uses this prompt on a Large Language Model (such as Codex) in zero-shot to generate a fix. We compare our approach with the various prompt variations and state of the art methods in the task of performance bug fixing. Our evaluation shows that RAPGen can generate performance improvement suggestions equivalent or better than a developer in ~60% of the cases, getting ~42% of them verbatim, in an expert-verified dataset of past performance changes made by C# developers.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.17077",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-29",
    "authors": [
      {
        "authorId": "2113364164",
        "name": "Spandan Garg"
      },
      {
        "authorId": "1680427",
        "name": "Roshanak Zilouchian Moghaddam"
      },
      {
        "authorId": "145507437",
        "name": "Neel Sundaresan"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "f3122c5e7524aaacc4dea5583cd876ccc53fe92a",
    "url": "https://www.semanticscholar.org/paper/f3122c5e7524aaacc4dea5583cd876ccc53fe92a",
    "title": "CarExpert: Leveraging Large Language Models for In-Car Conversational Question Answering",
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance by following natural language instructions without fine-tuning them on domain-specific tasks and data. However, leveraging LLMs for domain-specific question answering suffers from severe limitations. The generated answer tends to hallucinate due to the training data collection time (when using off-the-shelf), complex user utterance and wrong retrieval (in retrieval-augmented generation). Furthermore, due to the lack of awareness about the domain and expected output, such LLMs may generate unexpected and unsafe answers that are not tailored to the target domain. In this paper, we propose CarExpert, an in-car retrieval-augmented conversational question-answering system leveraging LLMs for different tasks. Specifically, CarExpert employs LLMs to control the input, provide domain-specific documents to the extractive and generative answering components, and controls the output to ensure safe and domain-specific answers. A comprehensive empirical evaluation exhibits that CarExpert outperforms state-of-the-art LLMs in generating natural, safe and car-specific answers.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-14",
    "authors": [
      {
        "authorId": "120441491",
        "name": "Md. Rashad Al Hasan Rony"
      },
      {
        "authorId": "2258719317",
        "name": "Christian Suess"
      },
      {
        "authorId": "2258714509",
        "name": "Sinchana Ramakanth Bhat"
      },
      {
        "authorId": "2121276445",
        "name": "Viju Sudhi"
      },
      {
        "authorId": "2258720522",
        "name": "Julia Schneider"
      },
      {
        "authorId": "2258717442",
        "name": "Maximilian Vogel"
      },
      {
        "authorId": "2176809559",
        "name": "Roman Teucher"
      },
      {
        "authorId": "2258713594",
        "name": "Ken E. Friedl"
      },
      {
        "authorId": "2258762555",
        "name": "S. Sahoo"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "c59b23ee87ba01942a7bab79728f85e66ba34ec7",
    "url": "https://www.semanticscholar.org/paper/c59b23ee87ba01942a7bab79728f85e66ba34ec7",
    "title": "Glitter or gold? Deriving structured insights from sustainability reports via large language models",
    "abstract": "Over the last decade, several regulatory bodies have started requiring the disclosure of non-financial information from publicly listed companies, in light of the investors‚Äô increasing attention to Environmental, Social, and Governance (ESG) issues. Publicly released information on sustainability practices is often disclosed in diverse, unstructured, and multi-modal documentation. This poses a challenge in efficiently gathering and aligning the data into a unified framework to derive insights related to Corporate Social Responsibility (CSR). Thus, using Information Extraction (IE) methods becomes an intuitive choice for delivering insightful and actionable data to stakeholders. In this study, we employ Large Language Models (LLMs), In-Context Learning, and the Retrieval-Augmented Generation (RAG) paradigm to extract structured insights related to ESG aspects from companies‚Äô sustainability reports. We then leverage graph-based representations to conduct statistical analyses concerning the extracted insights. These analyses revealed that ESG criteria cover a wide range of topics, exceeding 500, often beyond those considered in existing categorizations, and are addressed by companies through a variety of initiatives. Moreover, disclosure similarities emerged among companies from the same region or sector, validating ongoing hypotheses in the ESG literature. Lastly, by incorporating additional company attributes into our analyses, we investigated which factors impact the most on companies‚Äô ESG ratings, showing that ESG disclosure affects the obtained ratings more than other financial or company data.",
    "venue": "EPJ Data Science",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.05628",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-09",
    "authors": [
      {
        "authorId": "2256993568",
        "name": "Marco Bronzini"
      },
      {
        "authorId": "2256990455",
        "name": "Carlo Nicolini"
      },
      {
        "authorId": "49305855",
        "name": "B. Lepri"
      },
      {
        "authorId": "2256993219",
        "name": "Andrea Passerini"
      },
      {
        "authorId": "2256994086",
        "name": "Jacopo Staiano"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.1886522358297
  },
  {
    "paperId": "bcc4288d1b346abd794bf69850f5c2b7b728970d",
    "url": "https://www.semanticscholar.org/paper/bcc4288d1b346abd794bf69850f5c2b7b728970d",
    "title": "Large Language Models and Medical Knowledge Grounding for Diagnosis Prediction",
    "abstract": "While large language models (LLMs) have showcased their potential in diverse language tasks, their application in the healthcare arena needs to ensure the minimization of diagnostic errors and the prevention of patient harm. A medical knowledge graph (KG) houses a wealth of structured medical concept relations sourced from authoritative references, such as UMLS, making it a valuable resource to ground LLM diagnostic process in knowledge. In this paper, we examine the synergistic potential of LLMs and medical KG in predicting diagnoses given electronic health records (EHR), under the framework of Retrieval-augmented generation (RAG). We proposed a novel graph model: DR.KNOWS, that selects the most relevant pathology knowledge paths based on the medical problem descriptions. In order to evaluate DR.KNOWS, we developed the first comprehensive human evaluation approach to assess the performance of LLMs for diagnosis prediction and examine the rationale behind their decision-making processes, aimed at improving diagnostic safety. Using real-world hospital datasets, our study serves to enrich the discourse on the role of medical KGs in grounding medical knowledge into LLMs, revealing both challenges and opportunities in harnessing external knowledge for explainable diagnostic pathway and the realization of AI-augmented diagnostic decision support systems.",
    "venue": "medRxiv",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://www.medrxiv.org/content/medrxiv/early/2023/11/27/2023.11.24.23298641.full.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": null,
    "publicationDate": "2023-11-27",
    "authors": [
      {
        "authorId": "2215233233",
        "name": "Y. Gao"
      },
      {
        "authorId": "2268429722",
        "name": "R. Li"
      },
      {
        "authorId": "2268376059",
        "name": "E. Croxford"
      },
      {
        "authorId": "2145148258",
        "name": "S. Tesch"
      },
      {
        "authorId": "2268372565",
        "name": "D. To"
      },
      {
        "authorId": "2268372692",
        "name": "J. Caskey"
      },
      {
        "authorId": "2268376093",
        "name": "B. W. Patterson"
      },
      {
        "authorId": "2250950846",
        "name": "M. Churpek"
      },
      {
        "authorId": "2268377531",
        "name": "T. Miller"
      },
      {
        "authorId": "2159485831",
        "name": "D. Dligach"
      },
      {
        "authorId": "2268372345",
        "name": "M. Afshar"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "31f013e79b51fcbabed37e855ac8394967279d7b",
    "url": "https://www.semanticscholar.org/paper/31f013e79b51fcbabed37e855ac8394967279d7b",
    "title": "Bridging the Language Gap: Dynamic Learning Strategies for Improving Multilingual Performance in LLMs",
    "abstract": "Large language models (LLMs) have revolutionized various domains but still struggle with non-Latin scripts and low-resource languages. This paper addresses the critical challenge of improving multilingual performance without extensive fine-tuning. We introduce a novel dynamic learning approach that optimizes prompt strategy, embedding model, and LLM per query at runtime. By adapting configurations dynamically, our method achieves significant improvements over static, best and random baselines. It operates efficiently in both offline and online settings, generalizing seamlessly across new languages and datasets. Leveraging Retrieval-Augmented Generation (RAG) with state-of-the-art multilingual embeddings, we achieve superior task performance across diverse linguistic contexts. Through systematic investigation and evaluation across 18 diverse languages using popular question-answering (QA) datasets we show our approach results in 10-15% improvements in multilingual performance over pre-trained models and 4x gains compared to fine-tuned, language-specific models.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-28",
    "authors": [
      {
        "authorId": "51464520",
        "name": "A. Nambi"
      },
      {
        "authorId": "1564592237",
        "name": "Vaibhav Balloli"
      },
      {
        "authorId": "1381199788",
        "name": "M. Ranjit"
      },
      {
        "authorId": "1785978",
        "name": "T. Ganu"
      },
      {
        "authorId": "52154863",
        "name": "Kabir Ahuja"
      },
      {
        "authorId": "3010457",
        "name": "Sunayana Sitaram"
      },
      {
        "authorId": "3086996",
        "name": "Kalika Bali"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "f41977c497c96c1da2e9e945315e9be6d6ad472e",
    "url": "https://www.semanticscholar.org/paper/f41977c497c96c1da2e9e945315e9be6d6ad472e",
    "title": "Towards reducing hallucination in extracting information from financial reports using Large Language Models",
    "abstract": "For a financial analyst, the question and answer (Q&A) segment of the company financial report is a crucial piece of information for various analysis and investment decisions. However, extracting valuable insights from the Q&A section has posed considerable challenges as the conventional methods such as detailed reading and note-taking lack scalability and are susceptible to human errors, and Optical Character Recognition (OCR) and similar techniques encounter difficulties in accurately processing unstructured transcript text, often missing subtle linguistic nuances that drive investor decisions. Here, we demonstrate the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracy‚Äîtransforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique as well as metadata. We evaluate the outcomes of various LLMs with and without using our proposed approach based on various objective metrics for evaluating Q&A systems, and empirically demonstrate superiority of our method.",
    "venue": "International Conference on AI-ML-Systems",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Economics",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2023-10-16",
    "authors": [
      {
        "authorId": "23570698",
        "name": "Bhaskarjit Sarmah"
      },
      {
        "authorId": "2258962314",
        "name": "Dhagash Mehta"
      },
      {
        "authorId": "2258960763",
        "name": "Stefano Pasquali"
      },
      {
        "authorId": "2258950844",
        "name": "Tianjie Zhu"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "7ed835ecf9f3f6a222ec830e7d8ee40fd809dae5",
    "url": "https://www.semanticscholar.org/paper/7ed835ecf9f3f6a222ec830e7d8ee40fd809dae5",
    "title": "Context-aware Decoding Reduces Hallucination in Query-focused Summarization",
    "abstract": "Query-focused summarization (QFS) aims to provide a summary of a single document/multi documents that can satisfy the information needs of a given query. It is useful for various real-world applications, such as abstractive snippet generation or more recent retrieval augmented generation (RAG). A prototypical QFS pipeline consists of a retriever (sparse or dense retrieval) and a generator (usually a large language model). However, applying large language models (LLM) potentially leads to hallucinations, especially when the evidence contradicts the prior belief of LLMs. There has been growing interest in developing new decoding methods to improve generation quality and reduce hallucination. In this work, we conduct a large-scale reproducibility study on one recently proposed decoding method -- Context-aware Decoding (CAD). In addition to replicating CAD's experiments on news summarization datasets, we include experiments on QFS datasets, and conduct more rigorous analysis on computational complexity and hyperparameter sensitivity. Experiments with eight different language models show that performance-wise, CAD improves QFS quality by (1) reducing factuality errors/hallucinations while (2) mostly retaining the match of lexical patterns, measured by ROUGE scores, while also at a cost of increased inference-time FLOPs and reduced decoding speed. The code implementation based on Huggingface Library is made available https://github.com/zhichaoxu-shufe/context-aware-decoding-qfs",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-12-21",
    "authors": [
      {
        "authorId": "2276361696",
        "name": "Zhichao Xu"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "c40d26830646699aa93ddc5b4a00fa1f6e75a015",
    "url": "https://www.semanticscholar.org/paper/c40d26830646699aa93ddc5b4a00fa1f6e75a015",
    "title": "UniMC: A Unified Framework for Long-Term Memory Conversation via Relevance Representation Learning",
    "abstract": "Open-domain long-term memory conversation can establish long-term intimacy with humans, and the key is the ability to understand and memorize long-term dialogue history information. Existing works integrate multiple models for modelling through a pipeline, which ignores the coupling between different stages. In this paper, we propose a Unified framework for Long-term Memory Conversations (UniMC), which increases the connection between different stages by learning relevance representation. Specifically, we decompose the main task into three subtasks based on probability graphs: 1) conversation summarization, 2) memory retrieval, 3) memory-augmented generation. Each subtask involves learning a representation for calculating the relevance between the query and memory, which is modelled by inserting a special token at the beginning of the decoder input. The relevance representation learning strengthens the connection across subtasks through parameter sharing and joint training. Extensive experimental results show that the proposed method consistently improves over strong baselines and yields better dialogue consistency and engagingness.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.10543",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-18",
    "authors": [
      {
        "authorId": "1409910562",
        "name": "Kang Zhao"
      },
      {
        "authorId": "46641573",
        "name": "W. Liu"
      },
      {
        "authorId": "2067705842",
        "name": "Jian Luan"
      },
      {
        "authorId": "153481093",
        "name": "Ming-liang Gao"
      },
      {
        "authorId": "2220300688",
        "name": "Li Qian"
      },
      {
        "authorId": "2055665001",
        "name": "H. Teng"
      },
      {
        "authorId": "37722675",
        "name": "Bin Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "df16e8c7599e1a5f7dbc1beebf454cd20e938a59",
    "url": "https://www.semanticscholar.org/paper/df16e8c7599e1a5f7dbc1beebf454cd20e938a59",
    "title": "README: Bridging Medical Jargon and Lay Understanding for Patient Education through Data-Centric NLP",
    "abstract": "The advancement in healthcare has shifted focus toward patient-centric approaches, particularly in self-care and patient education, facilitated by access to Electronic Health Records (EHR). However, medical jargon in EHRs poses significant challenges in patient comprehension. To address this, we introduce a new task of automatically generating lay definitions, aiming to simplify complex medical terms into patient-friendly lay language. We first created the README dataset, an extensive collection of over 50,000 unique (medical term, lay definition) pairs and 300,000 mentions, each offering context-aware lay definitions manually annotated by domain experts. We have also engineered a data-centric Human-AI pipeline that synergizes data filtering, augmentation, and selection to improve data quality. We then used README as the training data for models and leveraged a Retrieval-Augmented Generation method to reduce hallucinations and improve the quality of model outputs. Our extensive automatic and human evaluations demonstrate that open-source mobile-friendly models, when fine-tuned with high-quality data, are capable of matching or even surpassing the performance of state-of-the-art closed-source large language models like ChatGPT. This research represents a significant stride in closing the knowledge gap in patient education and advancing patient-centric healthcare solutions.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-12-24",
    "authors": [
      {
        "authorId": "1576489304",
        "name": "Zonghai Yao"
      },
      {
        "authorId": "2276425805",
        "name": "Nandyala Siddharth Kantu"
      },
      {
        "authorId": "2249534497",
        "name": "Guanghao Wei"
      },
      {
        "authorId": "2263121822",
        "name": "Hieu Tran"
      },
      {
        "authorId": "2276425836",
        "name": "Zhangqi Duan"
      },
      {
        "authorId": "12693064",
        "name": "Sunjae Kwon"
      },
      {
        "authorId": "2261462978",
        "name": "Zhichao Yang"
      },
      {
        "authorId": "2276425268",
        "name": "Readme annotation team"
      },
      {
        "authorId": "2261455807",
        "name": "Hong Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "09bed28b3221f8aaac7f348cf0a5bb683513433b",
    "url": "https://www.semanticscholar.org/paper/09bed28b3221f8aaac7f348cf0a5bb683513433b",
    "title": "DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text",
    "abstract": "Large Language Models (LLMs) have exhibited impressive generation capabilities, but they suffer from hallucinations when solely relying on their internal knowledge, especially when answering questions that require less commonly known information. Retrieval-augmented LLMs have emerged as a potential solution to ground LLMs in external knowledge. Nonetheless, recent approaches have primarily emphasized retrieval from unstructured text corpora, owing to its seamless integration into prompts. When using structured data such as knowledge graphs, most methods simplify it into natural text, neglecting the underlying structures. Moreover, a significant gap in the current landscape is the absence of a realistic benchmark for evaluating the effectiveness of grounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and text). To fill this gap, we have curated a comprehensive dataset that poses two unique challenges: (1) Two-hop multi-source questions that require retrieving information from both open-domain structured and unstructured knowledge sources; retrieving information from structured knowledge sources is a critical component in correctly answering the questions. (2) The generation of symbolic queries (e.g., SPARQL for Wikidata) is a key requirement, which adds another layer of challenge. Our dataset is created using a combination of automatic generation through predefined reasoning chains and human annotation. We also introduce a novel approach that leverages multiple retrieval tools, including text passage retrieval and symbolic language-assisted retrieval. Our model outperforms previous approaches by a significant margin, demonstrating its effectiveness in addressing the above-mentioned reasoning challenges.",
    "venue": "NAACL-HLT",
    "year": 2023,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-31",
    "authors": [
      {
        "authorId": "2258378876",
        "name": "Wenting Zhao"
      },
      {
        "authorId": "2238385821",
        "name": "Ye Liu"
      },
      {
        "authorId": "2263863440",
        "name": "Tong Niu"
      },
      {
        "authorId": "2242935219",
        "name": "Yao Wan"
      },
      {
        "authorId": "2261463957",
        "name": "Philip S. Yu"
      },
      {
        "authorId": "2708940",
        "name": "Shafiq R. Joty"
      },
      {
        "authorId": "2118860628",
        "name": "Yingbo Zhou"
      },
      {
        "authorId": "3014143",
        "name": "Semih Yavuz"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "f63fdbbdf9005245d960ac1912cf4d0805e274a8",
    "url": "https://www.semanticscholar.org/paper/f63fdbbdf9005245d960ac1912cf4d0805e274a8",
    "title": "Minimizing Factual Inconsistency and Hallucination in Large Language Models",
    "abstract": "Large Language Models (LLMs) are widely used in critical fields such as healthcare, education, and finance due to their remarkable proficiency in various language-related tasks. However, LLMs are prone to generating factually incorrect responses or\"hallucinations,\"which can lead to a loss of credibility and trust among users. To address this issue, we propose a multi-stage framework that generates the rationale first, verifies and refines incorrect ones, and uses them as supporting references to generate the answer. The generated rationale enhances the transparency of the answer and our framework provides insights into how the model arrived at this answer, by using this rationale and the references to the context. In this paper, we demonstrate its effectiveness in improving the quality of responses to drug-related inquiries in the life sciences industry. Our framework improves traditional Retrieval Augmented Generation (RAG) by enabling OpenAI GPT-3.5-turbo to be 14-25% more faithful and 16-22% more accurate on two datasets. Furthermore, fine-tuning samples based on our framework improves the accuracy of smaller open-access LLMs by 33-42% and competes with RAG on commercial models.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-23",
    "authors": [
      {
        "authorId": "2268314767",
        "name": "Muneeswaran Irulandi"
      },
      {
        "authorId": "2184792648",
        "name": "Shreya Saxena"
      },
      {
        "authorId": "2268314981",
        "name": "Siva Prasad"
      },
      {
        "authorId": "2268310015",
        "name": "M. V. S. Prakash"
      },
      {
        "authorId": "2268310919",
        "name": "Advaith Shankar"
      },
      {
        "authorId": "2268310248",
        "name": "V. Varun"
      },
      {
        "authorId": "1419986651",
        "name": "Vishal Vaddina"
      },
      {
        "authorId": "1634991498",
        "name": "Saisubramaniam Gopalakrishnan"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "9a6e7438138d5308ff92b8e8ee719a5dbabe9922",
    "url": "https://www.semanticscholar.org/paper/9a6e7438138d5308ff92b8e8ee719a5dbabe9922",
    "title": "GeneGPT: Teaching Large Language Models to Use NCBI Web APIs",
    "abstract": "In this paper, we present GeneGPT, a novel method for teaching large language models (LLMs) to use the Web Application Programming Interfaces (APIs) of the National Center for Biotechnology Information (NCBI) and answer genomics questions. SpeciÔ¨Åcally, we prompt Codex ( code-davinci-002 ) to solve the GeneTuring tests with few-shot URL requests of NCBI API calls as demonstrations for in-context learning. During inference, we stop the decoding once a call request is detected and make the API call with the generated URL. We then append the raw execution results returned by NCBI APIs to the generated texts and continue the generation until the answer is found or another API call is detected. Our preliminary results show that GeneGPT achieves state-of-the-art results on three out of four one-shot tasks and four out of Ô¨Åve zero-shot tasks in the GeneTuring dataset. Overall, GeneGPT achieves a macro-average score of 0.76, which is much higher than retrieval-augmented LLMs such as the New Bing (0.44), biomedical LLMs such as BioMedLM (0.08) and BioGPT (0.04), as well as other LLMs such as GPT-3 (0.16) and ChatGPT (0.12).",
    "venue": "",
    "year": 2023,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "144255060",
        "name": "Qiao Jin"
      },
      {
        "authorId": "2108989300",
        "name": "Yifan Yang"
      },
      {
        "authorId": "48771891",
        "name": "Qingyu Chen"
      },
      {
        "authorId": "2152217328",
        "name": "Zhiyong Lu"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "1face14a7697fd9ff451eca5e8998c101491148c",
    "url": "https://www.semanticscholar.org/paper/1face14a7697fd9ff451eca5e8998c101491148c",
    "title": "Enhancing Drug Safety Documentation Search Capabilities with Large Language Models: A User-Centric Approach",
    "abstract": "Integrating Large Language Models (LLMs) to enhance complex business document retrieval represents an emerging field known as retrieval-augmented generation (RAG). In highly regulated domains like drug safety (pharmacovigilance), its application has remained largely unexplored. This technology brings numerous advantages, including expedited staff on-boarding, enhanced comprehension of contextual queries, and swift information retrieval through natural language inquiries, surpassing conventional keyword searches. This study delves into various operational tasks, such as locating regulatory process guidance, navigating intricate scenarios for advice, and ensuring the LLM's competence in recognizing uncertainties to prevent misinformation. LLMs empower users to engage with documentation using natural language, markedly improving search efficiency. The case study underscores LLM's effectiveness in delivering prompt guidance within pharmacovigilance and adverse event processing and reporting, offering a user-centric solution that streamlines the search for intricate business documentation.",
    "venue": "2023 International Conference on Computational Science and Computational Intelligence (CSCI)",
    "year": 2023,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2023-12-13",
    "authors": [
      {
        "authorId": "2306946860",
        "name": "Jeffery L. Painter"
      },
      {
        "authorId": "2300146987",
        "name": "Olivia Mahaux"
      },
      {
        "authorId": "2312046809",
        "name": "Marco Vanini"
      },
      {
        "authorId": "2167601392",
        "name": "V. Kara"
      },
      {
        "authorId": "2312045893",
        "name": "Christie Roshan"
      },
      {
        "authorId": "2312047161",
        "name": "Marcin Karwowski"
      },
      {
        "authorId": "2307009345",
        "name": "Venkateswara Rao Chalamalasetti"
      },
      {
        "authorId": "2306967615",
        "name": "Andrew Bate"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.47918433002164
  },
  {
    "paperId": "5f24adfcbff1f50b8caee51d9e5896014d3d83e5",
    "url": "https://www.semanticscholar.org/paper/5f24adfcbff1f50b8caee51d9e5896014d3d83e5",
    "title": "Effect of Pivot Language and Segment-Based Few-Shot Prompting for Cross-Domain Multi-Intent Identification in Low Resource Languages",
    "abstract": "NLU (Natural Language Understanding) has considerable difficulties in identifying multiple intentions across different domains in languages with limited resources. Our contributions involve utilizing pivot languages with similar semantics for NLU tasks, creating a vector database for efficient retrieval and indexing of language embeddings in high-resource languages for Retrieval Augmented Generation (RAG) in low-resource languages, and thoroughly investigating the effect of segmentbased strategies on complex user utterances across multiple domains and intents in the development of a Chain of Thought Prompting (COT) combined with Retrieval Augmented Generation. The study investigated recursive approaches to identify the most effective zeroshot instances for segment-based prompting. A comparison analysis was conducted to compare the effectiveness of sentence-based prompting vs segment-based prompting across different domains and multiple intents. This research offers a promising avenue to address the formidable challenges of NLU in low-resource languages, with potential applications in conversational agents and dialogue systems and a broader impact on linguistic understanding and inclusivity.",
    "venue": "ICON",
    "year": 2023,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2311119991",
        "name": "Mitra Kathakali"
      },
      {
        "authorId": "2311119007",
        "name": "Ashish Aditha Venkata Santosh"
      },
      {
        "authorId": "2311119039",
        "name": "Teotia Soumya"
      },
      {
        "authorId": "2311118522",
        "name": "Malapati Aruna"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "f78d23bc22ecf533adbdd40e7e05ae4a715a23f6",
    "url": "https://www.semanticscholar.org/paper/f78d23bc22ecf533adbdd40e7e05ae4a715a23f6",
    "title": "Methods and systems for augmented reality",
    "abstract": "The present invention provides methods and systems (terminals, devices) for the generation, the retrieval and the display of computer-generated holographic images through a head-mounted display. The holographic images may be used as virtual retrievable tags for display in augmented reality.",
    "venue": "",
    "year": 2014,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2014-09-03",
    "authors": [
      {
        "authorId": "1411360897",
        "name": "Ëê®Áº™Â∞î¬∑ÈòøÂ∞îÂ∞öÂçö"
      },
      {
        "authorId": "1411344954",
        "name": "Êù∞ÁΩóÁ±≥¬∑Ëä≠Ëé±Áâπ"
      },
      {
        "authorId": "1414186044",
        "name": "Â••Âæ∑¬∑È≤çÂ•áÂéÑ"
      },
      {
        "authorId": "1414185878",
        "name": "Èáë-‰øùÂ∞î¬∑Âç°ËØ∫"
      },
      {
        "authorId": "1415043072",
        "name": "Êà¥Áª¥Âæ∑¬∑ÂüÉÊñØÂáØÂ•á"
      },
      {
        "authorId": "1411351687",
        "name": "ÊñØÊ≥∞Âá°¬∑‰Ω©ÁΩóÁâπ"
      }
    ],
    "source": "semantic_scholar",
    "score": 60.39720770839918
  },
  {
    "paperId": "147be0de5c181ed2ef2a49b91b1efe0721ace529",
    "url": "https://www.semanticscholar.org/paper/147be0de5c181ed2ef2a49b91b1efe0721ace529",
    "title": "BINS APPROACH TO IMAGE RETRIEVAL USING STATISTICAL PARAMETERS BASED ON HISTOGRAM PARTITIONING OF R, G, B PLANES",
    "abstract": "In this paper we have proposed a novel technique to retrieve the images from large image databases based on the spatial contents of the image to extract the low level features from it for CBIR. In this work image is separated into 3 planes and for each plane we have calculated the histogram which is partitioned into three equal parts to obtain the 27 bins. These bins are holding the spatial-color information of the image in various forms which is generating the different types of the feature vector databases. Three different set of bins are designed to be used as feature vectors containing the total R,G, and B intensities, in R, G and B bins respectively, second form is containing the mean of R,G and B and third is holding standard deviation of R, G and B values in R, G, B bins respectively. This leads to generation of three feature vector databases where size of each feature vector in all databases is 27. Experimentation includes comparison of 100 query images with 1000 database images (Augmented Wang database) using two similarity measures named Euclidean distance and Absolute distance. Results obtained for three feature databases are compared based on the similarity measures reflecting the performance variations of different approaches used. These results are then analyzed and refined using three criteria in this work named Criterion1: Strong, Criterion2: Average and Criterion3: Weak. It has been observed in our results that bins holding standard deviation of R, G and B intensities performing better among all three approaches and refined results using cireterion3 giving very good results as compared to other two.",
    "venue": "",
    "year": 2012,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Mathematics"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "144908905",
        "name": "H. B. Kekre"
      },
      {
        "authorId": "7347461",
        "name": "Kavita Sonawane"
      }
    ],
    "source": "semantic_scholar",
    "score": 98.47424036192305
  },
  {
    "paperId": "075b58ca7efc5c2719019d3046f266b3bb5c3c76",
    "url": "https://www.semanticscholar.org/paper/075b58ca7efc5c2719019d3046f266b3bb5c3c76",
    "title": "Augmented maintenance of powerplants: a prototyping case study of a mobile AR system",
    "abstract": "Augmented reality (AR) research has progressed in great strides over the past few years. Most current demonstrations focus on providing robust tracking solutions since this is the most critical issue when demonstrating AR systems. An issue that is typically neglected concerns the online access, analysis and visualization of information. The information required by AR demonstration systems is kept to a minimum, is prepared ahead of time, and is stored locally in the form of three-dimensional geometric descriptions. In complex mobile settings, these simplifying assumptions do not work. The authors report on recent efforts at the TU Munich to analyze the information generation, retrieval, transmission, and visualization process in the context of maintenance procedures that are performed in nuclear power plants. The use of AR to present such information online has significant implications for the way information must be acquired, stored, and transmitted. The paper focuses on pointing out open questions, discussing options for addressing them, and evaluating them in prototypical implementations.",
    "venue": "Proceedings IEEE and ACM International Symposium on Augmented Reality",
    "year": 2001,
    "citationCount": 75,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2001-10-29",
    "authors": [
      {
        "authorId": "1715693",
        "name": "G. Klinker"
      },
      {
        "authorId": "2092954409",
        "name": "Oliver Creighton"
      },
      {
        "authorId": "153064007",
        "name": "A.H. Dutoit"
      },
      {
        "authorId": "2017827",
        "name": "R. Kobylinski"
      },
      {
        "authorId": "1710184",
        "name": "Christoph Vilsmeier"
      },
      {
        "authorId": "1692839",
        "name": "B. Br√ºgge"
      }
    ],
    "source": "semantic_scholar",
    "score": 130.96100010429495
  },
  {
    "paperId": "6ab36d2577f7c9487b28b2bcdf236191ba901aad",
    "url": "https://www.semanticscholar.org/paper/6ab36d2577f7c9487b28b2bcdf236191ba901aad",
    "title": "End-to-End Learning of Flowchart Grounded Task-Oriented Dialogs",
    "abstract": "We propose a novel problem within end-to-end learning of task oriented dialogs (TOD), in which the dialog system mimics a troubleshooting agent who helps a user by diagnosing their problem (e.g., car not starting). Such dialogs are grounded in domain-specific flowcharts, which the agent is supposed to follow during the conversation. Our task exposes novel technical challenges for neural TOD, such as grounding an utterance to the flowchart without explicit annotation, referring to additional manual pages when user asks a clarification question, and ability to follow unseen flowcharts at test time. We release a dataset (FLODIAL) consisting of 2,738 dialogs grounded on 12 different troubleshooting flowcharts. We also design a neural model, FLONET, which uses a retrieval-augmented generation architecture to train the dialog agent. Our experiments find that FLONET can do zero-shot transfer to unseen flowcharts, and sets a strong baseline for future research.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "citationCount": 19,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2021.emnlp-main.357.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-09-15",
    "authors": [
      {
        "authorId": "1916865",
        "name": "Dinesh Raghu"
      },
      {
        "authorId": "2114357288",
        "name": "Shantanu Agarwal"
      },
      {
        "authorId": "1703799",
        "name": "Sachindra Joshi"
      },
      {
        "authorId": "2674444",
        "name": "Mausam"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.93598410330986
  },
  {
    "paperId": "272b389c94b9592dc50875444e8816e545c4a6a9",
    "url": "https://www.semanticscholar.org/paper/272b389c94b9592dc50875444e8816e545c4a6a9",
    "title": "A Corpus for Understanding and Generating Moral Stories",
    "abstract": "Teaching morals is one of the most important purposes of storytelling. An essential ability for understanding and writing moral stories is bridging story plots and implied morals. Its challenges mainly lie in: (1) grasping knowledge about abstract concepts in morals, (2) capturing inter-event discourse relations in stories, and (3) aligning value preferences of stories and morals concerning good or bad behavior. In this paper, we propose two understanding tasks and two generation tasks to assess these abilities of machines. We present STORAL, a new dataset of Chinese and English human-written moral stories. We show the difficulty of the proposed tasks by testing various models with automatic and manual evaluation on STORAL. Furthermore, we present a retrieval-augmented algorithm that effectively exploits related concepts or events in training sets as additional guidance to improve performance on these tasks.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2022,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2204.09438",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-04-20",
    "authors": [
      {
        "authorId": "2323534462",
        "name": "Jian Guan"
      },
      {
        "authorId": "2117942138",
        "name": "Ziqi Liu"
      },
      {
        "authorId": "1730108",
        "name": "Minlie Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "e6ce185330c00e3f05171bf107df03c88e2a4875",
    "url": "https://www.semanticscholar.org/paper/e6ce185330c00e3f05171bf107df03c88e2a4875",
    "title": "KGI: An Integrated Framework for Knowledge Intensive Language Tasks",
    "abstract": "In this paper, we present a system to showcase the capabilities of the latest state-of-the-art retrieval augmented generation models trained on knowledge-intensive language tasks, such as slot filling, open domain question answering, dialogue, and fact-checking. Moreover, given a user query, we show how the output from these different models can be combined to cross-examine the outputs of each other. Particularly, we show how accuracy in dialogue can be improved using the question answering model. We are also releasing all models used in the demo as a contribution of this paper. A short video demonstrating the system is available at https://ibm.box.com/v/emnlp2022-demo.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2204.03985",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-04-08",
    "authors": [
      {
        "authorId": "8576392",
        "name": "Md. Faisal Mahbub Chowdhury"
      },
      {
        "authorId": "143742133",
        "name": "Michael R. Glass"
      },
      {
        "authorId": "3415700",
        "name": "Gaetano Rossiello"
      },
      {
        "authorId": "1711133",
        "name": "A. Gliozzo"
      },
      {
        "authorId": "2689774",
        "name": "Nandana Mihindukulasooriya"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "d9fdeeccdcc5262e5ed083a92fee72939e9fb70b",
    "url": "https://www.semanticscholar.org/paper/d9fdeeccdcc5262e5ed083a92fee72939e9fb70b",
    "title": "Smartphone Technologies for Social Network Data Generation and Infectious Disease Modeling",
    "abstract": "This paper presents a means of collecting and analyzing data related to personal social contact networks. A custom application is developed for smartphones that support Bluetooth connectivity, as representative of the ensemble of many consumer electronic products, to infer users' location and proximity to one another, the duration of such proximity ('contact'), and GPS-based information. In many instances of testing the application in this work, this is augmented by device meta-identity. The smartphone application and data storage and retrieval are discussed in detail. Preliminary data were collected (device-device proximity, proximity duration, and location) in pilot testing on the Blackberry Storm and HTC Hero (Android) smartphones. Data are presented as distributions and visualization tools for evolving contact graphs, including Pareto distributions and power law exponents representing face-to-face contacts. Extracted parameters are useful for estimating the potential of infection spread (e.g., respiratory illness), where a key transmission vector is person-person contact. A variant of the standard SEIR individual-based model is developed, with individual contact patterns guided by contact distributions extracted from the smartphone proximity data. Finally, a detailed agent-based model (ABM) of a small community is developed and the spread of an infectious disease is simulated. The data from the ABM is then analyzed in terms of proximity distributions across various demographic profiles, illustrating the utility of the proposed data collection technologies in supporting advancing modeling and simulation efforts associated with infectious diseases.",
    "venue": "",
    "year": 2012,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2012-08-01",
    "authors": [
      {
        "authorId": "144041455",
        "name": "J. Benavides"
      },
      {
        "authorId": "1973841",
        "name": "B. Demianyk"
      },
      {
        "authorId": "3262941",
        "name": "S. Mukhi"
      },
      {
        "authorId": "119655430",
        "name": "M. Laskowski"
      },
      {
        "authorId": "8486075",
        "name": "M. Friesen"
      },
      {
        "authorId": "1786401",
        "name": "R. McLeod"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.53877639491068
  },
  {
    "paperId": "8ee274a17615548f2db1d86bd4632dd3a7a20feb",
    "url": "https://www.semanticscholar.org/paper/8ee274a17615548f2db1d86bd4632dd3a7a20feb",
    "title": "Patron-augmented digital libraries",
    "abstract": "Digital library research is mostly focused on the generation of large collections of multimedia resources and state-of-the-art tools for their indexing and retrieval. However, digital libraries should provide more than advanced collection maintenance and retrieval services since the ultimate goal of any (academic) library is to serve the scholarly needs of its users. This paper begins by presenting a case for digital scholarship in which patrons perform all scholarly work electronically. A proposal is then made for patron-augmented digital libraries (PADLs), a class of digital libraries that supports the digital scholarship of its patrons. Finally, a prototype PADL (called Synchrony) providing access to video segments and associated textual transcripts is described. Synchrony allows patrons to search the library for artifacts, create annotations/original compositions, integrate these artifacts to form synchronized mixed text and video presentations and, after suitable review, publish these presentations into the digital library if desired. A study to evaluate the PADL concept and the usability of Synchrony is also discussed. The study revealed that participants were able to use Synchrony for the authoring and publishing of presentations and that attitudes toward PADLs were generally positive.",
    "venue": "Digital library",
    "year": 2000,
    "citationCount": 26,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/336597.336656",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2000-06-01",
    "authors": [
      {
        "authorId": "1731861",
        "name": "D. Goh"
      },
      {
        "authorId": "1805852",
        "name": "J. Leggett"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.43755299006494
  },
  {
    "paperId": "78474604a192bb82c77288f46b521b0cf9ee14f5",
    "url": "https://www.semanticscholar.org/paper/78474604a192bb82c77288f46b521b0cf9ee14f5",
    "title": "Improving Multiple Documents Grounded Goal-Oriented Dialog Systems via Diverse Knowledge Enhanced Pretrained Language Model",
    "abstract": "In this paper, we mainly discuss about our submission to MultiDoc2Dial task, which aims to model the goal-oriented dialogues grounded in multiple documents. The proposed task is split into grounding span prediction and agent response generation. The baseline for the task is the retrieval augmented generation model, which consists of a dense passage retrieval model for the retrieval part and the BART model for the generation part. The main challenge of this task is that the system requires a great amount of pre-trained knowledge to generate answers grounded in multiple documents. To overcome this challenge, we adopt model pretraining, fine-tuning, and multi-task learning to enhance our model‚Äôs coverage of pretrained knowledge. We experimented with various settings of our method to show the effectiveness of our approaches.",
    "venue": "Workshop on Document-grounded Dialogue and Conversational Question Answering",
    "year": 2022,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2022.dialdoc-1.15.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2164385860",
        "name": "Yunah Jang"
      },
      {
        "authorId": "1709263",
        "name": "Dongryeol Lee"
      },
      {
        "authorId": "2155823120",
        "name": "Hyung-joo Park"
      },
      {
        "authorId": "9414903",
        "name": "Taegwan Kang"
      },
      {
        "authorId": "2109339794",
        "name": "Hwanhee Lee"
      },
      {
        "authorId": "2165230479",
        "name": "Hyunkyung Bae"
      },
      {
        "authorId": "1731707",
        "name": "Kyomin Jung"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "44e6d5af9f41bd7a18044970de18d29cb5b8a553",
    "url": "https://www.semanticscholar.org/paper/44e6d5af9f41bd7a18044970de18d29cb5b8a553",
    "title": "Generating Cooperative System Responses in Information Retrieval Dialogues",
    "abstract": "This paper describes the Corinna system which integrates a theoretical approach to dialogue modeling with text generation techniques to conduct cooperative dialogues in natural language. It is shown how the dialogue model COR can be augmented by adding discourse relations as an additional level of description which is particularly valuable for the generation of dialogue acts.",
    "venue": "International Conference on Natural Language Generation",
    "year": 1994,
    "citationCount": 26,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "1994-06-21",
    "authors": [
      {
        "authorId": "2113448363",
        "name": "Markus Fischer"
      },
      {
        "authorId": "145825124",
        "name": "E. Maier"
      },
      {
        "authorId": "143692600",
        "name": "A. Stein"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.43755299006494
  },
  {
    "paperId": "6eed8bc20c20e059dd2cf25215fae8218709f3b8",
    "url": "https://www.semanticscholar.org/paper/6eed8bc20c20e059dd2cf25215fae8218709f3b8",
    "title": "Architectural issues in mobile augmented reality systems: a prototyping case study",
    "abstract": "Augmented Reality (AR), mobile computing, and ubiquitous computing have each individually provided technology enablers for new types of applications. Researchers have demonstrated over the years spectacular toy systems that fire researchers' imaginations. However, cross-fertilization between these areas has only just begun. Combining these technology enablers into robust information systems that can be used in the field present many software engineering challenges that have not been taken into account by the toy systems mentioned above. In this paper, we report our efforts at TU Munich to analyze information generation, retrieval, transmission, and visualization in the context of maintenance procedures in nuclear powerplants. The use of AR to present such information online has significant implications on the overall system architecture. This paper focuses on pointing out open questions, discussing options for addressing them, and evaluating them in prototypical implementations.",
    "venue": "Proceedings Eighth Asia-Pacific Software Engineering Conference",
    "year": 2001,
    "citationCount": 18,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2001-12-04",
    "authors": [
      {
        "authorId": "153064007",
        "name": "A.H. Dutoit"
      },
      {
        "authorId": "2092954409",
        "name": "Oliver Creighton"
      },
      {
        "authorId": "1715693",
        "name": "G. Klinker"
      },
      {
        "authorId": "2017827",
        "name": "R. Kobylinski"
      },
      {
        "authorId": "1710184",
        "name": "Christoph Vilsmeier"
      },
      {
        "authorId": "1692839",
        "name": "B. Br√ºgge"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.1665846874966
  },
  {
    "paperId": "bb39f795ce6a79a725ef1f129a9d9abcaf99e574",
    "url": "https://www.semanticscholar.org/paper/bb39f795ce6a79a725ef1f129a9d9abcaf99e574",
    "title": "FAST: Friends Augmented Search Techniques - System Design & Data-Management Issues",
    "abstract": "Improving web search solely based on algorithmic refinements has reached a plateau. The emerging generation of searching techniques tries to harness the ``wisdom of crowds'', using inputs from users in the spirit of Web 2.0. In this paper, we introduce a framework facilitating friends augmented search techniques (FAST). To that end, we present a browser add-on as front end for collaborative browsing and searching, supporting synchronous and asynchronous collaboration between users. We then describe the back end, a distributed key-value store for efficient information retrieval in the presence of an evolving knowledge base. The mechanisms we explore in supporting efficient query processing for FAST are applicable for many other recent Web 2.0 applications that rely on similar key-value stores. The specific collaborative search tool we present is expected to be an useful utility in its own right and spur further research on friends augmented search techniques, while the data-management techniques we developed are of general interest and applicability.",
    "venue": "2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology",
    "year": 2011,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2011-08-22",
    "authors": [
      {
        "authorId": "6131481",
        "name": "Christian von der Weth"
      },
      {
        "authorId": "145688317",
        "name": "Anwitaman Datta"
      }
    ],
    "source": "semantic_scholar",
    "score": 82.47918433002164
  },
  {
    "paperId": "273990598a6929bdc7c885f25bacfc6ebf51995e",
    "url": "https://www.semanticscholar.org/paper/273990598a6929bdc7c885f25bacfc6ebf51995e",
    "title": "Retrieve & Memorize: Dialog Policy Learning with Multi-Action Memory",
    "abstract": "Dialogue policy learning, a subtask that determines the content of system response generation and then the degree of task completion, is essential for task-oriented dialogue systems. However, the unbalanced distribution of system actions in dialogue datasets often causes difficulty in learning to generate desired actions and responses. In this paper, we propose a retrieve-and-memorize framework to enhance the learning of system actions. Specially, we first design a neural context-aware retrieval module to retrieve multiple candidate system actions from the training set given a dialogue context. Then, we propose a memory-augmented multi-decoder network to generate the system actions conditioned on the candidate actions, which allows the network to adaptively select key information in the candidate actions and ignore noises. We conduct experiments on the large-scale multi-domain task-oriented dialogue dataset MultiWOZ 2.0 and MultiWOZ 2.1. Experimental results show that our method achieves competitive performance among several state-of-the-art models in the context-to-response generation task.",
    "venue": "Findings",
    "year": 2021,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-06-04",
    "authors": [
      {
        "authorId": "2135329729",
        "name": "Yunhao Li"
      },
      {
        "authorId": "2116591052",
        "name": "Yunyi Yang"
      },
      {
        "authorId": "38472218",
        "name": "Xiaojun Quan"
      },
      {
        "authorId": "2155521332",
        "name": "Jianxing Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "100cdc47e3b6a7cc1d53b1ecb190f6dc1d88baf7",
    "url": "https://www.semanticscholar.org/paper/100cdc47e3b6a7cc1d53b1ecb190f6dc1d88baf7",
    "title": "Autonomic Network Management for Next Generation Networks",
    "abstract": "With the exponential growth of current computer networks, the need for autonomic network management is clearly increasing. While there have been many attempts to tackle autonomicity in network management environments, the platforms are too specific and apply only to very specific environments. A different approach is presented in this paper whose the concept of a dedicated network management platform based on autonomic principles is adopted. This platform encourages a more generic and streamlined approach for targeted research in autonomic network management by providing a lightweight, platform-agnostic middleware which gives access to communication primitives, platform-specific augmented support, basic data storage support and a query language for information retrieval in a dynamicallycomposed atomic functional units environment.",
    "venue": "",
    "year": 2010,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2268358",
        "name": "A. Louca"
      },
      {
        "authorId": "1749658",
        "name": "A. Mauthe"
      },
      {
        "authorId": "144661846",
        "name": "D. Hutchinson"
      }
    ],
    "source": "semantic_scholar",
    "score": 76.47918433002164
  },
  {
    "paperId": "89a28c83ed6a8b881cb8fef0034ce1d8e38a4154",
    "url": "https://www.semanticscholar.org/paper/89a28c83ed6a8b881cb8fef0034ce1d8e38a4154",
    "title": "Computing high quality phase-only holograms for holographic displays",
    "abstract": "Holography has demonstrated potential to achieve a wide field of view, focus supporting, optical see-through augmented reality display in an eyeglasses form factor. Although phase modulating spatial light modulators are becoming available, the phase-only hologram generation algorithms are still imprecise resulting in severe artifacts in the reconstructed imagery. Since the holographic phase retrieval problem is non-linear and non-convex and computationally expensive with the solutions being non-unique, the existing methods make several assumptions to make the phase-only hologram computation tractable. In this work, we deviate from any such approximations and solve the holographic phase retrieval problem as a quadratic problem using complex Wirtinger gradients and standard first-order optimization methods. Our approach results in high-quality phase hologram generation with at least an order of magnitude improvement over existing state-of-the-art approaches.",
    "venue": "AR, VR, MR",
    "year": 2020,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2020-02-21",
    "authors": [
      {
        "authorId": "51151674",
        "name": "Praneeth Chakravarthula"
      },
      {
        "authorId": "2111014438",
        "name": "Yifan Peng"
      },
      {
        "authorId": "48554766",
        "name": "J. Kollin"
      },
      {
        "authorId": "49857575",
        "name": "Felix Heide"
      },
      {
        "authorId": "145472944",
        "name": "H. Fuchs"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "a9ff85856358d5ff7c2a0af64040f67c1d102e8e",
    "url": "https://www.semanticscholar.org/paper/a9ff85856358d5ff7c2a0af64040f67c1d102e8e",
    "title": "Multi-modal Sign Icon Retrieval for Augmentative Communication",
    "abstract": null,
    "venue": "IEEE Pacific Rim Conference on Multimedia",
    "year": 2001,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2001-10-24",
    "authors": [
      {
        "authorId": "1681512",
        "name": "Chung-Hsien Wu"
      },
      {
        "authorId": "1762684",
        "name": "Yu-Hsien Chiu"
      },
      {
        "authorId": "2204416",
        "name": "K. Cheng"
      }
    ],
    "source": "semantic_scholar",
    "score": 66.79441541679836
  },
  {
    "paperId": "57c991a8c09e0809b43f00992f86e0392ff16a74",
    "url": "https://www.semanticscholar.org/paper/57c991a8c09e0809b43f00992f86e0392ff16a74",
    "title": "GQE-PRF: Generative Query Expansion with Pseudo-Relevance Feedback",
    "abstract": "Query expansion with pseudo-relevance feedback (PRF) is a powerful approach to enhance the effectiveness in information retrieval. Recently, with the rapid advance of deep learning techniques, neural text generation has achieved promising success in many natural language tasks. To leverage the strength of text generation for information retrieval, in this article, we propose a novel approach which effectively integrates text generation models into PRF-based query expansion. In particular, our approach generates augmented query terms via neural text generation models conditioned on both the initial query and pseudo-relevance feedback. Moreover, in order to train the generative model, we adopt the conditional generative adversarial nets (CGANs) and propose the PRF-CGAN method in which both the generator and the discriminator are conditioned on the pseudo-relevance feedback. We evaluate the performance of our approach on information retrieval tasks using two benchmark datasets. The experimental results show that our approach achieves comparable performance or outperforms traditional query expansion methods on both the retrieval and reranking tasks.",
    "venue": "arXiv.org",
    "year": 2021,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-08-13",
    "authors": [
      {
        "authorId": "47452150",
        "name": "Minghui Huang"
      },
      {
        "authorId": "2152687987",
        "name": "Dong Wang"
      },
      {
        "authorId": "2108588775",
        "name": "Shuang Liu"
      },
      {
        "authorId": "2147263012",
        "name": "Meizhen Ding"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "c84ef21cef04e2fa876c14bb5a30c7f1468169f9",
    "url": "https://www.semanticscholar.org/paper/c84ef21cef04e2fa876c14bb5a30c7f1468169f9",
    "title": "Learning to Memorize in Neural Task-Oriented Dialogue Systems",
    "abstract": "In this thesis, we leverage the neural copy mechanism and memory-augmented neural networks (MANNs) to address existing challenge of neural task-oriented dialogue learning. We show the effectiveness of our strategy by achieving good performance in multi-domain dialogue state tracking, retrieval-based dialogue systems, and generation-based dialogue systems. We first propose a transferable dialogue state generator (TRADE) that leverages its copy mechanism to get rid of dialogue ontology and share knowledge between domains. We also evaluate unseen domain dialogue state tracking and show that TRADE enables zero-shot dialogue state tracking and can adapt to new few-shot domains without forgetting the previous domains. Second, we utilize MANNs to improve retrieval-based dialogue learning. They are able to capture dialogue sequential dependencies and memorize long-term information. We also propose a recorded delexicalization copy strategy to replace real entity values with ordered entity types. Our models are shown to surpass other retrieval baselines, especially when the conversation has a large number of turns. Lastly, we tackle generation-based dialogue learning with two proposed models, the memory-to-sequence (Mem2Seq) and global-to-local memory pointer network (GLMP). Mem2Seq is the first model to combine multi-hop memory attention with the idea of the copy mechanism. GLMP further introduces the concept of response sketching and double pointers copying. We show that GLMP achieves the state-of-the-art performance on human evaluation.",
    "venue": "arXiv.org",
    "year": 2019,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/1905.07687",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2019-05-19",
    "authors": [
      {
        "authorId": "30340989",
        "name": "Chien-Sheng Wu"
      }
    ],
    "source": "semantic_scholar",
    "score": 91.87639203842082
  },
  {
    "paperId": "85639d0b6a886d13cae6a19601b9e2ca3609e748",
    "url": "https://www.semanticscholar.org/paper/85639d0b6a886d13cae6a19601b9e2ca3609e748",
    "title": "Dynamic Boundary of P-Set and Intelligent Acquisition for Two Types of Information Fusion",
    "abstract": "The development of information technology brings the challenge of data redundancy and data shortage to information fusion. Based on the dynamic boundary characteristics of p-set, this paper analyzes the structure and generation of p-augmented matrix, and then analyzes the dynamic generation of information equivalence class, and then proposes an intelligent acquisition algorithm of information equivalence class based on matrix reasoning. In addition, this paper analyzes two types of information fusion, namely information redundancy fusion and information supplement fusion. Then, the relationship among redundant information fusion, supplementary information fusion, and information equivalence classes is analyzed. Finally, this paper presents the application of intelligent acquisition of information equivalence class in information retrieval.",
    "venue": "De Computis",
    "year": 2020,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "https://www.mdpi.com/2073-431X/9/1/3/pdf?version=1580373160",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2020-01-16",
    "authors": [
      {
        "authorId": "35036224",
        "name": "Shouwei Li"
      },
      {
        "authorId": "2116643534",
        "name": "Yao Xiao"
      },
      {
        "authorId": "39500939",
        "name": "Kaiquan Shi"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "e6fe15db9e6848f59f7202c490566d094a34e072",
    "url": "https://www.semanticscholar.org/paper/e6fe15db9e6848f59f7202c490566d094a34e072",
    "title": "EAGER: Edge-Aided imaGe undERstanding System",
    "abstract": "Image understanding is a fundamental task for many multimedia and computer vision applications, such as self-driving, multimedia retrieval, and augmented reality, etc. In this paper, we demonstrate that edge detection could aid image understanding tasks such as semantic segmentation, optical flow estimation, and object proposal generation. Based on our recent research efforts on edge detection, we develop a robust and efficient Edge-Aided imaGe undERstanding system named as EAGER. EAGER is built on a compact and efficient edge detection module, which is constructed with a bi-directional cascade network, multi-scale feature enhancement, and layer-specific training supervision, respectively. Based on detected edges, EAGER achieves accurate semantic segment, optical flow estimation, as well as object bounding-box proposal generation for user-uploaded images and videos.",
    "venue": "International Conference on Multimedia Retrieval",
    "year": 2019,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle"
    ],
    "publicationDate": "2019-06-05",
    "authors": [
      {
        "authorId": "90991733",
        "name": "J. He"
      },
      {
        "authorId": "2142982436",
        "name": "Xiaobing Liu"
      },
      {
        "authorId": "1776581",
        "name": "Shiliang Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 75.39720770839918
  },
  {
    "paperId": "32be858f67b31fbe9252c0bf6cb97b1c39bc4d9c",
    "url": "https://www.semanticscholar.org/paper/32be858f67b31fbe9252c0bf6cb97b1c39bc4d9c",
    "title": "Information Exploration System for Sickle Cell Disease and Repurposing of Hydroxyfasudil",
    "abstract": "Background Sickle cell disease (SCD) is a fatal monogenic disorder with no effective cure and thus high rates of morbidity and sequelae. Efforts toward discovery of disease modifying drugs and curative strategies can be augmented by leveraging the plethora of information contained in available biomedical literature. To facilitate research in this direction we have developed a resource, Dragon Exploration System for Sickle Cell Disease (DESSCD) (http://cbrc.kaust.edu.sa/desscd/) that aims to promote the easy exploration of SCD-related data. Description The Dragon Exploration System (DES), developed based on text mining and complemented by data mining, processed 419,612 MEDLINE abstracts retrieved from a PubMed query using SCD-related keywords. The processed SCD-related data has been made available via the DESSCD web query interface that enables: a/information retrieval using specified concepts, keywords and phrases, and b/the generation of inferred association networks and hypotheses. The usefulness of the system is demonstrated by: a/reproducing a known scientific fact, the ‚ÄúSickle_Cell_Anemia‚ÄìHydroxyurea‚Äù association, and b/generating novel and plausible ‚ÄúSickle_Cell_Anemia‚ÄìHydroxyfasudil‚Äù hypothesis. A PCT patent (PCT/US12/55042) has been filed for the latter drug repurposing for SCD treatment. Conclusion We developed the DESSCD resource dedicated to exploration of text-mined and data-mined information about SCD. No similar SCD-related resource exists. Thus, we anticipate that DESSCD will serve as a valuable tool for physicians and researchers interested in SCD.",
    "venue": "PLoS ONE",
    "year": 2013,
    "citationCount": 14,
    "openAccessPdf": {
      "url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0065190&type=printable",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2013-06-10",
    "authors": [
      {
        "authorId": "46883364",
        "name": "M. Essack"
      },
      {
        "authorId": "3204206",
        "name": "A. Radovanovic"
      },
      {
        "authorId": "2550579",
        "name": "V. Bajic"
      }
    ],
    "source": "semantic_scholar",
    "score": 100.62075301653314
  },
  {
    "paperId": "d829f1615f2793392b10067a5e80cb2d580c1245",
    "url": "https://www.semanticscholar.org/paper/d829f1615f2793392b10067a5e80cb2d580c1245",
    "title": "Bags of phrases with codebooks alignment for near duplicate image detection",
    "abstract": "Image retrieval from large databases, such as popular social networks, collections of surveillance images and videos, or digital investigation archives, is a very important task for a number of applications. In digital investigation, hashing techniques are commonly used to index large quantities of images to detect copies from different archives. In the last few years, a number of image hashing techniques based on the Bags of Visual Words paradigm have been proposed. Recently, this paradigm has been augmented by using multiple descriptors (Bags of Visual Phrases) to exploit the coherence between different feature spaces. In this paper we propose to further improve the Bags of Visual Phrases approach exploiting the coherence between feature spaces not only in the image representation, but also in the codebooks generation. Experiments performed on real and synthetic near duplicate image datasets show the effectiveness of the proposed approach, which outperforms the original Bags of Visual Phrases approach.",
    "venue": "MiFor '10",
    "year": 2010,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2010-10-29",
    "authors": [
      {
        "authorId": "1742452",
        "name": "S. Battiato"
      },
      {
        "authorId": "1729739",
        "name": "G. Farinella"
      },
      {
        "authorId": "2725577",
        "name": "G. C. Guarnera"
      },
      {
        "authorId": "3219968",
        "name": "Tony Meccio"
      },
      {
        "authorId": "1739427",
        "name": "G. Puglisi"
      },
      {
        "authorId": "1859140",
        "name": "D. Rav√¨"
      },
      {
        "authorId": "35421247",
        "name": "Rosetta Rizzo"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.49820016084324
  },
  {
    "paperId": "4cf2235af5dddb4c2d82ce22ef6279371b53e3e7",
    "url": "https://www.semanticscholar.org/paper/4cf2235af5dddb4c2d82ce22ef6279371b53e3e7",
    "title": "DCT-DST Plane sectorization of Row wise Transformed color Images in CBIR",
    "abstract": "We have introduced a novel idea of sectoring of DCT-DST plane of Row wise transformed images and feature vector generation with and without augmentation of zeroth column component of DCT transformed image and the last column component of DST transformed image. Two similarity measures namely sum of absolute difference and Euclidean distance are used and results of them are compared. The cross over point performance of overall average of precision and recall for different sector sizes are studied and analyzed comparatively. The augmented Wang(20) image database of 1055 images is used, consisting of 12 different classes. DCT/DST plane has been divided into 4,8,12 and 16 sectors. The overall average precision and recall cross over point with augmentation of average of zeroeth column from DCT transformed image gives better result of retrieval. The use of sum of Absolute difference as similarity measure always gives lesser computational complexity and better relevant image retrieval rate compared to Euclidian distance. Keywords-CBIR, DCT, DST, Euclidian Distance, Sum of Absolute Difference, Precision and Recall.",
    "venue": "",
    "year": 2010,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Mathematics"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "144908905",
        "name": "H. B. Kekre"
      },
      {
        "authorId": "10691428",
        "name": "D. Mishra"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.49820016084324
  },
  {
    "paperId": "64ce692eda3069d807f2e783adccbb204c607ae6",
    "url": "https://www.semanticscholar.org/paper/64ce692eda3069d807f2e783adccbb204c607ae6",
    "title": "Traversing the Linking Open Data Cloud to Create News from Tweets",
    "abstract": null,
    "venue": "OTM Workshops",
    "year": 2014,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2014-10-27",
    "authors": [
      {
        "authorId": "2863619",
        "name": "Francisco Berrizbeita"
      },
      {
        "authorId": "143858195",
        "name": "Maria-Esther Vidal"
      }
    ],
    "source": "semantic_scholar",
    "score": 56.479184330021646
  },
  {
    "paperId": "a44bfe3aa7c8fed9ecffd212e8cbd2d57ffcb50f",
    "url": "https://www.semanticscholar.org/paper/a44bfe3aa7c8fed9ecffd212e8cbd2d57ffcb50f",
    "title": "Meetings and meeting modeling in smart environments",
    "abstract": null,
    "venue": "Ai & Society",
    "year": 2006,
    "citationCount": 20,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2006-02-16",
    "authors": [
      {
        "authorId": "144483472",
        "name": "A. Nijholt"
      },
      {
        "authorId": "1782503",
        "name": "R. O. D. Akker"
      },
      {
        "authorId": "1678537",
        "name": "D. Heylen"
      }
    ],
    "source": "semantic_scholar",
    "score": 85.66783656585135
  },
  {
    "paperId": "c283fd49d82845b3eb375f56ebaf553ffef7e85e",
    "url": "https://www.semanticscholar.org/paper/c283fd49d82845b3eb375f56ebaf553ffef7e85e",
    "title": "Meetings and meeting modeling in smart surroundings",
    "abstract": "In this paper we survey our research on smart meeting rooms and its relevance for augmented \nreality meeting support and virtual reality generation of meetings in real-time or off-line. Intelligent \nreal-time and off-line generation requires understanding of what is going on during \na meeting. The research reported here takes place in the European 5th and 6th framework \nprogramme projects M4 (Multi-Modal Meeting Manager) and AMI (Augmented Multi-party \nInteraction). Both projects aim at building a smart meeting environment that is able to capture \nin a multimodal way the activities and discussions in a meeting room, with the aim to use \nthis information as input to tools that allow real-time support, browsing, retrieval and summarization \nof meetings. In these projects many European research groups participate. Our \naim is to research (semantic) representations of what takes place during meetings in order to \nallow generation, e.g. in virtual reality, of meeting activities (discussions, presentations, voting, \netcetera). Being able to do so also allows us to look at tools that provide support during \na meeting and at tools that allow those not able to be physically present during a meeting \nto take part in a virtual way. This may lead to situations where the differences between \nreal meeting participants, human-controlled virtual participants and (semi-) autonomous virtual \nparticipants disappear. In this paper we introduce our research aims and ideas and we \nillustrate them with examples taken from many different projects in related areas.",
    "venue": "",
    "year": 2004,
    "citationCount": 14,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "144483472",
        "name": "A. Nijholt"
      },
      {
        "authorId": "2076015",
        "name": "H. O. D. Akker"
      },
      {
        "authorId": "1678537",
        "name": "D. Heylen"
      }
    ],
    "source": "semantic_scholar",
    "score": 100.62075301653314
  },
  {
    "paperId": "916be31cbf847faa65cad0549e153f0c25b9f424",
    "url": "https://www.semanticscholar.org/paper/916be31cbf847faa65cad0549e153f0c25b9f424",
    "title": "Few-shot Learning with Retrieval Augmented Language Models",
    "abstract": "Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.",
    "venue": "Journal of machine learning research",
    "year": 2022,
    "citationCount": 607,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2208.03299",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-08-05",
    "authors": [
      {
        "authorId": "1410231361",
        "name": "Gautier Izacard"
      },
      {
        "authorId": "145222654",
        "name": "Patrick Lewis"
      },
      {
        "authorId": "3376175",
        "name": "M. Lomeli"
      },
      {
        "authorId": "26360550",
        "name": "Lucas Hosseini"
      },
      {
        "authorId": "40052301",
        "name": "F. Petroni"
      },
      {
        "authorId": "32246932",
        "name": "Timo Schick"
      },
      {
        "authorId": "2129456957",
        "name": "Jane A. Yu"
      },
      {
        "authorId": "2319608",
        "name": "Armand Joulin"
      },
      {
        "authorId": "48662861",
        "name": "Sebastian Riedel"
      },
      {
        "authorId": "3024698",
        "name": "Edouard Grave"
      }
    ],
    "source": "semantic_scholar",
    "score": 166.15262322949252
  },
  {
    "paperId": "832fff14d2ed50eb7969c4c4b976c35776548f56",
    "url": "https://www.semanticscholar.org/paper/832fff14d2ed50eb7969c4c4b976c35776548f56",
    "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
    "abstract": "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.",
    "venue": "International Conference on Machine Learning",
    "year": 2020,
    "citationCount": 1763,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2020-02-10",
    "authors": [
      {
        "authorId": "2091768",
        "name": "Kelvin Guu"
      },
      {
        "authorId": "2544107",
        "name": "Kenton Lee"
      },
      {
        "authorId": "9941702",
        "name": "Zora Tung"
      },
      {
        "authorId": "2616463",
        "name": "Panupong Pasupat"
      },
      {
        "authorId": "1744179",
        "name": "Ming-Wei Chang"
      }
    ],
    "source": "semantic_scholar",
    "score": 182.13008854850105
  },
  {
    "paperId": "cc78babfacce48e715dac56886d7dd9746cfcab0",
    "url": "https://www.semanticscholar.org/paper/cc78babfacce48e715dac56886d7dd9746cfcab0",
    "title": "RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit",
    "abstract": "Although Large Language Models (LLMs) have demonstrated extraordinary capabilities in many domains, they still have a tendency to hallucinate and generate fictitious responses to user requests. This problem can be alleviated by augmenting LLMs with information retrieval (IR) systems (also known as retrieval-augmented LLMs). Applying this strategy, LLMs can generate more factual texts in response to user input according to the relevant content retrieved by IR systems from external corpora as references. In addition, by incorporating external knowledge, retrieval-augmented LLMs can answer in-domain questions that cannot be answered by solely relying on the world knowledge stored in parameters. To support research in this area and facilitate the development of retrieval-augmented LLM systems, we develop RETA-LLM, a {RET}reival-{A}ugmented LLM toolkit. In RETA-LLM, we create a complete pipeline to help researchers and users build their customized in-domain LLM-based systems. Compared with previous retrieval-augmented LLM systems, RETA-LLM provides more plug-and-play modules to support better interaction between IR systems and LLMs, including {request rewriting, document retrieval, passage extraction, answer generation, and fact checking} modules. Our toolkit is publicly available at https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 49,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.05212",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-08",
    "authors": [
      {
        "authorId": "1830383266",
        "name": "Jiongnan Liu"
      },
      {
        "authorId": "4376097",
        "name": "Jiajie Jin"
      },
      {
        "authorId": "2243360876",
        "name": "Zihan Wang"
      },
      {
        "authorId": "2219726925",
        "name": "Jiehan Cheng"
      },
      {
        "authorId": "1897235",
        "name": "Zhicheng Dou"
      },
      {
        "authorId": "153693432",
        "name": "Ji-rong Wen"
      }
    ],
    "source": "semantic_scholar",
    "score": 128.6803450814222
  },
  {
    "paperId": "a925dc735c1ba7c85255a54c5f63407fd9f3ee46",
    "url": "https://www.semanticscholar.org/paper/a925dc735c1ba7c85255a54c5f63407fd9f3ee46",
    "title": "READSUM: Retrieval-Augmented Adaptive Transformer for Source Code Summarization",
    "abstract": "Code summarization is the process of automatically generating brief and informative summaries of source code to aid in software comprehension and maintenance. In this paper, we propose a novel model called READSUM, REtrieval-augmented ADaptive transformer for source code SUMmarization, that combines both abstractive and extractive approaches. Our proposed model generates code summaries in an abstractive manner, taking into account both the structural and sequential information of the input code, while also utilizing an extractive approach that leverages a retrieved summary of similar code to increase the frequency of important keywords. To effectively blend the original code and the retrieved similar code at the embedding layer stage, we obtain the augmented representation of the original code and the retrieved code through multi-head self-attention. In addition, we develop a self-attention network that adaptively learns the structural and sequential information for the representations in the encoder stage. Furthermore, we design a fusion network to capture the relation between the original code and the retrieved summary at the decoder stage. The fusion network effectively guides summary generation based on the retrieved summary. Finally, READSUM extracts important keywords using an extractive approach and generates high-quality summaries using an abstractive approach that considers both the structural and sequential information of the source code. We demonstrate the superiority of READSUM through various experiments and an ablation study. Additionally, we perform a human evaluation to assess the quality of the generated summary.",
    "venue": "IEEE Access",
    "year": 2023,
    "citationCount": 12,
    "openAccessPdf": {
      "url": "https://ieeexplore.ieee.org/ielx7/6287639/10005208/10113620.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "46252888",
        "name": "YunSeok Choi"
      },
      {
        "authorId": "2080438236",
        "name": "CheolWon Na"
      },
      {
        "authorId": "2109649519",
        "name": "Hyojun Kim"
      },
      {
        "authorId": "30603322",
        "name": "Jee-Hyong Lee"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.47424036192305
  },
  {
    "paperId": "3ed1c94ec4fdd2a9235afeb2d929fde965b1d723",
    "url": "https://www.semanticscholar.org/paper/3ed1c94ec4fdd2a9235afeb2d929fde965b1d723",
    "title": "Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models",
    "abstract": "Despite recent progress, it has been difficult to prevent semantic hallucinations in generative Large Language Models. One common solution to this is augmenting LLMs with a retrieval system and making sure that the generated output is attributable to the retrieved information. Given this new added constraint, it is plausible to expect that the overall quality of the output will be affected, for example, in terms of fluency. Can scaling language models help? Here we examine the relationship between fluency and attribution in LLMs prompted with retrieved evidence in knowledge-heavy dialog settings. Our experiments were implemented with a set of auto-metrics that are aligned with human preferences. They were used to evaluate a large set of generations, produced under varying parameters of LLMs and supplied context. We show that larger models tend to do much better in both fluency and attribution, and that (naively) using top-k retrieval versus top-1 retrieval improves attribution but hurts fluency. We next propose a recipe that could allow smaller models to both close the gap with larger models and preserve the benefits of top-k retrieval while avoiding its drawbacks.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 16,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2302.05578",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-02-11",
    "authors": [
      {
        "authorId": "2205544066",
        "name": "Renat Aksitov"
      },
      {
        "authorId": "2152948655",
        "name": "Chung-Ching Chang"
      },
      {
        "authorId": "1781409",
        "name": "D. Reitter"
      },
      {
        "authorId": "2944868",
        "name": "Siamak Shakeri"
      },
      {
        "authorId": "2305450",
        "name": "Yun-Hsuan Sung"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.49820016084324
  },
  {
    "paperId": "af5c7848417882012203ac21399977ebda695a2b",
    "url": "https://www.semanticscholar.org/paper/af5c7848417882012203ac21399977ebda695a2b",
    "title": "RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation",
    "abstract": "The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark are publicly available: https://github.com/microsoft/CodeT/tree/main/RepoCoder",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 155,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2303.12570",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-03-22",
    "authors": [
      {
        "authorId": "2158120018",
        "name": "Fengji Zhang"
      },
      {
        "authorId": "143876723",
        "name": "B. Chen"
      },
      {
        "authorId": "2211964951",
        "name": "Yue Zhang"
      },
      {
        "authorId": "2155352529",
        "name": "Jin Liu"
      },
      {
        "authorId": "2134434187",
        "name": "Daoguang Zan"
      },
      {
        "authorId": "145469202",
        "name": "Yi Mao"
      },
      {
        "authorId": "153249455",
        "name": "Jian-Guang Lou"
      },
      {
        "authorId": "2109136147",
        "name": "Weizhu Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 145.74784010874305
  },
  {
    "paperId": "114dcd4be9eb6f5a7257fbfdb542cfab3153a292",
    "url": "https://www.semanticscholar.org/paper/114dcd4be9eb6f5a7257fbfdb542cfab3153a292",
    "title": "Learning Retrieval Augmentation for Personalized Dialogue Generation",
    "abstract": "Personalized dialogue generation, focusing on generating highly tailored responses by leveraging persona profiles and dialogue context, has gained significant attention in conversational AI applications. However, persona profiles, a prevalent setting in current personalized dialogue datasets, typically composed of merely four to five sentences, may not offer comprehensive descriptions of the persona about the agent, posing a challenge to generate truly personalized dialogues. To handle this problem, we propose $\\textbf{L}$earning Retrieval $\\textbf{A}$ugmentation for $\\textbf{P}$ersonalized $\\textbf{D}$ial$\\textbf{O}$gue $\\textbf{G}$eneration ($\\textbf{LAPDOG}$), which studies the potential of leveraging external knowledge for persona dialogue generation. Specifically, the proposed LAPDOG model consists of a story retriever and a dialogue generator. The story retriever uses a given persona profile as queries to retrieve relevant information from the story document, which serves as a supplementary context to augment the persona profile. The dialogue generator utilizes both the dialogue history and the augmented persona profile to generate personalized responses. For optimization, we adopt a joint training framework that collaboratively learns the story retriever and dialogue generator, where the story retriever is optimized towards desired ultimate metrics (e.g., BLEU) to retrieve content for the dialogue generator to generate personalized responses. Experiments conducted on the CONVAI2 dataset with ROCStory as a supplementary data source show that the proposed LAPDOG method substantially outperforms the baselines, indicating the effectiveness of the proposed method. The LAPDOG model code is publicly available for further exploration. https://github.com/hqsiswiliam/LAPDOG",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.emnlp-main.154.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-27",
    "authors": [
      {
        "authorId": "2274015208",
        "name": "Qiushi Huang"
      },
      {
        "authorId": "2274132285",
        "name": "Shuai Fu"
      },
      {
        "authorId": "2110814131",
        "name": "Xubo Liu"
      },
      {
        "authorId": "2239051433",
        "name": "Wenwu Wang"
      },
      {
        "authorId": "2273320275",
        "name": "Tom Ko"
      },
      {
        "authorId": "2273525536",
        "name": "Yu Zhang"
      },
      {
        "authorId": "2189113746",
        "name": "Lilian H. Y. Tang"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "5616be414472881c170abca7ac2472378c63fc74",
    "url": "https://www.semanticscholar.org/paper/5616be414472881c170abca7ac2472378c63fc74",
    "title": "CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation",
    "abstract": "Building high-quality datasets for specialized tasks is a time-consuming and resource-intensive process that often requires specialized domain knowledge. We propose Corpus Retrieval and Augmentation for Fine-Tuning (CRAFT), a method for generating synthetic datasets, given a small number of user-written few-shots that demonstrate the task to be performed. Given the few-shot examples, we use large-scale public web-crawled corpora and similarity-based document retrieval to find other relevant human-written documents. Lastly, instruction-tuned large language models (LLMs) augment the retrieved documents into custom-formatted task samples, which then can be used for fine-tuning. We demonstrate that CRAFT can efficiently generate large-scale task-specific training datasets for four diverse tasks: biology question-answering (QA), medicine QA and commonsense QA as well as summarization. Our experiments show that CRAFT-based models outperform or achieve comparable performance to general LLMs for QA tasks, while CRAFT-based summarization models outperform models trained on human-curated data by 46 preference points.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-03",
    "authors": [
      {
        "authorId": "2319415753",
        "name": "Ingo Ziegler"
      },
      {
        "authorId": "1999179692",
        "name": "Abdullatif K√∂ksal"
      },
      {
        "authorId": "2326119320",
        "name": "Desmond Elliott"
      },
      {
        "authorId": "2130001188",
        "name": "Hinrich Schutze"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "9bbcc6eb7ab49ebed302118a98c9e28ea88987b2",
    "url": "https://www.semanticscholar.org/paper/9bbcc6eb7ab49ebed302118a98c9e28ea88987b2",
    "title": "Retrieval is Accurate Generation",
    "abstract": "Standard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary. We introduce a novel method that selects context-aware phrases from a collection of supporting documents. One of the most significant challenges for this paradigm shift is determining the training oracles, because a string of text can be segmented in various ways and each segment can be retrieved from numerous possible documents. To address this, we propose to initialize the training oracles using linguistic heuristics and, more importantly, bootstrap the oracles through iterative self-reinforcement. Extensive experiments show that our model not only outperforms standard language models on a variety of knowledge-intensive tasks but also demonstrates improved generation quality in open-ended text generation. For instance, compared to the standard language model counterpart, our model raises the accuracy from 23.47% to 36.27% on OpenbookQA, and improves the MAUVE score from 42.61% to 81.58% in open-ended text generation. Remarkably, our model also achieves the best performance and the lowest latency among several retrieval-augmented baselines. In conclusion, we assert that retrieval is more accurate generation and hope that our work will encourage further research on this new paradigm shift.",
    "venue": "International Conference on Learning Representations",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-27",
    "authors": [
      {
        "authorId": "2209367631",
        "name": "Bowen Cao"
      },
      {
        "authorId": "2266753374",
        "name": "Deng Cai"
      },
      {
        "authorId": "2279792419",
        "name": "Leyang Cui"
      },
      {
        "authorId": null,
        "name": "Xuxin Cheng"
      },
      {
        "authorId": "2237804371",
        "name": "Wei Bi"
      },
      {
        "authorId": "2260859476",
        "name": "Yuexian Zou"
      },
      {
        "authorId": "2257446263",
        "name": "Shuming Shi"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "d621b3445c568b976fa186b3be14e45348aa7df1",
    "url": "https://www.semanticscholar.org/paper/d621b3445c568b976fa186b3be14e45348aa7df1",
    "title": "GRACE: Gradient-guided Controllable Retrieval for Augmenting Attribute-based Text Generation",
    "abstract": ",",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.findings-acl.530.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2067933778",
        "name": "Zhihua Wen"
      },
      {
        "authorId": "33992653",
        "name": "Zhiliang Tian"
      },
      {
        "authorId": "2151323750",
        "name": "Zhen Huang"
      },
      {
        "authorId": "2116104944",
        "name": "Yuxin Yang"
      },
      {
        "authorId": "2113769250",
        "name": "Z. Jian"
      },
      {
        "authorId": "2146408568",
        "name": "Changjian Wang"
      },
      {
        "authorId": "2163335292",
        "name": "Dongsheng Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 84.1415686865115
  },
  {
    "paperId": "de549c1592a62c129b8d49c8c0137aa6859b103f",
    "url": "https://www.semanticscholar.org/paper/de549c1592a62c129b8d49c8c0137aa6859b103f",
    "title": "Internet-Augmented Dialogue Generation",
    "abstract": "The largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giving access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training. In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information. We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledgedriven discussions in order to ground their responses. We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or FAISS-based retrieval (Lewis et al., 2020b).",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "citationCount": 262,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2022.acl-long.579.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-07-15",
    "authors": [
      {
        "authorId": "100653935",
        "name": "M. Komeili"
      },
      {
        "authorId": "35752280",
        "name": "Kurt Shuster"
      },
      {
        "authorId": "145183709",
        "name": "J. Weston"
      }
    ],
    "source": "semantic_scholar",
    "score": 153.58231048266646
  },
  {
    "paperId": "51ca900b85799205f161a5c84a21276e31ff04ee",
    "url": "https://www.semanticscholar.org/paper/51ca900b85799205f161a5c84a21276e31ff04ee",
    "title": "Unsupervised Dense Retrieval Deserves Better Positive Pairs: Scalable Augmentation with Query Extraction and Generation",
    "abstract": "Dense retrievers have made signiÔ¨Åcant strides in obtaining state-of-the-art results on text retrieval and open-domain question answering (ODQA). Yet most of these achievements were made possible with the help of large annotated datasets, unsupervised learning for dense retrieval models remains an open problem. In this work, we explore two categories of meth-ods for creating pseudo query-document pairs, named query extraction (QExt) and transferred query generation (TQGen), to augment the retriever training in an annotation-free and scalable manner. SpeciÔ¨Åcally, QExt extracts pseudo queries by document structures or selecting salient random spans, and TQGen utilizes generation models trained for other NLP tasks (e.g., summarization) to produce pseudo queries. Extensive experiments show that dense retrievers trained with individual augmentation methods can perform comparably well with multiple strong baselines, and combining them leads to further improvements, achieving state-of-the-art performance of un-supervised dense retrieval on both BEIR and ODQA datasets.",
    "venue": "arXiv.org",
    "year": 2022,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2212.08841",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2087884364",
        "name": "Rui Meng"
      },
      {
        "authorId": "2108334982",
        "name": "Ye Liu"
      },
      {
        "authorId": "3014143",
        "name": "Semih Yavuz"
      },
      {
        "authorId": "2057234256",
        "name": "Divyansh Agarwal"
      },
      {
        "authorId": "3376969",
        "name": "Lifu Tu"
      },
      {
        "authorId": "143878318",
        "name": "Ning Yu"
      },
      {
        "authorId": "2108313930",
        "name": "Jianguo Zhang"
      },
      {
        "authorId": "48648832",
        "name": "Meghana Moorthy Bhat"
      },
      {
        "authorId": "2118860628",
        "name": "Yingbo Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "003c1005c719d8f5691fd963f4ed8a0b2d50f4f7",
    "url": "https://www.semanticscholar.org/paper/003c1005c719d8f5691fd963f4ed8a0b2d50f4f7",
    "title": "Retrieval-Free Knowledge-Grounded Dialogue Response Generation with Adapters",
    "abstract": "To diversify and enrich generated dialogue responses, knowledge-grounded dialogue has been investigated in recent years. The existing methods tackle the knowledge grounding challenge by retrieving the relevant sentences over a large corpus and augmenting the dialogues with explicit extra information. Despite their success, however, the existing works have drawbacks on the inference efficiency. This paper proposes KnowExpert, an end-to-end framework to bypass the explicit retrieval process and inject knowledge into the pre-trained language models with lightweight adapters and adapt to the knowledge-grounded dialogue task. To the best of our knowledge, this is the first attempt to tackle this challenge without retrieval in this task under an open-domain chit-chat scenario. The experimental results show that KnowExpert performs comparably with some retrieval-based baselines while being time-efficient in inference, demonstrating the effectiveness of our proposed method.",
    "venue": "Workshop on Document-grounded Dialogue and Conversational Question Answering",
    "year": 2021,
    "citationCount": 40,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2022.dialdoc-1.10.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-05-13",
    "authors": [
      {
        "authorId": "98271906",
        "name": "Yan Xu"
      },
      {
        "authorId": "38524906",
        "name": "Etsuko Ishii"
      },
      {
        "authorId": "2117941142",
        "name": "Zihan Liu"
      },
      {
        "authorId": "9162688",
        "name": "Genta Indra Winata"
      },
      {
        "authorId": "144610224",
        "name": "Dan Su"
      },
      {
        "authorId": "3064807",
        "name": "Andrea Madotto"
      },
      {
        "authorId": "40539650",
        "name": "Pascale Fung"
      }
    ],
    "source": "semantic_scholar",
    "score": 125.70358100056461
  },
  {
    "paperId": "ecc5927d4f263c09ace502626033c0d9e636edf7",
    "url": "https://www.semanticscholar.org/paper/ecc5927d4f263c09ace502626033c0d9e636edf7",
    "title": "REASONS: A benchmark for REtrieval and Automated citationS Of scieNtific Sentences using Public and Proprietary LLMs",
    "abstract": "Automatic citation generation for sentences in a document or report is paramount for intelligence analysts, cybersecurity, news agencies, and education personnel. In this research, we investigate whether large language models (LLMs) are capable of generating references based on two forms of sentence queries: (a) Direct Queries, LLMs are asked to provide author names of the given research article, and (b) Indirect Queries, LLMs are asked to provide the title of a mentioned article when given a sentence from a different article. To demonstrate where LLM stands in this task, we introduce a large dataset called REASONS comprising abstracts of the 12 most popular domains of scientific research on arXiv. From around 20K research articles, we make the following deductions on public and proprietary LLMs: (a) State-of-the-art, often called anthropomorphic GPT-4 and GPT-3.5, suffers from high pass percentage (PP) to minimize the hallucination rate (HR). When tested with Perplexity.ai (7B), they unexpectedly made more errors; (b) Augmenting relevant metadata lowered the PP and gave the lowest HR; (c) Advance retrieval-augmented generation (RAG) using Mistral demonstrates consistent and robust citation support on indirect queries and matched performance to GPT-3.5 and GPT-4. The HR across all domains and models decreased by an average of 41.93%, and the PP was reduced to 0% in most cases. In terms of generation quality, the average F1 Score and BLEU were 68.09% and 57.51%, respectively; (d) Testing with adversarial samples showed that LLMs, including the Advance RAG Mistral, struggle to understand context, but the extent of this issue was small in Mistral and GPT-4-Preview. Our study contributes valuable insights into the reliability of RAG for automated citation generation tasks.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-05-03",
    "authors": [
      {
        "authorId": "2141126407",
        "name": "Deepa Tilwani"
      },
      {
        "authorId": "2298271532",
        "name": "Yash Saxena"
      },
      {
        "authorId": "2307078365",
        "name": "Seyedali Mohammadi"
      },
      {
        "authorId": "2338828613",
        "name": "Edward Raff"
      },
      {
        "authorId": "2262731589",
        "name": "Amit P. Sheth"
      },
      {
        "authorId": "2275282544",
        "name": "Srinivasan Parthasarathy"
      },
      {
        "authorId": "1491238594",
        "name": "Manas Gaur"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "785431fea0f297c63383a2ce5954d685f25064f3",
    "url": "https://www.semanticscholar.org/paper/785431fea0f297c63383a2ce5954d685f25064f3",
    "title": "Optimizing biomedical information retrieval with a keyword frequency-driven prompt enhancement strategy",
    "abstract": "Background Mining the enormous pool of biomedical literature to extract accurate responses and relevant references is a daunting challenge, made more difficult by the domain‚Äôs interdisciplinary nature, specialized jargon, and continuous evolution. Early natural language processing (NLP) approaches faced considerable hurdles in comprehending the nuances of natural language which frequently led to false answers. However, the discovery of transformer models has immensely boosted the cause by enabling researchers to create larger and more potent language models known as large language models (LLMs). These LLMs have opened up new possibilities for enhancing question-answering (QA) tasks. Even with these technological advances, the current LLM-based solutions for querying specialized domains like biology and biomedicine still have issues in generating up to date responses while preventing ‚Äúhallucination‚Äù or the generation of plausible but factually incorrect responses. Results This paper focuses on prompt enhancement using retrieval-augmented architecture as a strategy to guide LLMs towards generating more meaningful responses for biomedical question-answering tasks. We evaluated two prompt enhancement approaches using GPT-3 and GPT-4, examining their effectiveness in retrieving relevant information from complex landscape of biomedical literature. Our proposed approach leverages explicit signals in user queries to extract meaningful contexts from a vast pool of information, addressing the shortcomings of traditional text embedding-based prompt enhancement methods. We developed a QA bot ‚ÄòWeiseEule‚Äô (https://github.com/wasimaftab/WeiseEule-LocalHost) utilizing these prompt enhancement methods and allowing for comparative analysis of their performances. Conclusions Our findings highlight the importance of prompt enhancement methods that utilize explicit signals in user‚Äôs query over traditional text embedding based counterparts to improve LLM-generated responses in specialized domains such as biology and biomedicine. By providing users complete control over the information that goes into the LLM, our approach tackles some of the major drawbacks of existing web-based chatbots and LLM-based QA systems including hallucinations and the generation of irrelevant or outdated responses.",
    "venue": "bioRxiv",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "https://www.biorxiv.org/content/biorxiv/early/2024/04/28/2024.04.23.590746.full.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Biology",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-28",
    "authors": [
      {
        "authorId": "90732919",
        "name": "Wasim Aftab"
      },
      {
        "authorId": "6944461",
        "name": "Zivkos Apostolou"
      },
      {
        "authorId": "50349498",
        "name": "Karim Bouazoune"
      },
      {
        "authorId": "2290873665",
        "name": "Tobias Straub"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "8236c7cb26222b98af6dbcb15f9166f957b6ea67",
    "url": "https://www.semanticscholar.org/paper/8236c7cb26222b98af6dbcb15f9166f957b6ea67",
    "title": "Knowledge-Augmented Language Model Verification",
    "abstract": "Recent Language Models (LMs) have shown impressive capabilities in generating texts with the knowledge internalized in parameters. Yet, LMs often generate the factually incorrect responses to the given queries, since their knowledge may be inaccurate, incomplete, and outdated. To address this problem, previous works propose to augment LMs with the knowledge retrieved from an external knowledge source. However, such approaches often show suboptimal text generation performance due to two reasons: 1) the model may fail to retrieve the knowledge relevant to the given query, or 2) the model may not faithfully reflect the retrieved knowledge in the generated text. To overcome these, we propose to verify the output and the knowledge of the knowledge-augmented LMs with a separate verifier, which is a small LM that is trained to detect those two types of errors through instruction-finetuning. Then, when the verifier recognizes an error, we can rectify it by either retrieving new knowledge or generating new text. Further, we use an ensemble of the outputs from different instructions with a single verifier to enhance the reliability of the verification processes. We validate the effectiveness of the proposed verification steps on multiple question answering benchmarks, whose results show that the proposed verifier effectively identifies retrieval and generation errors, allowing LMs to provide more factually correct outputs. Our code is available at https://github.com/JinheonBaek/KALMV.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-19",
    "authors": [
      {
        "authorId": "90765684",
        "name": "Jinheon Baek"
      },
      {
        "authorId": "8599185",
        "name": "Soyeong Jeong"
      },
      {
        "authorId": "120434407",
        "name": "Minki Kang"
      },
      {
        "authorId": "2109285560",
        "name": "Jong C. Park"
      },
      {
        "authorId": "2260611009",
        "name": "Sung Ju Hwang"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "bb9f3c9c8bcf78814740559ae6fb82ca1012ea90",
    "url": "https://www.semanticscholar.org/paper/bb9f3c9c8bcf78814740559ae6fb82ca1012ea90",
    "title": "A Comprehensive Evaluation of Tool-Assisted Generation Strategies",
    "abstract": "A growing area of research investigates augmenting language models with tools (e.g., search engines, calculators) to overcome their shortcomings (e.g., missing or incorrect knowledge, incorrect logical inferences). Various few-shot tool-usage strategies have been proposed. However, there is no systematic and fair comparison across different strategies, or between these strategies and strong baselines that do not leverage tools. We conduct an extensive empirical analysis, finding that (1) across various datasets, example difficulty levels, and models, strong no-tool baselines are competitive to tool-assisted strategies, implying that effectively using tools with in-context demonstrations is a difficult unsolved problem; (2) for knowledge-retrieval tasks, strategies that *refine* incorrect outputs with tools outperform strategies that retrieve relevant information *ahead of* or *during generation*; (3) tool-assisted strategies are expensive in the number of tokens they require to work -- incurring additional costs by orders of magnitude -- which does not translate into significant improvement in performance. Overall, our findings suggest that few-shot tool integration is still an open challenge, emphasizing the need for comprehensive evaluations of future strategies to accurately assess their *benefits* and *costs*.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-16",
    "authors": [
      {
        "authorId": "41016275",
        "name": "Alon Jacovi"
      },
      {
        "authorId": "27743758",
        "name": "Avi Caciularu"
      },
      {
        "authorId": "2253566854",
        "name": "Jonathan Herzig"
      },
      {
        "authorId": "2335771",
        "name": "Roee Aharoni"
      },
      {
        "authorId": "2266464503",
        "name": "Bernd Bohnet"
      },
      {
        "authorId": "22245981",
        "name": "Mor Geva"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "06fcd9dfbc26b13b81cc4bae573e43c37e671903",
    "url": "https://www.semanticscholar.org/paper/06fcd9dfbc26b13b81cc4bae573e43c37e671903",
    "title": "RLEG: Vision-Language Representation Learning with Diffusion-based Embedding Generation",
    "abstract": "Vision-language representation learning models ( e.g. , CLIP) have achieved state-of-the-art performance on various downstream tasks, which usually need large-scale training data to learn discriminative representation. Recent progress on generative diffusion models ( e.g. , DALL-E 2) has demonstrated that diverse high-quality samples can be synthesized by randomly sampling from generative distribution. By virtue of generative capability in this paper, we propose a novel vision-language R epresentation L earning method with diffusion-based E mbedding G eneration ( RLEG ), which exploits diffusion models to generate feature embedding online for learning effective vision-language representation. Specifically, we first adopt image and text encoders to extract the corresponding embeddings. Secondly, pretrained diffusion-based embedding generators are harnessed to transfer the embedding modality online between vision and language domains. The embeddings generated from the generators are then served as augmented embedding-level samples, which are applied to contrastive learning with the variant of the CLIP framework. Experimental re-sults show that the proposed method could learn effective representation and achieve state-of-the-art performance on various tasks including image classification, image-text retrieval, object detection, semantic segmentation, and text-conditional image generation.",
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "46586815",
        "name": "Liming Zhao"
      },
      {
        "authorId": "84005711",
        "name": "Kecheng Zheng"
      },
      {
        "authorId": "2149513498",
        "name": "Yun Zheng"
      },
      {
        "authorId": "1678783",
        "name": "Deli Zhao"
      },
      {
        "authorId": "1709595",
        "name": "Jingren Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "6cc6c65421d98839ff8c30d4c7eb1a4dd2a7e3e1",
    "url": "https://www.semanticscholar.org/paper/6cc6c65421d98839ff8c30d4c7eb1a4dd2a7e3e1",
    "title": "Multi-auxiliary Augmented Collaborative Variational Auto-encoder for Tag Recommendation",
    "abstract": "Recommending appropriate tags to items can facilitate content organization, retrieval, consumption, and other applications, where hybrid tag recommender systems have been utilized to integrate collaborative information and content information for better recommendations. In this article, we propose a multi-auxiliary augmented collaborative variational auto-encoder (MA-CVAE) for tag recommendation, which couples item collaborative information and item multi-auxiliary information, i.e., content and social graph, by defining a generative process. Specifically, the model learns deep latent embeddings from different item auxiliary information using variational auto-encoders (VAE), which could form a generative distribution over each auxiliary information by introducing a latent variable parameterized by deep neural network. Moreover, to recommend tags for new items, item multi-auxiliary latent embeddings are utilized as a surrogate through the item decoder for predicting recommendation probabilities of each tag, where reconstruction losses are added in the training phase to constrain the generation for feedback predictions via different auxiliary embeddings. In addition, an inductive variational graph auto-encoder is designed to infer latent embeddings of new items in the test phase, such that item social information could be exploited for new items. Extensive experiments on MovieLens and citeulike datasets demonstrate the effectiveness of our method.",
    "venue": "ACM Trans. Inf. Syst.",
    "year": 2022,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2204.09422",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-04-20",
    "authors": [
      {
        "authorId": "2114337267",
        "name": "Jing Yi"
      },
      {
        "authorId": "2163180478",
        "name": "Xubin Ren"
      },
      {
        "authorId": "2150914949",
        "name": "Zhenzhong Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.96842909197557
  },
  {
    "paperId": "6dabbd090c87e53a0f736d8cade46d05e02046ba",
    "url": "https://www.semanticscholar.org/paper/6dabbd090c87e53a0f736d8cade46d05e02046ba",
    "title": "Crafting the Path: Robust Query Rewriting for Information Retrieval",
    "abstract": "Query rewriting aims to generate a new query that can complement the original query to improve the information retrieval system. Recent studies on query rewriting, such as query2doc, query2expand and querey2cot, rely on the internal knowledge of Large Language Models (LLMs) to generate a relevant passage to add information to the query. Nevertheless, the efficacy of these methodologies may markedly decline in instances where the requisite knowledge is not encapsulated within the model's intrinsic parameters. In this paper, we propose a novel structured query rewriting method called Crafting the Path tailored for retrieval systems. Crafting the Path involves a three-step process that crafts query-related information necessary for finding the passages to be searched in each step. Specifically, the Crafting the Path begins with Query Concept Comprehension, proceeds to Query Type Identification, and finally conducts Expected Answer Extraction. Experimental results show that our method outperforms previous rewriting methods, especially in less familiar domains for LLMs. We demonstrate that our method is less dependent on the internal parameter knowledge of the model and generates queries with fewer factual inaccuracies. Furthermore, we observe that \\name{} demonstrates superior performance in the retrieval-augmented generation scenarios.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-17",
    "authors": [
      {
        "authorId": "2311698905",
        "name": "Ingeol Baek"
      },
      {
        "authorId": "2311822003",
        "name": "Jimin Lee"
      },
      {
        "authorId": "2297051680",
        "name": "Joonho Yang"
      },
      {
        "authorId": "2311744808",
        "name": "Hwanhee Lee"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "1d75535b7783fb1c7b8121f002740b6a90921209",
    "url": "https://www.semanticscholar.org/paper/1d75535b7783fb1c7b8121f002740b6a90921209",
    "title": "CgAT: Center-Guided Adversarial Training for Deep Hashing-Based Retrieval",
    "abstract": "Deep hashing has been extensively utilized in massive image retrieval because of its efficiency and effectiveness. However, deep hashing models are vulnerable to adversarial examples, making it essential to develop adversarial defense methods for image retrieval. Existing solutions achieved limited defense performance because of using weak adversarial samples for training and lacking discriminative optimization objectives to learn robust features. In this paper, we present a min-max based Center-guided Adversarial Training, namely CgAT, to improve the robustness of deep hashing networks through worst adversarial examples. Our key idea is to formulate a hash code (dubbed center code) as a discriminative semantic representation of the original sample, which can be used to guide the generation of the powerful adversarial example and as an accurate optimization objective for adversarial training. Specifically, we first formulate the center code as a semantically-discriminative representative of the input image content, which preserves the semantic similarity with positive samples and dissimilarity with negative examples. We prove that a mathematical formula can calculate the center code immediately. After obtaining the center codes in each optimization iteration of the deep hashing network, they are adopted to guide the adversarial training process. On the one hand, CgAT generates the worst adversarial examples as augmented data by maximizing the Hamming distance between the hash codes of the adversarial examples and the center codes. On the other hand, CgAT learns to mitigate the effects of adversarial samples by minimizing the Hamming distance to the center codes. Extensive experiments on the benchmark datasets demonstrate the effectiveness of our adversarial training algorithm in defending against adversarial attacks for deep hashing-based retrieval. Compared with the current state-of-the-art defense method, we significantly improve the defense performance by an average of 18.61%, 12.35%, and 11.56% on FLICKR-25K, NUS-WIDE, and MS-COCO, respectively. The code is available at https://github.com/xunguangwang/CgAT.",
    "venue": "The Web Conference",
    "year": 2022,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2204.10779",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2022-04-18",
    "authors": [
      {
        "authorId": "1701156804",
        "name": "Xunguang Wang"
      },
      {
        "authorId": "2190166320",
        "name": "Yinqun Lin"
      },
      {
        "authorId": "48569608",
        "name": "X. Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "9ec3166f4a9f00c37ce080c4dc1fff04b719dccf",
    "url": "https://www.semanticscholar.org/paper/9ec3166f4a9f00c37ce080c4dc1fff04b719dccf",
    "title": "Logical Form Generation via Multi-task Learning for Complex Question Answering over Knowledge Bases",
    "abstract": "Question answering over knowledge bases (KBQA) for complex questions is a challenging task in natural language processing. Recently, generation-based methods that translate natural language questions to executable logical forms have achieved promising performance. These methods use auxiliary information to augment the logical form generation of questions with unseen KB items or novel combinations, but the noise introduced can also leads to more incorrect results. In this work, we propose GMT-KBQA, a Generation-based KBQA method via Multi-Task learning, to better retrieve and utilize auxiliary information. GMT-KBQA first obtains candidate entities and relations through dense retrieval, and then introduces a multi-task model which jointly learns entity disambiguation, relation classification, and logical form generation. Experimental results show that GMT-KBQA achieves state-of-the-art results on both ComplexWebQuestions and WebQuestionsSP datasets. Furthermore, the detailed evaluation demonstrates that GMT-KBQA benefits from the auxiliary tasks and has a strong generalization capability.",
    "venue": "International Conference on Computational Linguistics",
    "year": 2022,
    "citationCount": 21,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1576053308",
        "name": "Xixin Hu"
      },
      {
        "authorId": "48661434",
        "name": "X. Wu"
      },
      {
        "authorId": "1406331721",
        "name": "Yiheng Shu"
      },
      {
        "authorId": "1887019",
        "name": "Yuzhong Qu"
      }
    ],
    "source": "semantic_scholar",
    "score": 116.36563680037474
  },
  {
    "paperId": "28727e96023b06a8f1b3fa43730e512b07516317",
    "url": "https://www.semanticscholar.org/paper/28727e96023b06a8f1b3fa43730e512b07516317",
    "title": "Image hashing retrieval based on generative adversarial networks",
    "abstract": null,
    "venue": "Applied intelligence (Boston)",
    "year": 2022,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-08-05",
    "authors": [
      {
        "authorId": "2066702057",
        "name": "Lei Lei"
      },
      {
        "authorId": "2766071",
        "name": "Dongen Guo"
      },
      {
        "authorId": "2111638621",
        "name": "Zhen Shen"
      },
      {
        "authorId": "2109576734",
        "name": "Zechen Wu"
      }
    ],
    "source": "semantic_scholar",
    "score": 66.47918433002164
  },
  {
    "paperId": "26217a04c8a2ce36e1d027cca7b35bf83cab957b",
    "url": "https://www.semanticscholar.org/paper/26217a04c8a2ce36e1d027cca7b35bf83cab957b",
    "title": "ZusammenQA: Data Augmentation with Specialized Models for Cross-lingual Open-retrieval Question Answering System",
    "abstract": "This paper introduces our proposed system for the MIA Shared Task on Cross-lingual Openretrieval Question Answering (COQA). In this challenging scenario, given an input question the system has to gather evidence documents from a multilingual pool and generate from them an answer in the language of the question. We devised several approaches combining different model variants for three main components: Data Augmentation, Passage Retrieval, and Answer Generation. For passage retrieval, we evaluated the monolingual BM25 ranker against the ensemble of re-rankers based on multilingual pretrained language models (PLMs) and also variants of the shared task baseline, re-training it from scratch using a recently introduced contrastive loss that maintains a strong gradient signal throughout training by means of mixed negative samples. For answer generation, we focused on languageand domain-specialization by means of continued language model (LM) pretraining of existing multilingual encoders. Additionally, for both passage retrieval and answer generation, we augmented the training data provided by the task organizers with automatically generated question-answer pairs created from Wikipedia passages to mitigate the issue of data scarcity, particularly for the low-resource languages for which no training data were provided. Our results show that language- and domain-specialization as well as data augmentation help, especially for low-resource languages.",
    "venue": "MIA",
    "year": 2022,
    "citationCount": 2,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2205.14981",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-05-30",
    "authors": [
      {
        "authorId": "11618346",
        "name": "Chia-Chien Hung"
      },
      {
        "authorId": "2124689664",
        "name": "Tommaso Green"
      },
      {
        "authorId": "46249177",
        "name": "Robert Litschko"
      },
      {
        "authorId": "2139739556",
        "name": "Tornike Tsereteli"
      },
      {
        "authorId": "9390836",
        "name": "Sotaro Takeshita"
      },
      {
        "authorId": "2042527842",
        "name": "Marco Bombieri"
      },
      {
        "authorId": "1666177566",
        "name": "Goran Glavavs"
      },
      {
        "authorId": "2029669151",
        "name": "Simone Paolo Ponzetto"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "747fb3abae56dbc1607d9626fbfcf9528151ee7a",
    "url": "https://www.semanticscholar.org/paper/747fb3abae56dbc1607d9626fbfcf9528151ee7a",
    "title": "Combining Linked Data and Statistical Information Retrieval - Next Generation Information Systems",
    "abstract": null,
    "venue": "Extended Semantic Web Conference",
    "year": 2014,
    "citationCount": 17,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2014-05-25",
    "authors": [
      {
        "authorId": "2370666",
        "name": "Ricardo Usbeck"
      }
    ],
    "source": "semantic_scholar",
    "score": 83.35557636844247
  },
  {
    "paperId": "8673d46f2b5abce2f11bb0960a0d095bab8081ea",
    "url": "https://www.semanticscholar.org/paper/8673d46f2b5abce2f11bb0960a0d095bab8081ea",
    "title": "Knowledge-Augmented Visual Question Answering With Natural Language Explanation",
    "abstract": "Visual question answering with natural language explanation (VQA-NLE) is a challenging task that requires models to not only generate accurate answers but also to provide explanations that justify the relevant decision-making processes. This task is accomplished by generating natural language sentences based on the given question-image pair. However, existing methods often struggle to ensure consistency between the answers and explanations due to their disregard of the crucial interactions between these factors. Moreover, existing methods overlook the potential benefits of incorporating additional knowledge, which hinders their ability to effectively bridge the semantic gap between questions and images, leading to less accurate explanations. In this paper, we present a novel approach denoted the knowledge-based iterative consensus VQA-NLE (KICNLE) model to address these limitations. To maintain consistency, our model incorporates an iterative consensus generator that adopts a multi-iteration generative method, enabling multiple iterations of the answer and explanation in each generation. In each iteration, the current answer is utilized to generate an explanation, which in turn guides the generation of a new answer. Additionally, a knowledge retrieval module is introduced to provide potentially valid candidate knowledge, guide the generation process, effectively bridge the gap between questions and images, and enable the production of high-quality answer-explanation pairs. Extensive experiments conducted on three different datasets demonstrate the superiority of our proposed KICNLE model over competing state-of-the-art approaches. Our code is available at https://github.com/Gary-code/KICNLE.",
    "venue": "IEEE Transactions on Image Processing",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-28",
    "authors": [
      {
        "authorId": "1387837930",
        "name": "Jiayuan Xie"
      },
      {
        "authorId": "2279360884",
        "name": "Yi Cai"
      },
      {
        "authorId": "2108090448",
        "name": "Jiali Chen"
      },
      {
        "authorId": "2293908983",
        "name": "Ruohang Xu"
      },
      {
        "authorId": "2273933485",
        "name": "Jiexin Wang"
      },
      {
        "authorId": "2293742286",
        "name": "Qing Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.79441541679836
  },
  {
    "paperId": "1a1e99514d8d175459f7c61cfd0c394b46e63359",
    "url": "https://www.semanticscholar.org/paper/1a1e99514d8d175459f7c61cfd0c394b46e63359",
    "title": "Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training",
    "abstract": "Prior work on Data-To-Text Generation, the task of converting knowledge graph (KG) triples into natural text, focused on domain-specific benchmark datasets. In this paper, however, we verbalize the entire English Wikidata KG, and discuss the unique challenges associated with a broad, open-domain, large-scale verbalization. We further show that verbalizing a comprehensive, encyclopedic KG like Wikidata can be used to integrate structured KGs and natural language corpora. In contrast to the many architectures that have been developed to integrate these two sources, our approach converts the KG into natural text, allowing it to be seamlessly integrated into existing language models. It carries the further advantages of improved factual accuracy and reduced toxicity in the resulting language model. We evaluate this approach by augmenting the retrieval corpus in a retrieval language model and showing significant improvements on the knowledge intensive tasks of open domain QA and the LAMA knowledge probe.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2021,
    "citationCount": 156,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2021.naacl-main.278.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-06-01",
    "authors": [
      {
        "authorId": "2257175603",
        "name": "Oshin Agarwal"
      },
      {
        "authorId": "2257216155",
        "name": "Heming Ge"
      },
      {
        "authorId": "2944868",
        "name": "Siamak Shakeri"
      },
      {
        "authorId": "1388360943",
        "name": "Rami Al-Rfou"
      }
    ],
    "source": "semantic_scholar",
    "score": 145.8436870802246
  },
  {
    "paperId": "3f9a4f5c1a462bbf113282b35ed3791f48606fac",
    "url": "https://www.semanticscholar.org/paper/3f9a4f5c1a462bbf113282b35ed3791f48606fac",
    "title": "Generation of Asset Administration Shell With Large Language Model Agents: Toward Semantic Interoperability in Digital Twins in the Context of Industry 4.0",
    "abstract": "This research introduces a novel approach for achieving semantic interoperability in digital twins and assisting the creation of Asset Administration Shell (AAS) as digital twin model within the context of Industry 4.0. The foundational idea of our research is that the communication based on semantics and the generation of meaningful textual data are directly linked, and we posit that these processes are equivalent if the exchanged information can be serialized in text form. Based on this, we construct a ‚Äúsemantic node‚Äù data structure in our research to capture the semantic essence of textual data. Then, a system powered by large language models is designed and implemented to process the ‚Äúsemantic node‚Äù and generate standardized digital twin models (AAS instance models in the context of Industry 4.0) from raw textual data collected from datasheets describing technical assets. Our evaluation demonstrates an effective generation rate of 62-79%, indicating a substantial proportion of the information from the source text can be translated error-free to the target digital twin instance model with the generative capability of large language models. This result has a direct application in the context of Industry 4.0, and the designed system is implemented as a data model generation tool for reducing the manual effort in creating AAS model by automatically translating unstructured textual data into a standardized AAS model. The generated AAS model can be integrated into AAS-compliant digital twin software for seamless information exchange and communication. In our evaluation, a comparative analysis of different LLMs and an in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms provide insights into the effectiveness of LLM systems for interpreting technical concepts and translating data. Our findings emphasize LLMs‚Äô capability to automate AAS instance creation and contribute to the broader field of semantic interoperability for digital twins in industrial applications. The prototype implementation and evaluation results are presented on our GitHub Repository: https://github.com/YuchenXia/AASbyLLM.",
    "venue": "IEEE Access",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-25",
    "authors": [
      {
        "authorId": "152428280",
        "name": "Yuchen Xia"
      },
      {
        "authorId": "2294154000",
        "name": "Zhewen Xiao"
      },
      {
        "authorId": "2509708",
        "name": "N. Jazdi"
      },
      {
        "authorId": "2273478",
        "name": "M. Weyrich"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.1886522358297
  },
  {
    "paperId": "6eb426af01b8871cf88fe01bff85f9b2ec8e5a04",
    "url": "https://www.semanticscholar.org/paper/6eb426af01b8871cf88fe01bff85f9b2ec8e5a04",
    "title": "TegTok: Augmenting Text Generation via Task-specific and Open-world Knowledge",
    "abstract": "Generating natural and informative texts has been a long-standing problem in NLP. Much effort has been dedicated into incorporating pre-trained language models (PLMs) with various open-world knowledge, such as knowledge graphs or wiki pages. However, their ability to access and manipulate the task-specific knowledge is still limited on downstream tasks, as this type of knowledge is usually not well covered in PLMs and is hard to acquire. To address the problem, we propose augmenting TExt Generation via Task-specific and Open-world Knowledge (TegTok) in a unified framework. Our model selects knowledge entries from two types of knowledge sources through dense retrieval and then injects them into the input encoding and output decoding stages respectively on the basis of PLMs. With the help of these two types of knowledge, our model can learn what and how to generate. Experiments on two text generation tasks of dialogue generation and question generation, and on two datasets show that our method achieves better performance than various baseline models.",
    "venue": "Findings",
    "year": 2022,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2203.08517",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-03-16",
    "authors": [
      {
        "authorId": "2111728713",
        "name": "Chao-Hong Tan"
      },
      {
        "authorId": "3028818",
        "name": "Jia-Chen Gu"
      },
      {
        "authorId": "8801869",
        "name": "Chongyang Tao"
      },
      {
        "authorId": "2072392338",
        "name": "Zhen-Hua Ling"
      },
      {
        "authorId": "46747953",
        "name": "Can Xu"
      },
      {
        "authorId": "2144026961",
        "name": "Huang Hu"
      },
      {
        "authorId": "2442662",
        "name": "Xiubo Geng"
      },
      {
        "authorId": "2086994543",
        "name": "Daxin Jiang"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "bdbbbde626296772961cf9791cf4f1d89009fa04",
    "url": "https://www.semanticscholar.org/paper/bdbbbde626296772961cf9791cf4f1d89009fa04",
    "title": "Leveraging Event Schema to Ask Clarifying Questions for Conversational Legal Case Retrieval",
    "abstract": "Legal case retrieval is a special IR task aiming to retrieve supporting cases for a given query case. Existing works have shown that conversational search paradigm can improve users' search experience in legal case retrieval. One of the keys to a practical conversational search system is how to ask high-quality clarifying questions to initiate conversations with users and understand their search intents. Recently, Large Language Models, such as ChatGPT and GPT-4, have shown superior ability in both open-domain QA and conversations with human. Thus it is natural to believe that they could be applied to legal conversational search as well. However, our preliminary study has shown that generating clarifying questions in legal conversational search with SOTA LLMs (e.g., GPT-4) often suffers from several problems such as duplication and low-utility contents. To address these problems, we propose LeClari, which leverages legal event schema as external knowledge to instruct LLMs to generate effective clarifying questions for legal conversational search. LeClari is constructed with a prompt module and a novel legal event selection module. The former defines a prompt with legal events for clarifying question generation and the latter selects potential event types by modeling the relationships of legal event types, conversational context, and candidate cases. We also propose ranking-oriented rewards and employ the reward augmented maximum likelihood (RAML) method to optimize LeClari directly based on the final retrieval performance of the conversational legal search system. Empirical results over two widely adopted legal case retrieval datasets demonstrate the effectiveness of our approach as compared with the state-of-the-art baselines.",
    "venue": "International Conference on Information and Knowledge Management",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3583780.3614953",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-21",
    "authors": [
      {
        "authorId": "150936790",
        "name": "Bulou Liu"
      },
      {
        "authorId": "2212045428",
        "name": "Yiran Hu"
      },
      {
        "authorId": "2256982003",
        "name": "Qingyao Ai"
      },
      {
        "authorId": "2260835922",
        "name": "Yiqun Liu"
      },
      {
        "authorId": "2257126236",
        "name": "Yueyue Wu"
      },
      {
        "authorId": "2136338739",
        "name": "Chenliang Li"
      },
      {
        "authorId": "2211946754",
        "name": "Weixing Shen"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "c011a59b155965caf2ab5cdf9e61e6544142a981",
    "url": "https://www.semanticscholar.org/paper/c011a59b155965caf2ab5cdf9e61e6544142a981",
    "title": "X-TRA: Improving Chest X-ray Tasks with Cross-Modal Retrieval Augmentation",
    "abstract": "An important component of human analysis of medical images and their context is the ability to relate newly seen things to related instances in our memory. In this paper we mimic this ability by using multi-modal retrieval augmentation and apply it to several tasks in chest X-ray analysis. By retrieving similar images and/or radiology reports we expand and regularize the case at hand with additional knowledge, while maintaining factual knowledge consistency. The method consists of two components. First, vision and language modalities are aligned using a pre-trained CLIP model. To enforce that the retrieval focus will be on detailed disease-related content instead of global visual appearance it is fine-tuned using disease class information. Subsequently, we construct a non-parametric retrieval index, which reaches state-of-the-art retrieval levels. We use this index in our downstream tasks to augment image representations through multi-head attention for disease classification and report retrieval. We show that retrieval augmentation gives considerable improvements on these tasks. Our downstream report retrieval even shows to be competitive with dedicated report generation methods, paving the path for this method in medical imaging.",
    "venue": "Information Processing in Medical Imaging",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2302.11352",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-02-22",
    "authors": [
      {
        "authorId": "150324472",
        "name": "Tom van Sonsbeek"
      },
      {
        "authorId": "2067075870",
        "name": "M. Worring"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "77db3d91786935f91656d4193a742220651f869f",
    "url": "https://www.semanticscholar.org/paper/77db3d91786935f91656d4193a742220651f869f",
    "title": "Multi-stage Training with Improved Negative Contrast for Neural Passage Retrieval",
    "abstract": "In the context of neural passage retrieval, we study three promising techniques: synthetic data generation, negative sampling, and fusion. We systematically investigate how these techniques contribute to the performance of the retrieval system and how they complement each other. We propose a multi-stage framework comprising of pre-training with synthetic data, fine-tuning with labeled data, and negative sampling at both stages. We study six negative sampling strategies and apply them to the fine-tuning stage and, as a noteworthy novelty, to the synthetic data that we use for pre-training. Also, we explore fusion methods that combine negatives from different strategies. We evaluate our system using two passage retrieval tasks for open-domain QA and using MS MARCO. Our experiments show that augmenting the negative contrast in both stages is effective to improve passage retrieval accuracy and, importantly, they also show that synthetic data generation and negative sampling have additive benefits. Moreover, using the fusion of different kinds allows us to reach performance that establishes a new state-of-the-art level in two of the tasks we evaluated.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "citationCount": 26,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2021.emnlp-main.492.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "12916118",
        "name": "Jing-Bin Lu"
      },
      {
        "authorId": "2056957",
        "name": "Gustavo Hern√°ndez √Åbrego"
      },
      {
        "authorId": "2109919783",
        "name": "Ji Ma"
      },
      {
        "authorId": "2148023",
        "name": "Jianmo Ni"
      },
      {
        "authorId": "2118771180",
        "name": "Yinfei Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.43755299006494
  },
  {
    "paperId": "aead76141d5c338578d4e4e0e8933a4052599c54",
    "url": "https://www.semanticscholar.org/paper/aead76141d5c338578d4e4e0e8933a4052599c54",
    "title": "Chest X-Ray Report Generation from Chest-X Ray Images",
    "abstract": "The automatic generation of highly clinically accurate radiology reports could improve clinical outcomes by reducing radiologist workload, prioritizing severe cases, and augmenting existing radiograph processing pipelines. In this project, we are going to focus on an encoder-decoder models for a generative approach, and retrieval model approach based on existing radiology reports",
    "venue": "",
    "year": 2022,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2220791003",
        "name": "Stanford CS224N"
      },
      {
        "authorId": "2285220441",
        "name": "Custom Project"
      },
      {
        "authorId": "2118278662",
        "name": "M. Tan"
      },
      {
        "authorId": "2286313788",
        "name": "Kathy Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 75.39720770839918
  },
  {
    "paperId": "35f0cf9f70c408dbaf106e6a675244e2867c164e",
    "url": "https://www.semanticscholar.org/paper/35f0cf9f70c408dbaf106e6a675244e2867c164e",
    "title": "Pearl: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers",
    "abstract": "Powerful large language models have facilitated the development of writing assistants that promise to significantly improve the quality and efficiency of composition and communication. However, a barrier to effective assistance is the lack of personalization in LLM outputs to the author‚Äôs communication style, specialized knowledge, and values. In this paper, we address this challenge by proposing Pearl, a LLM writing assistant personalized with a retriever that is trained to be generation-calibrated for personalization. Generation calibration ensures that our retriever selects historic user authored documents to augment an LLM prompt such that they are likely to help an LLM generation better adhere to a users‚Äô preferences. We propose two key novelties for training such a retriever: (1) A training data selection method that identifies user requests likely to benefit from personalization and documents that provide that benefit; and (2) A scale-calibrating KL-divergence objective that ensures that our retriever scores remain proportional to the downstream generation quality from using the document for personalized generation. In a series of holistic evaluations, we demonstrate the effectiveness of Pearl in generating long-form texts on multiple social media datasets. Finally, we demonstrate how a generation-calibrated retriever can double as a performance predictor ‚Äì detecting low quality retrieval, and improving potentially under-performing outputs via revision with LLMs.",
    "venue": "CUSTOMNLP4U",
    "year": 2023,
    "citationCount": 22,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-15",
    "authors": [
      {
        "authorId": "9076501",
        "name": "Sheshera Mysore"
      },
      {
        "authorId": "2266767060",
        "name": "Zhuoran Lu"
      },
      {
        "authorId": "2266754711",
        "name": "Mengting Wan"
      },
      {
        "authorId": "2266801772",
        "name": "Longqi Yang"
      },
      {
        "authorId": "2266756625",
        "name": "Steve Menezes"
      },
      {
        "authorId": "2266756161",
        "name": "Tina Baghaee"
      },
      {
        "authorId": "2267049385",
        "name": "Emmanuel Barajas Gonzalez"
      },
      {
        "authorId": "2243181687",
        "name": "Jennifer Neville"
      },
      {
        "authorId": "2243180014",
        "name": "Tara Safavi"
      }
    ],
    "source": "semantic_scholar",
    "score": 117.03241323893724
  },
  {
    "paperId": "06c366d57a3526ec1a138b5f0de2b06e9b11add7",
    "url": "https://www.semanticscholar.org/paper/06c366d57a3526ec1a138b5f0de2b06e9b11add7",
    "title": "Fingerprint retrieval by spatial modelling and distorted sample generation",
    "abstract": "In this study, the authors extend and refine the process of fingerprint retrieval, with the goal of boosting recognition rates for the first rank candidate and low penetration rates. On top of a baseline retrieval system which extracts Gabor features in multiple directions from fingerprint images, the authors propose spatial modelling techniques to generate artificial samples for training the system. Translational modelling, rotational modelling and distorted sample generation techniques are used to augment the original training set in order to boost the accuracy of fingerprint retrieval. The effectiveness of the models is evaluated using the well-known National Institute of Standards and Technology database 4. Experimental results, with reference to some leading fingerprint retrieval rates reported in the literature, confirm that the authors‚Äô proposed system is promising in recognition performance.",
    "venue": "IET Computer Vision",
    "year": 2013,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1049/iet-cvi.2011.0161",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2013-12-12",
    "authors": [
      {
        "authorId": "2198285",
        "name": "K. Leung"
      },
      {
        "authorId": "1899547",
        "name": "C. Leung"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.79441541679836
  },
  {
    "paperId": "f7c46e38ce42b30b1871d7a5d0905c03869bb7c9",
    "url": "https://www.semanticscholar.org/paper/f7c46e38ce42b30b1871d7a5d0905c03869bb7c9",
    "title": "Enhancing Arabic Information Retrieval for Question Answering",
    "abstract": "In the modern landscape of Natural Language Processing (NLP), intelligent chatbots like ChatGPT 3.5 and Google‚Äôs Bard have shown remarkable competence in generic question-answering (QA) tasks. However, their performance falters when navigating domain-specific QA, particularly in the Arabic language, which is celebrated for its complex morphology and syntax. This paper presents a comprehensive approach to address these issues. The aim of this research is to build a chatbot tailored for a university community. We first create an extensive Arabic Q&A dataset by extracting data from academic documents, employing state-of-the-art Optical Character Recognition (OCR) tools. Then, we evaluate multiple text similarity measures like Pooled FastText Word embedding, BM25 ranking functions, and various semantic sentence embedding models. A thorough performance assessment reveals that the domain-specific model excels at both sentence-level similarity and context-relevance tasks. The developed web application chatbot, leveraging LangChain library and Retrieval Augmented Generation (RAG) methods, outperforms existing chatbots in domain-specific, Arabic language QA scenarios.",
    "venue": "International Conference on Future Networks and Distributed Systems",
    "year": 2023,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book"
    ],
    "publicationDate": "2023-12-21",
    "authors": [
      {
        "authorId": "2301211750",
        "name": "Muath Alghamdi"
      },
      {
        "authorId": "2301210691",
        "name": "Mohammed Abushawarib"
      },
      {
        "authorId": "2200546479",
        "name": "Mahmoud Ellouh"
      },
      {
        "authorId": "2283372904",
        "name": "Mustafa Ghaleb"
      },
      {
        "authorId": "2301211108",
        "name": "Muhamad Felemban"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "b360427d0991143013da6a208ccf28bcc8028fab",
    "url": "https://www.semanticscholar.org/paper/b360427d0991143013da6a208ccf28bcc8028fab",
    "title": "Large Scale Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training",
    "abstract": "Generating natural sentences from Knowledge Graph (KG) triples, known as Data-To-Text Generation, is a task with many datasets for which numerous complex systems have been developed. However, no prior work has attempted to perform this generation at scale by converting an entire KG into natural text. In this paper, we verbalize the entire Wikidata KG, and create a KG-Text aligned corpus in the training process. We discuss the challenges in verbalizing an entire KG versus verbalizing smaller datasets. We further show that verbalizing an entire KG can be used to integrate structured and natural language data. In contrast to the many architectures that have been developed to integrate the structural differences between these two sources, our approach converts the KG into the same format as natural text allowing it to be seamlessly plugged into existing natural language systems. We evaluate this approach by augmenting the retrieval corpus in REALM and showing improvements, both on the LAMA knowledge probe and open domain QA.",
    "venue": "arXiv.org",
    "year": 2020,
    "citationCount": 38,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2020-10-23",
    "authors": [
      {
        "authorId": "47016717",
        "name": "Oshin Agarwal"
      },
      {
        "authorId": "31856237",
        "name": "Heming Ge"
      },
      {
        "authorId": "2944868",
        "name": "Siamak Shakeri"
      },
      {
        "authorId": "1388360943",
        "name": "Rami Al-Rfou"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.95342469194469
  },
  {
    "paperId": "ef223941dd0bd142ee1de03b496305ecd89a88e6",
    "url": "https://www.semanticscholar.org/paper/ef223941dd0bd142ee1de03b496305ecd89a88e6",
    "title": "Augmented TIRG for CBIR Using Combined Text and Image Features",
    "abstract": "In this paper we propose a methodology for Content Based Image Retrieval, CBIR, using query inputs in the form of a source image and text modifiers. The proposed methodology augments the methodology proposed in [21], TIRG, with a trained module. The trained module aims at enhancing the relationship between a) the composed image-text features and b) the target image features (e.g. input an image of blue dress along with a textual description and ask for the same dress but in red). Our study used two trained modules (Linear Regression, LR, and Non-Linear Multilayered Perceptron, NMLP). The proposed models were tested using the well-known fashion 200K dataset. The LR model reduced the Mean Squared Error, MSE, significantly. A joint LR model outperformed the TIRG on the testing Dataset. Two NMLP trained models were used: MSE-optimized and Cosine similarity optimized. The performance of the two models was very similar. The NMLP models, in general, outperformed TIRG over the Training dataset. The study also indicates that combining image-text features should be kept to later stages to obtain their recall intersections. Moreover, the study showed that text-features generation based on words assigned numbers irrelated to their semantics requires semantic hub to bridge to a semantic-numbers.",
    "venue": "2021 International Conference on Electrical, Computer and Energy Technologies (ICECET)",
    "year": 2021,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2021-12-09",
    "authors": [
      {
        "authorId": "2154107524",
        "name": "Mohamed Aboali"
      },
      {
        "authorId": "2154109221",
        "name": "Islam Elmaddah"
      },
      {
        "authorId": "2154158290",
        "name": "Hossam El-Din Hassan"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "1a5cc4e66d50a21289799373876334385456b9fa",
    "url": "https://www.semanticscholar.org/paper/1a5cc4e66d50a21289799373876334385456b9fa",
    "title": "Response Generation by Context-aware Prototype Editing",
    "abstract": "Open domain response generation has achieved remarkable progress in recent years, but sometimes yields short and uninformative responses. We propose a new paradigm, prototypethen-edit for response generation, that first retrieves a prototype response from a pre-defined index and then edits the prototype response according to the differences between the prototype context and current context. Our motivation is that the retrieved prototype provides a good start-point for generation because it is grammatical and informative, and the post-editing process further improves the relevance and coherence of the prototype. In practice, we design a contextaware editing model that is built upon an encoder-decoder framework augmented with an editing vector. We first generate an edit vector by considering lexical differences between a prototype context and current context. After that, the edit vector and the prototype response representation are fed to a decoder to generate a new response. Experiment results on a large scale dataset demonstrate that our new paradigm significantly increases the relevance, diversity and originality of generation results, compared to traditional generative models. Furthermore, our model outperforms retrieval-based methods in terms of relevance and originality.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2018,
    "citationCount": 117,
    "openAccessPdf": {
      "url": "https://ojs.aaai.org/index.php/AAAI/article/download/4714/4592",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2018-06-19",
    "authors": [
      {
        "authorId": "2142240763",
        "name": "Yu Wu"
      },
      {
        "authorId": "49807919",
        "name": "Furu Wei"
      },
      {
        "authorId": "3110003",
        "name": "Shaohan Huang"
      },
      {
        "authorId": "1707275",
        "name": "Zhoujun Li"
      },
      {
        "authorId": "92660691",
        "name": "Ming Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 136.56026936698498
  },
  {
    "paperId": "6065fe83bbab89763e1637a16d64676bbda6b6bd",
    "url": "https://www.semanticscholar.org/paper/6065fe83bbab89763e1637a16d64676bbda6b6bd",
    "title": "Unified Multimodal Pre-training and Prompt-based Tuning for Vision-Language Understanding and Generation",
    "abstract": "Most existing vision-language pre-training methods focus on understanding tasks and use BERT-like objectives (masked language modeling and image-text matching) during pretraining. Although they perform well in many understanding downstream tasks, e.g., visual question answering, image-text retrieval and visual entailment, they do not possess the ability to generate. To tackle this problem, we propose Unified multimodal pre-training for both Vision-Language understanding and generation (UniVL). The proposed UniVL is capable of handling both understanding tasks and generative tasks. We augment existing pretraining paradigms that only use random masks with causal masks, i.e., triangular masks that mask out future tokens, such that the pre-trained models can have autoregressive generation abilities by design. We formulate several previous understanding tasks as a text generation task and propose to use prompt-based method for fine-tuning on different downstream tasks. Our experiments show that there is a trade-off between understanding tasks and generation tasks while using the same model, and a feasible way to improve both tasks is to use more data. Our UniVL framework attains comparable performance to recent vision-language pre-training methods on both understanding tasks and generation tasks. Moreover, we demostrate that prompt-based finetuning is more data-efficient - it outperforms discriminative methods in few-shot scenarios.",
    "venue": "arXiv.org",
    "year": 2021,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-12-10",
    "authors": [
      {
        "authorId": "2636832",
        "name": "Tianyi Liu"
      },
      {
        "authorId": "3099139",
        "name": "Zuxuan Wu"
      },
      {
        "authorId": "22253126",
        "name": "Wenhan Xiong"
      },
      {
        "authorId": "2108536365",
        "name": "Jingjing Chen"
      },
      {
        "authorId": "1717861",
        "name": "Yu-Gang Jiang"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "ade8f79a2918c52c6a2aa3d07977cbb157dac6d6",
    "url": "https://www.semanticscholar.org/paper/ade8f79a2918c52c6a2aa3d07977cbb157dac6d6",
    "title": "IMAGE-TO-IMAGE TRANSLATION FOR ENHANCED FEATURE MATCHING, IMAGE RETRIEVAL AND VISUAL LOCALIZATION",
    "abstract": "Abstract. The performance of machine learning and deep learning algorithms for image analysis depends significantly on the quantity and quality of the training data. The generation of annotated training data is often costly, time-consuming and laborious. Data augmentation is a powerful option to overcome these drawbacks. Therefore, we augment training data by rendering images with arbitrary poses from 3D models to increase the quantity of training images. These training images usually show artifacts and are of limited use for advanced image analysis. Therefore, we propose to use image-to-image translation to transform images from a rendered domain to a captured domain. We show that translated images in the captured domain are of higher quality than the rendered images. Moreover, we demonstrate that image-to-image translation based on rendered 3D models enhances the performance of common computer vision tasks, namely feature matching, image retrieval and visual localization. The experimental results clearly show the enhancement on translated images over rendered images for all investigated tasks. In addition to this, we present the advantages utilizing translated images over exclusively captured images for visual localization.\n",
    "venue": "ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences",
    "year": 2019,
    "citationCount": 21,
    "openAccessPdf": {
      "url": "https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/IV-2-W7/111/2019/isprs-annals-IV-2-W7-111-2019.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2019-09-16",
    "authors": [
      {
        "authorId": "2191673",
        "name": "M. S. Mueller"
      },
      {
        "authorId": "1959475",
        "name": "Torsten Sattler"
      },
      {
        "authorId": "1742208",
        "name": "M. Pollefeys"
      },
      {
        "authorId": "2198031",
        "name": "B. Jutzi"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.36563680037474
  },
  {
    "paperId": "93db638ccd1342ed20ffd0d28fcc4007b836d547",
    "url": "https://www.semanticscholar.org/paper/93db638ccd1342ed20ffd0d28fcc4007b836d547",
    "title": "Automated Generation of Storytelling Vocabulary from Photographs for use in AAC",
    "abstract": "Research on the application of NLP in symbol-based Augmentative and Alternative Communication (AAC) tools for improving social interaction support is scarce. We contribute a novel method for generating context-related vocabulary from photographs of personally relevant events aimed at supporting people with language impairments in retelling their past experiences. Performance was calculated with information retrieval concepts on the relevance of vocabulary generated for communicating a corpus of 9730 narrative phrases about events depicted in 1946 photographs. In comparison to a baseline generation composed of frequent English words, our method generated vocabulary with a 4.6 gain in mean average precision, regardless of the level of contextual information in the input photographs, and 6.9 for photographs in which contextual information was extracted correctly. We conclude by discussing how our findings provide insights for system optimization and usage.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2021,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2021.acl-long.108.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2055706844",
        "name": "M. Vargas"
      },
      {
        "authorId": "1698401",
        "name": "Karyn Moffatt"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "f3c1bdf3abaf6f0399faf8acba6b87a8acd98211",
    "url": "https://www.semanticscholar.org/paper/f3c1bdf3abaf6f0399faf8acba6b87a8acd98211",
    "title": "Knowledge Graph Generation From Text Using Neural Machine Translation Techniques",
    "abstract": "As the applications of data science become pervasive in daily life, there arises a dire need to represent data in machine-understandable forms like knowledge graphs. Over the years, there have been numerous developments in extracting entities and their relations for augmenting knowledge graphs, but many of them depend on external dependencies like dependency parsers and part-of-speech taggers. These approaches, while indeed accomplishing this task, induce a certain degree of inflexibility in their implementation. Recent explorations in this domain have attempted to utilize Neural Machine Translation techniques to convert natural language to SPARQL queries, with a focus on information retrieval from pre-established Knowledge Graphs. We explore in detail, the variety of approaches followed for SPARQL machine translation, with a keen focus on insertion of extracted knowledge into the graphs.As part of our research, we curated a dataset- Scientists-100, extracted from Dbpedia, for the task of translation of natural language to SPARQL insertion statements. We also propose two models ‚Äì an Attention RNN and a Transformer for the same. These models achieve an accuracy of 99.27% and a 98.61% respectively on the dataset. In addition to this, we present a metric for examining the syntactic accuracy of the generated SPARQL statements. Our models exhibit 99.25% and 98.71% syntactic accuracy as calculated on the same.",
    "venue": "International Conference on Communication, Information & Computing Technology",
    "year": 2021,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2021-06-25",
    "authors": [
      {
        "authorId": "2123092486",
        "name": "Athang Gupte"
      },
      {
        "authorId": "2123092357",
        "name": "Saumitra Sapre"
      },
      {
        "authorId": "39786233",
        "name": "S. Sonawane"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "32c52698df99f3c1f8585e3938e2d09bbcaca87d",
    "url": "https://www.semanticscholar.org/paper/32c52698df99f3c1f8585e3938e2d09bbcaca87d",
    "title": "GENERATION AND ENCODING OF THE PROJECT INTREX AUGMENTED CATALOG DATA BASE",
    "abstract": "Abstract : A flexible, analytically-structured, catalog-record format was designed to aid in meeting the objectives of the display-oriented Project Intrex augmented catalog experiments. The analytical format, and the catalog data elements and their encoding for machine readability are discussed. The selection of documents from the literature of materials science and engineering for the Intrex data base, the generation of catalog records of those documents, and the initial processing of those records for computer-storage are covered. Initial studies that were made to evaluate the processing of catalog records receive attention. One study shows that data input at an on-line terminal in our current MIT CTSS operating environment is twice as expensive as our normal off-line data input using punched paper tape. Attention is also given to the creation from each document of a set of complete index term phrases and to the problems of matching these unconstrained terms with similarly unconstrained subject request phrases. Computer programs for phrase decomposition and word stemming, and interactive man-machine dialog, will help solve the problems of subject retrieval. The main development phase of the experimental time-shared augmented catalog is nearing completion.",
    "venue": "",
    "year": 1968,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "1968-08-01",
    "authors": [
      {
        "authorId": "2955885",
        "name": "Alan R. Benenfeld"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.87639203842082
  },
  {
    "paperId": "4b63916bb755a1b441c6093930b4763c20941cbe",
    "url": "https://www.semanticscholar.org/paper/4b63916bb755a1b441c6093930b4763c20941cbe",
    "title": "Ptychography retrieval of fully polarized holograms from geometric-phase metasurfaces",
    "abstract": null,
    "venue": "Nature Communications",
    "year": 2020,
    "citationCount": 157,
    "openAccessPdf": {
      "url": "https://www.nature.com/articles/s41467-020-16437-9.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Physics",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2020-05-27",
    "authors": [
      {
        "authorId": "1716615346",
        "name": "Q. Song"
      },
      {
        "authorId": "152699018",
        "name": "Arthur Baroni"
      },
      {
        "authorId": "143833715",
        "name": "Rajath Sawant"
      },
      {
        "authorId": "28677619",
        "name": "P. Ni"
      },
      {
        "authorId": "1720789110",
        "name": "V. Brandli"
      },
      {
        "authorId": "3158249",
        "name": "S. Chenot"
      },
      {
        "authorId": "12983789",
        "name": "S. V√©zian"
      },
      {
        "authorId": "12284069",
        "name": "B. Damilano"
      },
      {
        "authorId": "6958665",
        "name": "P. de Mierry"
      },
      {
        "authorId": "7661972",
        "name": "S. Khadir"
      },
      {
        "authorId": "2668084",
        "name": "P. Ferrand"
      },
      {
        "authorId": "4033160",
        "name": "P. Genevet"
      }
    ],
    "source": "semantic_scholar",
    "score": 135.9389254954045
  },
  {
    "paperId": "92de943c8e846233588204435025f7887dc5ba48",
    "url": "https://www.semanticscholar.org/paper/92de943c8e846233588204435025f7887dc5ba48",
    "title": "FoleyGAN: Visually Guided Generative Adversarial Network-Based Synchronous Sound Generation in Silent Videos",
    "abstract": "Deep learning based visual-to-sound generation systems have been developed that identify and create audio features from video signals. However, these techniques often fail to consider the time-synchronicity of the visual and audio features. In this paper we introduce a novel method for guiding a class-conditioned GAN to synthesize representative audio with temporally-extracted visual information. We accomplish this visual-to-sound generation task by adapting the synchronicity traits between the audio-visual modalities. Our proposed FoleyGAN model is capable of conditioning action sequences of visual events leading to the generation of visually aligned realistic soundtracks. We expanded our previously proposed Automatic Foley data set. We evaluated FoleyGAN‚Äôs synthesized sound output through human surveys that show noteworthy (on average 81%) audio-visual synchronicity performance. Our approach outperforms other baseline models and audio-visual data sets in statistical and ablation experiments achieving improved IS, FID and NDB scores. In ablation analysis we showed the significance of our visual and temporal feature extraction method as well as augmented performance of our generation network. Overall, our FoleyGAN model showed sound retrieval accuracy of 76.08% surpassing existing visual-to-audio synthesis deep neural networks.",
    "venue": "IEEE transactions on multimedia",
    "year": 2021,
    "citationCount": 22,
    "openAccessPdf": {
      "url": "https://ieeexplore.ieee.org/ielx7/6046/4456689/09782577.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2021-07-20",
    "authors": [
      {
        "authorId": "48359318",
        "name": "Sanchita Ghose"
      },
      {
        "authorId": "2845029",
        "name": "John J. Prevost"
      }
    ],
    "source": "semantic_scholar",
    "score": 123.03241323893724
  },
  {
    "paperId": "112e0260c960c02a808cbf191420b13ef824da1c",
    "url": "https://www.semanticscholar.org/paper/112e0260c960c02a808cbf191420b13ef824da1c",
    "title": "On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks",
    "abstract": "There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples--ranging from multiplication to simple planning--there persists a wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion. This belief seemingly rests on the assumption that verification of correctness should be easier than generation--a rather classical argument from computational complexity--which should be irrelevant to LLMs to the extent that what they are doing is approximate retrieval. In this paper, we set out to systematically investigate the effectiveness of iterative prompting in the context of reasoning and planning. We present a principled empirical study of the performance of GPT-4 in three domains: Game of 24, Graph Coloring, and STRIPS planning. We experiment both with the model critiquing its own answers and with an external correct reasoner verifying proposed solutions. In each case, we analyze whether the content of criticisms actually affects bottom line performance, and whether we can ablate elements of the augmented system without losing performance. We observe significant performance collapse with self-critique and significant performance gains with sound external verification. We also note that merely re-prompting with a sound verifier maintains most of the benefits of more involved setups.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 28,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-12",
    "authors": [
      {
        "authorId": "2260339788",
        "name": "Kaya Stechly"
      },
      {
        "authorId": "144982263",
        "name": "Karthik Valmeekam"
      },
      {
        "authorId": "2047340230",
        "name": "Subbarao Kambhampati"
      }
    ],
    "source": "semantic_scholar",
    "score": 120.5094374497971
  },
  {
    "paperId": "d49ee588e540ea9a75aead078c4bd6ac0580d308",
    "url": "https://www.semanticscholar.org/paper/d49ee588e540ea9a75aead078c4bd6ac0580d308",
    "title": "Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations",
    "abstract": "Long-form generations from large language models (LLMs) contain a mix of factual and non-factual claims, making evaluating factuality difficult. Prior works evaluate the factuality of a long paragraph by decomposing it into multiple facts, verifying those facts independently, and aggregating the results. Such methods assume that combining factual claims forms a factual paragraph. The above assumption can be violated: we show that strong open-source models like Llama-chat can generate paragraphs that contain verifiable facts, but the facts are combined into a non-factual paragraph due to entity ambiguity. We further reveal that existing factuality metrics, including FActScore and citation recall, cannot properly evaluate these non-factual paragraphs and overestimate their factuality. To address this, we introduce an enhanced metric, D-FActScore, specifically designed for content with ambiguous entities. We evaluate the D-FActScores of people biographies generated by retrieval-augmented LLMs. We show that D-FActScore can better assess the factuality of paragraphs with entity ambiguity than FActScore. We also find that four widely used open-source LLMs tend to mix information of distinct entities to form non-factual paragraphs, making their D-FActScore much lower than FActScore by over 10%.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-08",
    "authors": [
      {
        "authorId": "1992777064",
        "name": "Cheng-Han Chiang"
      },
      {
        "authorId": "2282959610",
        "name": "Hung-yi Lee"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "83c168e84bcd0f97af53acf34003090f38cb3dd1",
    "url": "https://www.semanticscholar.org/paper/83c168e84bcd0f97af53acf34003090f38cb3dd1",
    "title": "MedImageInsight: An Open-Source Embedding Model for General Domain Medical Imaging",
    "abstract": "In this work, we present MedImageInsight, an open-source medical imaging embedding model. MedImageInsight is trained on medical images with associated text and labels across a diverse collection of domains, including X-Ray, CT, MRI, dermoscopy, OCT, fundus photography, ultrasound, histopathology, and mammography. Rigorous evaluations demonstrate MedImageInsight's ability to achieve state-of-the-art (SOTA) or human expert level performance across classification, image-image search, and fine-tuning tasks. Specifically, on public datasets, MedImageInsight achieves SOTA in CT 3D medical image retrieval, as well as SOTA in disease classification and search for chest X-ray, dermatology, and OCT imaging. Furthermore, MedImageInsight achieves human expert performance in bone age estimation (on both public and partner data), as well as AUC above 0.9 in most other domains. When paired with a text decoder, MedImageInsight achieves near SOTA level single image report findings generation with less than 10\\% the parameters of other models. Compared to fine-tuning GPT-4o with only MIMIC-CXR data for the same task, MedImageInsight outperforms in clinical metrics, but underperforms on lexical metrics where GPT-4o sets a new SOTA. Importantly for regulatory purposes, MedImageInsight can generate ROC curves, adjust sensitivity and specificity based on clinical need, and provide evidence-based decision support through image-image search (which can also enable retrieval augmented generation). In an independent clinical evaluation of image-image search in chest X-ray, MedImageInsight outperformed every other publicly available foundation model evaluated by large margins (over 6 points AUC), and significantly outperformed other models in terms of AI fairness (across age and gender). We hope releasing MedImageInsight will help enhance collective progress in medical imaging AI research and development.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-09",
    "authors": [
      {
        "authorId": "2268318452",
        "name": "Noel Codella"
      },
      {
        "authorId": "2252430868",
        "name": "Ying Jin"
      },
      {
        "authorId": "2325227100",
        "name": "Shrey Jain"
      },
      {
        "authorId": "2260374573",
        "name": "Yu Gu"
      },
      {
        "authorId": "2268317049",
        "name": "Ho Hin Lee"
      },
      {
        "authorId": "2127117221",
        "name": "Asma Ben Abacha"
      },
      {
        "authorId": "2264626868",
        "name": "Alberto Santamar√≠a-Pang"
      },
      {
        "authorId": "2325094767",
        "name": "Will Guyman"
      },
      {
        "authorId": "2325092343",
        "name": "Naiteek Sangani"
      },
      {
        "authorId": "2267010012",
        "name": "Sheng Zhang"
      },
      {
        "authorId": "1759772",
        "name": "Hoifung Poon"
      },
      {
        "authorId": "2261608797",
        "name": "Stephanie L. Hyland"
      },
      {
        "authorId": "82303325",
        "name": "Shruthi Bannur"
      },
      {
        "authorId": "2029689505",
        "name": "Javier Alvarez-Valle"
      },
      {
        "authorId": "2325108542",
        "name": "Xue Li"
      },
      {
        "authorId": "2324988751",
        "name": "John Garrett"
      },
      {
        "authorId": "2325044878",
        "name": "Alan McMillan"
      },
      {
        "authorId": "80852197",
        "name": "Gaurav Rajguru"
      },
      {
        "authorId": "2325093634",
        "name": "Madhu Maddi"
      },
      {
        "authorId": "2325097344",
        "name": "Nilesh Vijayrania"
      },
      {
        "authorId": "2325094886",
        "name": "Rehaan Bhimai"
      },
      {
        "authorId": "2279750706",
        "name": "Nick Mecklenburg"
      },
      {
        "authorId": "2325116620",
        "name": "Rupal Jain"
      },
      {
        "authorId": "2279750514",
        "name": "Daniel Holstein"
      },
      {
        "authorId": "2325093842",
        "name": "Naveen Gaur"
      },
      {
        "authorId": "2257349985",
        "name": "Vijay Aski"
      },
      {
        "authorId": "2240538264",
        "name": "Jenq-Neng Hwang"
      },
      {
        "authorId": "2268318167",
        "name": "Thomas Lin"
      },
      {
        "authorId": "145368876",
        "name": "I. Tarapov"
      },
      {
        "authorId": "2188270295",
        "name": "M. Lungren"
      },
      {
        "authorId": "2072847758",
        "name": "Mu-Hsin Wei"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "6c776ab9d309ca2541ad087d8af784819d13357b",
    "url": "https://www.semanticscholar.org/paper/6c776ab9d309ca2541ad087d8af784819d13357b",
    "title": "Q-PEFT: Query-dependent Parameter Efficient Fine-tuning for Text Reranking with Large Language Models",
    "abstract": "Parameter Efficient Fine-Tuning (PEFT) methods have been extensively utilized in Large Language Models (LLMs) to improve the down-streaming tasks without the cost of fine-tuing the whole LLMs. Recent studies have shown how to effectively use PEFT for fine-tuning LLMs in ranking tasks with convincing performance; there are some limitations, including the learned prompt being fixed for different documents, overfitting to specific tasks, and low adaptation ability. In this paper, we introduce a query-dependent parameter efficient fine-tuning (Q-PEFT) approach for text reranking to leak the information of the true queries to LLMs and then make the generation of true queries from input documents much easier. Specifically, we utilize the query to extract the top-$k$ tokens from concatenated documents, serving as contextual clues. We further augment Q-PEFT by substituting the retrieval mechanism with a multi-head attention layer to achieve end-to-end training and cover all the tokens in the documents, guiding the LLMs to generate more document-specific synthetic queries, thereby further improving the reranking performance. Extensive experiments are conducted on four public datasets, demonstrating the effectiveness of our proposed approach.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-06",
    "authors": [
      {
        "authorId": "2113952662",
        "name": "Zhiyuan Peng"
      },
      {
        "authorId": "2293800171",
        "name": "Xuyang Wu"
      },
      {
        "authorId": "2261393439",
        "name": "Qifan Wang"
      },
      {
        "authorId": "2215626161",
        "name": "Sravanthi Rajanala"
      },
      {
        "authorId": "2293772877",
        "name": "Yi Fang"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "3723fb7ab37f2b67c2b95bf5c11e9dc9c9957e4d",
    "url": "https://www.semanticscholar.org/paper/3723fb7ab37f2b67c2b95bf5c11e9dc9c9957e4d",
    "title": "Empowering Large Language Models to Leverage Domain-Specific Knowledge in E-Learning",
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks. However, their performance in domain-specific contexts, such as E-learning, is hindered by the lack of specific domain knowledge. This paper adopts a novel approach of retrieval augment generation to empower LLMs with domain-specific knowledge in the field of E-learning. The approach leverages external knowledge sources, such as E-learning lectures or research papers, to enhance the LLM‚Äôs understanding and generation capabilities. Experimental evaluations demonstrate the effectiveness and superiority of our approach compared to existing methods in capturing and generating E-learning-specific information.",
    "venue": "Applied Sciences",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "https://www.mdpi.com/2076-3417/14/12/5264/pdf?version=1718697053",
      "status": "GOLD"
    },
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-18",
    "authors": [
      {
        "authorId": "2055872063",
        "name": "Ruei-Shan Lu"
      },
      {
        "authorId": "2217992245",
        "name": "Ching-Chang Lin"
      },
      {
        "authorId": "2458769",
        "name": "Hsiu-Yuan Tsao"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1415686865115
  },
  {
    "paperId": "5831c72295ebdd7f698deba914c30b907c801e6a",
    "url": "https://www.semanticscholar.org/paper/5831c72295ebdd7f698deba914c30b907c801e6a",
    "title": "Halu-J: Critique-Based Hallucination Judge",
    "abstract": "Large language models (LLMs) frequently generate non-factual content, known as hallucinations. Existing retrieval-augmented-based hallucination detection approaches typically address this by framing it as a classification task, evaluating hallucinations based on their consistency with retrieved evidence. However, this approach usually lacks detailed explanations for these evaluations and does not assess the reliability of these explanations. Furthermore, deficiencies in retrieval systems can lead to irrelevant or partially relevant evidence retrieval, impairing the detection process. Moreover, while real-world hallucination detection requires analyzing multiple pieces of evidence, current systems usually treat all evidence uniformly without considering its relevance to the content. To address these challenges, we introduce Halu-J, a critique-based hallucination judge with 7 billion parameters. Halu-J enhances hallucination detection by selecting pertinent evidence and providing detailed critiques. Our experiments indicate that Halu-J outperforms GPT-4o in multiple-evidence hallucination detection and matches its capability in critique generation and evidence selection. We also introduce ME-FEVER, a new dataset designed for multiple-evidence hallucination detection. Our code and dataset can be found in https://github.com/GAIR-NLP/factool .",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-17",
    "authors": [
      {
        "authorId": "2307185797",
        "name": "Binjie Wang"
      },
      {
        "authorId": "2224851117",
        "name": "Steffi Chern"
      },
      {
        "authorId": "2273658317",
        "name": "Ethan Chern"
      },
      {
        "authorId": "2307778855",
        "name": "Pengfei Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "93a361e8b3f8b90f3b99622704b0103cdf015682",
    "url": "https://www.semanticscholar.org/paper/93a361e8b3f8b90f3b99622704b0103cdf015682",
    "title": "When Does Perceptual Alignment Benefit Vision Representations?",
    "abstract": "Humans judge perceptual similarity according to diverse visual attributes, including scene layout, subject location, and camera pose. Existing vision models understand a wide range of semantic abstractions but improperly weigh these attributes and thus make inferences misaligned with human perception. While vision representations have previously benefited from alignment in contexts like image generation, the utility of perceptually aligned representations in more general-purpose settings remains unclear. Here, we investigate how aligning vision model representations to human perceptual judgments impacts their usability across diverse computer vision tasks. We finetune state-of-the-art models on human similarity judgments for image triplets and evaluate them across standard vision benchmarks. We find that aligning models to perceptual judgments yields representations that improve upon the original backbones across many downstream tasks, including counting, segmentation, depth estimation, instance retrieval, and retrieval-augmented generation. In addition, we find that performance is widely preserved on other tasks, including specialized out-of-distribution domains such as in medical imaging and 3D environment frames. Our results suggest that injecting an inductive bias about human perceptual knowledge into vision models can contribute to better representations.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-10-14",
    "authors": [
      {
        "authorId": "2061848423",
        "name": "Shobhita Sundaram"
      },
      {
        "authorId": "2277614007",
        "name": "Stephanie Fu"
      },
      {
        "authorId": "72543861",
        "name": "Lukas Muttenthaler"
      },
      {
        "authorId": "48842501",
        "name": "Netanel Y. Tamir"
      },
      {
        "authorId": "51322829",
        "name": "Lucy Chai"
      },
      {
        "authorId": "2135550613",
        "name": "Simon Kornblith"
      },
      {
        "authorId": "2327334625",
        "name": "Trevor Darrell"
      },
      {
        "authorId": "2257281262",
        "name": "Phillip Isola"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "0cd53afc6ac0033806d501886fb0f1dab5641057",
    "url": "https://www.semanticscholar.org/paper/0cd53afc6ac0033806d501886fb0f1dab5641057",
    "title": "Enhancing Question Answering for Enterprise Knowledge Bases using Large Language Models",
    "abstract": "Efficient knowledge management plays a pivotal role in augmenting both the operational efficiency and the innovative capacity of businesses and organizations. By indexing knowledge through vectorization, a variety of knowledge retrieval methods have emerged, significantly enhancing the efficacy of knowledge management systems. Recently, the rapid advancements in generative natural language processing technologies paved the way for generating precise and coherent answers after retrieving relevant documents tailored to user queries. However, for enterprise knowledge bases, assembling extensive training data from scratch for knowledge retrieval and generation is a formidable challenge due to the privacy and security policies of private data, frequently entailing substantial costs. To address the challenge above, in this paper, we propose EKRG, a novel Retrieval-Generation framework based on large language models (LLMs), expertly designed to enable question-answering for Enterprise Knowledge bases with limited annotation costs. Specifically, for the retrieval process, we first introduce an instruction-tuning method using an LLM to generate sufficient document-question pairs for training a knowledge retriever. This method, through carefully designed instructions, efficiently generates diverse questions for enterprise knowledge bases, encompassing both fact-oriented and solution-oriented knowledge. Additionally, we develop a relevance-aware teacher-student learning strategy to further enhance the efficiency of the training process. For the generation process, we propose a novel chain of thought (CoT) based fine-tuning method to empower the LLM-based generator to adeptly respond to user questions using retrieved documents. Finally, extensive experiments on real-world datasets have demonstrated the effectiveness of our proposed framework.",
    "venue": "International Conference on Database Systems for Advanced Applications",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-10",
    "authors": [
      {
        "authorId": "2273692566",
        "name": "Feihu Jiang"
      },
      {
        "authorId": "2084610514",
        "name": "Chuan Qin"
      },
      {
        "authorId": "40143334",
        "name": "Kaichun Yao"
      },
      {
        "authorId": "2259820435",
        "name": "Chuyu Fang"
      },
      {
        "authorId": "2266240304",
        "name": "Fuzhen Zhuang"
      },
      {
        "authorId": "1968806",
        "name": "Hengshu Zhu"
      },
      {
        "authorId": "2274428072",
        "name": "Hui Xiong"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "5e206bc2e276f43a530fd593fdb208a8c76afad7",
    "url": "https://www.semanticscholar.org/paper/5e206bc2e276f43a530fd593fdb208a8c76afad7",
    "title": "MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets",
    "abstract": "Development of multimodal interactive systems is hindered by the lack of rich, multimodal (text, images) conversational data, which is needed in large quantities for LLMs. Previous approaches augment textual dialogues with retrieved images, posing privacy, diversity, and quality constraints. In this work, we introduce Multimodal Augmented Generative Images Dialogues (MAGID), a framework to augment text-only dialogues with diverse and high-quality images . Subsequently, a diffusion model is applied to craft corresponding images, ensuring alignment with the identified text. Finally, MAGID incorporates an innovative feedback loop between an image description generation module (textual LLM) and image quality modules (addressing aesthetics, image-text matching, and safety), that work in tandem to generate high-quality and multi-modal dialogues. We compare MAGID to other SOTA baselines on three dialogue datasets, using automated and human evaluation. Our results show that MAGID is comparable to or better than baselines, with significant improvements in human evaluation, especially against retrieval baselines where the image database is small.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-03-05",
    "authors": [
      {
        "authorId": "2290018018",
        "name": "Hossein Aboutalebi"
      },
      {
        "authorId": "2260612278",
        "name": "Hwanjun Song"
      },
      {
        "authorId": "2290210451",
        "name": "Yusheng Xie"
      },
      {
        "authorId": "144877669",
        "name": "Arshit Gupta"
      },
      {
        "authorId": "2290066632",
        "name": "Justin Sun"
      },
      {
        "authorId": "2260901186",
        "name": "Hang Su"
      },
      {
        "authorId": "2260409785",
        "name": "Igor Shalyminov"
      },
      {
        "authorId": "2283784519",
        "name": "Nikolaos Pappas"
      },
      {
        "authorId": "2260441495",
        "name": "Siffi Singh"
      },
      {
        "authorId": "39674628",
        "name": "Saab Mansour"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "59641c10ed7431a3cf841f308367dc2dc0281b74",
    "url": "https://www.semanticscholar.org/paper/59641c10ed7431a3cf841f308367dc2dc0281b74",
    "title": "What Makes Good In-Context Examples for GPT-3?",
    "abstract": "GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting in-context examples (relative to random sampling) that better leverage GPT-3‚Äôs in-context learning capabilities.Inspired by the recent success of leveraging a retrieval module to augment neural networks, we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt. Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3‚Äôs power of text generation. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline. Moreover, it is observed that the sentence encoders fine-tuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-to-text generation (44.3% on the ToTTo dataset) and open-domain question answering (45.5% on the NQ dataset).",
    "venue": "Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out",
    "year": 2021,
    "citationCount": 1186,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2022.deelio-1.10.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-01-17",
    "authors": [
      {
        "authorId": "8807538",
        "name": "Jiachang Liu"
      },
      {
        "authorId": "19178763",
        "name": "Dinghan Shen"
      },
      {
        "authorId": "48378494",
        "name": "Yizhe Zhang"
      },
      {
        "authorId": "66648221",
        "name": "Bill Dolan"
      },
      {
        "authorId": "145006560",
        "name": "L. Carin"
      },
      {
        "authorId": "2109136147",
        "name": "Weizhu Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 176.18776591914502
  },
  {
    "paperId": "3bddda884c5264d2d3ae7087c2d570243dbe1db4",
    "url": "https://www.semanticscholar.org/paper/3bddda884c5264d2d3ae7087c2d570243dbe1db4",
    "title": "MechGPT, a language-based strategy for mechanics and materials modeling that connects knowledge across scales, disciplines and modalities",
    "abstract": "\n For centuries, researchers have sought out ways to connect disparate areas of knowledge. While early scholars (Galileo, da Vinci, etc.) were experts across fields, specialization has taken hold later. With the advent of Artificial Intelligence, we can now explore relationships across areas (e.g., mechanics-biology) or disparate domains (e.g., failure mechanics-art). To achieve this, we use a fine-tuned Large Language Model (LLM), here for a subset of knowledge in multiscale materials failure. The approach includes the use of a general-purpose LLM to distill question-answer pairs from raw sources followed by LLM fine-tuning. The resulting MechGPT LLM foundation model is used in a series of computational experiments to explore its capacity for knowledge retrieval, various language tasks, hypothesis generation, and connecting knowledge across disparate areas. While the model has some ability to recall knowledge from training, we find that LLMs are particularly useful to extract structural insights through Ontological Knowledge Graphs. These interpretable graph structures provide explanatory insights, frameworks for new research questions, and visual representations of knowledge that also can be used in retrieval-augmented generation. Three versions of MechGPT are discussed, featuring different sizes from 13 billion to 70 billion parameters, and reaching context lengths of more than 10,000 tokens. This provides ample capacity for sophisticated retrieval augmented strategies, as well as agent-based modeling where multiple LLMs interact collaboratively and/or adversarially, the incorporation of new data from the literature or web searches, as well as multimodality.",
    "venue": "Applied Mechanics Review",
    "year": 2023,
    "citationCount": 40,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.10445",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Physics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-16",
    "authors": [
      {
        "authorId": "2273480",
        "name": "M. Buehler"
      }
    ],
    "source": "semantic_scholar",
    "score": 125.70358100056461
  },
  {
    "paperId": "eb36681fc4c5dfce4f3e05540fc92b007de278ca",
    "url": "https://www.semanticscholar.org/paper/eb36681fc4c5dfce4f3e05540fc92b007de278ca",
    "title": "SelfEvolve: A Code Evolution Framework via Large Language Models",
    "abstract": "Large language models (LLMs) have already revolutionized code generation, after being pretrained on publicly available code data. However, while various methods have been proposed to augment LLMs with retrieved knowledge and enhance the quality of code generation, the performance of these retrieval-based methods is limited by the strength of the retrievers used. In addition, while LLMs show great emergent ability, they still struggle to produce the correct code in one turn. To address these challenges, we propose a novel two-step pipeline, called \\autoknow, that leverages LLMs as both knowledge providers and self-reflective programmers. Unlike retrieval-based methods, \\autoknow~obtains the knowledge from input prompts and generates intermediate code based on the generated knowledge. After that, \\autoknow~asks LLM to act as an expert programmer to perform debugging for the generated code. This is achieved by receiving the error message from the interpreter, without requiring special test cases for correctness verification. We evaluate \\autoknow~on three code generation datasets, including DS-1000 for data science code, HumanEval for software engineering code, and TransCoder for C++-to-Python translation. Our empirical experiments show that \\autoknow~outperforms strong baselines by a significant margin on all datasets. We also conduct exhaustive analytical experiments to validate the effectiveness of the two stages of \\autoknow, and find that both are superior to other prompting-based methods. Further scalability analysis demonstrates that \\autoknow~can be adapted to other more advanced models, such as GPT-4, and bring consistent efficacy improvement.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 27,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.02907",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-05",
    "authors": [
      {
        "authorId": "2119327053",
        "name": "Shuyang Jiang"
      },
      {
        "authorId": "2198466349",
        "name": "Yuhao Wang"
      },
      {
        "authorId": "2153609392",
        "name": "Yu Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.98306765262805
  },
  {
    "paperId": "8408175ee39cda1ab5107dadc3bf711ea45fa734",
    "url": "https://www.semanticscholar.org/paper/8408175ee39cda1ab5107dadc3bf711ea45fa734",
    "title": "Concept-Aware Video Captioning: Describing Videos With Effective Prior Information",
    "abstract": "Concepts, a collective term for meaningful words that correspond to objects, actions, and attributes, can act as an intermediary for video captioning. While many efforts have been made to augment video captioning with concepts, most methods suffer from limited precision of concept detection and insufficient utilization of concepts, which could provide caption generation with inaccurate and inadequate prior information. Considering these issues, we propose a Concept-awARE video captioning framework (CARE) to facilitate plausible caption generation. Based on the encoder-decoder structure, CARE detects concepts precisely via multimodal-driven concept detection (MCD) and offers sufficient prior information to caption generation by global-local semantic guidance (G-LSG). Specifically, we implement MCD by leveraging video-to-text retrieval and the multimedia nature of videos. To achieve G-LSG, given the concept probabilities predicted by MCD, we weight and aggregate concepts to mine the video‚Äôs latent topic to affect decoding globally and devise a simple yet efficient hybrid attention module to exploit concepts and video content to impact decoding locally. Finally, to develop CARE, we emphasize on the knowledge transfer of a contrastive vision-language pre-trained model (i.e., CLIP) in terms of visual understanding and video-to-text retrieval. With the multi-role CLIP, CARE can outperform CLIP-based strong video captioning baselines with affordable extra parameter and inference latency costs. Extensive experiments on MSVD, MSR-VTT, and VATEX datasets demonstrate the versatility of our approach for different encoder-decoder networks and the superiority of CARE against state-of-the-art methods. Our code is available at https://github.com/yangbang18/CARE.",
    "venue": "IEEE Transactions on Image Processing",
    "year": 2023,
    "citationCount": 19,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-08-28",
    "authors": [
      {
        "authorId": "2115355581",
        "name": "Bang Yang"
      },
      {
        "authorId": "2057072853",
        "name": "Meng Cao"
      },
      {
        "authorId": "26981150",
        "name": "Yuexian Zou"
      }
    ],
    "source": "semantic_scholar",
    "score": 120.93598410330986
  },
  {
    "paperId": "e516665a91d7e76c75796a106332882153323e6b",
    "url": "https://www.semanticscholar.org/paper/e516665a91d7e76c75796a106332882153323e6b",
    "title": "Guiding Language Models of Code with Global Context using Monitors",
    "abstract": "Language models of code (LMs) work well when the surrounding code in the vicinity of generation provides sufficient context. This is not true when it becomes necessary to use types or functionality defined in another module or library, especially those not seen during training. LMs suffer from limited awareness of such global context and end up hallucinating, e.g., using types defined in other files incorrectly. Recent work tries to overcome this issue by retrieving global information to augment the local context. However, this bloats the prompt or requires architecture modifications and additional training. Integrated development environments (IDEs) assist developers by bringing the global context at their fingertips using static analysis. We extend this assistance, enjoyed by developers, to the LMs. We propose a notion of monitors that use static analysis in the background to guide the decoding. Unlike a priori retrieval, static analysis is invoked iteratively during the entire decoding process, providing the most relevant suggestions on demand. We demonstrate the usefulness of our proposal by monitoring for type-consistent use of identifiers whenever an LM generates code for object dereference. To evaluate our approach, we curate PragmaticCode, a dataset of open-source projects with their development environments. On models of varying parameter scale, we show that monitor-guided decoding consistently improves the ability of an LM to not only generate identifiers that match the ground truth but also improves compilation rates and agreement with ground truth. We find that LMs with fewer parameters, when guided with our monitor, can outperform larger LMs. With monitor-guided decoding, SantaCoder-1.1B achieves better compilation rate and next-identifier match than the much larger text-davinci-003 model. The datasets and code will be released at https://aka.ms/monitors4codegen .",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 17,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.10763",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-19",
    "authors": [
      {
        "authorId": "2029486431",
        "name": "Lakshya A Agrawal"
      },
      {
        "authorId": "2594759",
        "name": "Aditya Kanade"
      },
      {
        "authorId": "144260125",
        "name": "Navin Goyal"
      },
      {
        "authorId": "145474353",
        "name": "Shuvendu K. Lahiri"
      },
      {
        "authorId": "1685546",
        "name": "S. Rajamani"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.35557636844247
  },
  {
    "paperId": "f40ef03bc4b773909ba615320853a50cdf7d2481",
    "url": "https://www.semanticscholar.org/paper/f40ef03bc4b773909ba615320853a50cdf7d2481",
    "title": "CONTENT BASED FORENSICS TATTOO IMAGE RETRIEVAL AND IDENTIFICATION METHOD FOR DATABASE APPLICATIONS",
    "abstract": ": The success of automatic fingerprint systems in law enforcement and forensics around the World has prompted the use of biometrics in various civil identification systems. Although tremendous progress has been made in biometrics and forensics, many situations exist where the primary biometric traits such as fingerprint, face, and iris alone cannot identify an individual with sufficiently high accuracy . Next Generation Identification (NGI) system for identifying criminals by using additional biometric modalities, such as a palm print and iris, to augment fingerprint evidence. The NGI system will include soft biometric traits, including scars, marks, and tattoos the current work focuses on one such soft biometric, namely tattoo images, which are routinely collected by law enforcement agencies and used in apprehending criminals and identifying suspects. The current practice of tattoo matching and retrieval, based on ANSI/NIST classes, is prone to significant errors due to limited vocabulary and the subjective nature of labeling. The proposed system uses the Tattoo-ID content-based image retrieval (CBIR) system which automatically extracts features from a query image and retrieves near-duplicate tattoo and also improves the retrieval accuracy particularly for queries with low quality images from a database with 90.5 percent.",
    "venue": "",
    "year": 2015,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "107620666",
        "name": "M. Kanamadi"
      },
      {
        "authorId": "2285264277",
        "name": "DR D.S.BHOSLE"
      }
    ],
    "source": "semantic_scholar",
    "score": 81.47918433002164
  },
  {
    "paperId": "dbd0d1db5ce43d9038a9865cdd879d245f4b3e2a",
    "url": "https://www.semanticscholar.org/paper/dbd0d1db5ce43d9038a9865cdd879d245f4b3e2a",
    "title": "Considering Nutrients During the Generation of Recipes by Process-Oriented Case-Based Reasoning",
    "abstract": null,
    "venue": "International Conference on Case-Based Reasoning",
    "year": 2018,
    "citationCount": 1,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2018-07-09",
    "authors": [
      {
        "authorId": "40803643",
        "name": "Christian Zeyen"
      },
      {
        "authorId": "1704822154",
        "name": "Maximilian Hoffmann"
      },
      {
        "authorId": "38412773",
        "name": "Gilbert M√ºller"
      },
      {
        "authorId": "145550612",
        "name": "R. Bergmann"
      }
    ],
    "source": "semantic_scholar",
    "score": 55.39720770839918
  },
  {
    "paperId": "7ec393a898521e4e9a3f510be424861f5a518109",
    "url": "https://www.semanticscholar.org/paper/7ec393a898521e4e9a3f510be424861f5a518109",
    "title": "Large Language Models as Topological Structure Enhancers for Text-Attributed Graphs",
    "abstract": "The latest advancements in large language models (LLMs) have revolutionized the field of natural language processing (NLP). Inspired by the success of LLMs in NLP tasks, some recent work has begun investigating the potential of applying LLMs in graph learning tasks. However, most of the existing work focuses on utilizing LLMs as powerful node feature augmenters, leaving employing LLMs to enhance graph topological structures an understudied problem. In this work, we explore how to leverage the information retrieval and text generation capabilities of LLMs to refine/enhance the topological structure of text-attributed graphs (TAGs) under the node classification setting. First, we propose using LLMs to help remove unreliable edges and add reliable ones in the TAG. Specifically, we first let the LLM output the semantic similarity between node attributes through delicate prompt designs, and then perform edge deletion and edge addition based on the similarity. Second, we propose using pseudo-labels generated by the LLM to improve graph topology, that is, we introduce the pseudo-label propagation as a regularization to guide the graph neural network (GNN) in learning proper edge weights. Finally, we incorporate the two aforementioned LLM-based methods for graph topological refinement into the process of GNN training, and perform extensive experiments on four real-world datasets. The experimental results demonstrate the effectiveness of LLM-based graph topology refinement (achieving a 0.15%--2.47% performance gain on public benchmarks).",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-24",
    "authors": [
      {
        "authorId": "2268390978",
        "name": "Shengyin Sun"
      },
      {
        "authorId": "2265170795",
        "name": "Yuxiang Ren"
      },
      {
        "authorId": "2268408070",
        "name": "Chen Ma"
      },
      {
        "authorId": "2258787460",
        "name": "Xuecang Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "0815a43e8868136db3c20d81c2713a1a0a4eb28c",
    "url": "https://www.semanticscholar.org/paper/0815a43e8868136db3c20d81c2713a1a0a4eb28c",
    "title": "CISA: Context Substitution for Image Semantics Augmentation",
    "abstract": "Large datasets catalyze the rapid expansion of deep learning and computer vision. At the same time, in many domains, there is a lack of training data, which may become an obstacle for the practical application of deep computer vision models. To overcome this problem, it is popular to apply image augmentation. When a dataset contains instance segmentation masks, it is possible to apply instance-level augmentation. It operates by cutting an instance from the original image and pasting to new backgrounds. This article challenges a dataset with the same objects present in various domains. We introduce the Context Substitution for Image Semantics Augmentation framework (CISA), which is focused on choosing good background images. We compare several ways to find backgrounds that match the context of the test set, including Contrastive Language‚ÄìImage Pre-Training (CLIP) image retrieval and diffusion image generation. We prove that our augmentation method is effective for classification, segmentation, and object detection with different dataset complexity and different model types. The average percentage increase in accuracy across all the tasks on a fruits and vegetables recognition dataset is 4.95%. Moreover, we show that the Fr√©chet Inception Distance (FID) metrics has a strong correlation with model accuracy, and it can help to choose better backgrounds without model training. The average negative correlation between model accuracy and the FID between the augmented and test datasets is 0.55 in our experiments.",
    "venue": "Mathematics",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "https://www.mdpi.com/2227-7390/11/8/1818/pdf?version=1681222327",
      "status": "GOLD"
    },
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-04-11",
    "authors": [],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "801981a7a071e5b76adcd54f4275006073fe4380",
    "url": "https://www.semanticscholar.org/paper/801981a7a071e5b76adcd54f4275006073fe4380",
    "title": "Reinforcement-Learning-Guided Source Code Summarization Using Hierarchical Attention",
    "abstract": "Code summarization (aka comment generation) provides a high-level natural language description of the function performed by code, which can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, the state-of-the-art approaches follow an encoder-decoder framework which encodes source code into a hidden space and later decodes it into a natural language space. Such approaches suffer from the following drawbacks: (a) they are mainly input by representing code as a sequence of tokens while ignoring code hierarchy; (b) most of the encoders only input simple features (e.g., tokens) while ignoring the features that can help capture the correlations between comments and code; (c) the decoders are typically trained to predict subsequent words by maximizing the likelihood of subsequent ground truth words, while in real world, they are excepted to generate the entire word sequence from scratch. As a result, such drawbacks lead to inferior and inconsistent comment generation accuracy. To address the above limitations, this paper presents a new code summarization approach using hierarchical attention network by incorporating multiple code features, including type-augmented abstract syntax trees and program control flows. Such features, along with plain code sequences, are injected into a deep reinforcement learning (DRL) framework (e.g., actor-critic network) for comment generation. Our approach assigns weights (pays ‚Äúattention‚Äù) to tokens and statements when constructing the code representation to reflect the hierarchical code structure under different contexts regarding code features (e.g., control flows and abstract syntax trees). Our reinforcement learning mechanism further strengthens the prediction results through the actor network and the critic network, where the actor network provides the confidence of predicting subsequent words based on the current state, and the critic network computes the reward values of all the possible extensions of the current state to provide global guidance for explorations. Eventually, we employ an advantage reward to train both networks and conduct a set of experiments on a real-world dataset. The experimental results demonstrate that our approach outperforms the baselines by around 22 to 45 percent in BLEU-1 and outperforms the state-of-the-art approaches by around 5 to 60 percent in terms of S-BLEU and C-BLEU.",
    "venue": "IEEE Transactions on Software Engineering",
    "year": 2022,
    "citationCount": 95,
    "openAccessPdf": {
      "url": "https://opus.lib.uts.edu.au/bitstream/10453/139555/3/Binder1.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-01-01",
    "authors": [
      {
        "authorId": "2108976833",
        "name": "Wenhua Wang"
      },
      {
        "authorId": "2108444948",
        "name": "Yuqun Zhang"
      },
      {
        "authorId": "34296085",
        "name": "Yulei Sui"
      },
      {
        "authorId": "2389866",
        "name": "Yao Wan"
      },
      {
        "authorId": "47122432",
        "name": "Zhou Zhao"
      },
      {
        "authorId": "2115904008",
        "name": "Jian Wu"
      },
      {
        "authorId": "144019071",
        "name": "Philip S. Yu"
      },
      {
        "authorId": "1747560",
        "name": "Guandong Xu"
      }
    ],
    "source": "semantic_scholar",
    "score": 144.46522287201753
  },
  {
    "paperId": "5272acad9e4201e93dabe3fd99bd7ead9b1a544d",
    "url": "https://www.semanticscholar.org/paper/5272acad9e4201e93dabe3fd99bd7ead9b1a544d",
    "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models",
    "abstract": "As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc. This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types. This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs. Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 131,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-01-02",
    "authors": [
      {
        "authorId": "2146190509",
        "name": "S. Tonmoy"
      },
      {
        "authorId": "2277446934",
        "name": "S. M. M. Zaman"
      },
      {
        "authorId": "2212131028",
        "name": "Vinija Jain"
      },
      {
        "authorId": "2106627712",
        "name": "Anku Rani"
      },
      {
        "authorId": "9460529",
        "name": "Vipula Rawte"
      },
      {
        "authorId": "40016108",
        "name": "Aman Chadha"
      },
      {
        "authorId": "2258322706",
        "name": "Amitava Das"
      }
    ],
    "source": "semantic_scholar",
    "score": 143.24202883879556
  },
  {
    "paperId": "3eb714c5e226d8d223f80b135cf5bbd9f8c37e02",
    "url": "https://www.semanticscholar.org/paper/3eb714c5e226d8d223f80b135cf5bbd9f8c37e02",
    "title": "Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence",
    "abstract": "The rapid advancement of large language models (LLMs) has paved the way for the development of highly capable autonomous agents. However, existing multi-agent frameworks often struggle with integrating diverse capable third-party agents due to reliance on agents defined within their own ecosystems. They also face challenges in simulating distributed environments, as most frameworks are limited to single-device setups. Furthermore, these frameworks often rely on hard-coded communication pipelines, limiting their adaptability to dynamic task requirements. Inspired by the concept of the Internet, we propose the Internet of Agents (IoA), a novel framework that addresses these limitations by providing a flexible and scalable platform for LLM-based multi-agent collaboration. IoA introduces an agent integration protocol, an instant-messaging-like architecture design, and dynamic mechanisms for agent teaming and conversation flow control. Through extensive experiments on general assistant tasks, embodied AI tasks, and retrieval-augmented generation benchmarks, we demonstrate that IoA consistently outperforms state-of-the-art baselines, showcasing its ability to facilitate effective collaboration among heterogeneous agents. IoA represents a step towards linking diverse agents in an Internet-like environment, where agents can seamlessly collaborate to achieve greater intelligence and capabilities. Our codebase has been released at \\url{https://github.com/OpenBMB/IoA}.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 19,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-09",
    "authors": [
      {
        "authorId": "2306423185",
        "name": "Weize Chen"
      },
      {
        "authorId": "2310435548",
        "name": "Ziming You"
      },
      {
        "authorId": "2310570226",
        "name": "Ran Li"
      },
      {
        "authorId": "2310653853",
        "name": "Yitong Guan"
      },
      {
        "authorId": "2214580084",
        "name": "Cheng Qian"
      },
      {
        "authorId": "2310816486",
        "name": "Chenyang Zhao"
      },
      {
        "authorId": "2257052321",
        "name": "Cheng Yang"
      },
      {
        "authorId": "2257007994",
        "name": "Ruobing Xie"
      },
      {
        "authorId": "2269703458",
        "name": "Zhiyuan Liu"
      },
      {
        "authorId": "2273551430",
        "name": "Maosong Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.93598410330986
  },
  {
    "paperId": "0e151acf98d2661dc70ae49e56cd217d0b15c17d",
    "url": "https://www.semanticscholar.org/paper/0e151acf98d2661dc70ae49e56cd217d0b15c17d",
    "title": "Region-Aware Face Swapping",
    "abstract": "This paper presents a novel Region-Aware Face Swapping (RAFSwap) network to achieve identity-consistent harmonious high-resolution face generation in a local-global manner: 1) Local Facial Region-Aware (FRA) branch augments local identity-relevant features by introducing the Transformer to effectively model misaligned crossscale semantic interaction. 2) Global Source Feature-Adaptive (SFA) branch further complements global identity-relevant cues for generating identity-consistent swapped faces. Besides, we propose a Face Mask Predictor (FMP) module incorporated with StyleGAN2 to predict identity-relevant soft facial masks in an unsupervised manner that is more practical for generating harmonious high-resolution faces. Abundant experiments qualitatively and quantitatively demonstrate the superiority of our method for generating more identity-consistent high-resolution swapped faces over SOTA methods, e.g., obtaining 96.70 ID retrieval that outperforms SOTA MegaFS by $5.87\\uparrow$.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2022,
    "citationCount": 45,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2203.04564",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-03-09",
    "authors": [
      {
        "authorId": "2155590464",
        "name": "Chao Xu"
      },
      {
        "authorId": "73329364",
        "name": "Jiangning Zhang"
      },
      {
        "authorId": "2052406572",
        "name": "Miao Hua"
      },
      {
        "authorId": "2152880412",
        "name": "Qian He"
      },
      {
        "authorId": "39737792",
        "name": "Zili Yi"
      },
      {
        "authorId": "2144385063",
        "name": "Yong Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 127.42962094733642
  },
  {
    "paperId": "06eb9a9a3c7455fe7631e20037e8667acb1967a1",
    "url": "https://www.semanticscholar.org/paper/06eb9a9a3c7455fe7631e20037e8667acb1967a1",
    "title": "LLM-Based Framework for Administrative Task Automation in Healthcare",
    "abstract": "Artificial Intelligence (AI) has been transformative in the healthcare sector, leading to enhanced precision in medical diagnosis, more effective treatment options, and a significant improvement in patient safety. However, computer-based administrative tasks, such as retrieval of medical and health records, patient registration, medical billing, filing and documentation, and appointment scheduling, still impose a heavy burden on healthcare professionals, causing a reduced quality of care and efficiency. In light of these challenges, this paper proposes a large language model (LLM)-based multi-agent framework designed to automate some of the administrative work in clinical settings. In our proposed solution, these LLM agents coordinate to parse instructions, breakdown tasks, and execute a sequence of actions in a workflow. They are equipped to not only execute documentation process at the database level but also operate directly on web-based electronic medical record (EMR) platforms. Moreover, the framework integrates data sources through a retrieval-augmented generation (RAG) system to allow streamlined interaction with patient information and medical records, mediated through an agent interface. The framework is designed with security in mind to defend against malicious prompts. We demonstrate the practicality of our solution by testing on various complex tasks that require the use of multiple tools and an EMR website. The result show the framework's effectiveness in handling diverse healthcare administrative tasks.",
    "venue": "International Symposium on Digital Forensics and Security",
    "year": 2024,
    "citationCount": 15,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-04-29",
    "authors": [
      {
        "authorId": "2194018681",
        "name": "Senay A. Gebreab"
      },
      {
        "authorId": "2255837230",
        "name": "Khaled Salah"
      },
      {
        "authorId": "2112132253",
        "name": "Raja Jayaraman"
      },
      {
        "authorId": "121706668",
        "name": "Muhammad Habib ur Rehman"
      },
      {
        "authorId": "84089407",
        "name": "Samer Ellaham"
      }
    ],
    "source": "semantic_scholar",
    "score": 111.58883083359672
  },
  {
    "paperId": "edea07a00665e3531c0e3a25b41bc4ee135661f0",
    "url": "https://www.semanticscholar.org/paper/edea07a00665e3531c0e3a25b41bc4ee135661f0",
    "title": "Blockchain Integration with Machine Learning for Securing Fog Computing Vulnerability in Smart City Sustainability",
    "abstract": "The advent of a smart city-based industrial Internet of Things (IIoT) is confidently built on the combined protocols of a virtual IPv6 addressing scheme and the fifth generation (5G) mobile network. For better network service and to achieve Quality of Experience (QoE) in the architecture. But this intelligent city architecture is vulnerable to several cyber-attack and malicious actors at the different layers which make it exposed to the same attacks as in the conventional IPv4 wireless sensor networks. However, this work aims to develop a blockchain-based machine learning (BML) security framework that secures the fog computing layer vulnerability in the smart city‚Äôs sustainability. The machine learning approach is firstly implemented between the edge layer and fog server nodes of the city architecture for the variants of intrusion detection using different ML algorithms for the attack‚Äôs discovery and classification. While the augmented blockchain technology is implemented between the fog layer and cloud computing to enhance the privacy and confidentiality of packet traffic broadcast to the public. The results obtained from ML-IDS show high-performance detection accuracy and low processing time. While the blockchain framework is also evaluated based on the certmcate generation, and retrieval size in bytes and time in milliseconds.",
    "venue": "International Conference on Artificial Intelligence and Soft Computing",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2023-01-23",
    "authors": [
      {
        "authorId": "72097354",
        "name": "Lukman Adewale Ajao"
      },
      {
        "authorId": "30026332",
        "name": "S. T. Apeh"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "6d4a9f1c41b078846901362ba0dce8295dd6a2a8",
    "url": "https://www.semanticscholar.org/paper/6d4a9f1c41b078846901362ba0dce8295dd6a2a8",
    "title": "End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering",
    "abstract": "We present an end-to-end differentiable training method for retrieval-augmented open-domain question answering systems that combine information from multiple retrieved documents when generating answers. We model retrieval decisions as latent variables over sets of relevant documents. Since marginalizing over sets of retrieved documents is computationally hard, we approximate this using an expectation-maximization algorithm. We iteratively estimate the value of our latent variable (the set of relevant documents for a given question) and then use this estimate to update the retriever and reader parameters. We hypothesize that such end-to-end training allows training signals to flow to the reader and then to the retriever better than staged-wise training. This results in a retriever that is able to select more relevant documents for a question and a reader that is trained on more accurate documents to generate an answer. Experiments on three benchmark datasets demonstrate that our proposed method outperforms all existing approaches of comparable size by 2-3% absolute exact match points, achieving new state-of-the-art results. Our results also demonstrate the feasibility of learning to retrieve to improve answer generation without explicit supervision of retrieval decisions.",
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "citationCount": 154,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-06-09",
    "authors": [
      {
        "authorId": "39670454",
        "name": "Devendra Singh Sachan"
      },
      {
        "authorId": "145732771",
        "name": "Siva Reddy"
      },
      {
        "authorId": "2057555891",
        "name": "William Hamilton"
      },
      {
        "authorId": "2053229802",
        "name": "Chris Dyer"
      },
      {
        "authorId": "1755465",
        "name": "Dani Yogatama"
      }
    ],
    "source": "semantic_scholar",
    "score": 152.6513767537887
  },
  {
    "paperId": "896f1d9348336da581e3e89f1d8a4e4eaeb0b3f8",
    "url": "https://www.semanticscholar.org/paper/896f1d9348336da581e3e89f1d8a4e4eaeb0b3f8",
    "title": "Probabilistic Representations for Video Contrastive Learning",
    "abstract": "This paper presents Probabilistic Video Contrastive Learning, a self-supervised representation learning method that bridges contrastive learning with probabilistic representation. We hypothesize that the clips composing the video have different distributions in short-term duration, but can represent the complicated and sophisticated video distribution through combination in a common embedding space. Thus, the proposed method represents video clips as normal distributions and combines them into a Mixture of Gaussians to model the whole video distribution. By sampling embeddings from the whole video distribution, we can circumvent the careful sampling strategy or transformations to generate augmented views of the clips, unlike previous deterministic methods that have mainly focused on such sample generation strategies for contrastive learning. We further propose a stochastic contrastive loss to learn proper video distributions and handle the inherent uncertainty from the nature of the raw video. Experimental results verify that our probabilistic embedding stands as a state-of-the-art video representation learning for action recognition and video retrieval on the most popular benchmarks, including UCF101 and HMDB51.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2022,
    "citationCount": 33,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2204.03946",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-04-08",
    "authors": [
      {
        "authorId": "121638702",
        "name": "Jungin Park"
      },
      {
        "authorId": "1601207488",
        "name": "Jiyoung Lee"
      },
      {
        "authorId": "2161965217",
        "name": "Ig-Jae Kim"
      },
      {
        "authorId": "144442279",
        "name": "K. Sohn"
      }
    ],
    "source": "semantic_scholar",
    "score": 122.89540786924243
  },
  {
    "paperId": "1c811f618f40349e6e7130083ddbacaf78858737",
    "url": "https://www.semanticscholar.org/paper/1c811f618f40349e6e7130083ddbacaf78858737",
    "title": "Chunk-based Nearest Neighbor Machine Translation",
    "abstract": "Semi-parametric models, which augment generation with retrieval, have led to impressive results in language modeling and machine translation, due to their ability to retrieve fine-grained information from a datastore of examples. One of the most prominent approaches, kNN-MT, exhibits strong domain adaptation capabilities by retrieving tokens from domain-specific datastores (Khandelwal et al., 2021). However, kNN-MT requires an expensive retrieval operation for every single generated token, leading to a very low decoding speed (around 8 times slower than a parametric model). In this paper, we introduce a chunk-based kNN-MT model which retrieves chunks of tokens from the datastore, instead of a single token. We propose several strategies for incorporating the retrieved chunks into the generation process, and for selecting the steps at which the model needs to search for neighbors in the datastore. Experiments on machine translation in two settings, static and ‚Äúon-the-fly‚Äù domain adaptation, show that the chunk-based kNN-MT model leads to significant speed-ups (up to 4 times) with only a small drop in translation quality.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "citationCount": 27,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2205.12230",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-05-24",
    "authors": [
      {
        "authorId": "144869806",
        "name": "Pedro Henrique Martins"
      },
      {
        "authorId": "2566656",
        "name": "Zita Marinho"
      },
      {
        "authorId": "2069905347",
        "name": "Andr√© Martins"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.98306765262805
  },
  {
    "paperId": "19856469e29d13b63adde4d198d4785a65204997",
    "url": "https://www.semanticscholar.org/paper/19856469e29d13b63adde4d198d4785a65204997",
    "title": "cTBL: Augmenting Large Language Models for Conversational Tables",
    "abstract": "An open challenge in multimodal conversational AI requires augmenting large language models with information from textual and non-textual sources for multi-turn dialogue. To address this problem, this paper introduces Conversational Tables ( C TBL), a three-step encoder-decoder approach to retrieve tabular information and generate dialogue responses grounded on the retrieved information. C TBL uses Transformer encoder embeddings for Dense Table Retrieval and obtains up to 5% relative improvement in Top-1 and Top-3 accuracy over sparse retrieval on the H YRBI D IA - LOGUE dataset. Additionally, C TBL performs tabular knowledge retrieval using encoder and decoder models, resulting in up to 46% relative improvement in ROUGE scores and better human evaluation for response generation on H YRBI D IALOGUE .",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 1,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2303.12024",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2161133253",
        "name": "Anirudh S. Sundar"
      },
      {
        "authorId": "2257280755",
        "name": "Larry Heck"
      }
    ],
    "source": "semantic_scholar",
    "score": 80.39720770839918
  },
  {
    "paperId": "2e20395786ebaf45b3cee398b3db3531bc4851d6",
    "url": "https://www.semanticscholar.org/paper/2e20395786ebaf45b3cee398b3db3531bc4851d6",
    "title": "Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments",
    "abstract": "Autonomous agents powered by large language models (LLMs) have the potential to enhance human capabilities, assisting with digital tasks from sending emails to performing data analysis. The abilities of existing LLMs at such tasks are often hindered by the lack of high-quality agent data from the corresponding environments they interact with. We propose Learn-by-interact, a data-centric framework to adapt LLM agents to any given environments without human annotations. Learn-by-interact synthesizes trajectories of agent-environment interactions based on documentations, and constructs instructions by summarizing or abstracting the interaction histories, a process called backward construction. We assess the quality of our synthetic data by using them in both training-based scenarios and training-free in-context learning (ICL), where we craft innovative retrieval approaches optimized for agents. Extensive experiments on SWE-bench, WebArena, OSWorld and Spider2-V spanning across realistic coding, web, and desktop environments show the effectiveness of Learn-by-interact in various downstream agentic tasks -- baseline results are improved by up to 12.2\\% for ICL with Claude-3.5 and 19.5\\% for training with Codestral-22B. We further demonstrate the critical role of backward construction, which provides up to 14.0\\% improvement for training. Our ablation studies demonstrate the efficiency provided by our synthesized data in ICL and the superiority of our retrieval pipeline over alternative approaches like conventional retrieval-augmented generation (RAG). We expect that Learn-by-interact will serve as a foundation for agent data synthesis as LLMs are increasingly deployed at real-world environments.",
    "venue": "",
    "year": 2025,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2025-01-18",
    "authors": [
      {
        "authorId": "2152173042",
        "name": "Hongjin Su"
      },
      {
        "authorId": "2313049168",
        "name": "Ruoxi Sun"
      },
      {
        "authorId": "2256335437",
        "name": "Jinsung Yoon"
      },
      {
        "authorId": "2265492003",
        "name": "Pengcheng Yin"
      },
      {
        "authorId": null,
        "name": "Tao Yu"
      },
      {
        "authorId": "2676352",
        "name": "Sercan √ñ. Arik"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "c7ea05d7c20fd8aaaca6b5756e1cb6cfa3164411",
    "url": "https://www.semanticscholar.org/paper/c7ea05d7c20fd8aaaca6b5756e1cb6cfa3164411",
    "title": "Enhancing Patent Search with Content-Based Image Retrieval",
    "abstract": null,
    "venue": "Professional Search in the Modern World",
    "year": 2014,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "3019137",
        "name": "S. Vrochidis"
      },
      {
        "authorId": "2559834",
        "name": "A. Moumtzidou"
      },
      {
        "authorId": "1715604",
        "name": "Y. Kompatsiaris"
      }
    ],
    "source": "semantic_scholar",
    "score": 66.87639203842082
  },
  {
    "paperId": "366441034ec03b2fd72e29c246c49389a50b8ad8",
    "url": "https://www.semanticscholar.org/paper/366441034ec03b2fd72e29c246c49389a50b8ad8",
    "title": "Online Adaptation of Language Models with a Memory of Amortized Contexts",
    "abstract": "Due to the rapid generation and dissemination of information, large language models (LLMs) quickly run out of date despite enormous development costs. To address the crucial need to keep models updated, online learning has emerged as a critical tool when utilizing LLMs for real-world applications. However, given the ever-expanding corpus of unseen documents and the large parameter space of modern LLMs, efficient adaptation is essential. To address these challenges, we propose Memory of Amortized Contexts (MAC), an efficient and effective online adaptation framework for LLMs with strong knowledge retention. We propose a feature extraction and memory-augmentation approach to compress and extract information from new documents into compact modulations stored in a memory bank. When answering questions, our model attends to and extracts relevant knowledge from this memory bank. To learn informative modulations in an efficient manner, we utilize amortization-based meta-learning, which substitutes an otherwise required optimization process with a single forward pass of the encoder. Subsequently, we learn to choose from and aggregate selected documents into a single modulation by conditioning on the question, allowing us to adapt a frozen language model during test time without requiring further gradient updates. Our experiment demonstrates the superiority of MAC in multiple aspects, including online adaptation performance, time, and memory efficiency. In addition, we show how MAC can be combined with and improve the performance of popular alternatives such as retrieval augmented generations (RAGs). Code is available at: https://github.com/jihoontack/MAC.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-07",
    "authors": [
      {
        "authorId": "1750599181",
        "name": "Jihoon Tack"
      },
      {
        "authorId": "2116671354",
        "name": "Jaehyung Kim"
      },
      {
        "authorId": "2290187088",
        "name": "Eric Mitchell"
      },
      {
        "authorId": "2261688831",
        "name": "Jinwoo Shin"
      },
      {
        "authorId": "2290184641",
        "name": "Yee Whye Teh"
      },
      {
        "authorId": "2290185444",
        "name": "Jonathan Richard Schwarz"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "4773398fd2fdf1feddb194aa3e90da270fd3db11",
    "url": "https://www.semanticscholar.org/paper/4773398fd2fdf1feddb194aa3e90da270fd3db11",
    "title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications",
    "abstract": "A graph is a fundamental data model to represent various entities and their complex relationships in society and nature, such as social networks, transportation networks, financial networks, and biomedical systems. Recently, large language models (LLMs) have showcased a strong generalization ability to handle various NLP and multi-mode tasks to answer users' arbitrary questions and specific-domain content generation. Compared with graph learning models, LLMs enjoy superior advantages in addressing the challenges of generalizing graph tasks by eliminating the need for training graph learning models and reducing the cost of manual annotation. In this survey, we conduct a comprehensive investigation of existing LLM studies on graph data, which summarizes the relevant graph analytics tasks solved by advanced LLM models and points out the existing remaining challenges and future directions. Specifically, we study the key problems of LLM-based generative graph analytics (LLM-GGA) with three categories: LLM-based graph query processing (LLM-GQP), LLM-based graph inference and learning (LLM-GIL), and graph-LLM-based applications. LLM-GQP focuses on an integration of graph analytics techniques and LLM prompts, including graph understanding and knowledge graph (KG) based augmented retrieval, while LLM-GIL focuses on learning and reasoning over graphs, including graph learning, graph-formed reasoning and graph representation. We summarize the useful prompts incorporated into LLM to handle different graph downstream tasks. Moreover, we give a summary of LLM model evaluation, benchmark datasets/tasks, and a deep pro and cons analysis of LLM models. We also explore open problems and future directions in this exciting interdisciplinary research area of LLMs and graph analytics.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-04-23",
    "authors": [
      {
        "authorId": "2297848659",
        "name": "Wenbo Shang"
      },
      {
        "authorId": "2298024639",
        "name": "Xin Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "881af971d00621709b4c772750cd3ea9d0fb11fd",
    "url": "https://www.semanticscholar.org/paper/881af971d00621709b4c772750cd3ea9d0fb11fd",
    "title": "Estimating Knowledge in Large Language Models Without Generating a Single Token",
    "abstract": "To evaluate knowledge in large language models (LLMs), current methods query the model and then evaluate its generated responses. In this work, we ask whether evaluation can be done before the model has generated any text. Concretely, is it possible to estimate how knowledgeable a model is about a certain entity, only from its internal computation? We study this question with two tasks: given a subject entity, the goal is to predict (a) the ability of the model to answer common questions about the entity, and (b) the factuality of open-ended responses generated by the model about the entity. Experiments with a variety of LLMs show that KEEN, a simple probe trained over internal subject representations, succeeds at both tasks - correlating with both the QA accuracy of the model per-subject and FActScore, a recent factuality metric in open-ended generation. Moreover, KEEN naturally aligns with the model's hedging behavior and faithfully reflects changes in the model's knowledge after fine-tuning. Lastly, we show a more interpretable yet equally performant variant of KEEN, which highlights a small set of tokens indicative of clusters and gaps in the model's knowledge. Being simple and lightweight, KEEN can be leveraged to guide decisions such as when it is appropriate to apply further training or augment queries with retrieval.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-18",
    "authors": [
      {
        "authorId": "2307080108",
        "name": "Daniela Gottesman"
      },
      {
        "authorId": "22245981",
        "name": "Mor Geva"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "de02ba19fb957ae30de7f09904ae3d983c3b50e7",
    "url": "https://www.semanticscholar.org/paper/de02ba19fb957ae30de7f09904ae3d983c3b50e7",
    "title": "GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding",
    "abstract": "Integrating large language models with knowledge graphs derived from domain-specific data represents an important advancement towards more powerful and factual reasoning. As these models grow more capable, it is crucial to enable them to perform multi-step inferences over real-world knowledge graphs while minimizing hallucination. While large language models excel at conversation and text generation, their ability to reason over domain-specialized graphs of interconnected entities remains limited. For example, can we query a model to identify the optimal contact in a professional network for a specific goal, based on relationships and attributes in a private database? The answer is no ‚Äì such capabilities lie beyond current methods. However, this question underscores a critical technical gap that must be addressed. Many high-value applications in areas such as science, security, and e-commerce rely on proprietary knowledge graphs encoding unique structures, relationships, and logical constraints. We introduce a fine-tuning framework for developing Graph-aligned Language Models (GaLM) that transforms a knowledge graph into an alternate text representation with labeled question-answer pairs. We demonstrate that grounding the models in specific graph-based knowledge expands the models‚Äô capacity for structure-based reasoning. Our methodology leverages the large-language model's generative capabilities to create the dataset and proposes an efficient alternate to retrieval-augmented generation styled methods.",
    "venue": "AAAI Spring Symposia",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-02-09",
    "authors": [
      {
        "authorId": "2283844550",
        "name": "Stefan Dernbach"
      },
      {
        "authorId": "39073194",
        "name": "Khushbu Agarwal"
      },
      {
        "authorId": "2283844552",
        "name": "Alejandro Zuniga"
      },
      {
        "authorId": "2283848833",
        "name": "Michael Henry"
      },
      {
        "authorId": "7617146",
        "name": "Sutanay Choudhury"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "718f8b17fc0d67230aa16e92f428b7c19fd38992",
    "url": "https://www.semanticscholar.org/paper/718f8b17fc0d67230aa16e92f428b7c19fd38992",
    "title": "Uncovering Limitations of Large Language Models in Information Seeking from Tables",
    "abstract": "Tables are recognized for their high information density and widespread usage, serving as essential sources of information. Seeking information from tables (TIS) is a crucial capability for Large Language Models (LLMs), serving as the foundation of knowledge-based Q&A systems. However, this field presently suffers from an absence of thorough and reliable evaluation. This paper introduces a more reliable benchmark for Table Information Seeking (TabIS). To avoid the unreliable evaluation caused by text similarity-based metrics, TabIS adopts a single-choice question format (with two options per question) instead of a text generation format. We establish an effective pipeline for generating options, ensuring their difficulty and quality. Experiments conducted on 12 LLMs reveal that while the performance of GPT-4-turbo is marginally satisfactory, both other proprietary and open-source models perform inadequately. Further analysis shows that LLMs exhibit a poor understanding of table structures, and struggle to balance between TIS performance and robustness against pseudo-relevant tables (common in retrieval-augmented systems). These findings uncover the limitations and potential challenges of LLMs in seeking information from tables. We release our data and code to facilitate further research in this field.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-06",
    "authors": [
      {
        "authorId": "2257003180",
        "name": "Chaoxu Pang"
      },
      {
        "authorId": "10034341",
        "name": "Yixuan Cao"
      },
      {
        "authorId": "2305233603",
        "name": "Chunhao Yang"
      },
      {
        "authorId": "2256989703",
        "name": "Ping Luo"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "b12ae41e3761a5bf487e36d8d8ff47423659bd12",
    "url": "https://www.semanticscholar.org/paper/b12ae41e3761a5bf487e36d8d8ff47423659bd12",
    "title": "Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles",
    "abstract": "The generation of corner cases has become increasingly crucial for efficiently testing autonomous vehicles prior to road deployment. However, existing methods struggle to accommodate diverse testing requirements and often lack the ability to generalize to unseen situations, thereby reducing the convenience and usability of the generated scenarios. A method that facilitates easily controllable scenario generation for efficient autonomous vehicles (AV) testing with realistic and challenging situations is greatly needed. To address this, we proposed OmniTester: a multimodal Large Language Model (LLM) based framework that fully leverages the extensive world knowledge and reasoning capabilities of LLMs. OmniTester is designed to generate realistic and diverse scenarios within a simulation environment, offering a robust solution for testing and evaluating AVs. In addition to prompt engineering, we employ tools from Simulation of Urban Mobility to simplify the complexity of codes generated by LLMs. Furthermore, we incorporate Retrieval-Augmented Generation and a self-improvement mechanism to enhance the LLM's understanding of scenarios, thereby increasing its ability to produce more realistic scenes. In the experiments, we demonstrated the controllability and realism of our approaches in generating three types of challenging and complex scenarios. Additionally, we showcased its effectiveness in reconstructing new scenarios described in crash report, driven by the generalization capability of LLMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-10",
    "authors": [
      {
        "authorId": "2292804341",
        "name": "Qiujing Lu"
      },
      {
        "authorId": "2320473347",
        "name": "Xuanhan Wang"
      },
      {
        "authorId": "2321126971",
        "name": "Yiwei Jiang"
      },
      {
        "authorId": "2111399079",
        "name": "Guangming Zhao"
      },
      {
        "authorId": "2287935448",
        "name": "Mingyue Ma"
      },
      {
        "authorId": "2321669649",
        "name": "Shuo Feng"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "25738c43c0c4788d803981eaf5d397691aba0958",
    "url": "https://www.semanticscholar.org/paper/25738c43c0c4788d803981eaf5d397691aba0958",
    "title": "MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter",
    "abstract": "Language Models (LMs) have demonstrated impressive molecule understanding ability on various 1D text-related tasks. However, they inherently lack 2D graph perception - a critical ability of human professionals in comprehending molecules' topological structures. To bridge this gap, we propose MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter. MolCA enables an LM (e.g., Galactica) to understand both text- and graph-based molecular contents via the cross-modal projector. Specifically, the cross-modal projector is implemented as a Q-Former to connect a graph encoder's representation space and an LM's text space. Further, MolCA employs a uni-modal adapter (i.e., LoRA) for the LM's efficient adaptation to downstream tasks. Unlike previous studies that couple an LM with a graph encoder via cross-modal contrastive learning, MolCA retains the LM's ability of open-ended text generation and augments it with 2D graph information. To showcase its effectiveness, we extensively benchmark MolCA on tasks of molecule captioning, IUPAC name prediction, and molecule-text retrieval, on which MolCA significantly outperforms the baselines. Our codes and checkpoints can be found at https://github.com/acharkq/MolCA.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 61,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-10-19",
    "authors": [
      {
        "authorId": "1390625267",
        "name": "Zhiyuan Liu"
      },
      {
        "authorId": "2261249035",
        "name": "Sihang Li"
      },
      {
        "authorId": "2231664238",
        "name": "Yancheng Luo"
      },
      {
        "authorId": "46959445",
        "name": "Hao Fei"
      },
      {
        "authorId": "2258806194",
        "name": "Yixin Cao"
      },
      {
        "authorId": "2260546802",
        "name": "Kenji Kawaguchi"
      },
      {
        "authorId": "2260296919",
        "name": "Xiang Wang"
      },
      {
        "authorId": "2257036129",
        "name": "Tat-Seng Chua"
      }
    ],
    "source": "semantic_scholar",
    "score": 131.90701577567637
  },
  {
    "paperId": "be34462a39a10ad8e25b738975066616c3e88ddb",
    "url": "https://www.semanticscholar.org/paper/be34462a39a10ad8e25b738975066616c3e88ddb",
    "title": "ORCAS: 18 Million Clicked Query-Document Pairs for Analyzing Search",
    "abstract": "Users of Web search engines reveal their information needs through queries and clicks, making click logs a useful asset for information retrieval. However, click logs have not been publicly released for academic use, because they can be too revealing of personally or commercially sensitive information. This paper describes a click data release related to the TREC Deep Learning Track document corpus. After aggregation and filtering, including a k -anonymity requirement, we find 1.4 million of the TREC DL URLs have 18 million connections to 10 million distinct queries. Our dataset of these queries and connections to TREC documents is of similar size to proprietary datasets used in previous papers on query mining and ranking. We perform some preliminary experiments using the click data to augment the TREC DL training data, offering by comparison: 28x more queries, with 49x more connections to 4.4x more URLs in the corpus. We present a description of the dataset's generation process, characteristics, use in ranking and other potential uses.",
    "venue": "International Conference on Information and Knowledge Management",
    "year": 2020,
    "citationCount": 79,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2006.05324",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2020-06-09",
    "authors": [
      {
        "authorId": "2286321410",
        "name": "Nick Craswell"
      },
      {
        "authorId": "144081089",
        "name": "Daniel Fernando Campos"
      },
      {
        "authorId": "116506812",
        "name": "Bhaskar Mitra"
      },
      {
        "authorId": "49724730",
        "name": "Emine Yilmaz"
      },
      {
        "authorId": "1748336",
        "name": "B. Billerbeck"
      }
    ],
    "source": "semantic_scholar",
    "score": 135.73039952010822
  },
  {
    "paperId": "6058ce3819d72c3e429bea58d78d80c719cb4bdb",
    "url": "https://www.semanticscholar.org/paper/6058ce3819d72c3e429bea58d78d80c719cb4bdb",
    "title": "Data Augmentation for Biomedical Factoid Question Answering",
    "abstract": "We study the effect of seven data augmentation (DA) methods in factoid question answering, focusing on the biomedical domain, where obtaining training instances is particularly difficult. We experiment with data from the BIOASQ challenge, which we augment with training instances obtained from an artificial biomedical machine reading comprehension dataset, or via back-translation, information retrieval, word substitution based on WORD2VEC embeddings, or masked language modeling, question generation, or extending the given passage with additional context. We show that DA can lead to very significant performance gains, even when using large pre-trained Transformers, contributing to a broader discussion of if/when DA benefits large pre-trained models. One of the simplest DA methods, WORD2VEC-based word substitution, performed best and is recommended. We release our artificial training instances and code.",
    "venue": "Workshop on Biomedical Natural Language Processing",
    "year": 2022,
    "citationCount": 11,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2204.04711",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-04-10",
    "authors": [
      {
        "authorId": "36753496",
        "name": "Dimitris Pappas"
      },
      {
        "authorId": "1950133",
        "name": "Prodromos Malakasiotis"
      },
      {
        "authorId": "1752430",
        "name": "Ion Androutsopoulos"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "42bab238258f8c862684945e30a37aca076decb4",
    "url": "https://www.semanticscholar.org/paper/42bab238258f8c862684945e30a37aca076decb4",
    "title": "Partial-match retrieval via the method of superimposed codes",
    "abstract": "This paper presents and analyzes an effective and practical method of accomplishing partial-match retrieval on a computer file containing a large number of information records. In partial-match retrieval a subset of the records in the file is selected and retrieved by specifying a query set consisting of a small number of key values; the records selected and retrieved are those having a match to all the key values in the query set. Partial-match retrieval can be a powerful capability when used in information retrieval systems, and a stored file augmented with such a capability is equivalent to an associative or content-addressable store. The method presented in this paper is based upon the use of superimposed codes, a technique used previously with some success in notched-edge card filing systems in which card selection was accomplished mechanically with long metal needles. A new algorithm is presented for generating superimposed codes without the use of a stored code dictionary. The new algorithm executes rapidly on a digital computer and employs a hash function and a pseudo-random number generator; it allows the generation of binary codes having any desired width and weight. It is pointed out that the use of variableweight binary codes gives an advantage on files in which the key values occur with substantially different frequencies. It is shown that organizing the bits of the superimposed code words properly in storage leads to the property that only a small fraction of these bits need be retrieved and processed on each query; this substantially reduces the time required to execute a query. Also, because of the simplicity of the bit operations required in the processing of superimposed code words, a very fast query processor could be designed and built with currently available digital hardware devices and techniques. With the query processor implemented in such digital hardware, partial-match query rates as high as 100/s or even higher appear to be feasible. In order to demonstrate experimentally the power of the method of superimposed codes, the method was implemented in software on a file consisting of the Suffolk County, NY, telephone-directory listings.",
    "venue": "Proceedings of the IEEE",
    "year": 1979,
    "citationCount": 202,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "1979-12-01",
    "authors": [
      {
        "authorId": "2286018080",
        "name": "Ievrwiav ICNlaWl"
      },
      {
        "authorId": "2286025294",
        "name": "Ivd IWlsLIp"
      }
    ],
    "source": "semantic_scholar",
    "score": 145.69808968562683
  },
  {
    "paperId": "d221972ac498182030dfd6bd904f4a810864b7ca",
    "url": "https://www.semanticscholar.org/paper/d221972ac498182030dfd6bd904f4a810864b7ca",
    "title": "Generating Relevant and Informative uestions for Open-domain Conversations ‚àó",
    "abstract": "Recent research has highlighted the importance of mixed-initiative interactions in conversational search. To enable mixed-initiative interactions, information retrieval systems should be able to ask diverse questions, such as information-seeking, clariication, and open-ended ones. Question generation (QG) of open-domain conversational systems aims at enhancing the interactiveness and persistence of human-machine interactions. The task is challenging because of the sparsity of QG-speciic data in conversations. Current work is limited to single-turn interaction scenarios. We propose a context-enhanced neural question generation (CNQG) model that leverages the conversational context to predict question content and pattern, then perform question decoding. A hierarchical encoder framework is employed to obtain the discourse-level context representation. Based on this, we propose Review and Transit mechanisms to respectively select contextual keywords and predict new topic words to further construct the question content. Conversational context and the predicted question content are used to produce the question pattern, which in turn guides the question decoding process implemented by a recurrent decoder with a ‚àó A preliminary version of this work appeared as a short paper in the proceedings of TheWebConf 2020 [31]. In this extension, we (1) extend the context-enhanced neural question generation model by employing a hierarchical conversational context encoder, proposing a new question content prediction method, optimizing the question pattern prediction method, augmenting the question decoder with a joint attention, designing a multi-task learning based on self-supervised annotations to fully utilize the limited QG-speciic training data, and exploring a decaying strategy to automatically set the weights of various loss functions; (2) conduct extensive experiments and provide more detailed discussions, including investigating model performance on question generation, question pattern prediction and question content prediction, adding human evaluation, providing detailed analysis of the multi-task learning based on self-supervised annotations, examining model performance under diferent context lengths, and providing case studies to illustrate both the positive and negative generated results; and (3) include more related work.",
    "venue": "",
    "year": 2022,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2653094",
        "name": "Yanxiang Ling"
      },
      {
        "authorId": "145030663",
        "name": "Fei Cai"
      },
      {
        "authorId": "2157175938",
        "name": "Jun Liu"
      },
      {
        "authorId": "2108476186",
        "name": "Honghui Chen"
      },
      {
        "authorId": "1696030",
        "name": "M. de Rijke"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "650ae86d4d78e697c812bb94fe2562b84cfe9a00",
    "url": "https://www.semanticscholar.org/paper/650ae86d4d78e697c812bb94fe2562b84cfe9a00",
    "title": "Embedding Expansion: Augmentation in Embedding Space for Deep Metric Learning",
    "abstract": "Learning the distance metric between pairs of samples has been studied for image retrieval and clustering. With the remarkable success of pair-based metric learning losses, recent works have proposed the use of generated synthetic points on metric learning losses for augmentation and generalization. However, these methods require additional generative networks along with the main network, which can lead to a larger model size, slower training speed, and harder optimization. Meanwhile, post-processing techniques, such as query expansion and database augmentation, have proposed the combination of feature points to obtain additional semantic information. In this paper, inspired by query expansion and database augmentation, we propose an augmentation method in an embedding space for pair-based metric learning losses, called embedding expansion. The proposed method generates synthetic points containing augmented information by a combination of feature points and performs hard negative pair mining to learn with the most informative feature representations. Because of its simplicity and flexibility, it can be used for existing metric learning losses without affecting model size, training speed, or optimization difficulty. Finally, the combination of embedding expansion and representative metric learning losses outperforms the state-of-the-art losses and previous sample generation methods in both image retrieval and clustering tasks. The implementation is publicly available.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2020,
    "citationCount": 53,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2003.02546",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2020-03-05",
    "authors": [
      {
        "authorId": "9726578",
        "name": "ByungSoo Ko"
      },
      {
        "authorId": "2707832",
        "name": "Geonmo Gu"
      }
    ],
    "source": "semantic_scholar",
    "score": 129.83476069846412
  },
  {
    "paperId": "4ca1b798869362068091913cc6f1ca91fe0b101b",
    "url": "https://www.semanticscholar.org/paper/4ca1b798869362068091913cc6f1ca91fe0b101b",
    "title": "TauREx 3: A Fast, Dynamic, and Extendable Framework for Retrievals",
    "abstract": "TauREx 3 is the next generation of the TauREx exoplanet atmospheric retrieval framework for Windows, Mac, and Linux. It is a complete rewrite with a full Python stack that makes it easy-to-use, high-performance, dynamic, and flexible. The new main TauREx program is built with modularity in mind, allowing the user to augment its functionalities with custom code and efficiently perform retrievals on custom parameters. We achieve this result by dynamic determination of fitting parameters, whereby TauREx 3 can detect new parameters for retrieval from user code through a simple interface. TauREx 3 can act as a library with a simple import taurex command, providing a rich set of classes and functions related to atmospheric modeling. A 10√ó speedup in forward model computations is achieved as compared to the previous version with a sixfold reduction in retrieval times while maintaining robust results. TauREx 3 is intended as a standalone, all-in-one package for retrievals while the TauREx 3 Python library can build or augment a user‚Äôs custom data pipeline easily.",
    "venue": "",
    "year": 2019,
    "citationCount": 85,
    "openAccessPdf": {
      "url": "https://iopscience.iop.org/article/10.3847/1538-4357/ac0252/pdf",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Physics",
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2019-12-16",
    "authors": [
      {
        "authorId": "1402008120",
        "name": "A. Al-Refaie"
      },
      {
        "authorId": "88741066",
        "name": "Q. Changeat"
      },
      {
        "authorId": "3863724",
        "name": "I. Waldmann"
      },
      {
        "authorId": "1882832",
        "name": "G. Tinetti"
      }
    ],
    "source": "semantic_scholar",
    "score": 131.81520944380262
  },
  {
    "paperId": "604f0778247080421d4369b66e45adda1fae6087",
    "url": "https://www.semanticscholar.org/paper/604f0778247080421d4369b66e45adda1fae6087",
    "title": "Watch Less and Uncover More: Could Navigation Tools Help Users Search and Explore Videos?",
    "abstract": "Prior research has shown how ‚Äòcontent preview tools‚Äô improve speed and accuracy of user relevance judgements across different information retrieval tasks. This paper describes a novel user interface tool, the Content Flow Bar, designed to allow users to quickly identify relevant fragments within informational videos to facilitate browsing, through a cognitively augmented form of navigation. It achieves this by providing semantic ‚Äúsnippets‚Äù that enable the user to rapidly scan through video content. The tool provides visually-appealing pop-ups that appear in a time series bar at the bottom of each video, allowing to see in advance and at a glance how topics evolve in the content. We conducted a user study to evaluate how the tool changes the users search experience in video retrieval, as well as how it supports exploration and information seeking. The user questionnaire revealed that participants found the Content Flow Bar helpful and enjoyable for finding relevant information in videos. The interaction logs of the user study, where participants interacted with the tool for completing two informational tasks, showed that it holds promise for enhancing discoverability of content both across and within videos. This discovered potential could leverage a new generation of navigation tools in search and information retrieval.",
    "venue": "Conference on Human Information Interaction and Retrieval",
    "year": 2022,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "https://discovery.ucl.ac.uk/10142920/1/2201.03408v1.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Review"
    ],
    "publicationDate": "2022-01-10",
    "authors": [
      {
        "authorId": "1393680545",
        "name": "M. P√©rez-Ortiz"
      },
      {
        "authorId": "2143115611",
        "name": "Sahan Bulathwela"
      },
      {
        "authorId": "34960866",
        "name": "C. Dormann"
      },
      {
        "authorId": "2149411170",
        "name": "Meghana Verma"
      },
      {
        "authorId": "3100966",
        "name": "S. Kreitmayer"
      },
      {
        "authorId": "144160028",
        "name": "R. Noss"
      },
      {
        "authorId": "1404459229",
        "name": "J. Shawe-Taylor"
      },
      {
        "authorId": "1685816",
        "name": "Y. Rogers"
      },
      {
        "authorId": "2084638888",
        "name": "Emine Yilmaz"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "9ea1bd012d502cf26f54f2c31c34879fe3d8b002",
    "url": "https://www.semanticscholar.org/paper/9ea1bd012d502cf26f54f2c31c34879fe3d8b002",
    "title": "Evaluation of cross-language information retrieval systems : Second Workshop of the Cross-Language Evaluation Forum, CLEF 2001, Darmstadt, Germany, September 3-4, 2001 : revised papers",
    "abstract": "System Evaluation Experiments at CLEF 2001.- CLEF 2001 - Overview of Results.- Mainly Cross-Language.- Report on CLEF-2001 Experiments: Effective Combined Query-Translation Approach.- Multilingual Information Retrieval Using English and Chinese Queries.- Exeter at CLEF 2001: Experiments with Machine Translation for Bilingual Retrieval.- TNO at CLEF-2001: Comparing Translation Resources.- ITC-irst at CLEF 2001: Monolingual and Bilingual Tracks.- Experiments with the Eurospider Retrieval System for CLEF 2001.- Using Co-occurrence, Augmented Restrictions, and C-E WordNet for Chinese-English Cross-Language Information Retrieval at CLEF 2001.- Utaclir @ CLEF 2001 - Effects of Compound Splitting and N-Gram Techniques.- Using Statistical Translation Models for Bilingual IR.- Cross-Lingual Pseudo-Relevance Feedback Using a Comparable Corpus.- Investigation on Disambiguation in CLIR: Aligned Corpus and Bi-directional Translation-Based Strategies.- Vector-Based Semantic Analysis Using Random Indexing for Cross-Lingual Query Expansion.- Query Expansion Techniques for the CLEF Bilingual Track.- Intelligent Information Access Systems (SINAI) at CLEF 2001: Calculating Translation Probabilities with SemCor.- JHU/APL Experiments at CLEF: Translation Resources and Score Normalization.- Dictionary-Based Thai CLIR: An Experimental Survey of Thai CLIR.- English-Dutch CLIR Using Query Translation Techniques.- Thomson Legal and Regulatory at CLEF 2001: Monolingual and Bilingual Experiments.- Working with Russian Queries for the GIRT, Bilingual, and Multilingual CLEF Tasks.- IR-n: A Passage Retrieval System at CLEF-2001.- Monolingual Experiments.- Spanish Monolingual Track: The Impact of Stemming on Retrieval.- Shallow Morphological Analysis in Monolingual Information Retrieval for Dutch, German, and Italian.- Stemming Evaluated in 6 Languages by Hummingbird SearchServer(TM) at CLEF 2001.- Minimalistic Test Runs of the Eidetica Indexer.- Across the Bridge: CLEF 2001 - Non-english Monolingual Retrieval. The French Task.- Mpro-IR in CLEF 2001.- Some Terms Are More Interchangeable than Others.- Interactive Track.- The CLEF 2001 Interactive Track.- Noun Phrase Translations for Cross-Language Document Selection.- iCLEF at Sheffield.- iCLEF 2001 at Maryland: Comparing Term-for-Term Gloss and MT.- Evaluation Issues and Results.- The Philosophy of Information Retrieval Evaluation.- CLIR System Evaluation at the Second NTCIR Workshop.- Multilingual Topic Generation within the CLEF 2001 Experiments.- CLEF Methodology and Metrics.",
    "venue": "",
    "year": 2002,
    "citationCount": 25,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "69547719",
        "name": "Cross-Language Evaluation Forum"
      },
      {
        "authorId": "144423157",
        "name": "C. Peters"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.87144807032223
  },
  {
    "paperId": "a4f43cc96fbb3287dc055db75dd9f7737f077763",
    "url": "https://www.semanticscholar.org/paper/a4f43cc96fbb3287dc055db75dd9f7737f077763",
    "title": "Engineering Tissue Fabrication With Machine Intelligence: Generating a Blueprint for Regeneration",
    "abstract": "Regenerating lost or damaged tissue is the primary goal of Tissue Engineering. 3D bioprinting technologies have been widely applied in many research areas of tissue regeneration and disease modeling with unprecedented spatial resolution and tissue-like complexity. However, the extraction of tissue architecture and the generation of high-resolution blueprints are challenging tasks for tissue regeneration. Traditionally, such spatial information is obtained from a collection of microscopic images and then combined together to visualize regions of interest. To fabricate such engineered tissues, rendered microscopic images are transformed to code to inform a 3D bioprinting process. If this process is augmented with data-driven approaches and streamlined with machine intelligence, identification of an optimal blueprint can become an achievable task for functional tissue regeneration. In this review, our perspective is guided by an emerging paradigm to generate a blueprint for regeneration with machine intelligence. First, we reviewed recent articles with respect to our perspective for machine intelligence-driven information retrieval and fabrication. After briefly introducing recent trends in information retrieval methods from publicly available data, our discussion is focused on recent works that use machine intelligence to discover tissue architectures from imaging and spectral data. Then, our focus is on utilizing optimization approaches to increase print fidelity and enhance biomimicry with machine learning (ML) strategies to acquire a blueprint ready for 3D bioprinting.",
    "venue": "Frontiers in Bioengineering and Biotechnology",
    "year": 2020,
    "citationCount": 25,
    "openAccessPdf": {
      "url": "https://www.frontiersin.org/articles/10.3389/fbioe.2019.00443/pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "Review",
      "JournalArticle"
    ],
    "publicationDate": "2020-01-10",
    "authors": [
      {
        "authorId": "2145454983",
        "name": "Joohyun Kim"
      },
      {
        "authorId": "1481706684",
        "name": "Jane A. McKee"
      },
      {
        "authorId": "2065083525",
        "name": "Jake J. Fontenot"
      },
      {
        "authorId": "5543315",
        "name": "Jangwook P. Jung"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.87144807032223
  },
  {
    "paperId": "f5fcf6adeb34e69ee709eedf8555a744ee73724b",
    "url": "https://www.semanticscholar.org/paper/f5fcf6adeb34e69ee709eedf8555a744ee73724b",
    "title": "An interactive video content-based retrieval system",
    "abstract": "The actual generation of video search engines offers low-level abstractions of the data while users seek for high-level semantics. The main challenge in video retrieval remains bridging the semantic gap. Thus, the effectiveness of video retrieval is based on the result of the interaction between query selection and a goal-oriented human user. The system exploits the human capability for rapidly scanning imagery augmenting it with an active learning loop, which tries to always present the most relevant material based on the current information. We describe in this paper, a machine learning system for interactive video retrieval. The core of this system is a kernel-based SVM classifier. The video retrieval uses the core as an active learning classifier. We perform an experiment against the 2005 NIST TRECVID benchmark in the high-level task.",
    "venue": "International Conference on Systems, Signals, and Image Processing",
    "year": 2008,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2008-06-25",
    "authors": [
      {
        "authorId": "1407376234",
        "name": "G. Camara-Chavez"
      },
      {
        "authorId": "1699175",
        "name": "F. Precioso"
      },
      {
        "authorId": "51021910",
        "name": "M. Cord"
      },
      {
        "authorId": "1407376283",
        "name": "S. Phillip-Foliguet"
      },
      {
        "authorId": "2149574979",
        "name": "A. Ara√∫jo"
      }
    ],
    "source": "semantic_scholar",
    "score": 95.96842909197557
  },
  {
    "paperId": "906e5524057f9742628dd91d3d43fa1b1751e4b5",
    "url": "https://www.semanticscholar.org/paper/906e5524057f9742628dd91d3d43fa1b1751e4b5",
    "title": "Next generation content networks: trends and challenges",
    "abstract": "Content Delivery Networks (CDNs) [1] have emerged to overcome the inherent limitations of the Internet in terms of user perceived Quality of Service (QoS) when accessing Web content. They offer infrastructure and mechanisms to deliver content and services in a scalable manner, and enhance users' Web experience. However, modern applications do not just perform retrieval or access operations on content but also create content, modify and manage content, and actively place content at appropriate locations. In order to deal with such new requirements, along with the proliferation, formation, and consolidation of the CDN landscape, new forms of Content Networks (CNs) are coming into picture. Thus, distribution and management of content is introducing new challenges in this domain through raising new issues in the architecture, design and implementation of CNs. Moreover, the evolution of next-generation CNs in a large-scale heterogeneous environment demands for a paradigm shift within the research community in terms of the technologies used. Therefore, the integrated uses of existing emerging as well as stable technologies (e.g. agent, P2P, grid, data mining) are anticipated to augment the effectiveness and boost the efficiency of future CN infrastructures [2, 3, 4].\n The aim of this panel session is to discuss about new ideas and results in the content networking domain. It will capture the state-of-the-art in content networking domain in terms of organizational structure, content distribution mechanisms, request redirection techniques, and performance measurement methodologies. It will also identify potential research directions and technologies that will drive innovations within this domain. In particular, the talks will be focused on agents for content management and delivery, P2P-based CNs, mobile multimedia CDNs, integration between CDNs and cloud computing, and content delivery on wireless networks.",
    "venue": "Use of P2P, GRID and Agents for the Development of Content Networks",
    "year": 2009,
    "citationCount": 9,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2009-06-09",
    "authors": [
      {
        "authorId": "1691577",
        "name": "G. Fortino"
      },
      {
        "authorId": "1802472",
        "name": "C. Mastroianni"
      },
      {
        "authorId": "2524347",
        "name": "Mukaddim Pathan"
      },
      {
        "authorId": "1741423",
        "name": "A. Vakali"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.53877639491068
  },
  {
    "paperId": "1cfbfdb1e772f11ca93a7ee262fada0942cb5bca",
    "url": "https://www.semanticscholar.org/paper/1cfbfdb1e772f11ca93a7ee262fada0942cb5bca",
    "title": "Automated Generation of Graphic Sketches by Example",
    "abstract": "Hand-crafting effective visual presentations is time-consuming and requires design skills. Here we present a case-based graphic sketch generation algorithm, which uses a database of existing graphic examples (cases) to automatically create a sketch of a presentation for a new user request. As the first case-based learning approach to graphics generation, our work offers three unique contributions. First, we augment a similarity metric with a set of adequacy evaluation criteria to retrieve a case that is most similar to the request and is also usable in sketch synthesis. To facilitate the retrieval of case fragments, we develop a systematic approach to case/request decomposition when a usable case cannot be found. Second, we improve case retrieval speed by organizing cases into hierarchical clusters based on their similarity distances and by using dynamically selected cluster representatives. Third, we develop a general case composition method to synthesize a new sketch from multiple retrieved cases. Furthermore, we have implemented our casebased sketch generation algorithm in a user-system cooperative graphics design system called IMPROVISE, which helps users to generate creative and tailored presentations.",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2003,
    "citationCount": 19,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2003-08-09",
    "authors": [
      {
        "authorId": "1705742",
        "name": "Michelle X. Zhou"
      },
      {
        "authorId": "2157216188",
        "name": "Min Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.93598410330986
  },
  {
    "paperId": "0734e6d51c84da7ee1b54028910a046f520c0a23",
    "url": "https://www.semanticscholar.org/paper/0734e6d51c84da7ee1b54028910a046f520c0a23",
    "title": "EdgeXAR: A 6-DoF Camera Multi-target Interaction Framework for MAR with User-friendly Latency Compensation",
    "abstract": "The computational capabilities of recent mobile devices enable the processing of natural features for Augmented Reality (AR), but the scalability is still limited by the devices' computation power and available resources. In this paper, we propose EdgeXAR, a mobile AR framework that utilizes the advantages of edge computing through task offloading to support flexible camera-based AR interaction. We propose a hybrid tracking system for mobile devices that provides lightweight tracking with 6 Degrees of Freedom and hides the offloading latency from users' perception. A practical, reliable and unreliable communication mechanism is used to achieve fast response and consistency of crucial information. We also propose a multi-object image retrieval pipeline that executes fast and accurate image recognition tasks on the cloud and edge servers. Extensive experiments are carried out to evaluate the performance of EdgeXAR by building mobile AR apps upon it. Regarding the Quality of Experience (QoE), the mobile AR apps powered by EdgeXAR framework run on average at the speed of 30 frames per second with precise tracking of only 1-2 pixel errors and accurate image recognition of at least 97% accuracy. As compared to Vuforia, one of the leading commercial AR frameworks, EdgeXAR transmits 87% less data while providing a stable 30FPS performance and reducing the offloading latency by 50 to 70% depending on the transmission medium. Our work facilitates the large-scale deployment of AR as the next generation of ubiquitous interfaces.",
    "venue": "Proc. ACM Hum. Comput. Interact.",
    "year": 2021,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2111.05173",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-11-09",
    "authors": [
      {
        "authorId": "2108143998",
        "name": "Wenxiao Zhang"
      },
      {
        "authorId": "2145200456",
        "name": "Sikun Lin"
      },
      {
        "authorId": "3197928",
        "name": "Farshid Hassani Bijarbooneh"
      },
      {
        "authorId": "10391344",
        "name": "H. Cheng"
      },
      {
        "authorId": "2136102137",
        "name": "Tristan Braud"
      },
      {
        "authorId": "72050450",
        "name": "Pengyuan Zhou"
      },
      {
        "authorId": "41177600",
        "name": "Lik-Hang Lee"
      },
      {
        "authorId": "2106066733",
        "name": "Pan Hui"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.19162312519754
  },
  {
    "paperId": "bd127e5d75012fba198385c5239dbf57c3398244",
    "url": "https://www.semanticscholar.org/paper/bd127e5d75012fba198385c5239dbf57c3398244",
    "title": "Automatically Paraphrasing via Sentence Reconstruction and Round-trip Translation",
    "abstract": "Paraphrase generation plays key roles in NLP tasks such as question answering, machine translation, and information retrieval. In this paper, we propose a novel framework for paraphrase generation. It simultaneously decodes the output sentence using a pretrained wordset-to-sequence model and a round-trip translation model. We evaluate this framework on Quora, WikiAnswers, MSCOCO and Twitter, and show its advantage over previous state-of-the-art unsupervised methods and distantly-supervised methods by significant margins on all datasets. For Quora and WikiAnswers, our framework even performs better than some strongly supervised methods with domain adaptation. Further, we show that the generated paraphrases can be used to augment the training data for machine translation to achieve substantial improvements.",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2021,
    "citationCount": 7,
    "openAccessPdf": {
      "url": "https://www.ijcai.org/proceedings/2021/0525.pdf",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-08-01",
    "authors": [
      {
        "authorId": "2149261950",
        "name": "Zilu Guo"
      },
      {
        "authorId": "2109670639",
        "name": "Zhongqiang Huang"
      },
      {
        "authorId": "1796651",
        "name": "Kenny Q. Zhu"
      },
      {
        "authorId": "23121526",
        "name": "Guandan Chen"
      },
      {
        "authorId": "2153280666",
        "name": "Kaibo Zhang"
      },
      {
        "authorId": "2152687324",
        "name": "Boxing Chen"
      },
      {
        "authorId": "2117426607",
        "name": "Fei Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 101.19162312519754
  },
  {
    "paperId": "0768cacd594fe087a6187c5464770c3af6b66ee7",
    "url": "https://www.semanticscholar.org/paper/0768cacd594fe087a6187c5464770c3af6b66ee7",
    "title": "Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering",
    "abstract": "In this paper, we illustrate how to fine-tune the entire Retrieval Augment Generation (RAG) architecture in an end-to-end manner. We highlighted the main engineering challenges that needed to be addressed to achieve this objective. We also compare how end-to-end RAG architecture outperforms the original RAG architecture for the task of question answering. We have open-sourced our implementation in the HuggingFace Transformers library.",
    "venue": "arXiv.org",
    "year": 2021,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-06-22",
    "authors": [
      {
        "authorId": "51516859",
        "name": "Shamane Siriwardhana"
      },
      {
        "authorId": "52001535",
        "name": "Rivindu Weerasekera"
      },
      {
        "authorId": "2114425044",
        "name": "Elliott Wen"
      },
      {
        "authorId": "1486464114",
        "name": "Suranga Nanayakkara"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.19162312519754
  },
  {
    "paperId": "3265935e23ff0c7a670ab3169313b042c07cb756",
    "url": "https://www.semanticscholar.org/paper/3265935e23ff0c7a670ab3169313b042c07cb756",
    "title": "Ground Camera Image and Large-Scale 3-D Image-Based Point Cloud Registration Based on Learning Domain Invariant Feature Descriptors",
    "abstract": "Multisource data are captured from different sensors or generated with different generation mechanisms. Ground camera images (images taken from ground-based camera) and rendered images (synthesized by the position information from 3-D image-based point cloud) are different-source geospatial data, called cross-domain images. Particularly, in outdoor environments, the registration relationship between the above cross-domain images is available to establish the spatial relationship between 2-D and 3-D space, which is an indirect solution for virtual‚Äìreal registration of augmented reality (AR). However, the traditional handcrafted feature descriptors cannot match the above cross-domain images because of the low quality of rendered images and the domain gap between cross-domain images. In this article, inspired by the success achieved by deep learning in computer vision, we first propose an end-to-end network, DIFD-Net, to learn domain invariant feature descriptors (DIFDs) for cross-domain image patches. The DIFDs are used for cross-domain image patch retrieval to the registration of ground camera and rendered images. Second, we construct a domain-kept consistent loss function, which balances the feature descriptors for narrowing the gap in different domains, to optimize DIFD-Net. Specially, the negative samples are generated from positive during training, and the introduced constraint of intermediate feature maps increases extra supervision information to learn feature descriptors. Finally, experiments show the superiority of DIFDs for the retrieval of cross-domain image patches, which achieves state-of-the-art retrieval performance. Additionally, we use DIFDs to match ground camera images and rendered images, and verify the feasibility of the derived AR virtual‚Äìreal registration in open outdoor environments",
    "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
    "year": 2021,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "48152302",
        "name": "Weiquan Liu"
      },
      {
        "authorId": "1693384403",
        "name": "Baiqi Lai"
      },
      {
        "authorId": "115877724",
        "name": "Cheng Wang"
      },
      {
        "authorId": "47831618",
        "name": "Guorong Cai"
      },
      {
        "authorId": "2118000466",
        "name": "Yanfei Su"
      },
      {
        "authorId": "1381288626",
        "name": "Xuesheng Bian"
      },
      {
        "authorId": "2110462967",
        "name": "Yongchuan Li"
      },
      {
        "authorId": "2107926256",
        "name": "Shuting Chen"
      },
      {
        "authorId": "2109043577",
        "name": "Jonathan Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.19162312519754
  },
  {
    "paperId": "57348a5e75b89c1d3e8d5b85027872c35ebc6d36",
    "url": "https://www.semanticscholar.org/paper/57348a5e75b89c1d3e8d5b85027872c35ebc6d36",
    "title": "Relevance Transformer: Generating Concise Code Snippets with Relevance Feedback",
    "abstract": "Tools capable of automatic code generation have the potential to augment programmer's capabilities. While straightforward code retrieval is incorporated into many IDEs, an emerging area is explicit code generation. Code generation is currently approached as a Machine Translation task, with Recurrent Neural Network (RNN) based encoder-decoder architectures trained on code-description pairs. In this work we introduce and study modern Transformer architectures for this task. We further propose a new model called the Relevance Transformer that incorporates external knowledge using pseudo-relevance feedback. The Relevance Transformer biases the decoding process to be similar to existing retrieved code while enforcing diversity. We perform experiments on multiple standard benchmark datasets for code generation including Django, Hearthstone, and CoNaLa. The results show improvements over state-of-the-art methods based on BLEU evaluation. The Relevance Transformer model shows the potential of Transformer-based architectures for code generation and introduces a method of incorporating pseudo-relevance feedback during inference.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2020,
    "citationCount": 11,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2007.02609",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2020-04-23",
    "authors": [
      {
        "authorId": "1796270950",
        "name": "Carlos Gemmell"
      },
      {
        "authorId": "48890086",
        "name": "Federico Rossetto"
      },
      {
        "authorId": "145269114",
        "name": "Jeffrey Dalton"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.27359974682
  },
  {
    "paperId": "20b726c3c2544dad980647f6c5d67edf460b59d4",
    "url": "https://www.semanticscholar.org/paper/20b726c3c2544dad980647f6c5d67edf460b59d4",
    "title": "Automated Question-Answering for Interactive Decision Support in Operations & Maintenance of Wind Turbines",
    "abstract": "Intelligent question-answering (QA) systems have witnessed increased interest in recent years, particularly in their ability to facilitate information access, data interpretation or decision support. The wind energy sector is one of the most promising sources of renewable energy, yet turbines regularly suffer from failures and operational inconsistencies, leading to downtimes and significant maintenance costs. Addressing these issues requires rapid interpretation of complex and dynamic data patterns under time-critical conditions. In this article, we present a novel approach that leverages interactive, natural language-based decision support for operations & maintenance (O&M) of wind turbines. The proposed interactive QA system allows engineers to pose domain-specific questions in natural language, and provides answers (in natural language) based on the automated retrieval of information on turbine sub-components, their properties and interactions, from a bespoke domain-specific knowledge graph. As data for specific faults is often sparse, we propose the use of paraphrase generation as a way to augment the existing dataset. Our QA system leverages encoder-decoder models to generate Cypher queries to obtain domain-specific facts from the KG database in response to user-posed natural language questions. Experiments with an attention-based sequence-to-sequence (Seq2Seq) model and a transformer show that the transformer accurately predicts up to 89.75% of responses to input questions, outperforming the Seq2Seq model marginally by 0.76%, though being 9.46 times more computationally efficient. The proposed QA system can help support engineers and technicians during O&M to reduce turbine downtime and operational costs, thus improving the reliability of wind energy as a source of renewable energy.",
    "venue": "IEEE Access",
    "year": 2022,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/09852225.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "31650568",
        "name": "Joyjit Chatterjee"
      },
      {
        "authorId": "3198238",
        "name": "Nina Dethlefs"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.95836866004329
  },
  {
    "paperId": "e302914bf04e36207f38f632d8d57cf9d387a97e",
    "url": "https://www.semanticscholar.org/paper/e302914bf04e36207f38f632d8d57cf9d387a97e",
    "title": "CoSe-Co: Text Conditioned Generative CommonSense Contextualizer",
    "abstract": "Pre-trained Language Models (PTLMs) have been shown to perform well on natural language tasks. Many prior works have leveraged structured commonsense present in the form of entities linked through labeled relations in Knowledge Graphs (KGs) to assist PTLMs. Retrieval approaches use KG as a separate static module which limits coverage since KGs contain finite knowledge. Generative methods train PTLMs on KG triples to improve the scale at which knowledge can be obtained. However, training on symbolic KG entities limits their applicability in tasks involving natural language text where they ignore overall context. To mitigate this, we propose a CommonSense Contextualizer (CoSe-Co) conditioned on sentences as input to make it generically usable in tasks for generating knowledge relevant to the overall context of input text. To train CoSe-Co, we propose a novel dataset comprising of sentence and commonsense knowledge pairs. The knowledge inferred by CoSe-Co is diverse and contain novel entities not present in the underlying KG. We augment generated knowledge in Multi-Choice QA and Open-ended CommonSense Reasoning tasks leading to improvements over current best methods on CSQA, ARC, QASC and OBQA datasets. We also demonstrate its applicability in improving performance of a baseline model for paraphrase generation task.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2022,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2206.05706",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-06-12",
    "authors": [
      {
        "authorId": "80494139",
        "name": "Rachit Bansal"
      },
      {
        "authorId": "6657914",
        "name": "Milan Aggarwal"
      },
      {
        "authorId": "2085742",
        "name": "S. Bhatia"
      },
      {
        "authorId": "2084554148",
        "name": "J. Kaur"
      },
      {
        "authorId": "145846953",
        "name": "Balaji Krishnamurthy"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "38d2b0887f377b611a384bc43230ce27f2ca00ad",
    "url": "https://www.semanticscholar.org/paper/38d2b0887f377b611a384bc43230ce27f2ca00ad",
    "title": "Utilizing sub-topical structure of documents for information retrieval",
    "abstract": "Text segmentation in natural language processing typically refers to the process of decomposing a document into constituent subtopics. Our work centers on the application of text segmentation techniques within information retrieval (IR) tasks. For example, for scoring a document by combining the retrieval scores of its constituent segments, exploiting the proximity of query terms in documents for ad-hoc search, and for question answering (QA), where retrieved passages from multiple documents are aggregated and presented as a single document to a searcher. Feedback in ad-hoc IR task is shown to benefit from the use of extracted sentences instead of terms from the pseudo relevant documents for query expansion. Retrieval effectiveness for patent prior art search task is enhanced by applying text segmentation to the patent queries. Another aspect of our work involves augmenting text segmentation techniques to produce segments which are more readable with less unresolved anaphora. This is particularly useful for QA and snippet generation tasks where the objective is to aggregate relevant and novel information from multiple documents satisfying user information need on one hand, and ensuring that the automatically generated content presented to the user is easily readable without reference to the original source document.",
    "venue": "Ph.D. Workshop on Information and Knowledge Management",
    "year": 2011,
    "citationCount": 10,
    "openAccessPdf": {
      "url": "http://doras.dcu.ie/16518/1/Utilizing_sub-topical_structure_of_documents_for_Information_Retrieval.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2011-10-28",
    "authors": [
      {
        "authorId": "1698027",
        "name": "Debasis Ganguly"
      },
      {
        "authorId": "1725514",
        "name": "Johannes Leveling"
      },
      {
        "authorId": "143723939",
        "name": "G. Jones"
      }
    ],
    "source": "semantic_scholar",
    "score": 95.96842909197557
  },
  {
    "paperId": "05628bce9204c95b96a5411ff703a4a125014858",
    "url": "https://www.semanticscholar.org/paper/05628bce9204c95b96a5411ff703a4a125014858",
    "title": "Bounds on Topological Descriptors of the Corona Product of  $F$ -Sum of Connected Graphs",
    "abstract": "The present-day trend of the numerical coding of chemical structures with topological indices (TIs) has established quite successful in medicinal chemistry and bioinformatics. This strategy provides the annotation, comparison, rapid collection, mining, and retrieval of chemical structures within large databases. Afterward, TIs can be used to look for quantitative structure-activity relationships and quantitative structure-property relationships, which are models that associate chemical structure with biological activity. In these analyses, degree-based TIs have secured a significant place among the different types of descriptors because of the ease of generation and the momentum with which these computations can be executed. In this paper, we compute the lower and upper bounds of the first, second, and third Zagreb, the first and the second multiple Zagreb, the geometric-arithmetic, the general sum connectivity, the general Randi<inline-formula> <tex-math notation=\"LaTeX\">$\\acute {c}$ </tex-math></inline-formula>, the atom-bond connectivity, the augmented Zagreb, and the harmonic indices of the corona product of <inline-formula> <tex-math notation=\"LaTeX\">$F$ </tex-math></inline-formula>-sum of connected graphs in the form of their factor graphs by applying combinatorial inequalities.",
    "venue": "IEEE Access",
    "year": 2019,
    "citationCount": 18,
    "openAccessPdf": {
      "url": "https://ieeexplore.ieee.org/ielx7/6287639/8600701/08643762.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Mathematics",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2153710784",
        "name": "Wei Gao"
      },
      {
        "authorId": "152503581",
        "name": "Zahid Iqbal"
      },
      {
        "authorId": "2053053202",
        "name": "Muhammad Ishaq"
      },
      {
        "authorId": "49885781",
        "name": "A. Aslam"
      },
      {
        "authorId": "2099928919",
        "name": "M. Aamir"
      },
      {
        "authorId": "4835579",
        "name": "M. Binyamin"
      }
    ],
    "source": "semantic_scholar",
    "score": 115.1665846874966
  },
  {
    "paperId": "d89991e0487c3315c2ac565e4f54524a6c2ad498",
    "url": "https://www.semanticscholar.org/paper/d89991e0487c3315c2ac565e4f54524a6c2ad498",
    "title": "Place Recognition in Semi-Dense Maps",
    "abstract": "For robotics and augmented reality systems operating in large and dynamic environments, \nplace recognition and tracking using vision represent very challenging tasks. Additionally, \nwhen these systems need to reliably operate for very long time periods, such \nas months or years, further challenges are introduced by severe environmental changes, \nthat can significantly alter the visual appearance of a scene. Thus, to unlock long term, \nlarge scale visual place recognition, it is necessary to develop new methodologies for \nimproving localization under difficult conditions. As shown in previous work, gains in \nrobustness can be achieved by exploiting the 3D structural information of a scene. The \nlatter, extracted from image sequences, carries in fact more discriminative clues than \nindividual images only. In this paper, we propose to represent a scene‚Äôs structure with \nsemi-dense point clouds, due to their highly informative power, and the simplicity of their \ngeneration through mature visual odometry and SLAM systems. Then we cast place \nrecognition as an instance of pose retrieval and evaluate several techniques, including \nrecent learning based approaches, to produce discriminative descriptors of semi-dense \npoint clouds. Our proposed methodology, evaluated on the recently published and challenging \nOxford Robotcar Dataset, shows to outperform image-based place recognition, \nwith improvements up to 30% in precision across strong appearance changes. To the best \nof our knowledge, we are the first to propose place recognition in semi-dense maps.",
    "venue": "British Machine Vision Conference",
    "year": 2017,
    "citationCount": 16,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Geology",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2017-09-07",
    "authors": [
      {
        "authorId": "51301761",
        "name": "Yawei Ye"
      },
      {
        "authorId": "2017998",
        "name": "Titus Cieslewski"
      },
      {
        "authorId": "20580939",
        "name": "Antonio Loquercio"
      },
      {
        "authorId": "2075371",
        "name": "D. Scaramuzza"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.49820016084324
  },
  {
    "paperId": "e42f9f9bf78236dcf36f73dfee1b59bc83739bdb",
    "url": "https://www.semanticscholar.org/paper/e42f9f9bf78236dcf36f73dfee1b59bc83739bdb",
    "title": "Overview of the Special Issue on Contextual Search and Recommendation",
    "abstract": "Information systems that leverage contextual knowledge about their users and their search situations ‚Äì such as histories, demographics, surroundings, constraints or devices ‚Äì can provide tailored search experiences and higher-quality task outcomes. Within information retrieval, there is a growing focus on how knowledge of user interests, intentions, and context can improve aspects of search and recommendation such as ranking and query suggestion, especially for exploratory and/or complex tasks that can span multiple queries or search sessions. The interactions that occur during these complex tasks provide context that can be leveraged by search systems to support users‚Äô broader information-seeking activities. Next-generation recommender systems face analogous challenges, including integrating signals from user exploration to update recommendations in real time. Within the space of search, much of the work on modeling context and search personalization has focused on constructing topical profiles of the user‚Äôs shortand long-term search history [Gauch et al. 2004; Chirita et al. 2005; Speretta and Gauch 2005; Ma et al. 2007; Bennett et al. 2010; White et al. 2010; Xiang et al. 2010; Sontag et al. 2012] or more generally, models of their query and result-click sequences [Cao et al. 2008; Cao et al. 2009; Mihalkova and Mooney 2009]. Related research has also considered a more content-driven representation such as language-model based approaches [Tan et al. 2006] or weighted term vectors derived from long-term desktop search activities [Teevan et al. 2005; Matthijs and Radlinski 2011]. However, a variety of recent investigations to contextualize search include a broader set of factors based on: a user‚Äôs location [Bennett et al. 2011], a user‚Äôs task-based search activity [Jones and Klinkner 2008; Kanoulas et al. 2011b; 2011a; Kanoulas et al. 2012; Sontag et al. 2012; Melucci 2012; Raman et al. 2014], the long-term vs. short-term interests of the user [Sugiyama et al. 2004; Li et al. 2007; Bennett et al. 2012], the ability of users to consume information at differing levels of complexity [Collins-Thompson et al. 2011], and patterns of re-finding the same search result over time [Teevan et al. 2011; Shokouhi et al. 2013]. The growth in the types of context explored and the information available to search systems derives from the timely convergence of several factors. The rapid growth in the use of different devices ‚Äì most notably smartphones and tablets, but also including stationary devices such as game consoles, smart televisions, and augmented conference rooms ‚Äì provides opportunities to obtain both raw and derived contextual signals that could power next-generation search and recommendation systems. The use of such signals in search and recommendation tasks has been recently explored in such venues as the Context-awareness in Retrieval and Recommendation workshops at IUI 20112012 [Luca et al. 2011; Luca et al. 2012], WSDM 2013 [Bohmer et al. 2013], and ECIR 2014 [Said et al. 2014]. Furthermore, a variety of recent work and venues have noted that much information retrieval research on web search has focused on optimizing and evaluating single queries, even though a significant fraction of queries are associated with more complex tasks [Jones and Klinkner 2008; Kanoulas et al. 2011b; 2011a; Belkin et al. 2012a;",
    "venue": "ACM Trans. Inf. Syst.",
    "year": 2015,
    "citationCount": 18,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/2691351",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2015-02-17",
    "authors": [
      {
        "authorId": "144609235",
        "name": "Paul N. Bennett"
      },
      {
        "authorId": "1403434962",
        "name": "Kevyn Collins-Thompson"
      },
      {
        "authorId": "144859929",
        "name": "D. Kelly"
      },
      {
        "authorId": "34286525",
        "name": "Ryen W. White"
      },
      {
        "authorId": "2153910463",
        "name": "Yi Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 115.1665846874966
  },
  {
    "paperId": "e213afc124dd27560adde446ec72c2e8d7b7c981",
    "url": "https://www.semanticscholar.org/paper/e213afc124dd27560adde446ec72c2e8d7b7c981",
    "title": "Managing Semantic Content for the Web",
    "abstract": "By associating meaning with content, the Semantic Web will facilitate search, interoperability, and the composition of complex applications. The paper discusses the Semantic Content Organization and Retrieval Engine (SCORE, see vvww.voquette.com), which is based on research transferred from the University of Georgia's Large Scale Distributed Information Systems. SCORE belongs to a new generation of technologies for the emerging Semantic Web. It provides facilities to define ontological components that software agents can maintain. These agents use regular expression based rules in conjunction with various semantic techniques to extract ontology-driven metadata from structured and semistructured content. Automatic classification and information-extraction techniques augment these results and also let the system deal with unstructured text.",
    "venue": "IEEE Internet Computing",
    "year": 2002,
    "citationCount": 213,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2002-07-01",
    "authors": [
      {
        "authorId": "144463965",
        "name": "A. Sheth"
      },
      {
        "authorId": "2404324",
        "name": "Clemens Bertram"
      },
      {
        "authorId": "3062778",
        "name": "David Avant"
      },
      {
        "authorId": "145487657",
        "name": "B. Hammond"
      },
      {
        "authorId": "1685452",
        "name": "K. Kochut"
      },
      {
        "authorId": "2729853",
        "name": "Yashodhan S. Warke"
      }
    ],
    "source": "semantic_scholar",
    "score": 146.48964022532778
  },
  {
    "paperId": "001bcd6b1be2ae71260d6e31f957ca5231cda8e3",
    "url": "https://www.semanticscholar.org/paper/001bcd6b1be2ae71260d6e31f957ca5231cda8e3",
    "title": "Information organization and retrieval with collaboratively generated content",
    "abstract": "Proliferation of ubiquitous access to the Internet enables millions of Web users to collaborate online on a variety of activities. Many of these activities result in the construction of large repositories of knowledge, either as their primary aim (e.g., Wikipedia) or as a by-product (e.g., Yahoo! Answers). In this tutorial, we will discuss organizing and exploiting Collaboratively Generated Content (CGC) for information organization and retrieval. Specifically, we intend to cover two complementary areas of the problem: (1) using such content as a powerful enabling resource for knowledge-enriched, intelligent representations and new information retrieval algorithms, and (2) development of supporting technologies for extracting, filtering, and organizing collaboratively created content. The unprecedented amounts of information in CGC enable new, knowledge-rich approaches to information access, which are significantly more powerful than the conventional word-based methods. Considerable progress has been made in this direction over the last few years. Examples include explicit manipulation of human-defined concepts and their use to augment the bag of words (cf. Explicit Semantic Analysis), using large-scale taxonomies of topics from Wikipedia or the Open Directory Project to construct additional class-based features, or using Wikipedia for better word sense disambiguation. However, the quality and comprehensiveness of collaboratively created content vary widely, and in order for this resource to be useful, a significant amount of preprocessing, filtering, and organization is necessary. Consequently, new methods for analyzing CGC and corresponding user interactions are required to effectively harness the resulting knowledge. Thus, not only the content repositories can be used to improve IR methods, but the reverse pollination is also possible, as better information extraction methods can be used for automatically collecting more knowledge, or verifying the contributed content. This natural connection between modeling the generation process of CGC and effectively using the accumulated knowledge suggests covering both areas together in a single tutorial. The intended audience of the tutorial includes IR researchers and graduate students, who would like to learn about the recent advances and research opportunities in working with collaboratively generated content. The emphasis of the tutorial is on comparing the existing approaches and presenting practical techniques that IR practitioners can use in their research. We also cover open research challenges, as well as survey available resources (software tools and data) for getting started in this research field.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2011,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference",
      "Review"
    ],
    "publicationDate": "2011-07-24",
    "authors": [
      {
        "authorId": "1685296",
        "name": "Eugene Agichtein"
      },
      {
        "authorId": "1718798",
        "name": "E. Gabrilovich"
      }
    ],
    "source": "semantic_scholar",
    "score": 82.47918433002164
  },
  {
    "paperId": "881bb4c2c5371216a8c9c2dbff8f7e7af46dcd1e",
    "url": "https://www.semanticscholar.org/paper/881bb4c2c5371216a8c9c2dbff8f7e7af46dcd1e",
    "title": "The Multimedia Satellite Task at MediaEval 2017",
    "abstract": "This paper provides a description of the MediaEval 2017 Multimedia Satellite Task. The primary goal of the task is to extract and fuse content of events which are present in Satellite Imagery and Social Media. Establishing a link from Satellite Imagery to Social Multimedia can yield to a comprehensive event representation which is vital for numerous applications. Focusing on natural disaster events in this year, the main objective of the task is to leverage the combined event representation withing the context of emergency response and environmental monitoring. In particular, our task focuses this year on flooding events and consists of two subtasks. The first Disaster Image Retrieval form Social Media subtask requires participants to retrieve images from Social Media which show a direct evidence of the flooding event. The second task Flood Detection in Satellite Images aims to extract regions in satellite images which are affected by a flooding event. Extracted content from both tasks can be fused by means of the geographic information. The task seeks to go beyond state-of-the-art flooding map generation towards recent approaches in Deep-Learning while augmenting the satellite information at the same time with rich social multimedia.",
    "venue": "MediaEval Benchmarking Initiative for Multimedia Evaluation",
    "year": 2017,
    "citationCount": 58,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "3457221",
        "name": "B. Bischke"
      },
      {
        "authorId": "26431319",
        "name": "P. Helber"
      },
      {
        "authorId": "2248802637",
        "name": "Christian Schulze"
      },
      {
        "authorId": "2248296571",
        "name": "Venkat Srinivasan"
      },
      {
        "authorId": "2248061971",
        "name": "A. Dengel"
      },
      {
        "authorId": "1772549",
        "name": "Damian Borth"
      }
    ],
    "source": "semantic_scholar",
    "score": 126.1630616585858
  },
  {
    "paperId": "e54f3dfe75426b2bea48657f88ffcf13c320ca20",
    "url": "https://www.semanticscholar.org/paper/e54f3dfe75426b2bea48657f88ffcf13c320ca20",
    "title": "Information retrieval challenges in computational advertising",
    "abstract": "Computational advertising is an emerging scientific sub-discipline, at the intersection of large scale search and text analysis, information retrieval, statistical modeling, machine learning, classification, optimization, and microeconomics. The central challenge of computational advertising is to find the \"best match\" between a given user in a given context and a suitable advertisement. The aim of this tutorial is to present the state of the art in Computational Advertising, in particular in its IR-related aspects, and to expose the participants to the current research challenges in this field. The tutorial does not assume any prior knowledge of Web advertising, and will begin with a comprehensive background survey. Going deeper, our focus will be on using a textual representation of the user context to retrieve relevant ads. At first approximation, this process can be reduced to a conventional setup by constructing a query that describes the user context and executing the query against a large inverted index of ads. We show how to augment this approach using query expansion and text classification techniques tuned for the ad-retrieval problem. In particular, we show how to use the Web as a repository of query-specific knowledge and use the Web search results retrieved by the query as a form of a relevance feedback and query expansion. We also present solutions that go beyond the conventional bag of words indexing by constructing additional features using a large external taxonomy and a lexicon of named entities obtained by analyzing the entire Web as a corpus. The last part of the tutorial will be devoted to a potpourri of recent research results and open problems inspired by Computational Advertising challenges in text summarization, natural language generation, named entity recognition, computer-human interaction, and other SIGIR-relevant areas.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2010,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference",
      "Review"
    ],
    "publicationDate": "2010-07-19",
    "authors": [
      {
        "authorId": "1774745",
        "name": "A. Broder"
      },
      {
        "authorId": "1718798",
        "name": "E. Gabrilovich"
      },
      {
        "authorId": "1679460",
        "name": "V. Josifovski"
      }
    ],
    "source": "semantic_scholar",
    "score": 82.47918433002164
  },
  {
    "paperId": "7038704829d5ac4c9f0aaf11ad7bc909d2ee8ad2",
    "url": "https://www.semanticscholar.org/paper/7038704829d5ac4c9f0aaf11ad7bc909d2ee8ad2",
    "title": "Comparing noun phrasing techniques for use with medical digital library tools",
    "abstract": "In an effort to assist medical researchers and professionals in accessing information necessary for their work, the A1 Lab at the University of Arizona is investigating the use of a natural language processing (NLP) technique called noun phrasing. The goal of this research is to determine whether noun phrasing could be a viable technique to include in medical information retrieval applications. Four noun phrase generation tools were evaluated as to their ability to isolate noun phrases from medical journal abstracts. Tests were conducted using the National Cancer Institute's CANCERLIT database. The NLP tools evaluated were Massachusetts Institute of Technology's (MIT's) Chopper, The University of Arizona's Automatic Indexer, Lingsoft's NPtool, and The University of Arizona's AZ Noun Phraser. In addition, the National Library of Medicine's SPECIALIST Lexicon was incorporated into two versions of the AZ Noun Phraser to be evaluated against the other tools as well as a nonaugmented version of the AZ Noun Phraser. Using the metrics relative subject recall and precision, our results show that, with the exception of Chopper, the phrasing tools were fairly comparable in recall and precision. It was also shown that augmenting the AZ Noun Phraser by including the SPECIALIST Lexicon from the National Library of Medicine resulted in improved recall and precision.",
    "venue": "Journal of the American Society for Information Science",
    "year": 2000,
    "citationCount": 170,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2000-03-01",
    "authors": [
      {
        "authorId": "2375744",
        "name": "K. Tolle"
      },
      {
        "authorId": "47666658",
        "name": "Hsinchun Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 147.1249533475399
  },
  {
    "paperId": "b39c4706503b4f6270abc85410efe3a0e7c26282",
    "url": "https://www.semanticscholar.org/paper/b39c4706503b4f6270abc85410efe3a0e7c26282",
    "title": "Augmenting Images for ASR and TTS Through Single-Loop and Dual-Loop Multimodal Chain Framework",
    "abstract": "Previous research has proposed a machine speech chain to enable automatic speech recognition (ASR) and text-to-speech synthesis (TTS) to assist each other in semi-supervised learning and to avoid the need for a large amount of paired speech and text data. However, that framework still requires a large amount of unpaired (speech or text) data. A prototype multimodal machine chain was then explored to further reduce the need for a large amount of unpaired data, which could improve ASR or TTS even when no more speech or text data were available. Unfortunately, this framework relied on the image retrieval (IR) model, and thus it was limited to handling only those images that were already known during training. Furthermore, the performance of this framework was only investigated with single-speaker artificial speech data. In this study, we revamp the multimodal machine chain framework with image generation (IG) and investigate the possibility of augmenting image data for ASR and TTS using single-loop and dual-loop architectures on multispeaker natural speech data. Experimental results revealed that both single-loop and dual-loop multimodal chain frameworks enabled ASR and TTS to improve their performance using an image-only dataset.",
    "venue": "Interspeech",
    "year": 2020,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2011.02099",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2020-10-25",
    "authors": [
      {
        "authorId": "46183056",
        "name": "Johanes Effendi"
      },
      {
        "authorId": "2894428",
        "name": "Andros Tjandra"
      },
      {
        "authorId": "1783949",
        "name": "S. Sakti"
      },
      {
        "authorId": "145223960",
        "name": "Satoshi Nakamura"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "e8874f8baa43516519832d87fd243d5cc09b645d",
    "url": "https://www.semanticscholar.org/paper/e8874f8baa43516519832d87fd243d5cc09b645d",
    "title": "Evaluation of aggregated query plans using heuristic approach",
    "abstract": "In the present era, swarm optimization techniques are quite adaptable towards achieving the flexibilities in terms of expansion of queries achieving the global optimization capabilities. The expansion of queries in general targets to achieve the desired query terms among the available query sets with proper identification of candidate keys. Practically, it is difficult to judge the potentiality of the query terms as well as large queries through traditional computing mechanisms. In such case, it is desirable to employ the optimization techniques to resolve the problems associated with expansion of queries. Also it is essential to make experimentation on parameters associated with the particle swarm optimization techniques. This analysis should focus on query expansion and link to retrieval mechanisms. To enhance the mechanisms of retrieval of information within the stipulated time period, the linked queries can be augmented involving the computational steps along with query expansion techniques. It will be a support to increase the effectiveness of retrieval mechanism of queries and eradicate the anomalies implementing the normalization. The reason of choosing particle swarm optimization in this case is to maintain the members as well as the complete population linked with the retrieval mechanism of queries and to filter the operation obtaining the optimal or near optimal solution. Accordingly, it is essential to update the present generation of particles considering as candidate solutions focusing on velocity, position which may be initialized randomly. During this process there is a great significance of search engine which aims to process huge facts as well as data. With the consistent increase of information, the task of retrieval mechanisms is to aggregate the queries and estimate the performance by simulating the data. In order to achieve better result during retrieval process, particle swarm optimization technique can be adopted to optimize the data and obtain better query formulation. The major merit in this case is its global convergence as well as robustness. Considering the mechanisms associated with data virtualization, it is seen that the logical layer integrates all types of linked data and unifies the same towards real time applications without disturbing the physical storage allocations. Accordingly some related approaches can be adopted towards real time applications. As such one of the most common approaches, heuristic approach has been proposed in this case to evaluate the performance of aggregated query plans.",
    "venue": "European Conference on Artificial Intelligence",
    "year": 2020,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2020-06-01",
    "authors": [
      {
        "authorId": "3467055",
        "name": "Z. P√≥lkowski"
      },
      {
        "authorId": "66053563",
        "name": "Jyotirmaya Mishra"
      },
      {
        "authorId": "1576667071",
        "name": "Suman Sourav Prasad"
      },
      {
        "authorId": "2157585",
        "name": "S. Mishra"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "67b955907a39e6405c2f5eae7c7b16baf2b1006c",
    "url": "https://www.semanticscholar.org/paper/67b955907a39e6405c2f5eae7c7b16baf2b1006c",
    "title": "A Novel Approach of Augmenting Training Data for Legal Text Segmentation by Leveraging Domain Knowledge",
    "abstract": null,
    "venue": "Intelligent Systems, Technologies and Applications",
    "year": 2019,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "32113796",
        "name": "R. Wagh"
      },
      {
        "authorId": "39770915",
        "name": "D. Anand"
      }
    ],
    "source": "semantic_scholar",
    "score": 74.1886522358297
  },
  {
    "paperId": "42545105672e5b68945bc7a08f3fdd84b60ad187",
    "url": "https://www.semanticscholar.org/paper/42545105672e5b68945bc7a08f3fdd84b60ad187",
    "title": "Information-Centric Networking Security",
    "abstract": "The selected articles cover topics including security mechanisms overview for named data networking (NDN), security for an edge named function environment, secure NDN with attribute- based cryptography and software-defined networking (SDN), content protection for NDN, and the design of a security monitoring plane in NDN. Over the last few decades, the sum of all forms of video data has grown explosively and is expected to reach 90 percent of all Internet traffic in the near future. Meanwhile, new applications, such as the Internet of Things (IoT), augmented reality, and automatic driving, are emerging as the major trends for the next evolution of the digital society, resulting in the generation and sharing of vast amounts of data. The existing Internet-based technologies, such as peer-to-peer networks, content distribution networks, and cloud computing, will be unable to handle such large amounts of data and provide the necessary quality of service (QoS) to the applications. In this context, information-centric networking (ICN), characterized by name-based data retrieval and in-network data caching, has emerged as a promising candidate for future networks. It provides access to named data as first-order network service, providing a built-in mechanism for data provenance and greater potential for optimizing forwarding behavior compared to the traditional host-centric communication systems ‚Äî the Internet today.",
    "venue": "IEEE Communications Magazine",
    "year": 2018,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "https://ieeexplore.ieee.org/ielx7/35/8539002/08539022.pdf",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2018-11-01",
    "authors": [
      {
        "authorId": "1799074",
        "name": "Xiaoming Fu"
      },
      {
        "authorId": "2054552",
        "name": "D. Kutscher"
      },
      {
        "authorId": "34279538",
        "name": "S. Misra"
      },
      {
        "authorId": "1731681",
        "name": "Ruidong Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 103.95836866004329
  },
  {
    "paperId": "83c151d8e6f5967ac031b021b30a1a10e890c2eb",
    "url": "https://www.semanticscholar.org/paper/83c151d8e6f5967ac031b021b30a1a10e890c2eb",
    "title": "A Large-Scale Comparative Evaluation of IR-Based Tools for Bug Localization",
    "abstract": "This paper reports on a large-scale comparative evaluation of IR-based tools for automatic bug localization. We have divided the tools in our evaluation into the following three generations: (1) The firstgeneration tools, now over a decade old, that are based purely on the Bag-of-Words (BoW) modeling of software libraries. (2) The somewhat more recent second-generation tools that augment BoW-based modeling with two additional pieces of information: historical data, such as change history, and structured information such as class names, method names, etc. And, finally, (3) The third-generation tools that are currently the focus of much research and that also exploit proximity, order, and semantic relationships between the terms. It is important to realize that the original authors of all these three generations of tools have mostly tested them on relatively small-sized datasets that typically consisted no more than a few thousand bug reports. Additionally, those evaluations only involved Java code libraries. The goal of the present paper is to present a comprehensive large-scale evaluation of all three generations of bug-localization tools with code libraries in multiple languages. Our study involves over 20,000 bug reports drawn from a diverse collection of Java, C/C++, and Python projects. Our results show that the third-generation tools are significantly superior to the older tools. We also show that the word embeddings generated using code files written in one language are effective for retrieval from code libraries in other languages.",
    "venue": "IEEE Working Conference on Mining Software Repositories",
    "year": 2020,
    "citationCount": 24,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2020-05-01",
    "authors": [
      {
        "authorId": "2732770",
        "name": "Shayan A. Akbar"
      },
      {
        "authorId": "1703247",
        "name": "A. Kak"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.28313737302301
  },
  {
    "paperId": "b4640647117539e45e4b4c46cb19023ccb882e49",
    "url": "https://www.semanticscholar.org/paper/b4640647117539e45e4b4c46cb19023ccb882e49",
    "title": "A Mobile LBS for Geo-Content Generation Facilitating Users to Share, Rate and Access Information in a Novel Manner",
    "abstract": null,
    "venue": "",
    "year": 2012,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Business"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1403911492",
        "name": "Helmut Schrom-Feiertag"
      },
      {
        "authorId": "1877017",
        "name": "P. Luley"
      },
      {
        "authorId": "2065653",
        "name": "L. Paletta"
      }
    ],
    "source": "semantic_scholar",
    "score": 60.79441541679836
  },
  {
    "paperId": "4425b7102d2d718c5619cb7f59d1851a50077caf",
    "url": "https://www.semanticscholar.org/paper/4425b7102d2d718c5619cb7f59d1851a50077caf",
    "title": "Aligning codebooks for near duplicate image detection",
    "abstract": null,
    "venue": "Multimedia tools and applications",
    "year": 2014,
    "citationCount": 17,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2014-09-01",
    "authors": [
      {
        "authorId": "1742452",
        "name": "S. Battiato"
      },
      {
        "authorId": "1729739",
        "name": "G. Farinella"
      },
      {
        "authorId": "1739427",
        "name": "G. Puglisi"
      },
      {
        "authorId": "1859140",
        "name": "D. Rav√¨"
      }
    ],
    "source": "semantic_scholar",
    "score": 83.35557636844247
  },
  {
    "paperId": "5b4506180a747095522b8fdbe5cc78577dfddc2c",
    "url": "https://www.semanticscholar.org/paper/5b4506180a747095522b8fdbe5cc78577dfddc2c",
    "title": "HCI Intelligent multimodal interaction environments",
    "abstract": "I: Multimodality and Conversational Dialogue.- Preferences and Patterns of Paralinguistic Voice Input to Interactive Media.- \"Show and Tell\": Using Semantically Processable Prosodic Markers for Spatial Expressions in an HCI System for Consumer Complaints.- Exploiting Speech-Gesture Correlation in Multimodal Interaction.- Pictogram Retrieval Based on Collective Semantics.- Enrich Web Applications with Voice Internet Persona Text-to-Speech for Anyone, Anywhere.- Using Recurrent Fuzzy Neural Networks for Predicting Word Boundaries in a Phoneme Sequence in Persian Language.- Subjective Measurement of Workload Related to a Multimodal Interaction Task: NASA-TLX vs. Workload Profile.- Menu Selection Using Auditory Interface.- Analysis of User Interaction with Service Oriented Chatbot Systems.- Performance Analysis of Perceptual Speech Quality and Modules Design for Management over IP Network.- A Tangible User Interface with Multimodal Feedback.- Minimal Parsing Key Concept Based Question Answering System.- Customized Message Generation and Speech Synthesis in Response to Characteristic Behavioral Patterns of Children.- Multi-word Expression Recognition Integrated with Two-Level Finite State Transducer.- Towards Multimodal User Interfaces Composition Based on UsiXML and MBD Principles.- m-LoCoS UI: A Universal Visible Language for Global Mobile Communication.- Developing a Conversational Agent Using Ontologies.- Conspeakuous: Contextualising Conversational Systems.- Persuasive Effects of Embodied Conversational Agent Teams.- Exploration of Possibility of Multithreaded Conversations Using a Voice Communication System.- A Toolkit for Multimodal Interface Design: An Empirical Investigation.- An Input-Parsing Algorithm Supporting Integration of Deictic Gesture in Natural Language Interface.- Multimodal Interfaces for In-Vehicle Applications.- Character Agents in E-Learning Interface Using Multimodal Real-Time Interaction.- An Empirical Study on Users' Acceptance of Speech Recognition Errors in Text-Messaging.- Flexible Multi-modal Interaction Technologies and User Interface Specially Designed for Chinese Car Infotainment System.- A Spoken Dialogue System Based on Keyword Spotting Technology.- II: Adaptive, Intelligent and Emotional User Interfaces.- Dynamic Association Rules Mining to Improve Intermediation Between User Multi-channel Interactions and Interactive e-Services.- Emotionally Expressive Avatars for Chatting, Learning and Therapeutic Intervention.- Can Virtual Humans Be More Engaging Than Real Ones?.- Automatic Mobile Content Conversion Using Semantic Image Analysis.- History Based User Interest Modeling in WWW Access.- Development of a Generic Design Framework for Intelligent Adaptive Systems.- Three Way Relationship of Human-Robot Interaction.- MEMORIA: Personal Memento Service Using Intelligent Gadgets.- A Location-Adaptive Human-Centered Audio Email Notification Service for Multi-user Environments.- Emotion-Based Textile Indexing Using Neural Networks.- Decision Theoretic Perspective on Optimizing Intelligent Help.- Human-Aided Cleaning Algorithm for Low-Cost Robot Architecture.- The Perception of Artificial Intelligence as \"Human\" by Computer Users.- Speaker Segmentation for Intelligent Responsive Space.- Emotion and Sense of Telepresence: The Effects of Screen Viewpoint, Self-transcendence Style, and NPC in a 3D Game Environment.- Emotional Interaction Through Physical Movement.- Towards Affective Sensing.- Affective User Modeling for Adaptive Intelligent User Interfaces.- A Multidimensional Classification Model for the Interaction in Reactive Media Rooms.- An Adaptive Web Browsing Method for Various Terminals: A Semantic Over-Viewing Method.- Evaluation of P2P Information Recommendation Based on Collaborative Filtering.- Understanding the Social Relationship Between Humans and Virtual Humans.- EREC-II in Use - Studies on Usability and Suitability of a Sensor System for Affect Detection and Human Performance Monitoring.- Development of an Adaptive Multi-agent Based Content Collection System for Digital Libraries.- Using Content-Based Multimedia Data Retrieval for Multimedia Content Adaptation.- Coping with Complexity Through Adaptive Interface Design.- Region-Based Model of Tour Planning Applied to Interactive Tour Generation.- A Learning Interface Agent for User Behavior Prediction.- Sharing Video Browsing Style by Associating Browsing Behavior with Low-Level Features of Videos.- Adaptation in Intelligent Tutoring Systems: Development of Tutoring and Domain Models.- Confidence Measure Based Incremental Adaptation for Online Language Identification.- Study on Speech Emotion Recognition System in E-Learning.- III: Gesture and Eye Gaze Recognition.- How Do Adults Solve Digital Tangram Problems? Analyzing Cognitive Strategies Through Eye Tracking Approach.- Gesture Interaction for Electronic Music Performance.- A New Method for Multi-finger Detection Using a Regular Diffuser.- Lip Contour Extraction Using Level Set Curve Evolution with Shape Constraint.- Visual Foraging of Highlighted Text: An Eye-Tracking Study.- Effects of a Dual-Task Tracking on Eye Fixation Related Potentials (EFRP).- Effect of Glance Duration on Perceived Complexity and Segmentation of User Interfaces.- Movement-Based Interaction and Event Management in Virtual Environments with Optical Tracking Systems.- Multiple People Gesture Recognition for Human-Robot Interaction.- Position and Pose Computation of a Moving Camera Using Geometric Edge Matching for Visual SLAM.- \"Shooting a Bird\": Game System Using Facial Feature for the Handicapped People.- Human Pose Estimation Using a Mixture of Gaussians Based Image Modeling.- Human Motion Modeling Using Multivision.- Real-Time Face Tracking System Using Adaptive Face Detector and Kalman Filter.- Kalman Filtering in the Design of Eye-Gaze-Guided Computer Interfaces.- Human Shape Tracking for Gait Recognition Using Active Contours with Mean Shift.- Robust Gaze Tracking Method for Stereoscopic Virtual Reality Systems.- EyeScreen: A Gesture Interface for Manipulating On-Screen Objects.- GART: The Gesture and Activity Recognition Toolkit.- Static and Dynamic Hand-Gesture Recognition for Augmented Reality Applications.- Multiple People Labeling and Tracking Using Stereo for Human Computer Interaction.- A Study of Human Vision Inspection for Mura.- Tracing Users' Behaviors in a Multimodal Instructional Material: An Eye-Tracking Study.- A Study on Interactive Artwork as an Aesthetic Object Using Computer Vision System.- Human-Computer Interaction System Based on Nose Tracking.- Evaluating Eye Tracking with ISO 9241 - Part 9.- Impact of Mental Rotation Strategy on Absolute Direction Judgments: Supplementing Conventional Measures with Eye Movement Data.- IV: Interactive TV and Media.- Beyond Mobile TV: Understanding How Mobile Interactive Systems Enable Users to Become Digital Producers.- Media Convergence, an Introduction.- An Improved H.264 Error Concealment Algorithm with User Feedback Design.- Classification of a Person Picture and Scenery Picture Using Structured Simplicity.- Designing Personalized Media Center with Focus on Ethical Issues of Privacy and Security.- Evaluation of VISTO: A New Vector Image Search TOol.- G-Tunes - Physical Interaction Design of Playing Music.- nan0sphere: Location-Driven Fiction for Groups of Users.- How Panoramic Photography Changed Multimedia Presentations in Tourism.- Frame Segmentation Used MLP-Based X-Y Recursive for Mobile Cartoon Content.- Browsing and Sorting Digital Pictures Using Automatic Image Classification and Quality Analysis.- A Usability Study on Personalized EPG (pEPG) UI of Digital TV.- Recognizing Cultural Diversity in Digital Television User Interface Design.- A Study on User Satisfaction Evaluation About the Recommendation Techniques of a Personalized EPG System on Digital TV.- Usability of Hybridmedia Services - PC and Mobile Applications Compared.- m-YouTube Mobile UI: Video Selection Based on Social Influence.- Can Video Support City-Based Communities?.- Watch, Press, and Catch - Impact of Divided Attention on Requirements of Audiovisual Quality.- Media Service Mediation Supporting Resident's Collaboration in ubiTV.- Implementation of a New H.264 Video Watermarking Algorithm with Usability Test.- Innovative TV: From an Old Standard to a New Concept of Interactive TV - An Italian Job.- Evaluating the Effectiveness of Digital Storytelling with Panoramic Images to Facilitate Experience Sharing.- User-Centered Design and Evaluation of a Concurrent Voice Communication and Media Sharing Application.- Customer-Dependent Storytelling Tool with Authoring and Viewing Functions.- Reliable Partner System Always Providing Users with Companionship Through Video Streaming.- Modeling of Places Based on Feature Distribution.- Knowledge Transfer in Semi-automatic Image Interpretation.",
    "venue": "",
    "year": 2007,
    "citationCount": 56,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2184437",
        "name": "J. Jacko"
      }
    ],
    "source": "semantic_scholar",
    "score": 120.64576901751826
  },
  {
    "paperId": "50fbcc1e72bcf321ec0ea76244828706876fc72d",
    "url": "https://www.semanticscholar.org/paper/50fbcc1e72bcf321ec0ea76244828706876fc72d",
    "title": "Similarity Search using Concept Graphs",
    "abstract": "The rapid proliferation of hand-held devices has led to the development of rich, interactive and immersive applications, such as e-readers for electronic books. These applications motivate retrieval systems that can implicitly satisfy any information need of the reader by exploiting the context of the user's interactions. Such retrieval systems differ from traditional search engines in that the queries constructed using the context are typically complex objects (including the document and its structure). In this paper, we develop an efficient retrieval system, only assuming an oracle access to a traditional search engine that admits 'succinct' keyword queries for retrieving objects of a desired media type. As part of query generation, we first map the complex query object to a concept graph and then use the concepts along with their relationships in the graph to compute a small set of keyword queries to the search engine. Next, as part of the result generation, we aggregate the results of these queries to identify relevant web content of the desired type, thereby eliminating the need for explicitly computing similarity between the query object and all web content. We present a theoretical analysis of our approach and carry out a detailed empirical evaluation to show the practicality of the approach for the task of augmenting electronic documents with high quality videos from the web.",
    "venue": "International Conference on Information and Knowledge Management",
    "year": 2014,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2014-11-03",
    "authors": [
      {
        "authorId": "144947410",
        "name": "R. Agrawal"
      },
      {
        "authorId": "144979147",
        "name": "Sreenivas Gollapudi"
      },
      {
        "authorId": "145721096",
        "name": "A. Kannan"
      },
      {
        "authorId": "1769861",
        "name": "K. Kenthapadi"
      }
    ],
    "source": "semantic_scholar",
    "score": 95.96842909197557
  },
  {
    "paperId": "02ca1da63e5b59e09457090ecfbcd33fb7b41c12",
    "url": "https://www.semanticscholar.org/paper/02ca1da63e5b59e09457090ecfbcd33fb7b41c12",
    "title": "Modeling and intercomparison of field and laboratory hyperspectral goniometer measurements with G-LiHT imagery of the Algodones Dunes",
    "abstract": "Abstract. We compare field hyperspectral bidirectional reflectance distribution function (BRDF) measurements acquired by a hyperspectral goniometer system known as the goniometer of the Rochester Institute of Technology (GRIT) during an experiment in the Algodones Dunes system in March 2015 with NASA Goddard‚Äôs light detection and ranging, hyperspectral, and thermal imagery of the site acquired during the experiment. We augment our field spectral data collection with laboratory hyperspectral BRDF measurements of samples brought back from the Algodones Dunes site using GRIT and our second-generation goniometer GRIT-two (GRIT-T). In these laboratory experiments, we vary geophysical parameters such as sediment density and grain size distribution of the sediments that would typically impact observed BRDF with the goal of extending the range of applicability of our resulting BRDF spectral libraries. Geotechnical measurements on site confirm the variability of geophysical parameters such as density and grain size distributions within the dune system, and measurements with GRIT and GRIT-T demonstrate the impact on observed spectral variation. By augmenting field spectral libraries with laboratory BRDF, we show that a greater proportion of the dune system is more faithfully represented in the expanded spectral library. Beyond developing appropriate calibration data for airborne and satellite imagery of the Algodones Dunes, laboratory and field studies also support goals to develop reliable retrieval methods for geophysical quantities such as sediment density directly from spectral imagery. We consider approaches based on the Hapke model. Our approaches use the invariance of the observed functional forms of the single scattering phase function, which must be invariant to differences in the illumination geometry. Fill factor is retrieved and correlates with expected direct measurements of sediment density in a laboratory setting.",
    "venue": "",
    "year": 2017,
    "citationCount": 26,
    "openAccessPdf": {
      "url": "https://www.spiedigitallibrary.org/journals/Journal-of-Applied-Remote-Sensing/volume-12/issue-1/012005/Modeling-and-intercomparison-of-field-and-laboratory-hyperspectral-goniometer-measurements/10.1117/1.JRS.12.012005.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Engineering",
      "Geology"
    ],
    "publicationTypes": null,
    "publicationDate": "2017-09-27",
    "authors": [
      {
        "authorId": "2247428",
        "name": "C. Bachmann"
      },
      {
        "authorId": "32760101",
        "name": "Rehman S. Eon"
      },
      {
        "authorId": "32782765",
        "name": "Brittany L. Ambeau"
      },
      {
        "authorId": "34433885",
        "name": "Justin Harms"
      },
      {
        "authorId": "13558238",
        "name": "Gregory P. Badura"
      },
      {
        "authorId": "3197289",
        "name": "C. Griffo"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.43755299006494
  },
  {
    "paperId": "7c693c193bc7f42a623c7fc191669faa73630721",
    "url": "https://www.semanticscholar.org/paper/7c693c193bc7f42a623c7fc191669faa73630721",
    "title": "A fiber-coupled laser hygrometer for airborne total water measurement",
    "abstract": "Abstract. The second-generation University of Colorado closed-path tunable-diode laser hygrometer (CLH-2) is an instrument for the airborne in situ measurement of total water content ‚Äì the sum of vapor-, liquid- and ice-phase water ‚Äì in clouds. This compact instrument has been flown on the NSF/NCAR Gulfstream-V aircraft in an underwing canister. It operates autonomously and uses fiber-coupled optics to eliminate the need for a supply of dry compressed gas. In operation, sample air is ingested into a forward-facing sub-isokinetic inlet; this sampling configuration results in particle concentrations that are enhanced relative to ambient and causes greater instrument sensitivity to condensed water particles. Heaters within the inlet vaporize the ingested water particles, and the resulting augmented water vapor mixing ratio is measured by absorption of near-infrared light in a single-pass optical cell. The condensed water content is then determined by subtracting the ambient water vapor content from the total and by accounting for the inertial enhancement of particles into the sampling inlet. The CLH-2 is calibrated in the laboratory over a range of pressures and water vapor mixing ratios; the uncertainty in CLH-2 condensed water retrievals is estimated to be 14.3% to 16.1% (1-œÉ). A vapor-only laboratory intercomparison with the first-generation University of Colorado closed-path tunable-diode laser hygrometer (CLH) shows agreement within the 2-œÉ uncertainty bounds of both instruments.",
    "venue": "",
    "year": 2013,
    "citationCount": 12,
    "openAccessPdf": {
      "url": "https://amt.copernicus.org/articles/7/215/2014/amt-7-215-2014.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Chemistry"
    ],
    "publicationTypes": null,
    "publicationDate": "2013-08-12",
    "authors": [
      {
        "authorId": "90692815",
        "name": "S. Dorsi"
      },
      {
        "authorId": "91461708",
        "name": "L. Kalnajs"
      },
      {
        "authorId": "2764927",
        "name": "D. Toohey"
      },
      {
        "authorId": "49136016",
        "name": "L. Avallone"
      }
    ],
    "source": "semantic_scholar",
    "score": 98.47424036192305
  },
  {
    "paperId": "0ccadc49fe9b3aebdbbe0d48a745ba031b9b5427",
    "url": "https://www.semanticscholar.org/paper/0ccadc49fe9b3aebdbbe0d48a745ba031b9b5427",
    "title": "Supporting sociable literacy in the international children's digital library",
    "abstract": "As each generation of children grows up in a world shaped by the affordances available to them in both physical and digital environments, their expectations of tools to support changing literacy practices make new demands on technologists and designers. To ensure that digital libraries (DLs) for young people support their understandings of libraries and reading (and not just adults' conceptions), an intergenerational design team (IDT) at the University of Baltimore (UB) used contextual inquiry and participatory design to develop concepts for augmenting the International Children's Digital Library (ICDL) to make it more appropriate for 10-14 year olds. Our prototype aims to support \"sociable literacy,\" a set of practices made possible by digital storage, retrieval and use of texts.",
    "venue": "International Conference on Interaction Design and Children",
    "year": 2004,
    "citationCount": 37,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Sociology",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2004-06-01",
    "authors": [
      {
        "authorId": "144464641",
        "name": "Nancy Kaplan"
      },
      {
        "authorId": "1736717",
        "name": "Yoram Chisik"
      },
      {
        "authorId": "48964112",
        "name": "K. Knudtzon"
      },
      {
        "authorId": "144686719",
        "name": "R. Kulkarni"
      },
      {
        "authorId": "1707888",
        "name": "Stuart Moulthrop"
      },
      {
        "authorId": "2056305331",
        "name": "K. Summers"
      },
      {
        "authorId": "35993320",
        "name": "H. Weeks"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.56379239589579
  },
  {
    "paperId": "5b14e84c8ddec9c7cf63dc648cec032f48f26556",
    "url": "https://www.semanticscholar.org/paper/5b14e84c8ddec9c7cf63dc648cec032f48f26556",
    "title": "Geographic Information System: Principles and Applications",
    "abstract": null,
    "venue": "",
    "year": 2018,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "39269408",
        "name": "G. Reddy"
      }
    ],
    "source": "semantic_scholar",
    "score": 77.95836866004329
  },
  {
    "paperId": "5ea785b16847690417ad879f7cb856ce1421f346",
    "url": "https://www.semanticscholar.org/paper/5ea785b16847690417ad879f7cb856ce1421f346",
    "title": "Watson: Anticipating and Contextualizing Information Needs",
    "abstract": "In this paper, we introduce a class of systems called Information Management Assistants (IMAs). IMAs automatically discover related material on behalf of the user by serving as an intermediary between the user and information retrieval systems. IMAs observe users interact with everyday applications and then anticipate their information needs using a model of the task at hand. IMAs then automatically fulfill these needs using the text of the document the user is manipulating and a knowledge of how to form queries to traditional information retrieval systems (e.g., Internet search engines, abstract databases, etc.). IMAs automatically query information systems on behalf of users as well as provide an interface by which the user can pose queries explicitly. Because IMAs are aware of the user‚Äôs task, they can augment their explicit query with terms representative of the context of this task. In this way, IMAs provide a framework for bringing implicit task context to bear on servicing explicit information requests, significantly reducing ambiguity. IMAs embody a just-in-time information infrastructure in which information is brought to users as they need it, without requiring explicit requests. In this paper, we present our work on an architecture for this class of system, and our progress implementing Watson, a prototype of such a system. Watson observes users in word processing and Web browsing applications and uses a simple model of the user‚Äôs tasks, knowledge of term importance, and an understanding of query generation to find relevant documents and service explicit queries. We close by discussing our experimental evaluations of the system.",
    "venue": "",
    "year": 1999,
    "citationCount": 166,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "1999-12-01",
    "authors": [
      {
        "authorId": "2820785",
        "name": "Jay Budzik"
      },
      {
        "authorId": "2263239",
        "name": "K. Hammond"
      }
    ],
    "source": "semantic_scholar",
    "score": 136.76990718625132
  },
  {
    "paperId": "ab482094b6122e0c7194e09750bb0648bf81aef0",
    "url": "https://www.semanticscholar.org/paper/ab482094b6122e0c7194e09750bb0648bf81aef0",
    "title": "Design mechanisms and constraints",
    "abstract": null,
    "venue": "",
    "year": 2005,
    "citationCount": 19,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2005-09-16",
    "authors": [
      {
        "authorId": "2939217",
        "name": "D. Pons"
      },
      {
        "authorId": "33436295",
        "name": "J. Raine"
      }
    ],
    "source": "semantic_scholar",
    "score": 84.93598410330986
  },
  {
    "paperId": "16722e4f1474ccad790e4b26b8044d7a04245472",
    "url": "https://www.semanticscholar.org/paper/16722e4f1474ccad790e4b26b8044d7a04245472",
    "title": "Engineering Human Computer Interaction and Interactive Systems, Joint Working Conferences EHCI-DSVIS 2004, Hamburg, Germany, July 11-13, 2004, Revised Selected Papers",
    "abstract": null,
    "venue": "EHCI/DS-VIS",
    "year": 2005,
    "citationCount": 56,
    "openAccessPdf": {
      "url": "https://link.springer.com/content/pdf/bfm:978-3-540-31961-0/1?pdf=chapter%20toc",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1404545509",
        "name": "Ehci-Dsvis"
      },
      {
        "authorId": "1793652",
        "name": "R. Bastide"
      },
      {
        "authorId": "1751033",
        "name": "Philippe A. Palanque"
      },
      {
        "authorId": "2113977474",
        "name": "J. Roth"
      }
    ],
    "source": "semantic_scholar",
    "score": 100.64576901751826
  },
  {
    "paperId": "d77278dbeb60609f780aff5b8b0672bfe19d3e6c",
    "url": "https://www.semanticscholar.org/paper/d77278dbeb60609f780aff5b8b0672bfe19d3e6c",
    "title": "Computer Aided Systems Theory - EUROCAST 2005, 10th International Conference on Computer Aided Systems Theory, Las Palmas de Gran Canaria, Spain, February 7-11, 2005, Revised Selected Papers",
    "abstract": null,
    "venue": "International Conference/Workshop on Computer Aided Systems Theory",
    "year": 2005,
    "citationCount": 45,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "71827067",
        "name": "R. Moreno-D√≠az"
      },
      {
        "authorId": "145807191",
        "name": "F. Pichler"
      },
      {
        "authorId": "67147468",
        "name": "Alexis Quesada Arencibia"
      }
    ],
    "source": "semantic_scholar",
    "score": 97.42962094733642
  },
  {
    "paperId": "40848eda8b7aacb01a938ada0ddc14959c61c95e",
    "url": "https://www.semanticscholar.org/paper/40848eda8b7aacb01a938ada0ddc14959c61c95e",
    "title": "Comparing Comparisons: Document Clustering Evaluation Using Two Manual Classifications",
    "abstract": "Text clustering divides a set of texts into clusters (parts), so that texts within each cluster are similar in content. It may be used to uncover the structure and content of unknown text sets as well as to give new perspectives on familiar ones. The main contributions of this thesis are an investigation of text representation for Swedish and some extensions of the work on how to use text clustering as an exploration tool. We have also done some work on synonyms and evaluation of clustering results. Text clustering, at least such as it is treated here, is performed using the vector space model, which is commonly used in information retrieval. This model represents texts by the words that appear in them and considers texts similar in content if they share many words. Languages differ in what is considered a word. We have investigated the impact of some of the characteristics of Swedish on text clustering. Swedish has more morphological variation than for instance English. We show that it is beneficial to use the lemma form of words rather than the word forms. Swedish has a rich production of solid compounds. Most of the constituents of these are used on their own as words and in several different compounds. In fact, Swedish solid compounds often correspond to phrases or open compounds in other languages. Our experiments show that it is beneficial to split solid compounds into their parts when building the representation. The vector space model does not regard word order. We have tried to extend it with nominal phrases in different ways. We have also tried to differentiate between homographs, words that look alike but mean different things, by augmenting all words with a tag indicating their part of speech. None of our experiments using phrases or part of speech information have shown any improvement over using the ordinary model. Evaluation of text clustering results is very hard. What is a good partition of a text set is inherently subjective. External quality measures compare a clustering with a (manual) categorization of the same text set. The theoretical best possible value for a measure is known, but it is not obvious what a good value is ‚Äì text sets differ in difficulty to cluster and categorizations are more or less adapted to a particular text set. We describe how evaluation can be improved for cases where a text set has more than one categorization. In such cases the result of a clustering can be compared with the result for one of the categorizations, which we assume is a good partition. In some related work we have built a dictionary of synonyms. We use it to compare two different principles for automatic word relation extraction through clustering of words. Text clustering can be used to explore the contents of a text set. We have developed a visualization method that aids such exploration, and implemented it in a tool, called Infomat. It presents the representation matrix directly in two dimensions. When the order of texts and words are changed, by for instance clustering, distributional patterns that indicate similarities between texts and words appear. We have used Infomat to explore a set of free text answers about occupation from a questionnaire given to over 40 000 Swedish twins. The questionnaire also contained a closed answer regarding smoking. We compared several clusterings of the text answers to the closed answer, regarded as a categorization, by means of clustering evaluation. A recurring text cluster of high quality led us to formulate the hypothesis that ‚Äúfarmers smoke less than the average‚Äù, which we later could verify by reading previous studies. This hypothesis generation method could be used on any set of texts that is coupled with data that is restricted to a limited number of possible values.",
    "venue": "",
    "year": 2004,
    "citationCount": 36,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "40030870",
        "name": "M. Rosell"
      },
      {
        "authorId": "1747198",
        "name": "V. Kann"
      },
      {
        "authorId": "48614712",
        "name": "Jan-Eric Litton"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.16376868966336
  },
  {
    "paperId": "831d7fddfdab66da601b1b9a2eb381bfe8e9410d",
    "url": "https://www.semanticscholar.org/paper/831d7fddfdab66da601b1b9a2eb381bfe8e9410d",
    "title": "PACE: A Browsable Graphical Interface",
    "abstract": "PACE (Public Access Catalogue Extension) is an alternative interface designed to enhance online catalogs. PACE simulates images of books and library shelves to help users browse through the catalog. PACE was tested in a college library against a text-based, online public access catalog (Best-Seller) in a real operational environment. The results show that a simple browsable retrieval interface performed as well as a second-generation OPAC in terms of retrieval speed and search success. The overwhelming majority of students, however, preferred the browsing capability of PACE through the familiar metaphor of books and library shelves to a text-based OPAC. Today many information resources can be accessed through the virtual library, including many online public access catalogs (OPACs) with sophisticated retrieval engines. Research shows, however, that novice users encounter difficulties in interacting with these systems (Borgman 1986). Yee (1991) has summarized these problems as finding appropriate subject terms, large numbers of hits and failure to reduce the retrieval sets, zero hits and failure to increase the retrieval sets, failure to understand cataloging rules, and spelling and typographical errors. In addition, lack of understanding of indexes, files, and basic database structure leads to the use of articles, stop words, entering author's first name before last name, and hyphenation problems. The interface and retrieval systems can also be a source of potential problems. Complex interfaces and the need for training and relearning when used infrequently, incomprehensible error and help messages, problems associated with displaying records and difficulties with Boolean logic have compounded the obstacles encountered by novice users. These restraints have prompted one researcher to state that the second-generation OPACs are \"powerful and efficient but are dumb, passive systems which require resourceful, active, intelligent human searchers to produce acceptable results\" (Hildreth 1989). Suggested Solutions To address the needs of end users and to alleviate the mentioned difficulties, many researchers have conducted experiments to enhance and improve OPACs. In general, research in this area may be divided into two broad categories: enhancing MARC records, and improving the retrieval engine and the interface. Many researchers (Cochrane 1986, Drabenstott et al. 1990, Chan 1990, Pejtersen 1989, Lawrence 1985) have demonstrated the value of augmenting MARC records and the online catalog with various schemes such as the inclusion of the Dewey Decimal Classification (DDC) schedules and relative indexes. Projects in Carnegie Mellon University and other institutions have attempted to enhance bibliographic records by adding table of contents of books (Greenwood 1989, Michalak 1990, Posey and Erdmann 1986). The addition of new information to MARC, however, can be very costly and unsuitable for individual libraries. Advancing the retrieval engine has involved ranking of documents, weighting index terms and automatic spell checking. Projects such as OKAPI (Online Keyword Access to Public Information) have attempted to use these techniques to build OPACs which do not require any user training (Greenwood 1989) Other researchers have strived to enhance the retrieval system by adding visual interfaces which use icons, graphical user interfaces, hypertext links and multimedia to help end users. Client/server architectures are used to take advantage of the power of today's microcomputers to present users with alternative interfaces. Kid's Catalog (Busey and Doerr 1993), OASIS (Buckland et al. 1992), Multimedia Visualizer (Lee 1991), the Science Library Catalog Project (Borgman et al. 1995) and XOkapi (Hancock-Beaulieu et.al. 1995) are just a few examples of these new interfaces. These OPACs are utilizing concepts that move users closer to direct manipulation of objects or documents. Hildreth (1989) states that these OPACs are more intuitive as the objects are manipulated directly \"avoiding previous layers of mental encoding/decoding and indirect representation searchers are usually required to pass through. ‚Ä¶",
    "venue": "",
    "year": 1996,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "1996-12-01",
    "authors": [
      {
        "authorId": "1796558",
        "name": "J. Beheshti"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.58585994422887
  },
  {
    "paperId": "9a3f9429d3b87a1007efe8cc2a8731d7eb9a117d",
    "url": "https://www.semanticscholar.org/paper/9a3f9429d3b87a1007efe8cc2a8731d7eb9a117d",
    "title": "SmartTable: Equipping Spreadsheets with Intelligent AssistanceFunctionalities",
    "abstract": "Tables are one of those \"universal tools'' that are practical and useful in many application scenarios. Tables can be used to collect and organize information from multiple sources and then turn that information into knowledge (and ultimately to support decision-making) by performing various operations, like sorting, filtering, and joins. Because of this, a large number of tables exist already out there on the Web, which represent a vast and rich source of structured information and could be utilized as resources. Recently, a growing body of work has begun to tap into utilizing the knowledge contained in tables. A wide and diverse range of tasks have been undertaken, including but not limited to (i) searching for tables[4], (ii) extracting knowledge from tables, and (iii) augmenting tables (e.g., with new columns and rows[1,3] ). The objective of this research is to develop a set of components for a tool called SmartTable, which is aimed at assisting the user in completing a complex task by providing intelligent assistance for working with tables. Imagine the scenario that a user is working with a table, and has already entered some data in the table. We can provide recommendations for the empty table cells, search for similar tables that can serve as a blueprint, or even generate automatically the entire table that the user needs. The table-making task can thus be simplified into just a few button clicks. Motivated by the above scenario, we propose a set of novel tasks such as row and column heading population, table search, and table generation. The following specific research questions are addressed: ( RQ1 ) How to populate table rows and column heading labels? ( RQ2 ) How to find relevant tables given a keyword query? ( RQ3 ) How to find tables relevant to the table the user is currently working on? ( RQ4 ) How to generate an output table as response to a free text query? For RQ1, the task of row population [1,3] relates to the task of entity set expansion, where a given set of entities is to be completed with additional entities. Row population focuses on populating entities in the \"core column'' of a relational table. We develop a two-step pipeline for this task utilizing a table corpus and a knowledge base. In the first step, candidate entities sharing the same categories with seed entities or co-occurring in similar tables are selected. In the second step, they are ranked by a probabilistic model. Column population shares similarities with the problem of schema complement, where a seed table is to be extended with additional columns. For column population, we regard column headings from similar tables as candidates and rank them using a probabilistic model. For RQ2 and RQ3, we address the problem of table search. This task is not only interesting on its own but is also being used as a fundamental building block in many other table-based information access scenarios, such as table completion or table mining. To search related tables, the query could be some keywords [2,4] or it can also be an existing (incomplete) table. Based on the query type, this task is divided into two sub-tasks, which are table retrieval for keyword query and query-by-table respectively. For RQ4, we introduce and address the task of the on-the-fly table generation: given a query, generate a relational table that contains relevant entities (as rows) along with their key properties (as columns) [5]. In terms of the table elements in a relational table, this task boils downing to core column entity ranking, schema determination and value look-up. We propose a feature-based approach for entity ranking and schema determination, combing deep semantic features with task-specific signals. For value lookup, we combine information from existing tables and a knowledge base. So far, we have proposed methods and evaluation resources for addressing the tasks of row/column population, table search, and table generation. Future research directions for this project include looking up table values, interacting with tables using natural language, and generating table embeddings.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2018,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2018-06-27",
    "authors": [
      {
        "authorId": "1390852432",
        "name": "Shuo Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 103.95836866004329
  },
  {
    "paperId": "8c8314bb81013436f8c963c9681bdef23bf58f8b",
    "url": "https://www.semanticscholar.org/paper/8c8314bb81013436f8c963c9681bdef23bf58f8b",
    "title": "A Proposal for a BBS with Visual Presentation for Online Data Analysis",
    "abstract": "The concept of a bulletin board system (BBS) equipped with information visualization techniques is proposed for supporting online data analysis. Although group discussion is known to be effective for analyzing data from various viewpoints, the number of participants is limited by time and space constraints. To solve that problem, this paper proposes to augment a BBS, a popular web based tool. In order for discussion participants to share data online, the system provides them with a visual representation of target data, which elicits comments from participants as well as compares these comments. In order to illustrate the concept's potential, a BBS equipped with KeyGraph is also developed for supporting online chance discovery. It has functions for making visual annotations on the KeyGraph as well as a function for retrieving similar scenarios. The experimental results show the effectiveness of the BBS in terms of the usefulness of scenario generation support functions as well as that of scenario retrieval engines.",
    "venue": "Data Science Journal",
    "year": 2007,
    "citationCount": 5,
    "openAccessPdf": {
      "url": "http://datascience.codata.org/articles/10.2481/dsj.6.S28/galley/381/download/",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2007-03-28",
    "authors": [
      {
        "authorId": "1768055",
        "name": "Y. Takama"
      },
      {
        "authorId": "1823715",
        "name": "Yuta Seo"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "37447d956fa188ff0eb3d8fcdf8cc0b9de6a7850",
    "url": "https://www.semanticscholar.org/paper/37447d956fa188ff0eb3d8fcdf8cc0b9de6a7850",
    "title": "Advanced multimedia content processing : first international conference, AMCP '98, Osaka, Japan, November 9-11, 1998 : proceedings",
    "abstract": "Content Summarization.- Video Summarization Based on Semantic Representation.- Valbum: Album-Oriented Video Storyboard for Editing and Viewing Video.- Augmented Reality Technology and Applications.- Beyond the Desktop Metaphor: Toward More Effective Display, Interaction, and Telecollaboration in the Office of the Future via a Multitude of Sensors and Displays.- A Method for Estimating Illumination Distribution of a Real Scene Based on Soft Shadows.- Integrating Real Space and Virtual Space in the 'Invisible Person' Communication Support System.- Content-Based Video Indexing and Classification.- News Dictation and Article Classification Using Automatically Extracted Announcer Utterance.- Automatic Video Indexing Based on Shot Classification.- Mutual Spotting Retrieval between Speech and Video Image Using Self-Organized Network Databases.- Content-Based Retrieval.- Content-Based Retrieval in Multimedia Databases Based on Feature Models.- An Efficient Index Structure for High Dimensional Image Data.- Color-Based Pseudo Object Model for Image Retrieval with Relevance Feedback.- System Environments for Virtual Reality.- InvenTcl: A Fast Prototyping Environment for 3D Graphics and Multimedia Applications.- The NAVL Distributed Virtual Reality System.- Content Broadcast Systems and Applications.- Research in Data Broadcast and Dissemination.- Multimedia Database System for TV Newscasts and Newspapers.- A TV News Recommendation System with Automatic Recomposition.- Extended Digital Video Broadcasting with Time-Lined Hypermedia.- Video Images and Virtual Space.- Active Image Capturing and Dynamic Scene Visualization by Cooperative Distributed Vision.- Videoplex: A New System Framework for Constructing Video-Based Three-Dimensional Space.- Construction of Virtual Environment from Video Data with Forward Motion.- Spatial Browsing for Video Databases.- Video Databases.- AI-STRATA: A User-Centered Model for Content-Based Description and Retrieval of Audiovisual Sequences.- Use of Action History Views for Indexing Continuous Media Objects.- Semantic Structures for Video Data Indexing.- Interactive Content Creation.- A Study of Emergent Computation of Life-like Behavior by Indefinite Observation.- An Interactive Digital Fishtank Based on Live Video Images.- Creation for Interactive Media.- Visual Modeling for Multimedia Content.- Visual Modeling for Multimedia Content.- Automatic Generation of Moving Crowds in the Virtual Environment.- Extracting Facial Motion Parameters by Tracking Feature Points.- Dynamic Media Contest Session.- Immersion Reconsidered.- Synthetic Characters: Behaving in Character.",
    "venue": "",
    "year": 1999,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "69479098",
        "name": "Amcp"
      },
      {
        "authorId": "2093802085",
        "name": "Ë•øÂ∞æ Á´†Ê≤ªÈÉé"
      },
      {
        "authorId": "2072240570",
        "name": "Â≤∏Èáé ÊñáÈÉé"
      }
    ],
    "source": "semantic_scholar",
    "score": 84.1415686865115
  },
  {
    "paperId": "71d2b6a463f3ed6a67ca81522db8a03d9e0b623c",
    "url": "https://www.semanticscholar.org/paper/71d2b6a463f3ed6a67ca81522db8a03d9e0b623c",
    "title": "Technologies for E-Learning and Digital Entertainment, Second International Conference, Edutainment 2007, Hong Kong, China, June 11-13, 2007, Proceedings",
    "abstract": null,
    "venue": "International Conference on E-learning and Games",
    "year": 2007,
    "citationCount": 19,
    "openAccessPdf": {
      "url": "https://link.springer.com/content/pdf/bfm:978-3-540-73011-8/1?pdf=chapter%20toc",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "143671487",
        "name": "K. Hui"
      }
    ],
    "source": "semantic_scholar",
    "score": 84.93598410330986
  },
  {
    "paperId": "95eee392a5cd2c45bd5c192b29a1e07dc70cf2d3",
    "url": "https://www.semanticscholar.org/paper/95eee392a5cd2c45bd5c192b29a1e07dc70cf2d3",
    "title": "Machine Learning for Intelligent Information Access",
    "abstract": null,
    "venue": "Machine Learning and Its Applications",
    "year": 2001,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2001-09-27",
    "authors": [
      {
        "authorId": "1770463",
        "name": "Grigoris I. Karakoulas"
      },
      {
        "authorId": "145467353",
        "name": "G. Semeraro"
      }
    ],
    "source": "semantic_scholar",
    "score": 60.79441541679836
  },
  {
    "paperId": "8ba12a8fb72aa6856365e7038cde80c445b7e500",
    "url": "https://www.semanticscholar.org/paper/8ba12a8fb72aa6856365e7038cde80c445b7e500",
    "title": "Advanced Concepts for Intelligent Vision Systems",
    "abstract": null,
    "venue": "Lecture Notes in Computer Science",
    "year": 2010,
    "citationCount": 13,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1402955152",
        "name": "J. Blanc-Talon"
      },
      {
        "authorId": "2257221694",
        "name": "Wilfried Philips"
      },
      {
        "authorId": "2270297409",
        "name": "Dan Popescu"
      },
      {
        "authorId": "1682088",
        "name": "P. Scheunders"
      }
    ],
    "source": "semantic_scholar",
    "score": 89.58585994422887
  },
  {
    "paperId": "0152aa7eb21cb1c9152fcaa50d5abde255d035e5",
    "url": "https://www.semanticscholar.org/paper/0152aa7eb21cb1c9152fcaa50d5abde255d035e5",
    "title": "Intelligent control and innovative computing",
    "abstract": null,
    "venue": "",
    "year": 2012,
    "citationCount": 7,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "70069764",
        "name": "Innovative Computing"
      },
      {
        "authorId": "116494420",
        "name": "S. Ao"
      },
      {
        "authorId": "119560021",
        "name": "Osca Castillo"
      },
      {
        "authorId": "2118520670",
        "name": "Xu Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 71.19162312519754
  },
  {
    "paperId": "9061cc45c64846498f572c9ad2cb14f76324d665",
    "url": "https://www.semanticscholar.org/paper/9061cc45c64846498f572c9ad2cb14f76324d665",
    "title": "Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA",
    "abstract": "Long-context modeling capabilities of Large Language Models (LLMs) have garnered widespread attention, leading to the emergence of LLMs with ultra-context windows. Meanwhile, benchmarks for evaluating long-context language models are gradually catching up. However, existing benchmarks employ irrelevant noise texts to artificially extend the length of test cases, diverging from the real-world scenarios of long-context applications. To bridge this gap, we propose a novel long-context benchmark, Loong, aligning with realistic scenarios through extended multi-document question answering (QA). Unlike typical document QA, in Loong‚Äôs test cases, each document is relevant to the final answer, ignoring any document will lead to the failure of the answer. Furthermore, Loong introduces four types of tasks with a range of context lengths: Spotlight Locating, Comparison, Clustering, and Chain of Reasoning, to facilitate a more realistic and comprehensive evaluation of long-context understanding. Extensive experiments indicate that existing long-context language models still exhibit considerable potential for enhancement. Retrieval augmented generation (RAG) achieves poor performance, demonstrating that Loong can reliably assess the model‚Äôs long-context modeling capabilities.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 22,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-06-25",
    "authors": [
      {
        "authorId": "2264515707",
        "name": "Minzheng Wang"
      },
      {
        "authorId": "2303412284",
        "name": "Longze Chen"
      },
      {
        "authorId": "2308978408",
        "name": "Cheng Fu"
      },
      {
        "authorId": "2308098837",
        "name": "Shengyi Liao"
      },
      {
        "authorId": "2305991598",
        "name": "Xinghua Zhang"
      },
      {
        "authorId": "2308412644",
        "name": "Bingli Wu"
      },
      {
        "authorId": "2288351000",
        "name": "Haiyang Yu"
      },
      {
        "authorId": "2308100932",
        "name": "Nan Xu"
      },
      {
        "authorId": "2274938171",
        "name": "Lei Zhang"
      },
      {
        "authorId": "2240531270",
        "name": "Run Luo"
      },
      {
        "authorId": "2204767592",
        "name": "Yunshui Li"
      },
      {
        "authorId": "2275101569",
        "name": "Min Yang"
      },
      {
        "authorId": "2257407873",
        "name": "Fei Huang"
      },
      {
        "authorId": "2287833084",
        "name": "Yongbin Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 117.03241323893724
  },
  {
    "paperId": "c4a53f4a12207ecce4ea5102fb0a05d8f8984561",
    "url": "https://www.semanticscholar.org/paper/c4a53f4a12207ecce4ea5102fb0a05d8f8984561",
    "title": "From human experts to machines: An LLM supported approach to ontology and knowledge graph construction",
    "abstract": "The conventional process of building Ontologies and Knowledge Graphs (KGs) heavily relies on human domain experts to define entities and relationship types, establish hierarchies, maintain relevance to the domain, fill the ABox (or populate with instances), and ensure data quality (including amongst others accuracy and completeness). On the other hand, Large Language Models (LLMs) have recently gained popularity for their ability to understand and generate human-like natural language, offering promising ways to automate aspects of this process. This work explores the (semi-)automatic construction of KGs facilitated by open-source LLMs. Our pipeline involves formulating competency questions (CQs), developing an ontology (TBox) based on these CQs, constructing KGs using the developed ontology, and evaluating the resultant KG with minimal to no involvement of human experts. We showcase the feasibility of our semi-automated pipeline by creating a KG on deep learning methodologies by exploiting scholarly publications. To evaluate the answers generated via Retrieval-Augmented-Generation (RAG) as well as the KG concepts automatically extracted using LLMs, we design a judge LLM, which rates the generated content based on ground truth. Our findings suggest that employing LLMs could potentially reduce the human effort involved in the construction of KGs, although a human-in-the-loop approach is recommended to evaluate automatically generated KGs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 17,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-13",
    "authors": [
      {
        "authorId": "98776340",
        "name": "Vamsi Krishna Kommineni"
      },
      {
        "authorId": "1389035902",
        "name": "B. K√∂nig-Ries"
      },
      {
        "authorId": "38718062",
        "name": "Sheeba Samuel"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.35557636844247
  },
  {
    "paperId": "1d500905ca70d40a85d10f60d44a018ec0d9349d",
    "url": "https://www.semanticscholar.org/paper/1d500905ca70d40a85d10f60d44a018ec0d9349d",
    "title": "Weaver: Foundation Models for Creative Writing",
    "abstract": "This work introduces Weaver, our first family of large language models (LLMs) dedicated to content creation. Weaver is pre-trained on a carefully selected corpus that focuses on improving the writing capabilities of large language models. We then fine-tune Weaver for creative and professional writing purposes and align it to the preference of professional writers using a suit of novel methods for instruction data synthesis and LLM alignment, making it able to produce more human-like texts and follow more diverse instructions for content creation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for different applications and can be dynamically dispatched by a routing agent according to query complexity to balance response quality and computation cost. Evaluation on a carefully curated benchmark for assessing the writing capabilities of LLMs shows Weaver models of all sizes outperform generalist LLMs several times larger than them. Notably, our most-capable Weaver Ultra model surpasses GPT-4, a state-of-the-art generalist LLM, on various writing scenarios, demonstrating the advantage of training specialized LLMs for writing purposes. Moreover, Weaver natively supports retrieval-augmented generation (RAG) and function calling (tool usage). We present various use cases of these abilities for improving AI-assisted writing systems, including integration of external knowledge bases, tools, or APIs, and providing personalized writing assistance. Furthermore, we discuss and summarize a guideline and best practices for pre-training and fine-tuning domain-specific LLMs.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 12,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-01-30",
    "authors": [
      {
        "authorId": "2118916175",
        "name": "Tiannan Wang"
      },
      {
        "authorId": "2281899463",
        "name": "Jiamin Chen"
      },
      {
        "authorId": "2281826284",
        "name": "Qingrui Jia"
      },
      {
        "authorId": "2241329032",
        "name": "Shuai Wang"
      },
      {
        "authorId": "2281825246",
        "name": "Ruoyu Fang"
      },
      {
        "authorId": "2281904344",
        "name": "Huilin Wang"
      },
      {
        "authorId": "2281851996",
        "name": "Zhaowei Gao"
      },
      {
        "authorId": "2281912686",
        "name": "Chunzhao Xie"
      },
      {
        "authorId": "2281907115",
        "name": "Chuou Xu"
      },
      {
        "authorId": "2281923463",
        "name": "Jihong Dai"
      },
      {
        "authorId": "2265516972",
        "name": "Yibin Liu"
      },
      {
        "authorId": "2240542238",
        "name": "Jialong Wu"
      },
      {
        "authorId": "2282097999",
        "name": "Shengwei Ding"
      },
      {
        "authorId": "2240651745",
        "name": "Long Li"
      },
      {
        "authorId": "2261275988",
        "name": "Zhiwei Huang"
      },
      {
        "authorId": "2281905378",
        "name": "Xinle Deng"
      },
      {
        "authorId": "2282248664",
        "name": "Teng Yu"
      },
      {
        "authorId": "2281860775",
        "name": "Gangan Ma"
      },
      {
        "authorId": "2114223496",
        "name": "Han Xiao"
      },
      {
        "authorId": "2256226039",
        "name": "Z. Chen"
      },
      {
        "authorId": "2281824853",
        "name": "Danjun Xiang"
      },
      {
        "authorId": "2244604217",
        "name": "Yunxia Wang"
      },
      {
        "authorId": "2281839906",
        "name": "Yuanyuan Zhu"
      },
      {
        "authorId": "2300044485",
        "name": "Yichen Xiao"
      },
      {
        "authorId": "2281870256",
        "name": "Jing Wang"
      },
      {
        "authorId": "2281902398",
        "name": "Yiru Wang"
      },
      {
        "authorId": "2282098001",
        "name": "Siran Ding"
      },
      {
        "authorId": "2281904666",
        "name": "Jiayang Huang"
      },
      {
        "authorId": "2281900041",
        "name": "Jiayi Xu"
      },
      {
        "authorId": "101830651",
        "name": "Yilihamujiang Tayier"
      },
      {
        "authorId": "2281908323",
        "name": "Zhenyu Hu"
      },
      {
        "authorId": "2278969278",
        "name": "Yuan Gao"
      },
      {
        "authorId": "2267427446",
        "name": "Chengfeng Zheng"
      },
      {
        "authorId": "2275577993",
        "name": "Yu-Jie Ye"
      },
      {
        "authorId": "2266724843",
        "name": "Yihan Li"
      },
      {
        "authorId": "2281833351",
        "name": "Lei Wan"
      },
      {
        "authorId": "2281902500",
        "name": "Xinyue Jiang"
      },
      {
        "authorId": "46394439",
        "name": "Yujie Wang"
      },
      {
        "authorId": "2258034882",
        "name": "Siyuan Cheng"
      },
      {
        "authorId": "2282459045",
        "name": "Zhule Song"
      },
      {
        "authorId": "47274259",
        "name": "Xiangru Tang"
      },
      {
        "authorId": "2281906558",
        "name": "Xiaohua Xu"
      },
      {
        "authorId": "2153010067",
        "name": "Ningyu Zhang"
      },
      {
        "authorId": "2144200945",
        "name": "Huajun Chen"
      },
      {
        "authorId": "2269831684",
        "name": "Yuchen Eleanor Jiang"
      },
      {
        "authorId": "150341221",
        "name": "Wangchunshu Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 108.47424036192305
  },
  {
    "paperId": "31babc068383c3966d322579eaaf79edc6a6bef4",
    "url": "https://www.semanticscholar.org/paper/31babc068383c3966d322579eaaf79edc6a6bef4",
    "title": "Generative AI in Cybersecurity: A Comprehensive Review of LLM Applications and Vulnerabilities",
    "abstract": "This paper provides a comprehensive review of the future of cybersecurity through Generative AI and Large Language Models (LLMs). We explore LLM applications across various domains, including hardware design security, intrusion detection, software engineering, design verification, cyber threat intelligence, malware detection, and phishing detection. We present an overview of LLM evolution and its current state, focusing on advancements in models such as GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends to LLM vulnerabilities, such as prompt injection, insecure output handling, data poisoning, DDoS attacks, and adversarial instructions. We delve into mitigation strategies to protect these models, providing a comprehensive look at potential attack scenarios and prevention techniques. Furthermore, we evaluate the performance of 42 LLM models in cybersecurity knowledge and hardware security, highlighting their strengths and weaknesses. We thoroughly evaluate cybersecurity datasets for LLM training and testing, covering the lifecycle from data creation to usage and identifying gaps for future research. In addition, we review new strategies for leveraging LLMs, including techniques like Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human Feedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank Adapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim to enhance real-time cybersecurity defenses and improve the sophistication of LLM applications in threat detection and response. Our paper provides a foundational understanding and strategic direction for integrating LLMs into future cybersecurity frameworks, emphasizing innovation and robust model deployment to safeguard against evolving cyber threats.",
    "venue": "",
    "year": 2024,
    "citationCount": 11,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": "2024-05-21",
    "authors": [
      {
        "authorId": "2864573",
        "name": "M. Ferrag"
      },
      {
        "authorId": "2258270534",
        "name": "Fatima Alwahedi"
      },
      {
        "authorId": "1574150859",
        "name": "A. Battah"
      },
      {
        "authorId": "2165439761",
        "name": "Bilel Cherif"
      },
      {
        "authorId": "2302400607",
        "name": "Abdechakour Mechri"
      },
      {
        "authorId": "2283303502",
        "name": "Norbert Tihanyi"
      },
      {
        "authorId": "1404353535",
        "name": "Tam√°s Bisztray"
      },
      {
        "authorId": "2065834880",
        "name": "M. Debbah"
      }
    ],
    "source": "semantic_scholar",
    "score": 107.27359974682
  },
  {
    "paperId": "d6f2ef1bd6fbfe60f03ec8da626af34fcf064900",
    "url": "https://www.semanticscholar.org/paper/d6f2ef1bd6fbfe60f03ec8da626af34fcf064900",
    "title": "MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine",
    "abstract": "This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities, with multigranular annotations for more than 65 diseases. These enriched annotations encompass both global textual information, such as disease/lesion type, modality, region-specific descriptions, and inter-regional relationships, as well as detailed local annotations for regions of interest (ROIs), including bounding boxes, segmentation masks. Unlike existing approach which is limited by the availability of image-text pairs, we have developed the first automated pipeline that scales up multimodal data by generating multigranular visual and texual annotations (in the form of image-ROI-description triplets) without the need for any paired text descriptions. Specifically, data from over 90 different sources have been collected, preprocessed, and grounded using domain-specific expert models to identify ROIs related to abnormal regions. We then build a comprehensive knowledge base and prompt multimodal large language models to perform retrieval-augmented generation with the identified ROIs as guidance, resulting in multigranular texual descriptions. Compared to existing datasets, MedTrinity-25M provides the most enriched annotations, supporting a comprehensive range of multimodal tasks such as captioning and report generation, as well as vision-centric tasks like classification and segmentation. Pretraining on MedTrinity-25M, our model achieves state-of-the-art performance on VQA-RAD and PathVQA, surpassing both multimodal large language models and other representative SoTA approaches. This dataset can also be utilized to support large-scale pre-training of multimodal medical AI models, contributing to the development of future foundation models in the medical domain.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 10,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-06",
    "authors": [
      {
        "authorId": "2315648431",
        "name": "Yunfei Xie"
      },
      {
        "authorId": "2315062394",
        "name": "Ce Zhou"
      },
      {
        "authorId": "2315058882",
        "name": "Lang Gao"
      },
      {
        "authorId": "2315276567",
        "name": "Juncheng Wu"
      },
      {
        "authorId": "2108261491",
        "name": "Xianhang Li"
      },
      {
        "authorId": "2315789194",
        "name": "Hong-Yu Zhou"
      },
      {
        "authorId": "2308266914",
        "name": "Sheng Liu"
      },
      {
        "authorId": "2266392022",
        "name": "Lei Xing"
      },
      {
        "authorId": "2204356217",
        "name": "James Y. Zou"
      },
      {
        "authorId": "3011497",
        "name": "Cihang Xie"
      },
      {
        "authorId": "2278976486",
        "name": "Yuyin Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.96842909197557
  },
  {
    "paperId": "17d7ce521fbc28cbf845b2675fda9e35b40076fa",
    "url": "https://www.semanticscholar.org/paper/17d7ce521fbc28cbf845b2675fda9e35b40076fa",
    "title": "The Sociodemographic Biases in Machine Learning Algorithms: A Biomedical Informatics Perspective",
    "abstract": "Artificial intelligence models represented in machine learning algorithms are promising tools for risk assessment used to guide clinical and other health care decisions. Machine learning algorithms, however, may house biases that propagate stereotypes, inequities, and discrimination that contribute to socioeconomic health care disparities. The biases include those related to some sociodemographic characteristics such as race, ethnicity, gender, age, insurance, and socioeconomic status from the use of erroneous electronic health record data. Additionally, there is concern that training data and algorithmic biases in large language models pose potential drawbacks. These biases affect the lives and livelihoods of a significant percentage of the population in the United States and globally. The social and economic consequences of the associated backlash cannot be underestimated. Here, we outline some of the sociodemographic, training data, and algorithmic biases that undermine sound health care risk assessment and medical decision-making that should be addressed in the health care system. We present a perspective and overview of these biases by gender, race, ethnicity, age, historically marginalized communities, algorithmic bias, biased evaluations, implicit bias, selection/sampling bias, socioeconomic status biases, biased data distributions, cultural biases and insurance status bias, conformation bias, information bias and anchoring biases and make recommendations to improve large language model training data, including de-biasing techniques such as counterfactual role-reversed sentences during knowledge distillation, fine-tuning, prefix attachment at training time, the use of toxicity classifiers, retrieval augmented generation and algorithmic modification to mitigate the biases moving forward.",
    "venue": "Life",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "https://www.mdpi.com/2075-1729/14/6/652/pdf?version=1716284761",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-05-21",
    "authors": [
      {
        "authorId": "2302647145",
        "name": "Gillian Franklin"
      },
      {
        "authorId": "2302645674",
        "name": "Rachel Stephens"
      },
      {
        "authorId": "2302645392",
        "name": "Muhammad Piracha"
      },
      {
        "authorId": "10702112",
        "name": "S. Tiosano"
      },
      {
        "authorId": "2069511681",
        "name": "Frank LeHouillier"
      },
      {
        "authorId": "2302644489",
        "name": "Ross Koppel"
      },
      {
        "authorId": "2295081197",
        "name": "Peter L Elkin"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "6990a7be4523a7b55229d720c739763fe78ceebf",
    "url": "https://www.semanticscholar.org/paper/6990a7be4523a7b55229d720c739763fe78ceebf",
    "title": "Current state of LLM Risks and AI Guardrails",
    "abstract": "Large language models (LLMs) have become increasingly sophisticated, leading to widespread deployment in sensitive applications where safety and reliability are paramount. However, LLMs have inherent risks accompanying them, including bias, potential for unsafe actions, dataset poisoning, lack of explainability, hallucinations, and non-reproducibility. These risks necessitate the development of\"guardrails\"to align LLMs with desired behaviors and mitigate potential harm. This work explores the risks associated with deploying LLMs and evaluates current approaches to implementing guardrails and model alignment techniques. We examine intrinsic and extrinsic bias evaluation methods and discuss the importance of fairness metrics for responsible AI development. The safety and reliability of agentic LLMs (those capable of real-world actions) are explored, emphasizing the need for testability, fail-safes, and situational awareness. Technical strategies for securing LLMs are presented, including a layered protection model operating at external, secondary, and internal levels. System prompts, Retrieval-Augmented Generation (RAG) architectures, and techniques to minimize bias and protect privacy are highlighted. Effective guardrail design requires a deep understanding of the LLM's intended use case, relevant regulations, and ethical considerations. Striking a balance between competing requirements, such as accuracy and privacy, remains an ongoing challenge. This work underscores the importance of continuous research and development to ensure the safe and responsible use of LLMs in real-world applications.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 8,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-16",
    "authors": [
      {
        "authorId": "2307472822",
        "name": "Suriya Ganesh Ayyamperumal"
      },
      {
        "authorId": "2335697196",
        "name": "Limin Ge"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "9165d49883a25d0661ac04fa3fb7b65cd70c885c",
    "url": "https://www.semanticscholar.org/paper/9165d49883a25d0661ac04fa3fb7b65cd70c885c",
    "title": "LLM-Powered Multimodal AI Conversations for Diabetes Prevention",
    "abstract": "The global prevalence of diabetes remains high despite rising life expectancy with improved quality and access to healthcare services. The significant burden that diabetes imposes warrants efforts to improve existing interventions in diabetes care. Present research on diabetes management has shown that artificial intelligence (AI) and Large Language Models (LLM) play an important role in various aspects of the diabetes continuum but a distinct lack of studies in diabetes prevention is observed. Our research introduces a comprehensive digital solution, leveraging the capabilities of GPT-3.5 models maintained by OpenAI, focused specifically on the active prevention of diabetes. The system encompasses a user-friendly interface accessible via mobile and web applications, an AI-powered chatbot for instant Q&A and advice, personalized reminder systems, a data analysis module for tailored guidance, resource aggregators for health-related information, and an emotional support module to ensure a holistic approach to prevention. Furthermore, our experiments involved testing the quality of responses generated by a fine-tuned GPT-3.5 model, utilizing the Assistants API or a retrieval-augmented generation (RAG) system powered by FAISS for enhanced context awareness and personalized advice. The testing focused on a structured dataset of questions and answers related to diabetes prevention, with results highlighting the superiority of the GPT-3.5 model combined with the Assistants API in providing relevant, detailed, and personalized responses, thus demonstrating its potential as an invaluable tool in the proactive prevention of diabetes.",
    "venue": "AIQAM@ICMR",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle"
    ],
    "publicationDate": "2024-06-10",
    "authors": [
      {
        "authorId": "2304955642",
        "name": "Dung Dao"
      },
      {
        "authorId": "2134684286",
        "name": "Jun Yi Claire Teo"
      },
      {
        "authorId": "2281073913",
        "name": "Wenru Wang"
      },
      {
        "authorId": "2305528733",
        "name": "Hoang D. Nguyen"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "3d814347e382fc3fcc071876744f139d2c101be7",
    "url": "https://www.semanticscholar.org/paper/3d814347e382fc3fcc071876744f139d2c101be7",
    "title": "Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent Communities",
    "abstract": "The rapid adoption of large language models (LLMs) in multi-agent systems has highlighted their impressive capabilities in various applications, such as collaborative problem-solving and autonomous negotiation. However, the security implications of these LLM-based multi-agent systems have not been thoroughly investigated, particularly concerning the spread of manipulated knowledge. In this paper, we investigate this critical issue by constructing a detailed threat model and a comprehensive simulation environment that mirrors real-world multi-agent deployments in a trusted platform. Subsequently, we propose a novel two-stage attack method involving Persuasiveness Injection and Manipulated Knowledge Injection to systematically explore the potential for manipulated knowledge (i.e., counterfactual and toxic knowledge) spread without explicit prompt manipulation. Our method leverages the inherent vulnerabilities of LLMs in handling world knowledge, which can be exploited by attackers to unconsciously spread fabricated information. Through extensive experiments, we demonstrate that our attack method can successfully induce LLM-based agents to spread both counterfactual and toxic knowledge without degrading their foundational capabilities during agent communication. Furthermore, we show that these manipulations can persist through popular retrieval-augmented generation frameworks, where several benign agents store and retrieve manipulated chat histories for future interactions. This persistence indicates that even after the interaction has ended, the benign agents may continue to be influenced by manipulated knowledge. Our findings reveal significant security risks in LLM-based multi-agent systems, emphasizing the imperative need for robust defenses against manipulated knowledge spread, such as introducing ``guardian'' agents and advanced fact-checking tools.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 6,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-07-10",
    "authors": [
      {
        "authorId": "2283307652",
        "name": "Tianjie Ju"
      },
      {
        "authorId": "2310653763",
        "name": "Yiting Wang"
      },
      {
        "authorId": "2141114505",
        "name": "Xinbei Ma"
      },
      {
        "authorId": "1988975660",
        "name": "Pengzhou Cheng"
      },
      {
        "authorId": "2146232955",
        "name": "Haodong Zhao"
      },
      {
        "authorId": "2283441039",
        "name": "Yulong Wang"
      },
      {
        "authorId": "2283444751",
        "name": "Lifeng Liu"
      },
      {
        "authorId": "2242192116",
        "name": "Jian Xie"
      },
      {
        "authorId": "2284695096",
        "name": "Zhuosheng Zhang"
      },
      {
        "authorId": "2267384727",
        "name": "Gongshen Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 99.1886522358297
  },
  {
    "paperId": "db96e019410006c3ee0ae0184800ab206f8704dd",
    "url": "https://www.semanticscholar.org/paper/db96e019410006c3ee0ae0184800ab206f8704dd",
    "title": "Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction",
    "abstract": "In this work, we are interested in automated methods for knowledge graph creation (KGC) from input text. Progress on large language models (LLMs) has prompted a series of recent works applying them to KGC, e.g., via zero/few-shot prompting. Despite successes on small domain-specific datasets, these models face difficulties scaling up to text common in many real-world applications. A principal issue is that, in prior methods, the KG schema has to be included in the LLM prompt to generate valid triplets; larger and more complex schemas easily exceed the LLMs‚Äô context window length. Furthermore, there are scenarios where a fixed pre-defined schema is not available and we would like the method to construct a high-quality KG with a succinct self-generated schema. To address these problems, we propose a three-phase framework named Extract-Define-Canonicalize (EDC): open information extraction followed by schema definition and post-hoc canonicalization. EDC is flexible in that it can be applied to settings where a pre-defined target schema is available and when it is not; in the latter case, it constructs a schema automatically and applies self-canonicalization. To further improve performance, we introduce a trained component that retrieves schema elements relevant to the input text; this improves the LLMs‚Äô extraction performance in a retrieval-augmented generation-like manner. We demonstrate on three KGC benchmarks that EDC is able to extract high-quality triplets without any parameter tuning and with significantly larger schemas compared to prior works. Code for EDC is available at https://github.com/clear-nus/edc.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-04-05",
    "authors": [
      {
        "authorId": "2295607413",
        "name": "Bowen Zhang"
      },
      {
        "authorId": "2295511978",
        "name": "Harold Soh"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "6baf554c223cb4eea666efa2d1eb507a60823808",
    "url": "https://www.semanticscholar.org/paper/6baf554c223cb4eea666efa2d1eb507a60823808",
    "title": "Eliminating Position Bias of Language Models: A Mechanistic Approach",
    "abstract": "Position bias has proven to be a prevalent issue of modern language models (LMs), where the models prioritize content based on its position within the given context. This bias often leads to unexpected model failures and hurts performance, robustness, and reliability across various applications. Our mechanistic analysis attributes the position bias to two components employed in nearly all state-of-the-art LMs: causal attention and relative positional encodings. Based on the analyses, we propose to eliminate position bias (e.g., different retrieved documents' orders in QA affect performance) with a training-free zero-shot approach. Our method changes the causal attention to bidirectional attention between documents and utilizes model attention values to decide the relative orders of documents instead of using the order provided in input prompts, therefore enabling Position-INvariant inferencE (PINE) at the document level. By eliminating position bias, models achieve better performance and reliability in downstream tasks, including LM-as-a-judge, retrieval-augmented QA, molecule generation, and math reasoning. Notably, PINE is especially useful when adapting LMs for evaluating reasoning pairs: it consistently provides 8 to 10 percentage points performance gains, making Llama-3-70B-Instruct perform even better than GPT-4-0125-preview and GPT-4o-2024-08-06 on the RewardBench reasoning set.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 5,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-07-01",
    "authors": [
      {
        "authorId": "2255392818",
        "name": "Ziqi Wang"
      },
      {
        "authorId": "2119078297",
        "name": "Hanlin Zhang"
      },
      {
        "authorId": "2118053386",
        "name": "Xiner Li"
      },
      {
        "authorId": "2295786180",
        "name": "Kuan-Hao Huang"
      },
      {
        "authorId": "2118642562",
        "name": "Chi Han"
      },
      {
        "authorId": "2279225650",
        "name": "Shuiwang Ji"
      },
      {
        "authorId": "144695232",
        "name": "S. Kakade"
      },
      {
        "authorId": "2288239343",
        "name": "Hao Peng"
      },
      {
        "authorId": "2290907632",
        "name": "Heng Ji"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.87639203842082
  },
  {
    "paperId": "22fe8270649b421a1c5b0b9f64793e345b7167ce",
    "url": "https://www.semanticscholar.org/paper/22fe8270649b421a1c5b0b9f64793e345b7167ce",
    "title": "ChatGPT and assistive AI in structured radiology reporting: A systematic review.",
    "abstract": "INTRODUCTION\nThe rise of transformer-based large language models (LLMs), such as ChatGPT, has captured global attention with recent advancements in artificial intelligence (AI). ChatGPT demonstrates growing potential in structured radiology reporting-a field where AI has traditionally focused on image analysis.\n\n\nMETHODS\nA comprehensive search of MEDLINE and Embase was conducted from inception through May 2024, and primary studies discussing ChatGPT's role in structured radiology reporting were selected based on their content.\n\n\nRESULTS\nOf the 268 articles screened, eight were ultimately included in this review. These articles explored various applications of ChatGPT, such as generating structured reports from unstructured reports, extracting data from free text, generating impressions from radiology findings and creating structured reports from imaging data. All studies demonstrated optimism regarding ChatGPT's potential to aid radiologists, though common critiques included data privacy concerns, reliability, medical errors, and lack of medical-specific training.\n\n\nCONCLUSION\nChatGPT and assistive AI have significant potential to transform radiology reporting, enhancing accuracy and standardization while optimizing healthcare resources. Future developments may involve integrating dynamic few-shot prompting, ChatGPT, and Retrieval Augmented Generation (RAG) into diagnostic workflows. Continued research, development, and ethical oversight are crucial to fully realize AI's potential in radiology.",
    "venue": "Current problems in diagnostic radiology",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-07-01",
    "authors": [
      {
        "authorId": "2275375851",
        "name": "Ethan Sacoransky"
      },
      {
        "authorId": "2310750180",
        "name": "B. Y. Kwan"
      },
      {
        "authorId": "2310739992",
        "name": "Donald Soboleski"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "3a559e36d340c740bf28a84d0b99d537be4f24e0",
    "url": "https://www.semanticscholar.org/paper/3a559e36d340c740bf28a84d0b99d537be4f24e0",
    "title": "Evaluating the OpenAI‚Äôs GPT-3.5 Turbo‚Äôs performance in extracting information from scientific articles on diabetic retinopathy",
    "abstract": null,
    "venue": "Systematic Reviews",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "https://systematicreviewsjournal.biomedcentral.com/counter/pdf/10.1186/s13643-024-02523-2",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "LettersAndComments",
      "JournalArticle"
    ],
    "publicationDate": "2024-05-16",
    "authors": [
      {
        "authorId": "2301462042",
        "name": "Celeste Ci Ying Gue"
      },
      {
        "authorId": "2301462146",
        "name": "Noorul Dharajath Abdul Rahim"
      },
      {
        "authorId": "2003772263",
        "name": "William Rojas-Carabali"
      },
      {
        "authorId": "2269223446",
        "name": "Rupesh Agrawal"
      },
      {
        "authorId": "2301483217",
        "name": "Palvannan Rk"
      },
      {
        "authorId": "2272990303",
        "name": "J. Abisheganaden"
      },
      {
        "authorId": "2239175408",
        "name": "W. Yip"
      }
    ],
    "source": "semantic_scholar",
    "score": 74.1415686865115
  },
  {
    "paperId": "4b2fc4597b1519207fd63c2b2b33a4665c988de7",
    "url": "https://www.semanticscholar.org/paper/4b2fc4597b1519207fd63c2b2b33a4665c988de7",
    "title": "DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models",
    "abstract": "Knowledge-intensive language understanding tasks require Language Models (LMs) to integrate relevant context, mitigating their inherent weaknesses, such as incomplete or outdated knowledge. However, conflicting knowledge can be present in the LM's parameters, termed intra-memory conflict, which can affect a model's propensity to accept contextual knowledge. To study the effect of intra-memory conflict on an LM's ability to accept relevant context, we utilize two knowledge conflict measures and a novel dataset containing inherently conflicting data, DynamicQA. This dataset includes facts with a temporal dynamic nature where facts can change over time and disputable dynamic facts, which can change depending on the viewpoint. DynamicQA is the first to include real-world knowledge conflicts and provide context to study the link between the different types of knowledge conflicts. We also evaluate several measures on their ability to reflect the presence of intra-memory conflict: semantic entropy and a novel coherent persuasion score. With our extensive experiments, we verify that LMs exhibit a greater degree of intra-memory conflict with dynamic facts compared to facts that have a single truth value. Furthermore, we reveal that facts with intra-memory conflict are harder to update with context, suggesting that retrieval-augmented generation will struggle with the most commonly adapted facts.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2407.17023",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-07-24",
    "authors": [
      {
        "authorId": "2284760365",
        "name": "Sara Vera Marjanovi'c"
      },
      {
        "authorId": "2298957995",
        "name": "Haeun Yu"
      },
      {
        "authorId": "145676297",
        "name": "Pepa Atanasova"
      },
      {
        "authorId": "1954475",
        "name": "Maria Maistro"
      },
      {
        "authorId": "2276440036",
        "name": "Christina Lioma"
      },
      {
        "authorId": "1736067",
        "name": "Isabelle Augenstein"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "e7a167996570e08ce4d006c8d674ea30ea62a868",
    "url": "https://www.semanticscholar.org/paper/e7a167996570e08ce4d006c8d674ea30ea62a868",
    "title": "PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering",
    "abstract": "Existing work on Temporal Question Answering (TQA) has predominantly focused on questions anchored to specific timestamps or events (e.g.\"Who was the US president in 1970?\"). Little work has studied questions whose temporal context is relative to the present time (e.g.\"Who was the previous US president?\"). We refer to this problem as Present-Anchored Temporal QA (PATQA). PATQA poses unique challenges: (1) large language models (LLMs) may have outdated knowledge, (2) complex temporal relationships (e.g. 'before', 'previous') are hard to reason, (3) multi-hop reasoning may be required, and (4) the gold answers of benchmarks must be continuously updated. To address these challenges, we introduce the PAT-Questions benchmark, which includes single and multi-hop temporal questions. The answers in PAT-Questions can be automatically refreshed by re-running SPARQL queries on a knowledge graph, if available. We evaluate several state-of-the-art LLMs and a SOTA temporal reasoning model (TEMPREASON-T5) on PAT-Questions through direct prompting and retrieval-augmented generation (RAG). The results highlight the limitations of existing solutions in PATQA and motivate the need for new methods to improve PATQA reasoning capabilities.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2024,
    "citationCount": 4,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-02-16",
    "authors": [
      {
        "authorId": "2040643555",
        "name": "Jannat Ara Meem"
      },
      {
        "authorId": "2072549229",
        "name": "Muhammad Shihab Rashid"
      },
      {
        "authorId": "2284691738",
        "name": "Yue Dong"
      },
      {
        "authorId": "1754970",
        "name": "Vagelis Hristidis"
      }
    ],
    "source": "semantic_scholar",
    "score": 94.1415686865115
  },
  {
    "paperId": "b4821e1e6627f7369e2e94b6d66a43920400d943",
    "url": "https://www.semanticscholar.org/paper/b4821e1e6627f7369e2e94b6d66a43920400d943",
    "title": "Synthetic continued pretraining",
    "abstract": "Pretraining on large-scale, unstructured internet text enables language models to acquire a significant amount of world knowledge. However, this knowledge acquisition is data-inefficient--to learn a given fact, models must be trained on hundreds to thousands of diverse representations of it. This poses a challenge when adapting a pretrained model to a small corpus of domain-specific documents, where each fact may appear rarely or only once. We propose to bridge this gap with synthetic continued pretraining: using the small domain-specific corpus to synthesize a large corpus more amenable to learning, and then performing continued pretraining on the synthesized corpus. We instantiate this proposal with EntiGraph, a synthetic data augmentation algorithm that extracts salient entities from the source documents and then generates diverse text by drawing connections between the sampled entities. Synthetic continued pretraining with EntiGraph enables a language model to answer questions and follow generic instructions related to the source documents without access to them. If, instead, the source documents are available at inference time, we show that the knowledge acquired through our approach compounds with retrieval-augmented generation. To better understand these results, we build a simple mathematical model of EntiGraph, and show how synthetic data augmentation can\"rearrange\"knowledge to enable more data-efficient learning.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-09-11",
    "authors": [
      {
        "authorId": "2283514777",
        "name": "Zitong Yang"
      },
      {
        "authorId": "2294359410",
        "name": "Neil Band"
      },
      {
        "authorId": "2320941550",
        "name": "Shuangping Li"
      },
      {
        "authorId": "2283307289",
        "name": "Emmanuel J. Candes"
      },
      {
        "authorId": "2294362683",
        "name": "Tatsunori Hashimoto"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "11bdbfe785b874fab4232c394e2d3a5aa389286a",
    "url": "https://www.semanticscholar.org/paper/11bdbfe785b874fab4232c394e2d3a5aa389286a",
    "title": "Towards Explainable Network Intrusion Detection using Large Language Models",
    "abstract": "Large Language Models (LLMs) have revolutionised natural language processing tasks, particularly as chat agents. However, their applicability to threat detection problems remains unclear. This paper examines the feasibility of employing LLMs as a Network Intrusion Detection System (NIDS), despite their high computational requirements, primarily for the sake of explainability. Furthermore, considerable resources have been invested in developing LLMs, and they may offer utility for NIDS. Current state-of-the-art NIDS rely on artificial benchmarking datasets, resulting in skewed performance when applied to real-world networking environments. Therefore, we compare the GPT-4 and LLama3 models against traditional architectures and transformer-based models to assess their ability to detect malicious NetFlows without depending on artificially skewed datasets, but solely on their vast pre-trained acquired knowledge. Our results reveal that, although LLMs struggle with precise attack detection, they hold significant potential for a path towards explainable NIDS. Our preliminary exploration shows that LLMs are unfit for the detection of Malicious NetFlows. Most promisingly, however, these exhibit significant potential as complementary agents in NIDS, particularly in providing explanations and aiding in threat response when integrated with Retrieval Augmented Generation (RAG) and function calling capabilities.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-08-08",
    "authors": [
      {
        "authorId": "2289452583",
        "name": "Paul R. B. Houssel"
      },
      {
        "authorId": "2315935308",
        "name": "Priyanka Singh"
      },
      {
        "authorId": "3242014",
        "name": "S. Layeghy"
      },
      {
        "authorId": "2105726298",
        "name": "Marius Portmann"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "92f11e0091f81d5f4807abcbc39eb5c796034edc",
    "url": "https://www.semanticscholar.org/paper/92f11e0091f81d5f4807abcbc39eb5c796034edc",
    "title": "Applications of LLMs for Generating Cyber Security Exercise Scenarios",
    "abstract": "This study proposes a novel approach leveraging Large Language Models (LLMs) to generate dynamic and complex adaptable cybersecurity exercise scenarios. Motivated by Turing‚Äôs seminal exploration into machine cognition, which questions the ability of machines to mimic human thought and intelligence. By exploiting the generative potential of LLMs, our methodology simulates a wide range of cyber threats, both known and novel, thereby enhancing cybersecurity training and awareness. This approach transforms the potential for ‚Äòhallucination‚Äô inherent in LLMs into a potential advantage, enabling the creation of complex exercise scenarios that push the boundaries of traditional cybersecurity training. The innovation lies in the sophisticated application of AI, aiming to advance the preparedness of security professionals against diverse cyber threats. The scenarios generated through this method were subject to meticulous testing and a rigorous evaluation process involving (Generated Pre-Trained Transformer) GPT models and expert review to ensure their realism and applicability. In this paper, we introduce ‚ÄòCyExec,‚Äô a novel approach leveraging GPT to dynamically generate cybersecurity training scenarios. Furthermore, the prompts provided to the LLMs were meticulously designed to adopt a Retrieval-Augmented Generation (RAG) approach, enriching the complexity and relevance of the scenarios. This incorporation of RAG, alongside the inspiration drawn from Turing‚Äôs exploration of machine intelligence, showcases an advanced application of AI in cybersecurity training, reflecting a deep understanding of how machines can augment our capabilities to anticipate and mitigate cyber threats.",
    "venue": "IEEE Access",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2310530710",
        "name": "Muhammad Mudassar Yamin"
      },
      {
        "authorId": "2292842024",
        "name": "Ehtesham Hashmi"
      },
      {
        "authorId": "2310503904",
        "name": "Mohib Ullah"
      },
      {
        "authorId": "1809786",
        "name": "Basel Katt"
      }
    ],
    "source": "semantic_scholar",
    "score": 96.79441541679836
  },
  {
    "paperId": "a74f781c561347408b0a5ef485aa9f1cac57c4b4",
    "url": "https://www.semanticscholar.org/paper/a74f781c561347408b0a5ef485aa9f1cac57c4b4",
    "title": "Generative AI in the Construction Industry: A State-of-the-art Analysis",
    "abstract": "The construction industry is a vital sector of the global economy, but it faces many productivity challenges in various processes, such as design, planning, procurement, inspection, and maintenance. Generative artificial intelligence (AI), which can create novel and realistic data or content, such as text, image, video, or code, based on some input or prior knowledge, offers innovative and disruptive solutions to address these challenges. However, there is a gap in the literature on the current state, opportunities, and challenges of generative AI in the construction industry. This study aims to fill this gap by providing a state-of-the-art analysis of generative AI in construction, with three objectives: (1) to review and categorize the existing and emerging generative AI opportunities and challenges in the construction industry; (2) to propose a framework for construction firms to build customized generative AI solutions using their own data, comprising steps such as data collection, dataset curation, training custom large language model (LLM), model evaluation, and deployment; and (3) to demonstrate the framework via a case study of developing a generative model for querying contract documents. The results show that retrieval augmented generation (RAG) improves the baseline LLM by 5.2, 9.4, and 4.8% in terms of quality, relevance, and reproducibility. This study provides academics and construction professionals with a comprehensive analysis and practical framework to guide the adoption of generative AI techniques to enhance productivity, quality, safety, and sustainability across the construction industry.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2024-02-15",
    "authors": [
      {
        "authorId": "2186491295",
        "name": "Ridwan Taiwo"
      },
      {
        "authorId": "2035680831",
        "name": "I. T. Bello"
      },
      {
        "authorId": "123466946",
        "name": "S. Abdulai"
      },
      {
        "authorId": "2212738159",
        "name": "Abdul-Mugis Yussif"
      },
      {
        "authorId": "101298311",
        "name": "B. Salami"
      },
      {
        "authorId": "2264882246",
        "name": "Abdullahi Saka"
      },
      {
        "authorId": "2242616006",
        "name": "Tarek Zayed"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "4a7d232ff1a806b257a93e31893319b1cd869cb0",
    "url": "https://www.semanticscholar.org/paper/4a7d232ff1a806b257a93e31893319b1cd869cb0",
    "title": "Semantic interlinking of Immigration Data using LLMs for Knowledge Graph Construction",
    "abstract": "The challenge of managing immigration data is exacerbated by its reliance on paper-based, evidence-driven records maintained by legal professionals, creating obstacles for efficient processing and analysis due to inherent trust issues with AI-based systems. This paper introduces a cutting-edge framework to surmount these hurdles by synergizing Large Language Models (LLMs) with Knowledge Graphs (KGs), revolutionizing traditional data handling methods. Our method transforms archaic, paper-based immigration records into a structured, interconnected knowledge network that intricately mirrors the legal and procedural nuances of immigration, ensuring a dynamic and trustworthy platform for data analysis. Utilizing LLMs, we extract vital entities and relationships from diverse legal documents to forge a comprehensive knowledge graph, encapsulating the complex legalities and procedural disparities in immigration processes and mapping the multifaceted interactions among stakeholders like applicants, sponsors, and legal experts. This graph not only facilitates a deep dive into the legal stipulations but also incorporates them, significantly boosting the system's reliability and precision. With the integration of Retrieval Augmented Generation (RAG) for exact, context-aware data retrieval and Augmented Knowledge Creation for developing a conversational interface via LLMs, our framework offers a scalable, adaptable solution to immigration data management. This innovative amalgamation of LLMs, KGs, and RAG techniques marks a paradigm shift towards more informed, efficient, and trustworthy decision-making in the sphere of global migration, setting a new benchmark for legal technology and data source management.",
    "venue": "The Web Conference",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3589335.3651557",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2024-05-13",
    "authors": [
      {
        "authorId": "2226521660",
        "name": "Radhakrishnan Venkatakrishnan"
      },
      {
        "authorId": "2301212268",
        "name": "Emrah Tanyildizi"
      },
      {
        "authorId": "3456930",
        "name": "M. A. Canbaz"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "a52ae36c6eedbd67e2214c6d8861e4f54bc00763",
    "url": "https://www.semanticscholar.org/paper/a52ae36c6eedbd67e2214c6d8861e4f54bc00763",
    "title": "Large language models as a diagnostic support tool in neuropathology",
    "abstract": "Abstract The WHO guidelines for classifying central nervous system (CNS) tumours are changing considerably with each release. The classification of CNS tumours is uniquely complex among most other solid tumours as it incorporates not just morphology, but also genetic and epigenetic features. Keeping current with these changes across medical fields can be challenging, even for clinical specialists. Large language models (LLMs) have demonstrated their ability to parse and process complex medical text, but their utility in neuro‚Äêoncology has not been systematically tested. We hypothesised that LLMs can effectively diagnose neuro‚Äêoncology cases from free‚Äêtext histopathology reports according to the latest WHO guidelines. To test this hypothesis, we evaluated the performance of ChatGPT‚Äê4o, Claude‚Äê3.5‚Äêsonnet, and Llama3 across 30 challenging neuropathology cases, which each presented a complex mix of morphological and genetic information relevant to the diagnosis. Furthermore, we integrated these models with the latest WHO guidelines through Retrieval‚ÄêAugmented Generation (RAG) and again assessed their diagnostic accuracy. Our data show that LLMs equipped with RAG, but not without RAG, can accurately diagnose the neuropathological tumour subtype in 90% of the tested cases. This study lays the groundwork for a new generation of computational tools that can assist neuropathologists in their daily reporting practice.",
    "venue": "The Journal of Pathology: Clinical Research",
    "year": 2024,
    "citationCount": 3,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-11-01",
    "authors": [
      {
        "authorId": "2065628669",
        "name": "K. Hewitt"
      },
      {
        "authorId": "2290247593",
        "name": "I. Wiest"
      },
      {
        "authorId": "6417799",
        "name": "Zunamys I. Carrero"
      },
      {
        "authorId": "2260938918",
        "name": "L. Bejan"
      },
      {
        "authorId": "2274758809",
        "name": "Thomas O Millner"
      },
      {
        "authorId": "2243077828",
        "name": "Sebastian Brandner"
      },
      {
        "authorId": "2262022936",
        "name": "J. N. Kather"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "dae177115f306b7005dde10acbef047b9f20d821",
    "url": "https://www.semanticscholar.org/paper/dae177115f306b7005dde10acbef047b9f20d821",
    "title": "UniOQA: A Unified Framework for Knowledge Graph Question Answering with Large Language Models",
    "abstract": "OwnThink stands as the most extensive Chinese open-domain knowledge graph introduced in recent times. Despite prior attempts in question answering over OwnThink (OQA), existing studies have faced limitations in model representation capabilities, posing challenges in further enhancing overall accuracy in question answering. In this paper, we introduce UniOQA, a unified framework that integrates two complementary parallel workflows. Unlike conventional approaches, UniOQA harnesses large language models (LLMs) for precise question answering and incorporates a direct-answer-prediction process as a cost-effective complement. Initially, to bolster representation capacity, we fine-tune an LLM to translate questions into the Cypher query language (CQL), tackling issues associated with restricted semantic understanding and hallucinations. Subsequently, we introduce the Entity and Relation Replacement algorithm to ensure the executability of the generated CQL. Concurrently, to augment overall accuracy in question answering, we further adapt the Retrieval-Augmented Generation (RAG) process to the knowledge graph. Ultimately, we optimize answer accuracy through a dynamic decision algorithm. Experimental findings illustrate that UniOQA notably advances SpCQL Logical Accuracy to 21.2% and Execution Accuracy to 54.9%, achieving the new state-of-the-art results on this benchmark. Through ablation experiments, we delve into the superior representation capacity of UniOQA and quantify its performance breakthrough.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 2,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-06-04",
    "authors": [
      {
        "authorId": "2304770576",
        "name": "Zhuoyang Li"
      },
      {
        "authorId": "2305352216",
        "name": "Liran Deng"
      },
      {
        "authorId": "2146672523",
        "name": "Hui Liu"
      },
      {
        "authorId": "2304939653",
        "name": "Qiaoqiao Liu"
      },
      {
        "authorId": "1683723",
        "name": "Junzhao Du"
      }
    ],
    "source": "semantic_scholar",
    "score": 86.47918433002164
  },
  {
    "paperId": "629f44f5fb78ec390ef66633dc627f1d04f3eb85",
    "url": "https://www.semanticscholar.org/paper/629f44f5fb78ec390ef66633dc627f1d04f3eb85",
    "title": "Knowledge Graph Prompting for Multi-Document Question Answering",
    "abstract": "The `pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or document structural relations. For graph traversal, we design an LLM-based graph traversal agent that navigates across nodes and gathers supporting passages assisting LLMs in MD-QA. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the graph traversal agent acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality. Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the potential of leveraging graphs in enhancing the prompt design and retrieval augmented generation for LLMs. Our code: https://github.com/YuWVandy/KG-LLM-MDQA.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2023,
    "citationCount": 68,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.11730",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-08-22",
    "authors": [
      {
        "authorId": "2153607948",
        "name": "Yu Wang"
      },
      {
        "authorId": "1793409",
        "name": "Nedim Lipka"
      },
      {
        "authorId": "2066337266",
        "name": "Ryan A. Rossi"
      },
      {
        "authorId": "2233085914",
        "name": "Alexa F. Siu"
      },
      {
        "authorId": "2283147661",
        "name": "Ruiyi Zhang"
      },
      {
        "authorId": "12524628",
        "name": "Tyler Derr"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.5115975689589
  },
  {
    "paperId": "8ac7df2d11170b0777b2d913d4a4b4887e127731",
    "url": "https://www.semanticscholar.org/paper/8ac7df2d11170b0777b2d913d4a4b4887e127731",
    "title": "A Comprehensive Evaluation of GPT-4V on Knowledge-Intensive Visual Question Answering",
    "abstract": "The emergence of multimodal large models (MLMs) has significantly advanced the field of visual understanding, offering remarkable capabilities in the realm of visual question answering (VQA). Yet, the true challenge lies in the domain of knowledge-intensive VQA tasks, which necessitate not just recognition of visual elements, but also a deep comprehension of the visual information in conjunction with a vast repository of learned knowledge. To uncover such capabilities of MLMs, particularly the newly introduced GPT-4V and Gemini, we provide an in-depth evaluation from three perspectives: 1) Commonsense Knowledge, which assesses how well models can understand visual cues and connect to general knowledge; 2) Fine-grained World Knowledge, which tests the model's skill in reasoning out specific knowledge from images, showcasing their proficiency across various specialized fields; 3) Comprehensive Knowledge with Decision-making Rationales, which examines model's capability to provide logical explanations for its inference, facilitating a deeper analysis from the interpretability perspective. Additionally, we utilize a visual knowledge-enhanced training strategy and multimodal retrieval-augmented generation approach to enhance MLMs, highlighting the future need for advancements in this research direction. Extensive experiments indicate that: a) GPT-4V demonstrates enhanced explanation generation when using composite images as few-shots; b) GPT-4V and other MLMs produce severe hallucinations when dealing with world knowledge; c) Visual knowledge enhanced training and prompting technicals present potential to improve performance. Codes: https://github.com/HITsz-TMG/Cognitive-Visual-Language-Mapper",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 20,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-11-13",
    "authors": [
      {
        "authorId": "2118046679",
        "name": "Yunxin Li"
      },
      {
        "authorId": "2111542852",
        "name": "Longyue Wang"
      },
      {
        "authorId": "33968873",
        "name": "Baotian Hu"
      },
      {
        "authorId": "2266425853",
        "name": "Xinyu Chen"
      },
      {
        "authorId": "2266437328",
        "name": "Wanqi Zhong"
      },
      {
        "authorId": "2266387313",
        "name": "Chenyang Lyu"
      },
      {
        "authorId": "2258690227",
        "name": "Min Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 115.66783656585135
  },
  {
    "paperId": "d566d6859e22cfdcd59d735f6865fb5f0b50f3c9",
    "url": "https://www.semanticscholar.org/paper/d566d6859e22cfdcd59d735f6865fb5f0b50f3c9",
    "title": "Chainpoll: A high efficacy method for LLM hallucination detection",
    "abstract": "Large language models (LLMs) have experienced notable advancements in generating coherent and contextually relevant responses. However, hallucinations - incorrect or unfounded claims - are still prevalent, prompting the creation of automated metrics to detect these in LLM outputs. Our contributions include: introducing ChainPoll, an innovative hallucination detection method that excels compared to its counterparts, and unveiling RealHall, a refined collection of benchmark datasets to assess hallucination detection metrics from recent studies. While creating RealHall, we assessed tasks and datasets from previous hallucination detection studies and observed that many are not suitable for the potent LLMs currently in use. Overcoming this, we opted for four datasets challenging for modern LLMs and pertinent to real-world scenarios. Using RealHall, we conducted a comprehensive comparison of ChainPoll with numerous hallucination metrics from recent studies. Our findings indicate that ChainPoll outperforms in all RealHall benchmarks, achieving an overall AUROC of 0.781. This surpasses the next best theoretical method by 11% and exceeds industry standards by over 23%. Additionally, ChainPoll is cost-effective and offers greater transparency than other metrics. We introduce two novel metrics to assess LLM hallucinations: Adherence and Correctness. Adherence is relevant to Retrieval Augmented Generation workflows, evaluating an LLM's analytical capabilities within given documents and contexts. In contrast, Correctness identifies logical and reasoning errors.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 18,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-22",
    "authors": [
      {
        "authorId": "2262445823",
        "name": "Robert Friel"
      },
      {
        "authorId": "2123005528",
        "name": "Atindriyo Sanyal"
      }
    ],
    "source": "semantic_scholar",
    "score": 114.1665846874966
  },
  {
    "paperId": "8e6c4425e48b09d64827c64d8de0008f41f9be54",
    "url": "https://www.semanticscholar.org/paper/8e6c4425e48b09d64827c64d8de0008f41f9be54",
    "title": "Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large Language Model",
    "abstract": "Integrating large language models (LLMs) into healthcare holds great potential but faces challenges. Pre-training LLMs from scratch for domains like medicine is resource-heavy and often unfeasible. On the other hand, sole reliance on Supervised Fine-tuning (SFT) can result in overconfident predictions and may not tap into domain-specific insights. In response, we present a multi-stage training method combining Domain-specific Continued Pre-training (DCPT), SFT, and Direct Preference Optimization (DPO). In addition, we publish a 3Gb Chinese Medicine (ChiMed) dataset, encompassing medical question answering, plain texts, knowledge graphs, and dialogues, segmented into three training stages. The medical LLM trained with our pipeline, Qilin-Med, shows substantial performance improvement. In the CPT and SFT phases, Qilin-Med achieved 38.4% and 40.0% accuracy on the CMExam test set, respectively. It outperformed the basemodel Baichuan-7B (accuracy: 33.5%), by 7.5%. In the DPO phase, it scored 16.66 in BLEU-1 and 27.44 in ROUGE-1 on the Huatuo-26M test set, bringing further improvement to the SFT phase (12.69 in BLEU-1 and 24.21 in ROUGE-1). Additionally, we have further enhanced the model's performance through the Retrieval Augmented Generation (RAG) approach. Experiments demonstrate that Qilin-Med-RAG achieves an accuracy rate of 42.8% on CMExam. These results highlight the contribution of our novel training approach in building LLMs for medical applications.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 16,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.09089",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-13",
    "authors": [
      {
        "authorId": "2258550058",
        "name": "Qichen Ye"
      },
      {
        "authorId": "2218869839",
        "name": "Junling Liu"
      },
      {
        "authorId": "52290752",
        "name": "Dading Chong"
      },
      {
        "authorId": "1800462890",
        "name": "Peilin Zhou"
      },
      {
        "authorId": "2147311343",
        "name": "Y. Hua"
      },
      {
        "authorId": "2170752745",
        "name": "Andrew Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 112.49820016084324
  },
  {
    "paperId": "bcc2b5a5b3dc9b07006a71a207f26e21e178beac",
    "url": "https://www.semanticscholar.org/paper/bcc2b5a5b3dc9b07006a71a207f26e21e178beac",
    "title": "Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding",
    "abstract": "Conversational AI systems such as Alexa need to understand defective queries to ensure robust conversational understanding and reduce user friction. These defective queries often arise from user ambiguities, mistakes, or errors in automatic speech recognition (ASR) and natural language understanding (NLU). Personalized query rewriting is an approach that focuses on reducing defects in queries by taking into account the user's individual behavior and preferences. It typically relies on an index of past successful user interactions with the conversational AI. However, unseen interactions within the user's history present additional challenges for personalized query rewriting. This paper presents our\"Collaborative Query Rewriting\"approach, which specifically addresses the task of rewriting new user interactions that have not been previously observed in the user's history. This approach builds a\"User Feedback Interaction Graph\"(FIG) of historical user-entity interactions and leverages multi-hop graph traversal to enrich each user's index to cover future unseen defective queries. The enriched user index is called a Collaborative User Index and contains hundreds of additional entries. To counteract precision degradation from the enlarged index, we add additional transformer layers to the L1 retrieval model and incorporate graph-based and guardrail features into the L2 ranking model. Since the user index can be pre-computed, we further investigate the utilization of a Large Language Model (LLM) to enhance the FIG for user-entity link prediction in the Video/Music domains. Specifically, this paper investigates the Dolly-V2 7B model. We found that the user index augmented by the fine-tuned Dolly-V2 generation significantly enhanced the coverage of future unseen user interactions, thereby boosting QR performance on unseen queries compared with the graph traversal only approach.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 9,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.14449",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-23",
    "authors": [
      {
        "authorId": "2141144864",
        "name": "Zheng Chen"
      },
      {
        "authorId": "2112347577",
        "name": "Ziyan Jiang"
      },
      {
        "authorId": "47829900",
        "name": "Fan Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "e9743a2bf9311ddaf07eda1ba501665d630bc0d6",
    "url": "https://www.semanticscholar.org/paper/e9743a2bf9311ddaf07eda1ba501665d630bc0d6",
    "title": "A comparison of SIFT, PCA-SIFT and SURF",
    "abstract": "This paper compares three robust feature detection methods, they are, Scale Invariant Feature Transform (SIFT), Principal Component Analysis (PCA) -SIFT and Speeded Up Robust Features (SURF). Lowe presented SIFT [1], which was successfully used in recognition, stitching and many other applications because of its robustness. Yan Ke [2] gave a change of SIFT by using PCA to normalize the gradient patch instead of histogram. H. Bay [3] presented a faster method for SURF, which used Fast-Hessian detector. The performance of the three methods is compared for scale changes, rotation , blur, illumination changes and affine transformations, all of which uses repeatability as an evaluation measurement. Additionally, RANSAC is used to reject the inconsistent matches [4]. SIFT presents its stability in most situation except rotation and illumination changes. SURF is the fastest one with good performance as the same as SIFT, PCASIFT shows its advantages in rotation, blur and illumination changes. CITED BY (442) 1 Han, C. H. (2010). Reduced Dimensional SURF Based Hand Gesture Recognition. 2 Su, C. R. (2014). Unsupervised Image Segmentation by Dual Morphological Operations and Peer-toPeer Content-Based Image Retrieval Applications (Doctoral dissertation). 3 Hu, H. H. (2013). Smartphone Positioning with Street View Image Database. 4 FlÔøΩrez Revuelta, F., & Chaaraoui, A. A. (2012). Interfaz de control domÔøΩtico basada en un sistema de detecciÔøΩn de postura. 5 Idrobo Pizo, G. A. (2014). Projeto de um descritor para o alinhamento de imagens de profundidade de superfÔøΩcies com aplicaÔøΩÔøΩo em visÔøΩo robÔøΩtica. 6 Einrichtung, B., & fÔøΩr Lehrerbildung, Z. OPUS-Passau. 7 Rhee, S. B. (2013). Efficient Image Stitching Using Fast Feature Descriptor Extraction and Matching. KIPS Transactions on Software and Data Engineering, 2(1), 65-70. 8 de Souza Tarallo, A., da Silva, F. A., Hiraga, A. K., de Paiva, M. S. V., & de Castro Jorge, L. A. Mosaicos de Imagens AÔøΩreas Sequenciais ConstruÔøΩdos Automaticamente. 9 Valenzuela, R. E., Schwartz, W. R., & Pedrini, H. ReduÔøΩÔøΩo de Dimensionalidade Aplicada ÔøΩ DescriÔøΩÔøΩo de CaracterÔøΩsticas Visuais. 10 COMITÔøΩ, Y. A. P. E. S. Eduardo Quintana Contreras. 11 Augereau, O. (2013). Reconnaissance et classification d'images de documents (Doctoral dissertation, UniversitÔøΩ Sciences et Technologies-Bordeaux I). 12 de Souza Tarallo, A., Hiraga, A. K., Martinez, G. A. G., de Paiva, M. S. V., de Castro Jorge, L. A., & Senger, H. Uso de mosaico de imagens aÔøΩreas como ferramenta de auxÔøΩlio ao diagnÔøΩstico de diversas culturas*. 13 GALLA, H. Z. (2015). PENGELOMPOKAN CITRA RAMBU LALU LINTAS DENGAN HIERARCHICAL AGGLOMERATIVE CLUSTERING BERBASIS SCALE INVARIANT FEATURE TRANSFORM. Skripsi, Fakultas Ilmu Komputer. MANUSCRIPT AUTHORS Miss Luo Juan Korea South qiuhehappy@hotmail.com Dr. Oubong Gwun Computer Graphics Lab, Chonbuk National University, Jeonju 561-756, South Korea South Korea A comparison of sift, pca-sift and surf 14 Renkens, I. M. (2015). Prometheus: from 2D to 3D. A reconstruction based on photographs (Doctoral dissertation, TU Delft, Delft University of Technology). 15 CIOU, J. J. (2014). Embedded Omni-Directional Wheeled Mobile Robot Visual SLAM based on Depth Image Database. 16 Prieto SÔøΩnchez, J. (2011). Reconocimiento de objetos por visiÔøΩn artificial en entornos controlados. 17 Campos, M. F. M. 2ÔøΩ Lista de ExercÔøΩcios 1 ExercÔøΩcios TeÔøΩricos. 18 Liedtke, A. Realisierung eines robusten Verfahrens zur Identifikation von Positionsmarken unter Verwendung von Bildverarbeitung in Videobildern. 19 Quintana Rosales, M. A. (2015). Registro de una secuencia temporal de nubes de puntos utilizando tecnologÔøΩa Kinect para la reconstrucciÔøΩn tridimensional de material arqueolÔøΩgico. 20 Mauricio, C. J. L., Edgar, F. S., & Samuel, R. H. E. (2012). Sistema MÔøΩvil de VirtualizaciÔøΩn, EdiciÔøΩn y VisualizaciÔøΩn de Objetos 3D (VIAR). 21 de Souza Tarallo, A. (2013). Escola de Engenharia de SÔøΩo Carlos (Doctoral dissertation, Universidade de SÔøΩo Paulo). 22 BOUACHIR, W., & DOCTOR, O. D. D. D. P. (2014). SUIVI DÔøΩOBJETS PAR CARACTÔøΩRISTIQUES LOCALES ENCODANT LA STRUCTURE. 23 Hung, C. Y. (2014). Reasearch and Validation of Application of Adaptive Template Building Technology to Lithography Pattern of Light Emitting Diode Chip. 24 Jatmiko, S. Analisis Dan Implementasi Penggunaan Scale Invariant Feature Transform (SIFT) Pada Sistem Verifikasi Tanda Tangan. 25 Pizarro, C. A. (2014). On the possibility to find coordinates by random features (Doctoral dissertation, Autonomous University of Madrid). 26 Creve, M. Navigatiehulp voor mensen met een visuele beperking. 27 Tarallo, A. D. S., Hiraga, A. K., Martinez, G. A. G., Paiva, M. S. V. D., Jorge, L. A. D. C., & Senger, H. (2013). Parallel Processing Applied to Image Mosaic Generation. In IX Workshop de VisÔøΩo Computacional (WVC 2013). Universidade Federal Fluminense (UFF). 28 Naftalianto, Y. (2012). RancangBangun Sensor Jarak Dengan Korespondensi Citra Dengan Ekstraksi Fitur SURF Dan Konsep Stereo Vision. Jurnal Sarjana Teknik Elektro, 1(1). 29 Meisel, A. Andreas Liedtke Realisierung eines robusten Verfahrens zur Identifikation von Positionsmarken unter Verwendung von Bildverarbeitung in Videobildern. 30 Kim, J. Y., Jeong, S. W., Jeong, M. B., Han, H. J., Kim, J. S., Park, H. M., & Chung, B. H. (2002). Application of total ear canal ablation and Ferreira, A. L. S., Santos, S. R. D., & de Miranda, L. C. (2012, May). TrueSight A Pedestrian Navigation System Based in Automatic Landmark Detection and Extraction on Android Smartphone. In Virtual and Augmente 31 Augereau, O., Journet, N., & Domenger, J. P. (2013). Reconnaissance et extraction de documents. Document numÔøΩrique, 16(2), 91-118. 32 Laura, T. L. (2013). Sistema de supervisÔøΩo aÔøΩrea baseado em navegaÔøΩÔøΩo visual para detecÔøΩÔøΩo de anomalias em instalaÔøΩÔøΩes de petrÔøΩleo e gÔøΩs. 33 Wang, P. H. (2011). Implementation of fast SIFT feature extraction and matching using GPU. 34 Mota, I. F. V. (2014). OlhÔøΩ-passarinho: uma extensÔøΩo do TweeProfiles para fotografias (Doctoral dissertation, MasterÔøΩs thesis, Faculdade de Engenharia da Universidade do Porto). 35 Salamon, N. Z. (2015). Re-identificaÔøΩÔøΩo de pessoas em imagens atravÔøΩs de caracterÔøΩsticas descritivas de cores e grupos. 36 Couto, L., & OsÔøΩrio, F. Auto-LocalizaÔøΩÔøΩo AutÔøΩnoma de RobÔøΩs MÔøΩveis por VisÔøΩo Computacional Baseada em Pontos de ReferÔøΩncia. 37 Shukla, A. P., & Saini, M. (2015). ÔøΩMoving Object Tracking of Vehicle DetectionÔøΩ: A Concise Review. International Journal of Signal Processing, Image Processing and Pattern Recognition, 8(3), 169-176. 38 Aguilar, W. G., & Angulo, C. (2015). Real-Time Model-Based Video Stabilization for Microaerial Vehicles. Neural Processing Letters, 1-19. 39 Solehah, S., Yaakob, S. N., Kadim, Z., & Woon, H. H. (2012, December). Moving object extraction in PTZ camera using the integration of background subtraction and local histogram processing. In Computer Applications and Industrial Electronics (ISCAIE), 2012 IEEE Symposium on (pp. 167-172). IEEE. 40 Xie, Z., Chen, J., Yao, T., & Sun, Y. (2015). Geometric structure-constraint tracking with confident parts. Signal Processing: Image Communication, 36, 43-52. 41 Jayachandran, G., Ekin, A., & De Haan, I. G. (2012). Landmark detection in MR brain images using SURF. 42 Lin, S. C. F., Wong, C. Y., Jiang, G., Rahman, M. A., & Kwok, N. M. (2014). Radial Fourier Analysis (RFA) Descriptor with Fourier-based Keypoint Orientation. International Journal of Image Processing (IJIP), 8(6), 397. 43 Xiong, X., & Choi, B. J. (2013, January). Estimation of Relative Self-Localization Based On Natural Landmark and an Improved SURF. In Proceedings of World Academy of Science, Engineering and Technology (No. 73, p. 900). World Academy of Science, Engineering and Technology (WASET). 44 Xiong, P., Liu, X., Gao, C., Zhou, Z., Gao, C., & Liu, Q. (2013, March). A Real-time Stitching Algorithm for UAV Aerial Images. In Proceedings of the 2nd International Conference on Computer Science and Electronics Engineering. Atlantis Press. 45 Averkin, A. N. (2010). Training image-correlation systems by optimizing their attribute representations. Journal of Optical Technology, 77(11), 712-720. 46 Koel, F., Maurer, R. K., & Prieler, Z. R. A Survey on Action Recognition Using Kinematic Feature Extraction Techniques. 47 Xiong, X., & Choi, B. J. (2013). Estimation of Relative Self-Localization for Indoor Mobile Robot and Its Error Analysis. International Journal of Smart Home, 7(4), 69-76. 48 Lin, C. H., & Chen, A. Y. Trip Characteristics Study through Social Media Data. 49 Wang, Z., & Qureshi, F. Z. (2013, May). Topic models for image localization. In Computer and Robot Vision (CRV), 2013 International Conference on (pp. 136-143). IEEE. 50 Li, X., Aouf, N., & Nemra, A. (2012, September). Estimation analysis in VSLAM for UAV application. In Multisensor Fusion and Integration for Intelligent Systems (MFI), 2012 IEEE Conference on (pp. 365370). IEEE. 51 Jain, S., & Kanwal, N. (2014, November). Overview on image registration. In Medical Imaging, mHealth and Emerging Communication Systems (MedCom), 2014 International Conference on (pp. 376-381). IEEE. 52 Potgieter, M., & Van Niekerk, J. (2013). MULTI-AGENT AUGMENTED COMPUTER VISION TECHNOLOGIES TO SUPPORT HUMAN MONITORING OF SECURE COMPUTING FACILITIES. SAIEE Africa Research Journal, 80. 53 Jurgensen, S. M. (2014). The rotated speeded-up robust features algorithm (R-SURF). NAVAL POSTGRADUATE SCHOOL MONTEREY CA. 54 Singh, K., & Chander, S. (2014). CONTENT BASED IMAGE RETRIEVAL WITH SURF, SVM AND COLOR HISTOGRAM. 55 Issac, A., & Velayutham, C. S. (2012). SaddleSURF: A Saddle Based Interest Point Detector. In Mathematical Modelling and Scientific Computation (pp. 413-420). Springer Berlin Heidelberg. 56 Sharma, B., & Sharma, A. (2015). A REVIEW: CONTENT BASED IMAGE RETRIEVAL WITH SURF AND COLOUR HISTOGRAM USING CROSS VALIDATION AND GRAPH MATCHING ON MEDICAL IMAGES. International Journal, 1(4). 57 Walsh, R., & Hornsby, A. (2011, January). Toward",
    "venue": "",
    "year": 2009,
    "citationCount": 813,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2065916430",
        "name": "Luo Juan"
      },
      {
        "authorId": "2610472",
        "name": "O. Gwun"
      }
    ],
    "source": "semantic_scholar",
    "score": 160.5294054900381
  },
  {
    "paperId": "6d47ab69e344ea332de51b935ce6a3605fe1a091",
    "url": "https://www.semanticscholar.org/paper/6d47ab69e344ea332de51b935ce6a3605fe1a091",
    "title": "Delving into the Digital Twin Developments and Applications in the Construction Industry: A PRISMA Approach",
    "abstract": "Construction 4.0 is witnessing exponential growth in digital twin (DT) technology developments and applications, revolutionizing the adoption of building information modelling (BIM) and other emerging technologies used throughout the built environment lifecycle. BIM provides technologies, procedures, and data schemas representing building components and systems. At the same time, the DT enhances this with real-time data for integrating cyber-physical systems, enabling live asset monitoring and better decision making. Despite being in the early stages of development, DT applications have rapidly progressed in the AEC sector, resulting in a diverse literature landscape due to the various technologies and parameters involved in fully developing the DT technology. The intricate complexities inherent in digital twin advancements have confused professionals and researchers. This confusion arises from the nuanced distinctions between the two technologies, i.e., BIM and DT, causing a convergence that hinders realizing their potential. To address this confusion and lead to a swift development of DT technology, this study provides a holistic review of the existing research focusing on the critical components responsible for developing the applications of DT technology in the construction industry. It highlights five crucial elements: technologies, maturity levels, data layers, enablers, and functionalities. Additionally, it identifies research gaps and proposes future avenues for streamlined DT developments and applications in the AEC sector. Future researchers and practitioners can target data integrity, integration and transmission, bi-directional interoperability, non-technical factors, and data security to achieve mature digital twin applications for AEC practices. This study highlights the growing significance of DTs in construction and provides a foundation for further advancements in this field to harness its potential to transform built environment practices. It also pinpoints the latest developments in AI, namely the large language model (LLM) and retrieval-augmented generation (RAG)‚Äôs implications for DT education, policies, and the construction industry‚Äôs practices.",
    "venue": "Sustainability",
    "year": 2023,
    "citationCount": 8,
    "openAccessPdf": {
      "url": "https://www.mdpi.com/2071-1050/15/23/16436/pdf?version=1701317782",
      "status": "GOLD"
    },
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-11-30",
    "authors": [
      {
        "authorId": "2260925044",
        "name": "Muhammad Afzal"
      },
      {
        "authorId": "2237945815",
        "name": "Rita Yi-man Li"
      },
      {
        "authorId": "2260964801",
        "name": "Muhammad Shoaib"
      },
      {
        "authorId": "2260970625",
        "name": "Muhammad Faisal Ayyub"
      },
      {
        "authorId": "144597886",
        "name": "L. Tagliabue"
      },
      {
        "authorId": "2260972802",
        "name": "Muhammad Bilal"
      },
      {
        "authorId": "2269535261",
        "name": "Habiba Ghafoor"
      },
      {
        "authorId": "2342191191",
        "name": "Otilia Manta"
      }
    ],
    "source": "semantic_scholar",
    "score": 102.95836866004329
  },
  {
    "paperId": "a75130799dc31ff57ec61b774f7ec56b35aec653",
    "url": "https://www.semanticscholar.org/paper/a75130799dc31ff57ec61b774f7ec56b35aec653",
    "title": "Bi-LS-AttM: A Bidirectional LSTM and Attention Mechanism Model for Improving Image Captioning",
    "abstract": "The discipline of automatic image captioning represents an integration of two pivotal branches of artificial intelligence, namely computer vision (CV) and natural language processing (NLP). The principal functionality of this technology lies in transmuting the extracted visual features into semantic information of a higher order. The bidirectional long short-term memory (Bi-LSTM) has garnered wide acceptance in executing image captioning tasks. Of late, scholarly attention has been focused on modifying suitable models for innovative and precise subtitle captions, although tuning the parameters of the model does not invariably yield optimal outcomes. Given this, the current research proposes a model that effectively employs the bidirectional LSTM and attention mechanism (Bi-LS-AttM) for image captioning endeavors. This model exploits the contextual comprehension from both anterior and posterior aspects of the input data, synergistically with the attention mechanism, thereby augmenting the precision of visual language interpretation. The distinctiveness of this research is embodied in its incorporation of Bi-LSTM and the attention mechanism to engender sentences that are both structurally innovative and accurately reflective of the image content. To enhance temporal efficiency and accuracy, this study substitutes convolutional neural networks (CNNs) with fast region-based convolutional networks (Fast RCNNs). Additionally, it refines the process of generation and evaluation of common space, thus fostering improved efficiency. Our model was tested for its performance on Flickr30k and MSCOCO datasets (80 object categories). Comparative analyses of performance metrics reveal that our model, leveraging the Bi-LS-AttM, surpasses unidirectional and Bi-LSTM models. When applied to caption generation and image-sentence retrieval tasks, our model manifests time economies of approximately 36.5% and 26.3% vis-a-vis the Bi-LSTM model and the deep Bi-LSTM model, respectively.",
    "venue": "Applied Sciences",
    "year": 2023,
    "citationCount": 6,
    "openAccessPdf": {
      "url": "https://www.mdpi.com/2076-3417/13/13/7916/pdf?version=1688626164",
      "status": "GOLD"
    },
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-07-06",
    "authors": [
      {
        "authorId": "2057038515",
        "name": "Tian Xie"
      },
      {
        "authorId": "2163787981",
        "name": "Weiping Ding"
      },
      {
        "authorId": "47539583",
        "name": "Jinbao Zhang"
      },
      {
        "authorId": "2221945512",
        "name": "Xusen Wan"
      },
      {
        "authorId": "2146043906",
        "name": "Jiehua Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 109.1886522358297
  },
  {
    "paperId": "ec0ca7eb1f365a8ee74247b9724e190651483706",
    "url": "https://www.semanticscholar.org/paper/ec0ca7eb1f365a8ee74247b9724e190651483706",
    "title": "Chatmap : Large Language Model Interaction with Cartographic Data",
    "abstract": "The swift advancement and widespread availability of foundational Large Language Models (LLMs), complemented by robust fine-tuning methodologies, have catalyzed their adaptation for innovative and industrious applications. Enabling LLMs to recognize and interpret geospatial data, while offering a linguistic access to vast cartographic datasets, is of significant importance. OpenStreetMap (OSM) is the most ambitious open-source global initiative offering detailed urban and rural geographic data, curated by a community of over 10 million contributors, which constitutes a great potential for LLM applications. In this study, we demonstrate the proof of concept and details of the process of fine-tuning a relatively small scale (1B parameters) LLM with a relatively small artificial dataset curated by a more capable teacher model, in order to provide a linguistic interface to the OSM data of an arbitrary urban region. Through this interface, users can inquire about a location's attributes, covering a wide spectrum of concepts, such as its touristic appeal or the potential profitability of various businesses in that vicinity. The study aims to provide an initial guideline for such generative artificial intelligence (AI) adaptations and demonstrate early signs of useful emerging abilities in this context even in minimal computational settings. The embeddings of artificially curated prompts including OSM data are also investigated in detail, which might be instrumental for potential geospatially aware urban Retrieval Augmented Generation (RAG) applications.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 3,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.01429",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-28",
    "authors": [
      {
        "authorId": "34669076",
        "name": "Eren Unlu"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.79441541679836
  },
  {
    "paperId": "8c1aa28c0018fbb3ec270b3ed5e7ee0df6771d33",
    "url": "https://www.semanticscholar.org/paper/8c1aa28c0018fbb3ec270b3ed5e7ee0df6771d33",
    "title": "Multimedia Generative Script Learning for Task Planning",
    "abstract": "Goal-oriented generative script learning aims to generate subsequent steps to reach a particular goal, which is an essential task to assist robots or humans in performing stereotypical activities. An important aspect of this process is the ability to capture historical states visually, which provides detailed information that is not covered by text and will guide subsequent steps. Therefore, we propose a new task, Multimedia Generative Script Learning, to generate subsequent steps by tracking historical states in both text and vision modalities, as well as presenting the first benchmark containing 5,652 tasks and 79,089 multimedia steps. This task is challenging in three aspects: the multimedia challenge of capturing the visual states in images, the induction challenge of performing unseen tasks, and the diversity challenge of covering different information in individual steps. We propose to encode visual state changes through a selective multimedia encoder to address the multimedia challenge, transfer knowledge from previously observed tasks using a retrieval-augmented decoder to overcome the induction challenge, and further present distinct information at each step by optimizing a diversity-oriented contrastive learning objective. We define metrics to evaluate both generation and inductive quality. Experiment results demonstrate that our approach significantly outperforms strong baselines.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2022,
    "citationCount": 9,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2208.12306",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-08-25",
    "authors": [
      {
        "authorId": "1786863",
        "name": "Qingyun Wang"
      },
      {
        "authorId": "2118482058",
        "name": "Manling Li"
      },
      {
        "authorId": "23181435",
        "name": "Hou Pong Chan"
      },
      {
        "authorId": "34170717",
        "name": "Lifu Huang"
      },
      {
        "authorId": "3118681",
        "name": "J. Hockenmaier"
      },
      {
        "authorId": "1733356",
        "name": "Girish V. Chowdhary"
      },
      {
        "authorId": "2113323573",
        "name": "Heng Ji"
      }
    ],
    "source": "semantic_scholar",
    "score": 104.53877639491068
  },
  {
    "paperId": "a0eff8d597f017e494cf289e87fa186ac602f474",
    "url": "https://www.semanticscholar.org/paper/a0eff8d597f017e494cf289e87fa186ac602f474",
    "title": "Detecting high-emitting methane sources in oil/gas fields using satellite observations",
    "abstract": "Abstract. Methane emissions from oil/gas fields originate from a large\nnumber of relatively small and densely clustered point sources. A small fraction of\nhigh-mode emitters can make a large contribution to the total methane emission. Here we\nconduct observation system simulation experiments (OSSEs) to examine the potential of\nrecently launched or planned satellites to detect and locate these high-mode emitters\nthrough measurements of atmospheric methane columns. We simulate atmospheric methane over\na generic oil/gas field (20‚Äì500 production sites of different size categories in a 50√ó50‚Äâkm2 domain) for a 1-week period using the WRF-STILT meteorological model\nwith 1.3√ó1.3‚Äâkm2 horizontal resolution. The\nsimulations consider many random realizations for the occurrence and distribution of\nhigh-mode emitters in the field by sampling bimodal probability density functions (PDFs)\nof emissions from individual sites. The atmospheric methane fields for each realization\nare observed virtually with different satellite and surface observing configurations.\nColumn methane enhancements observed from satellites are small relative to instrument\nprecision, even for high-mode emitters, so an inverse analysis is necessary. We compare\nL1 and L2 regularizations and show that L1 regularization effectively\nprovides sparse solutions for a bimodally distributed variable and enables the retrieval\nof high-mode emitters. We find that the recently launched TROPOMI instrument (low Earth\norbit, 7√ó7‚Äâkm2 nadir pixels, daily return time) and the planned GeoCARB\ninstrument (geostationary orbit, 2.7√ó3.0‚Äâkm2 pixels, 2 times or 4 times\nper day return times) are successful (>‚Äâ80‚Äâ% detection rate,\n<‚Äâ20‚Äâ% false alarm rate) at locating high-emitting sources for fields of\n20‚Äì50 emitters within the 50√ó50‚Äâkm2 domain as long as skies are clear.\nThey are unsuccessful for denser fields. GeoCARB does not benefit significantly from more\nfrequent observations (4 times per day vs. 2 times per day) because of a temporal error\ncorrelation in the inversion, unless under partly cloudy conditions where more frequent\nobservation increases the probability of clear sky. It becomes marginally successful when\nallowing a 5‚Äâkm error tolerance for localization. A next-generation geostationary\nsatellite instrument with 1.3√ó1.3‚Äâkm2 pixels, hourly return time, and\n1‚Äâppb precision can successfully detect and locate the high-mode emitters for a dense\nfield with up to 500¬†sites in the 50√ó50‚Äâkm2 domain. The capabilities of\nTROPOMI and GeoCARB can be usefully augmented with a surface air observation network of\n5‚Äì20 sites, and in turn the satellite instruments increase the detection capability that\ncan be achieved from the surface sites alone.\n",
    "venue": "Atmospheric Chemistry and Physics",
    "year": 2018,
    "citationCount": 45,
    "openAccessPdf": {
      "url": "https://acp.copernicus.org/articles/18/16885/2018/acp-18-16885-2018.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Environmental Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2018-07-31",
    "authors": [
      {
        "authorId": "93679592",
        "name": "D. Cusworth"
      },
      {
        "authorId": "36155926",
        "name": "D. Jacob"
      },
      {
        "authorId": "92920316",
        "name": "J. Sheng"
      },
      {
        "authorId": "14314250",
        "name": "J. Benmergui"
      },
      {
        "authorId": "67341871",
        "name": "A. Turner"
      },
      {
        "authorId": "35080098",
        "name": "J. Brandman"
      },
      {
        "authorId": "118643408",
        "name": "L. White"
      },
      {
        "authorId": "16717749",
        "name": "C. Randles"
      }
    ],
    "source": "semantic_scholar",
    "score": 122.42962094733642
  },
  {
    "paperId": "257369cd397764d8399c1637b94d321035c89faf",
    "url": "https://www.semanticscholar.org/paper/257369cd397764d8399c1637b94d321035c89faf",
    "title": "Deep Learning Techniques for Electronic Health Record (EHR) Analysis",
    "abstract": null,
    "venue": "",
    "year": 2020,
    "citationCount": 14,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1518610379",
        "name": "T. Poongodi"
      },
      {
        "authorId": "144977773",
        "name": "D. Sumathi"
      },
      {
        "authorId": "1518615777",
        "name": "P. Suresh"
      },
      {
        "authorId": "2456784",
        "name": "B. Balusamy"
      }
    ],
    "source": "semantic_scholar",
    "score": 90.62075301653314
  },
  {
    "paperId": "47e52c1cca76154e250f904d9ce1ccdc61706154",
    "url": "https://www.semanticscholar.org/paper/47e52c1cca76154e250f904d9ce1ccdc61706154",
    "title": "KI 2007: Advances in Artificial Intelligence, 30th Annual German Conference on AI, KI 2007, Osnabr√ºck, Germany, September 10-13, 2007, Proceedings",
    "abstract": null,
    "venue": "Deutsche Jahrestagung f√ºr K√ºnstliche Intelligenz",
    "year": 2007,
    "citationCount": 80,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1732149",
        "name": "J. Hertzberg"
      },
      {
        "authorId": "1746229",
        "name": "M. Beetz"
      },
      {
        "authorId": "3215719",
        "name": "R. Englert"
      }
    ],
    "source": "semantic_scholar",
    "score": 105.91673732008658
  }
]