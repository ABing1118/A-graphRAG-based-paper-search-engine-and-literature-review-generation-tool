[
  {
    "paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
    "url": "https://www.semanticscholar.org/paper/c8b25fab5608c3e033d34b4483ec47e68ba109b7",
    "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
    "abstract": "This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github.com/microsoft/Swin-Transformer.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 18035,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2103.14030",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2109371439",
        "name": "Ze Liu"
      },
      {
        "authorId": "51091819",
        "name": "Yutong Lin"
      },
      {
        "authorId": "2112823372",
        "name": "Yue Cao"
      },
      {
        "authorId": "1823518756",
        "name": "Han Hu"
      },
      {
        "authorId": "2107995927",
        "name": "Yixuan Wei"
      },
      {
        "authorId": "2148904543",
        "name": "Zheng Zhang"
      },
      {
        "authorId": "145676588",
        "name": "Stephen Lin"
      },
      {
        "authorId": "2261753424",
        "name": "B. Guo"
      }
    ],
    "source": "semantic_scholar",
    "score": 223.00187559311462
  },
  {
    "paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b",
    "url": "https://www.semanticscholar.org/paper/6c4b76232bb72897685d19b3d264c6ee3005bc2b",
    "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
    "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
    "venue": "Journal of machine learning research",
    "year": 2019,
    "citationCount": 17841,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Mathematics",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2019-10-23",
    "authors": [
      {
        "authorId": "2402716",
        "name": "Colin Raffel"
      },
      {
        "authorId": "1846258",
        "name": "Noam M. Shazeer"
      },
      {
        "authorId": "145625142",
        "name": "Adam Roberts"
      },
      {
        "authorId": "3844009",
        "name": "Katherine Lee"
      },
      {
        "authorId": "46617804",
        "name": "Sharan Narang"
      },
      {
        "authorId": "1380243217",
        "name": "Michael Matena"
      },
      {
        "authorId": "2389316",
        "name": "Yanqi Zhou"
      },
      {
        "authorId": "2157338362",
        "name": "Wei Li"
      },
      {
        "authorId": "35025299",
        "name": "Peter J. Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 211.83965761210595
  },
  {
    "paperId": "dbb6ded623159c867fbeca0772db7b2eb9489523",
    "url": "https://www.semanticscholar.org/paper/dbb6ded623159c867fbeca0772db7b2eb9489523",
    "title": "Spatial Transformer Networks",
    "abstract": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.",
    "venue": "Neural Information Processing Systems",
    "year": 2015,
    "citationCount": 7123,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2015-06-05",
    "authors": [
      {
        "authorId": "3093886",
        "name": "Max Jaderberg"
      },
      {
        "authorId": "34838386",
        "name": "K. Simonyan"
      },
      {
        "authorId": "1688869",
        "name": "Andrew Zisserman"
      },
      {
        "authorId": "2645384",
        "name": "K. Kavukcuoglu"
      }
    ],
    "source": "semantic_scholar",
    "score": 205.0683696661433
  },
  {
    "paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881",
    "url": "https://www.semanticscholar.org/paper/3e398bad2d8636491a1034cc938a5e024c7aa881",
    "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions",
    "abstract": "Although convolutional neural networks (CNNs) have achieved great success in computer vision, this work investigates a simpler, convolution-free backbone network use-fid for many dense prediction tasks. Unlike the recently-proposed Vision Transformer (ViT) that was designed for image classification specifically, we introduce the Pyramid Vision Transformer (PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several merits compared to current state of the arts. (1) Different from ViT that typically yields low-resolution outputs and incurs high computational and memory costs, PVT not only can be trained on dense partitions of an image to achieve high output resolution, which is important for dense prediction, but also uses a progressive shrinking pyramid to reduce the computations of large feature maps. (2) PVT inherits the advantages of both CNN and Transformer, making it a unified backbone for various vision tasks without convolutions, where it can be used as a direct replacement for CNN backbones. (3) We validate PVT through extensive experiments, showing that it boosts the performance of many downstream tasks, including object detection, instance and semantic segmentation. For example, with a comparable number of parameters, PVT+RetinaNet achieves 40.4 AP on the COCO dataset, surpassing ResNet50+RetinNet (36.3 AP) by 4.1 absolute AP (see Figure 2). We hope that PVT could, serre as an alternative and useful backbone for pixel-level predictions and facilitate future research.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 3296,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2102.12122",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-02-24",
    "authors": [
      {
        "authorId": "71074736",
        "name": "Wenhai Wang"
      },
      {
        "authorId": "41020000",
        "name": "Enze Xie"
      },
      {
        "authorId": null,
        "name": "Xiang Li"
      },
      {
        "authorId": "23999143",
        "name": "Deng-Ping Fan"
      },
      {
        "authorId": "50982078",
        "name": "Kaitao Song"
      },
      {
        "authorId": "152335674",
        "name": "Ding Liang"
      },
      {
        "authorId": "144720255",
        "name": "Tong Lu"
      },
      {
        "authorId": "144389940",
        "name": "P. Luo"
      },
      {
        "authorId": "144082425",
        "name": "Ling Shao"
      }
    ],
    "source": "semantic_scholar",
    "score": 197.51152364607594
  },
  {
    "paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2",
    "url": "https://www.semanticscholar.org/paper/925ad2897d1b5decbea320d07e99afa9110e09b2",
    "title": "Longformer: The Long-Document Transformer",
    "abstract": "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.",
    "venue": "arXiv.org",
    "year": 2020,
    "citationCount": 3620,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2020-04-10",
    "authors": [
      {
        "authorId": "46181066",
        "name": "Iz Beltagy"
      },
      {
        "authorId": "39139825",
        "name": "Matthew E. Peters"
      },
      {
        "authorId": "2527954",
        "name": "Arman Cohan"
      }
    ],
    "source": "semantic_scholar",
    "score": 192.91758264648462
  },
  {
    "paperId": "7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
    "url": "https://www.semanticscholar.org/paper/7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
    "title": "SwinIR: Image Restoration Using Swin Transformer",
    "abstract": "Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by up to 0.14∼0.45dB, while the total number of parameters can be reduced by up to 67%.",
    "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
    "year": 2021,
    "citationCount": 2384,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2108.10257",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-08-23",
    "authors": [
      {
        "authorId": "145270228",
        "name": "Jingyun Liang"
      },
      {
        "authorId": "2109811424",
        "name": "Jie Cao"
      },
      {
        "authorId": "15839174",
        "name": "Guolei Sun"
      },
      {
        "authorId": "144110274",
        "name": "K. Zhang"
      },
      {
        "authorId": "1681236",
        "name": "L. Gool"
      },
      {
        "authorId": "1732855",
        "name": "R. Timofte"
      }
    ],
    "source": "semantic_scholar",
    "score": 192.65431604983664
  },
  {
    "paperId": "5b9d8bcc46b766b47389c912a8e026f81b91b0d8",
    "url": "https://www.semanticscholar.org/paper/5b9d8bcc46b766b47389c912a8e026f81b91b0d8",
    "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting",
    "abstract": "Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L log L) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2020,
    "citationCount": 3307,
    "openAccessPdf": {
      "url": "https://ojs.aaai.org/index.php/AAAI/article/download/17325/17132",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2020-12-14",
    "authors": [
      {
        "authorId": "2395143",
        "name": "Haoyi Zhou"
      },
      {
        "authorId": "2437353",
        "name": "Shanghang Zhang"
      },
      {
        "authorId": "1752536568",
        "name": "J. Peng"
      },
      {
        "authorId": "2108433304",
        "name": "Shuai Zhang"
      },
      {
        "authorId": "1492113939",
        "name": "Jianxin Li"
      },
      {
        "authorId": "144467554",
        "name": "Hui Xiong"
      },
      {
        "authorId": "2108278607",
        "name": "Wan Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 191.56148584215373
  },
  {
    "paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221",
    "url": "https://www.semanticscholar.org/paper/13a0d8bb38f739990c8cd65a44061c6534f17221",
    "title": "OPT: Open Pre-trained Transformer Language Models",
    "abstract": "Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",
    "venue": "arXiv.org",
    "year": 2022,
    "citationCount": 3137,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-05-02",
    "authors": [
      {
        "authorId": "2108244542",
        "name": "Susan Zhang"
      },
      {
        "authorId": "3849208",
        "name": "Stephen Roller"
      },
      {
        "authorId": "39589154",
        "name": "Naman Goyal"
      },
      {
        "authorId": "2347956",
        "name": "Mikel Artetxe"
      },
      {
        "authorId": "2108267192",
        "name": "Moya Chen"
      },
      {
        "authorId": "1782969",
        "name": "Shuohui Chen"
      },
      {
        "authorId": "2065332326",
        "name": "Christopher Dewan"
      },
      {
        "authorId": "2138579860",
        "name": "Mona T. Diab"
      },
      {
        "authorId": "2116235416",
        "name": "Xian Li"
      },
      {
        "authorId": "143724481",
        "name": "Xi Victoria Lin"
      },
      {
        "authorId": "39980906",
        "name": "Todor Mihaylov"
      },
      {
        "authorId": "40511414",
        "name": "Myle Ott"
      },
      {
        "authorId": "88728159",
        "name": "Sam Shleifer"
      },
      {
        "authorId": "35752280",
        "name": "Kurt Shuster"
      },
      {
        "authorId": "2082239112",
        "name": "Daniel Simig"
      },
      {
        "authorId": "2146367061",
        "name": "Punit Singh Koura"
      },
      {
        "authorId": "5382923",
        "name": "Anjali Sridhar"
      },
      {
        "authorId": "1785372925",
        "name": "Tianlu Wang"
      },
      {
        "authorId": "1982950",
        "name": "Luke Zettlemoyer"
      }
    ],
    "source": "semantic_scholar",
    "score": 190.77011399939468
  },
  {
    "paperId": "0170fc76e934ee643f869df18fb617d5357e8b4e",
    "url": "https://www.semanticscholar.org/paper/0170fc76e934ee643f869df18fb617d5357e8b4e",
    "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
    "abstract": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.",
    "venue": "Interspeech",
    "year": 2020,
    "citationCount": 2853,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2005.08100",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2020-05-16",
    "authors": [
      {
        "authorId": "4478284",
        "name": "Anmol Gulati"
      },
      {
        "authorId": "47901308",
        "name": "James Qin"
      },
      {
        "authorId": "145039780",
        "name": "Chung-Cheng Chiu"
      },
      {
        "authorId": "3877127",
        "name": "Niki Parmar"
      },
      {
        "authorId": "2153632494",
        "name": "Yu Zhang"
      },
      {
        "authorId": "2338016291",
        "name": "Jiahui Yu"
      },
      {
        "authorId": "143911112",
        "name": "Wei Han"
      },
      {
        "authorId": "2108553866",
        "name": "Shibo Wang"
      },
      {
        "authorId": "2148905602",
        "name": "Zhengdong Zhang"
      },
      {
        "authorId": "48607963",
        "name": "Yonghui Wu"
      },
      {
        "authorId": "34320634",
        "name": "Ruoming Pang"
      }
    ],
    "source": "semantic_scholar",
    "score": 189.34715197055172
  },
  {
    "paperId": "b6382a7351c0c595f91472ac71d3b2d87b3c4844",
    "url": "https://www.semanticscholar.org/paper/b6382a7351c0c595f91472ac71d3b2d87b3c4844",
    "title": "ViViT: A Video Vision Transformer",
    "abstract": "We present pure-transformer based models for video classification, drawing upon the recent success of such models in image classification. Our model extracts spatiotemporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we propose several, efficient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple video classification benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 1897,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.15691",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-03-29",
    "authors": [
      {
        "authorId": "31638576",
        "name": "Anurag Arnab"
      },
      {
        "authorId": "3226635",
        "name": "Mostafa Dehghani"
      },
      {
        "authorId": "2280399",
        "name": "G. Heigold"
      },
      {
        "authorId": "1491624845",
        "name": "Chen Sun"
      },
      {
        "authorId": "34302129",
        "name": "Mario Lucic"
      },
      {
        "authorId": "2462253",
        "name": "C. Schmid"
      }
    ],
    "source": "semantic_scholar",
    "score": 189.2283396875481
  },
  {
    "paperId": "d40c77c010c8dbef6142903a02f2a73a85012d5d",
    "url": "https://www.semanticscholar.org/paper/d40c77c010c8dbef6142903a02f2a73a85012d5d",
    "title": "A Survey on Vision Transformer",
    "abstract": "Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.",
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "year": 2020,
    "citationCount": 1785,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2012.12556",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Medicine",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2020-12-23",
    "authors": [
      {
        "authorId": "3826388",
        "name": "Kai Han"
      },
      {
        "authorId": "2108702980",
        "name": "Yunhe Wang"
      },
      {
        "authorId": "2118023932",
        "name": "Hanting Chen"
      },
      {
        "authorId": "1736061",
        "name": "Xinghao Chen"
      },
      {
        "authorId": "2148899357",
        "name": "Jianyuan Guo"
      },
      {
        "authorId": "2125024057",
        "name": "Zhenhua Liu"
      },
      {
        "authorId": "103603255",
        "name": "Yehui Tang"
      },
      {
        "authorId": "1569696821",
        "name": "An Xiao"
      },
      {
        "authorId": "1691522",
        "name": "Chunjing Xu"
      },
      {
        "authorId": "2127897462",
        "name": "Yixing Xu"
      },
      {
        "authorId": "2116369043",
        "name": "Zhaohui Yang"
      },
      {
        "authorId": "2108440680",
        "name": "Yiman Zhang"
      },
      {
        "authorId": "143719920",
        "name": "D. Tao"
      }
    ],
    "source": "semantic_scholar",
    "score": 188.31600642154666
  },
  {
    "paperId": "dbe077f8521ecbe0a1477d6148c726d4f053d9c9",
    "url": "https://www.semanticscholar.org/paper/dbe077f8521ecbe0a1477d6148c726d4f053d9c9",
    "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet",
    "abstract": "Transformers, which are popular for language modeling, have been explored for solving vision tasks recently, e.g., the Vision Transformer (ViT) for image classification. The ViT model splits each image into a sequence of tokens with fixed length and then applies multiple Transformer layers to model their global relation for classification. However, ViT achieves inferior performance to CNNs when trained from scratch on a midsize dataset like ImageNet. We find it is because: 1) the simple tokenization of input images fails to model the important local structure such as edges and lines among neighboring pixels, leading to low training sample efficiency; 2) the redundant attention backbone design of ViT leads to limited feature richness for fixed computation budgets and limited training samples. To overcome such limitations, we propose a new Tokens-To-Token Vision Transformer (T2T-VTT), which incorporates 1) a layer-wise Tokens-to-Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating neighboring Tokens into one Token (Tokens-to-Token), such that local structure represented by surrounding tokens can be modeled and tokens length can be reduced; 2) an efficient backbone with a deep-narrow structure for vision transformer motivated by CNN architecture design after empirical study. Notably, T2T-ViT reduces the parameter count and MACs of vanilla ViT by half, while achieving more than 3.0% improvement when trained from scratch on ImageNet. It also outperforms ResNets and achieves comparable performance with MobileNets by directly training on ImageNet. For example, T2T-ViT with comparable size to ResNet50 (21.5M parameters) can achieve 83.3% top1 accuracy in image resolution 384x384 on ImageNet.1",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 1773,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2101.11986",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-01-28",
    "authors": [
      {
        "authorId": "2087091296",
        "name": "Li Yuan"
      },
      {
        "authorId": "2144861793",
        "name": "Yunpeng Chen"
      },
      {
        "authorId": null,
        "name": "Tao Wang"
      },
      {
        "authorId": "23476952",
        "name": "Weihao Yu"
      },
      {
        "authorId": "145356288",
        "name": "Yujun Shi"
      },
      {
        "authorId": "40983412",
        "name": "Francis E. H. Tay"
      },
      {
        "authorId": "33221685",
        "name": "Jiashi Feng"
      },
      {
        "authorId": "143653681",
        "name": "Shuicheng Yan"
      }
    ],
    "source": "semantic_scholar",
    "score": 188.21488244304288
  },
  {
    "paperId": "e775e649d815a02373eac840cf5e33a04ff85c95",
    "url": "https://www.semanticscholar.org/paper/e775e649d815a02373eac840cf5e33a04ff85c95",
    "title": "CvT: Introducing Convolutions to Vision Transformers",
    "abstract": "We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision Transformer (ViT) in performance and efficiency by introducing convolutions into ViT to yield the best of both de-signs. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (i.e. shift, scale, and distortion invariance) while maintaining the merits of Transformers (i.e. dynamic attention, global context, and better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters and lower FLOPs. In addition, performance gains are maintained when pretrained on larger datasets (e.g. ImageNet-22k) and fine-tuned to downstream tasks. Pretrained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7% on the ImageNet-1k val set. Finally, our results show that the positional encoding, a crucial component in existing Vision Transformers, can be safely re-moved in our model, simplifying the design for higher resolution vision tasks. Code will be released at https://github.com/microsoft/CvT.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 1728,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.15808",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-03-29",
    "authors": [
      {
        "authorId": "2119019500",
        "name": "Haiping Wu"
      },
      {
        "authorId": "2054421528",
        "name": "Bin Xiao"
      },
      {
        "authorId": "40589056",
        "name": "N. Codella"
      },
      {
        "authorId": "2152968847",
        "name": "Mengchen Liu"
      },
      {
        "authorId": "3386593",
        "name": "Xiyang Dai"
      },
      {
        "authorId": "145347147",
        "name": "Lu Yuan"
      },
      {
        "authorId": "2152828578",
        "name": "Lei Zhang"
      }
    ],
    "source": "semantic_scholar",
    "score": 187.82947728524937
  },
  {
    "paperId": "61ec7be824db60d4e40385c17ef5aab40f831195",
    "url": "https://www.semanticscholar.org/paper/61ec7be824db60d4e40385c17ef5aab40f831195",
    "title": "Loss of Life Transformer Prediction Based on Stacking Ensemble Improved by Genetic Algorithm By IJISRT",
    "abstract": "Prediction for loss of life transfomer is very important to ensure the reliability and efficiency of the power system. In this paper, an innovative model is proposed to improve the accuracy of lost of life transfomer prediction using stacking ensembles enhanced with genetic algorithm (GA). The aim is to develop a robust model to estimate the remaining life of a transformer in order to generally increase the reliability of the electrical energy distribution system. This approach involves integrating various machine learning models as a basic model, namely Support Vector Machines (SVM) and K-Nearest Neighbor (KNN). A stacking ensemble framework is then used to combine the predictions of these base models using a meta model namely Logistic Regression (LR). The results show a significant improvement in both transformers using stacking-GA, both TR-A and TR-B, with each prediction evaluation 99% and with a minimal error rate, namely approaching 0.the developed framework presents a promising solution for accurate and reliable transformer life prediction. By integrating a variety of basic models, applying improved stacking layouts using GA, these models offer valuable insights to improve maintenance strategies and system reliability in power grids.",
    "venue": "International Journal of Innovative Science and Research Technology",
    "year": 2024,
    "citationCount": 1319,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-26",
    "authors": [
      {
        "authorId": "2293648139",
        "name": "Rosena Shintabella"
      },
      {
        "authorId": "1574038086",
        "name": "Catur Edi Widodo"
      },
      {
        "authorId": "2293645742",
        "name": "Adi Wibowo"
      }
    ],
    "source": "semantic_scholar",
    "score": 187.78080523370625
  },
  {
    "paperId": "e1d082562981a9f51649c60663aa484ee623dbb0",
    "url": "https://www.semanticscholar.org/paper/e1d082562981a9f51649c60663aa484ee623dbb0",
    "title": "Point Transformer",
    "abstract": "In this work, we present Point Transformer, a deep neural network that operates directly on unordered and unstructured point sets. We design Point Transformer to extract local and global features and relate both representations by introducing the local-global attention mechanism, which aims to capture spatial point relations and shape information. For that purpose, we propose SortNet, as part of the Point Transformer, which induces input permutation invariance by selecting points based on a learned score. The output of Point Transformer is a sorted and permutation invariant feature list that can directly be incorporated into common computer vision applications. We evaluate our approach on standard classification and part segmentation benchmarks to demonstrate competitive results compared to the prior work.",
    "venue": "IEEE Access",
    "year": 2020,
    "citationCount": 1711,
    "openAccessPdf": {
      "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/09552005.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2020-11-02",
    "authors": [
      {
        "authorId": "2060173685",
        "name": "Nico Engel"
      },
      {
        "authorId": "1882784",
        "name": "Vasileios Belagiannis"
      },
      {
        "authorId": "1684594",
        "name": "K. Dietmayer"
      }
    ],
    "source": "semantic_scholar",
    "score": 187.6812633505253
  },
  {
    "paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
    "url": "https://www.semanticscholar.org/paper/c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
    "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
    "abstract": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "citationCount": 3535,
    "openAccessPdf": {
      "url": "https://www.aclweb.org/anthology/P19-1285.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2019-01-09",
    "authors": [
      {
        "authorId": "3422912",
        "name": "Zihang Dai"
      },
      {
        "authorId": "2109512754",
        "name": "Zhilin Yang"
      },
      {
        "authorId": "35729970",
        "name": "Yiming Yang"
      },
      {
        "authorId": "143712374",
        "name": "J. Carbonell"
      },
      {
        "authorId": "2827616",
        "name": "Quoc V. Le"
      },
      {
        "authorId": "145124475",
        "name": "R. Salakhutdinov"
      }
    ],
    "source": "semantic_scholar",
    "score": 187.56127135636302
  },
  {
    "paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a",
    "url": "https://www.semanticscholar.org/paper/74276a37bfa50f90dfae37f767b2b67784bd402a",
    "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
    "abstract": "The recent “Text-to-Text Transfer Transformer” (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent “accidental translation” in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2020,
    "citationCount": 2285,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2021.naacl-main.41.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2020-10-22",
    "authors": [
      {
        "authorId": "2692973",
        "name": "Linting Xue"
      },
      {
        "authorId": "40832517",
        "name": "Noah Constant"
      },
      {
        "authorId": "145625142",
        "name": "Adam Roberts"
      },
      {
        "authorId": "26688118",
        "name": "Mihir Kale"
      },
      {
        "authorId": "1388360943",
        "name": "Rami Al-Rfou"
      },
      {
        "authorId": "9356387",
        "name": "Aditya Siddhant"
      },
      {
        "authorId": "20407480",
        "name": "Aditya Barua"
      },
      {
        "authorId": "2402716",
        "name": "Colin Raffel"
      }
    ],
    "source": "semantic_scholar",
    "score": 186.01838266532133
  },
  {
    "paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
    "url": "https://www.semanticscholar.org/paper/c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500",
    "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling",
    "abstract": "We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.",
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "citationCount": 1418,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-06-02",
    "authors": [
      {
        "authorId": "2108435457",
        "name": "Lili Chen"
      },
      {
        "authorId": "2070275468",
        "name": "Kevin Lu"
      },
      {
        "authorId": "19275599",
        "name": "A. Rajeswaran"
      },
      {
        "authorId": "3436470",
        "name": "Kimin Lee"
      },
      {
        "authorId": "1954250",
        "name": "Aditya Grover"
      },
      {
        "authorId": "51093256",
        "name": "M. Laskin"
      },
      {
        "authorId": "1689992",
        "name": "P. Abbeel"
      },
      {
        "authorId": "41207614",
        "name": "A. Srinivas"
      },
      {
        "authorId": "2080746",
        "name": "Igor Mordatch"
      }
    ],
    "source": "semantic_scholar",
    "score": 185.86561515740064
  },
  {
    "paperId": "0ae67202f0584afccefa770865d14a46655d2975",
    "url": "https://www.semanticscholar.org/paper/0ae67202f0584afccefa770865d14a46655d2975",
    "title": "Transformer in Transformer",
    "abstract": "Transformer is a new kind of neural architecture which encodes the input data as powerful features via the attention mechanism. Basically, the visual transformers first divide the input images into several local patches and then calculate both representations and their relationship. Since natural images are of high complexity with abundant detail and color information, the granularity of the patch dividing is not fine enough for excavating features of objects in different scales and locations. In this paper, we point out that the attention inside these local patches are also essential for building visual transformers with high performance and we explore a new architecture, namely, Transformer iN Transformer (TNT). Specifically, we regard the local patches (e.g., 16$\\times$16) as\"visual sentences\"and present to further divide them into smaller patches (e.g., 4$\\times$4) as\"visual words\". The attention of each word will be calculated with other words in the given visual sentence with negligible computational costs. Features of both words and sentences will be aggregated to enhance the representation ability. Experiments on several benchmarks demonstrate the effectiveness of the proposed TNT architecture, e.g., we achieve an 81.5% top-1 accuracy on the ImageNet, which is about 1.7% higher than that of the state-of-the-art visual transformer with similar computational cost. The PyTorch code is available at https://github.com/huawei-noah/CV-Backbones, and the MindSpore code is available at https://gitee.com/mindspore/models/tree/master/research/cv/TNT.",
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "citationCount": 1394,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-02-27",
    "authors": [
      {
        "authorId": "3826388",
        "name": "Kai Han"
      },
      {
        "authorId": "1569696821",
        "name": "An Xiao"
      },
      {
        "authorId": "145344139",
        "name": "E. Wu"
      },
      {
        "authorId": "2148899357",
        "name": "Jianyuan Guo"
      },
      {
        "authorId": "1691522",
        "name": "Chunjing Xu"
      },
      {
        "authorId": "2108702980",
        "name": "Yunhe Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 185.609745413832
  },
  {
    "paperId": "8e33914d6051dd031a5e096962b9398fc1d16067",
    "url": "https://www.semanticscholar.org/paper/8e33914d6051dd031a5e096962b9398fc1d16067",
    "title": "Vision Transformers for Dense Prediction",
    "abstract": "We introduce dense prediction transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense prediction transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense prediction transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. Our models are available at https://github.com/intel-isl/DPT.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 1458,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.13413",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-03-24",
    "authors": [
      {
        "authorId": "2774325",
        "name": "René Ranftl"
      },
      {
        "authorId": "1651204675",
        "name": "Alexey Bochkovskiy"
      },
      {
        "authorId": "145231047",
        "name": "V. Koltun"
      }
    ],
    "source": "semantic_scholar",
    "score": 185.2825982278418
  },
  {
    "paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500",
    "url": "https://www.semanticscholar.org/paper/055fd6a9f7293269f1b22c1470e63bd02d8d9500",
    "title": "Reformer: The Efficient Transformer",
    "abstract": "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.",
    "venue": "International Conference on Learning Representations",
    "year": 2020,
    "citationCount": 2124,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2020-01-13",
    "authors": [
      {
        "authorId": "143808231",
        "name": "Nikita Kitaev"
      },
      {
        "authorId": "40527594",
        "name": "Lukasz Kaiser"
      },
      {
        "authorId": "6639036",
        "name": "Anselm Levskaya"
      }
    ],
    "source": "semantic_scholar",
    "score": 184.92290622037774
  },
  {
    "paperId": "7519a1e9e7371df79bd8a21cee871feb0ec597a5",
    "url": "https://www.semanticscholar.org/paper/7519a1e9e7371df79bd8a21cee871feb0ec597a5",
    "title": "UNETR: Transformers for 3D Medical Image Segmentation",
    "abstract": "Fully Convolutional Neural Networks (FCNNs) with contracting and expanding paths have shown prominence for the majority of medical image segmentation applications since the past decade. In FCNNs, the encoder plays an integral role by learning both global and local features and contextual representations which can be utilized for semantic output prediction by the decoder. Despite their success, the locality of convolutional layers in FCNNs, limits the capability of learning long-range spatial dependencies. Inspired by the recent success of transformers for Natural Language Processing (NLP) in long-range sequence learning, we reformulate the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem. We introduce a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information, while also following the successful \"U-shaped\" network design for the encoder and decoder. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV leaderboard.",
    "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
    "year": 2021,
    "citationCount": 1309,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.10504",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-03-18",
    "authors": [
      {
        "authorId": "31374559",
        "name": "Ali Hatamizadeh"
      },
      {
        "authorId": "144041873",
        "name": "Dong Yang"
      },
      {
        "authorId": "144531567",
        "name": "H. Roth"
      },
      {
        "authorId": "3262394",
        "name": "Daguang Xu"
      }
    ],
    "source": "semantic_scholar",
    "score": 183.66673624292795
  },
  {
    "paperId": "68f080e0ac836ea230cb5316fbed273c70422d75",
    "url": "https://www.semanticscholar.org/paper/68f080e0ac836ea230cb5316fbed273c70422d75",
    "title": "Segmenter: Transformer for Semantic Segmentation",
    "abstract": "Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. In contrast to convolution-based methods, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision Transformer (ViT) and extend it to semantic segmentation. To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embed-dings with a point-wise linear decoder or a mask trans-former decoder. We leverage models pre-trained for image classification and show that we can fine-tune them on moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on both ADE20K and Pascal Context datasets and is competitive on Cityscapes.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 1300,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2105.05633",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-05-12",
    "authors": [
      {
        "authorId": "86898863",
        "name": "Robin Strudel"
      },
      {
        "authorId": "2133420734",
        "name": "Ricardo Garcia Pinel"
      },
      {
        "authorId": "143991676",
        "name": "I. Laptev"
      },
      {
        "authorId": "2462253",
        "name": "C. Schmid"
      }
    ],
    "source": "semantic_scholar",
    "score": 183.56332717768757
  },
  {
    "paperId": "658a017302d29e4acf4ca789cb5d9f27983717ff",
    "url": "https://www.semanticscholar.org/paper/658a017302d29e4acf4ca789cb5d9f27983717ff",
    "title": "Masked-attention Mask Transformer for Universal Image Segmentation",
    "abstract": "Image segmentation groups pixels with different semantics, e.g., category or instance membership. Each choice of semantics defines a task. While only the semantics of each task differ, current research focuses on designing spe-cialized architectures for each task. We present Masked- attention Mask Transformer (Mask2Former), a new archi-tecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components in-clude masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a significant margin on four popular datasets. Most no-tably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU onADE20K).",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "citationCount": 1888,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2112.01527",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-12-02",
    "authors": [
      {
        "authorId": "50563570",
        "name": "Bowen Cheng"
      },
      {
        "authorId": "1806773",
        "name": "Ishan Misra"
      },
      {
        "authorId": "2068227",
        "name": "A. Schwing"
      },
      {
        "authorId": "144843400",
        "name": "Alexander Kirillov"
      },
      {
        "authorId": "3102850",
        "name": "Rohit Girdhar"
      }
    ],
    "source": "semantic_scholar",
    "score": 183.15704301252265
  },
  {
    "paperId": "0eff37167876356da2163b2e396df2719adf7de9",
    "url": "https://www.semanticscholar.org/paper/0eff37167876356da2163b2e396df2719adf7de9",
    "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification",
    "abstract": "The recently developed vision transformer (ViT) has achieved promising results on image classification compared to convolutional neural networks. Inspired by this, in this paper, we study how to learn multi-scale feature representations in transformer models for image classification. To this end, we propose a dual-branch transformer to com-bine image patches (i.e., tokens in a transformer) of different sizes to produce stronger image features. Our approach processes small-patch and large-patch tokens with two separate branches of different computational complexity and these tokens are then fused purely by attention multiple times to complement each other. Furthermore, to reduce computation, we develop a simple yet effective token fusion module based on cross attention, which uses a single token for each branch as a query to exchange information with other branches. Our proposed cross-attention only requires linear time for both computational and memory complexity instead of quadratic time otherwise. Extensive experiments demonstrate that our approach performs better than or on par with several concurrent works on vision transformer, in addition to efficient CNN models. For example, on the ImageNet1K dataset, with some architectural changes, our approach outperforms the recent DeiT by a large margin of 2% with a small to moderate increase in FLOPs and model parameters. Our source codes and models are available at https://github.com/IBM/CrossViT.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 1261,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.14899",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-03-27",
    "authors": [
      {
        "authorId": "48239920",
        "name": "Chun-Fu Chen"
      },
      {
        "authorId": "33421444",
        "name": "Quanfu Fan"
      },
      {
        "authorId": "1819152",
        "name": "Rameswar Panda"
      }
    ],
    "source": "semantic_scholar",
    "score": 183.10679564651736
  },
  {
    "paperId": "1e88d5afe19aea324d33541f60a90b7036894c32",
    "url": "https://www.semanticscholar.org/paper/1e88d5afe19aea324d33541f60a90b7036894c32",
    "title": "Restormer: Efficient Transformer for High-Resolution Image Restoration",
    "abstract": "Since convolutional neural networks (CNNs) perform well at learning generalizable image priors from large-scale data, these models have been extensively applied to image restoration and related tasks. Recently, another class of neural architectures, Transformers, have shown significant performance gains on natural language and high-level vision tasks. While the Transformer model mitigates the shortcomings of CNNs (i.e., limited receptive field and inadaptability to input content), its computational complexity grows quadratically with the spatial resolution, therefore making it infeasible to apply to most image restoration tasks involving high-resolution images. In this work, we propose an efficient Transformer model by making several key designs in the building blocks (multi-head attention and feed-forward network) such that it can capture long-range pixel interactions, while still remaining applicable to large images. Our model, named Restoration Transformer (Restormer), achieves state-of-the-art results on several image restoration tasks, including image deraining, single-image motion deblurring, defocus deblurring (single-image and dual-pixel data), and image denoising (Gaussian grayscale/color denoising, and real image denoising). The source code and pre-trained models are available at https://github.com/swz30/Restormer.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "citationCount": 1761,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2111.09881",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-11-18",
    "authors": [
      {
        "authorId": "3323621",
        "name": "Syed Waqas Zamir"
      },
      {
        "authorId": "153150198",
        "name": "Aditya Arora"
      },
      {
        "authorId": "152973423",
        "name": "Salman Hameed Khan"
      },
      {
        "authorId": "145684318",
        "name": "Munawar Hayat"
      },
      {
        "authorId": "2358803",
        "name": "F. Khan"
      },
      {
        "authorId": "37144787",
        "name": "Ming-Hsuan Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 182.1130720974419
  },
  {
    "paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e",
    "url": "https://www.semanticscholar.org/paper/c6c734e16f66fbfcefac7625cc64599e83292c1e",
    "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers",
    "abstract": "Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.",
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "citationCount": 1073,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2020-02-25",
    "authors": [
      {
        "authorId": "51456429",
        "name": "Wenhui Wang"
      },
      {
        "authorId": "49807919",
        "name": "Furu Wei"
      },
      {
        "authorId": "145307652",
        "name": "Li Dong"
      },
      {
        "authorId": "10699417",
        "name": "Hangbo Bao"
      },
      {
        "authorId": "144610884",
        "name": "Nan Yang"
      },
      {
        "authorId": "92660691",
        "name": "Ming Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 181.68717912603216
  },
  {
    "paperId": "18863dbfa32eaa1ccdb56ff180e6ab079a7f1ec6",
    "url": "https://www.semanticscholar.org/paper/18863dbfa32eaa1ccdb56ff180e6ab079a7f1ec6",
    "title": "Multiscale Vision Transformers",
    "abstract": "We present Multiscale Vision Transformers (MViT) for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models. Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features. We evaluate this fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and are 5-10× more costly in computation and parameters. We further remove the temporal dimension and apply our model for image classification where it outperforms prior work on vision transformers. Code is available at: https://github.com/facebookresearch/SlowFast.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 1122,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2104.11227",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-04-22",
    "authors": [
      {
        "authorId": "146884473",
        "name": "Haoqi Fan"
      },
      {
        "authorId": "144752314",
        "name": "Bo Xiong"
      },
      {
        "authorId": "11379939",
        "name": "K. Mangalam"
      },
      {
        "authorId": "3128506",
        "name": "Yanghao Li"
      },
      {
        "authorId": "151485208",
        "name": "Zhicheng Yan"
      },
      {
        "authorId": "153652147",
        "name": "J. Malik"
      },
      {
        "authorId": "2322150",
        "name": "Christoph Feichtenhofer"
      }
    ],
    "source": "semantic_scholar",
    "score": 181.35638432107663
  },
  {
    "paperId": "53a78a3fbbd3c7fc8674fe3c4323a68624c69ba6",
    "url": "https://www.semanticscholar.org/paper/53a78a3fbbd3c7fc8674fe3c4323a68624c69ba6",
    "title": "A three-phase soft-switched high power density DC/DC converter for high power applications",
    "abstract": "The authors present three DC/DC converter topologies suitable for high-power-density high-power applications. All three circuits operate in a soft-switched manner, making possible a reduction in device switching losses and an increase in switching frequency. The three-phase dual-bridge converter proposed is seen to have the most favorable characteristics. This converter consists of two three-phase inverter stages operating in a high frequency six-step mode. In contrast to existing single-phase AC-link DC/DC converters, low RMS current ratings are obtained for both the input and output filter capacitors. This is in addition to smaller filter element values due to the higher-frequency content of the input and output waveforms. The use of a three-phase symmetrical transformer instead of single-phase transformers and a better utilization of the available apparent power of the transformer (as a consequence of the controlled output inverter) significantly increase the power density attainable.<<ETX>>",
    "venue": "Conference Record of the 1988 IEEE Industry Applications Society Annual Meeting",
    "year": 1988,
    "citationCount": 2172,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "1988-10-02",
    "authors": [
      {
        "authorId": "34888860",
        "name": "R. D. Doncker"
      },
      {
        "authorId": "92953989",
        "name": "D. Divan"
      },
      {
        "authorId": "40078953",
        "name": "M. Kheraluwala"
      }
    ],
    "source": "semantic_scholar",
    "score": 181.25795970384644
  },
  {
    "paperId": "fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d",
    "url": "https://www.semanticscholar.org/paper/fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d",
    "title": "RT-1: Robotics Transformer for Real-World Control at Scale",
    "abstract": "By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at robotics-transformer1.github.io",
    "venue": "Robotics: Science and Systems",
    "year": 2022,
    "citationCount": 804,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2212.06817",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-12-13",
    "authors": [
      {
        "authorId": "118025075",
        "name": "Anthony Brohan"
      },
      {
        "authorId": "2161343011",
        "name": "Noah Brown"
      },
      {
        "authorId": "2196517336",
        "name": "Justice Carbajal"
      },
      {
        "authorId": "2527420",
        "name": "Yevgen Chebotar"
      },
      {
        "authorId": "2196517328",
        "name": "Joseph Dabis"
      },
      {
        "authorId": "46881670",
        "name": "Chelsea Finn"
      },
      {
        "authorId": "2161342233",
        "name": "K. Gopalakrishnan"
      },
      {
        "authorId": "1944801",
        "name": "Karol Hausman"
      },
      {
        "authorId": "1505793452",
        "name": "Alexander Herzog"
      },
      {
        "authorId": "2726592",
        "name": "Jasmine Hsu"
      },
      {
        "authorId": "46920727",
        "name": "Julian Ibarz"
      },
      {
        "authorId": "2704814",
        "name": "Brian Ichter"
      },
      {
        "authorId": "17818078",
        "name": "A. Irpan"
      },
      {
        "authorId": "2175779811",
        "name": "Tomas Jackson"
      },
      {
        "authorId": "2161341920",
        "name": "Sally Jesmonth"
      },
      {
        "authorId": "2052368480",
        "name": "Nikhil J. Joshi"
      },
      {
        "authorId": "144885996",
        "name": "Ryan C. Julian"
      },
      {
        "authorId": "48313860",
        "name": "Dmitry Kalashnikov"
      },
      {
        "authorId": "2161342687",
        "name": "Yuheng Kuang"
      },
      {
        "authorId": "2057988112",
        "name": "Isabel Leal"
      },
      {
        "authorId": "2145145412",
        "name": "Kuang-Huei Lee"
      },
      {
        "authorId": "1736651",
        "name": "S. Levine"
      },
      {
        "authorId": "2161346119",
        "name": "Yao Lu"
      },
      {
        "authorId": "51225879",
        "name": "U. Malla"
      },
      {
        "authorId": "2064295888",
        "name": "D. Manjunath"
      },
      {
        "authorId": "2080746",
        "name": "Igor Mordatch"
      },
      {
        "authorId": "7624658",
        "name": "Ofir Nachum"
      },
      {
        "authorId": "2057314286",
        "name": "Carolina Parada"
      },
      {
        "authorId": "2195951533",
        "name": "Jodilyn Peralta"
      },
      {
        "authorId": "2125121323",
        "name": "Emily Perez"
      },
      {
        "authorId": "31719101",
        "name": "Karl Pertsch"
      },
      {
        "authorId": "2161342191",
        "name": "Jornell Quiambao"
      },
      {
        "authorId": "2251957",
        "name": "Kanishka Rao"
      },
      {
        "authorId": "1766489",
        "name": "M. Ryoo"
      },
      {
        "authorId": "2196524735",
        "name": "Grecia Salazar"
      },
      {
        "authorId": "2840758",
        "name": "Pannag R. Sanketi"
      },
      {
        "authorId": "2196510642",
        "name": "Kevin Sayed"
      },
      {
        "authorId": "2196040785",
        "name": "Jaspiar Singh"
      },
      {
        "authorId": "34365421",
        "name": "S. Sontakke"
      },
      {
        "authorId": "2056868723",
        "name": "Austin Stone"
      },
      {
        "authorId": "2161386250",
        "name": "Clayton Tan"
      },
      {
        "authorId": "2195355151",
        "name": "Huong Tran"
      },
      {
        "authorId": "2657155",
        "name": "Vincent Vanhoucke"
      },
      {
        "authorId": "2195627101",
        "name": "Steve Vega"
      },
      {
        "authorId": "144579461",
        "name": "Q. Vuong"
      },
      {
        "authorId": "144956443",
        "name": "F. Xia"
      },
      {
        "authorId": "9961095",
        "name": "Ted Xiao"
      },
      {
        "authorId": "2153917744",
        "name": "Peng Xu"
      },
      {
        "authorId": "3068504",
        "name": "Sichun Xu"
      },
      {
        "authorId": "10909315",
        "name": "Tianhe Yu"
      },
      {
        "authorId": "2196524598",
        "name": "Brianna Zitkovich"
      }
    ],
    "source": "semantic_scholar",
    "score": 180.36263416127844
  },
  {
    "paperId": "0839722fb5369c0abaff8515bfc08299efc790a1",
    "url": "https://www.semanticscholar.org/paper/0839722fb5369c0abaff8515bfc08299efc790a1",
    "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision",
    "abstract": "Vision-and-Language Pre-training (VLP) has improved performance on various joint vision-and-language downstream tasks. Current approaches to VLP heavily rely on image feature extraction processes, most of which involve region supervision (e.g., object detection) and the convolutional architecture (e.g., ResNet). Although disregarded in the literature, we find it problematic in terms of both (1) efficiency/speed, that simply extracting input features requires much more computation than the multimodal interaction steps; and (2) expressive power, as it is upper bounded to the expressive power of the visual embedder and its predefined visual vocabulary. In this paper, we present a minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance. Our code and pre-trained weights are available at https://github.com/dandelin/vilt.",
    "venue": "International Conference on Machine Learning",
    "year": 2021,
    "citationCount": 1549,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Mathematics",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-02-05",
    "authors": [
      {
        "authorId": "2382193",
        "name": "Wonjae Kim"
      },
      {
        "authorId": "65842988",
        "name": "Bokyung Son"
      },
      {
        "authorId": "14972026",
        "name": "Ildoo Kim"
      }
    ],
    "source": "semantic_scholar",
    "score": 180.19015314869938
  },
  {
    "paperId": "561377c8e51bb864a1a689d830fab2f0881ae78e",
    "url": "https://www.semanticscholar.org/paper/561377c8e51bb864a1a689d830fab2f0881ae78e",
    "title": "Remote Sensing Image Change Detection With Transformers",
    "abstract": "Modern change detection (CD) has achieved remarkable success by the powerful discriminative ability of deep convolutions. However, high-resolution remote sensing CD remains challenging due to the complexity of objects in the scene. Objects with the same semantic concept may show distinct spectral characteristics at different times and spatial locations. Most recent CD pipelines using pure convolutions are still struggling to relate long-range concepts in space-time. Nonlocal self-attention approaches show promising performance via modeling dense relationships among pixels, yet are computationally inefficient. Here, we propose a bitemporal image transformer (BIT) to efficiently and effectively model contexts within the spatial-temporal domain. Our intuition is that the high-level concepts of the change of interest can be represented by a few visual words, that is, semantic tokens. To achieve this, we express the bitemporal image into a few tokens and use a transformer encoder to model contexts in the compact token-based space-time. The learned context-rich tokens are then fed back to the pixel-space for refining the original features via a transformer decoder. We incorporate BIT in a deep feature differencing-based CD framework. Extensive experiments on three CD datasets demonstrate the effectiveness and efficiency of the proposed method. Notably, our BIT-based model significantly outperforms the purely convolutional baseline using only three times lower computational costs and model parameters. Based on a naive backbone (ResNet18) without sophisticated structures (e.g., feature pyramid network (FPN) and UNet), our model surpasses several state-of-the-art CD methods, including better than four recent attention-based methods in terms of efficiency and accuracy. Our code is available at https://github.com/justchenhao/BIT_CD.",
    "venue": "IEEE Transactions on Geoscience and Remote Sensing",
    "year": 2021,
    "citationCount": 792,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.00208",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-02-27",
    "authors": [
      {
        "authorId": null,
        "name": "Hao Chen"
      },
      {
        "authorId": "2072539630",
        "name": "Zipeng Qi"
      },
      {
        "authorId": "1741174",
        "name": "Zhenwei Shi"
      }
    ],
    "source": "semantic_scholar",
    "score": 180.13734832452272
  },
  {
    "paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
    "url": "https://www.semanticscholar.org/paper/7e5709d81558d3ef4265de29ea75931afeb1f2dd",
    "title": "Efficient Transformers: A Survey",
    "abstract": "Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision, and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of “X-former” models have been proposed—Reformer, Linformer, Performer, Longformer, to name a few—which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this article characterizes a large and thoughtful selection of recent efficiency-flavored “X-former” models, providing an organized and comprehensive overview of existing work and models across multiple domains.",
    "venue": "ACM Computing Surveys",
    "year": 2020,
    "citationCount": 1031,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3530811",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2020-09-14",
    "authors": [
      {
        "authorId": "144447820",
        "name": "Yi Tay"
      },
      {
        "authorId": "3226635",
        "name": "Mostafa Dehghani"
      },
      {
        "authorId": "11774695",
        "name": "Dara Bahri"
      },
      {
        "authorId": "1680617",
        "name": "Donald Metzler"
      }
    ],
    "source": "semantic_scholar",
    "score": 180.0888091906226
  },
  {
    "paperId": "be0fbb810583930c071d0b9b2c5187fe260783f5",
    "url": "https://www.semanticscholar.org/paper/be0fbb810583930c071d0b9b2c5187fe260783f5",
    "title": "Swin Transformer V2: Scaling Up Capacity and Resolution",
    "abstract": "We present techniques for scaling Swin Transformer [35] up to 3 billion parameters and making it capable of training with images of up to 1,536x1,536 resolution. By scaling up capacity and resolution, Swin Transformer sets new records on four representative vision benchmarks: 84.0% top-1 accuracy on ImageNet- V2 image classification, 63.1 / 54.4 box / mask mAP on COCO object detection, 59.9 mIoU on ADE20K semantic segmentation, and 86.8% top-1 accuracy on Kinetics-400 video action classification. We tackle issues of training instability, and study how to effectively transfer models pre-trained at low resolutions to higher resolution ones. To this aim, several novel technologies are proposed: 1) a residual post normalization technique and a scaled cosine attention approach to improve the stability of large vision models; 2) a log-spaced continuous position bias technique to effectively transfer models pre-trained at low-resolution images and windows to their higher-resolution counterparts. In addition, we share our crucial implementation details that lead to significant savings of GPU memory consumption and thus make it feasi-ble to train large vision models with regular GPUs. Using these techniques and self-supervised pre-training, we suc-cessfully train a strong 3 billion Swin Transformer model and effectively transfer it to various vision tasks involving high-resolution images or windows, achieving the state-of-the-art accuracy on a variety of benchmarks. Code is avail-able at https://github.com/microsoft/Swin-Transformer.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "citationCount": 1512,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2111.09883",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-11-18",
    "authors": [
      {
        "authorId": "2109371439",
        "name": "Ze Liu"
      },
      {
        "authorId": "1823518756",
        "name": "Han Hu"
      },
      {
        "authorId": "51091819",
        "name": "Yutong Lin"
      },
      {
        "authorId": "32532300",
        "name": "Zhuliang Yao"
      },
      {
        "authorId": "2090394064",
        "name": "Zhenda Xie"
      },
      {
        "authorId": "2107995927",
        "name": "Yixuan Wei"
      },
      {
        "authorId": "2136771175",
        "name": "Jia Ning"
      },
      {
        "authorId": "2112823372",
        "name": "Yue Cao"
      },
      {
        "authorId": "2148904543",
        "name": "Zheng Zhang"
      },
      {
        "authorId": "145307652",
        "name": "Li Dong"
      },
      {
        "authorId": "49807919",
        "name": "Furu Wei"
      },
      {
        "authorId": "2261753424",
        "name": "B. Guo"
      }
    ],
    "source": "semantic_scholar",
    "score": 179.82774570682534
  },
  {
    "paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
    "url": "https://www.semanticscholar.org/paper/c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
    "title": "Linformer: Self-Attention with Linear Complexity",
    "abstract": "Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses $O(n^2)$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from $O(n^2)$ to $O(n)$ in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.",
    "venue": "arXiv.org",
    "year": 2020,
    "citationCount": 1509,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2020-06-08",
    "authors": [
      {
        "authorId": "2096387",
        "name": "Sinong Wang"
      },
      {
        "authorId": "46708422",
        "name": "Belinda Z. Li"
      },
      {
        "authorId": "2072010",
        "name": "Madian Khabsa"
      },
      {
        "authorId": "2087117615",
        "name": "Han Fang"
      },
      {
        "authorId": "2110815489",
        "name": "Hao Ma"
      }
    ],
    "source": "semantic_scholar",
    "score": 179.79797394713455
  },
  {
    "paperId": "6f6f73e69ee0d9d5d7d088bb882db1851d98175a",
    "url": "https://www.semanticscholar.org/paper/6f6f73e69ee0d9d5d7d088bb882db1851d98175a",
    "title": "Pre-Trained Image Processing Transformer",
    "abstract": "As the computing power of modern hardware is increasing strongly, pre-trained deep learning models (e.g., BERT, GPT-3) learned on large-scale datasets have shown their effectiveness over conventional methods. The big progress is mainly contributed to the representation ability of transformer and its variant architectures. In this paper, we study the low-level computer vision task (e.g., denoising, super-resolution and deraining) and develop a new pre-trained model, namely, image processing transformer (IPT). To maximally excavate the capability of transformer, we present to utilize the well-known ImageNet benchmark for generating a large amount of corrupted image pairs. The IPT model is trained on these images with multi-heads and multi-tails. In addition, the contrastive learning is introduced for well adapting to different image processing tasks. The pre-trained model can therefore efficiently employed on desired task after fine-tuning. With only one pre-trained model, IPT outperforms the current state-of-the-art methods on various low-level benchmarks. Code is available at https://github.com/huawei-noah/Pretrained-IPT and https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/IPT",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2020,
    "citationCount": 1499,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2012.00364",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2020-12-01",
    "authors": [
      {
        "authorId": "2118023932",
        "name": "Hanting Chen"
      },
      {
        "authorId": "2108702980",
        "name": "Yunhe Wang"
      },
      {
        "authorId": "50412584",
        "name": "Tianyu Guo"
      },
      {
        "authorId": null,
        "name": "Chang Xu"
      },
      {
        "authorId": "2115656092",
        "name": "Yiping Deng"
      },
      {
        "authorId": "2125024057",
        "name": "Zhenhua Liu"
      },
      {
        "authorId": "2217676532",
        "name": "Siwei Ma"
      },
      {
        "authorId": "1691522",
        "name": "Chunjing Xu"
      },
      {
        "authorId": "2004428678",
        "name": "Chao Xu"
      },
      {
        "authorId": "2153706048",
        "name": "Wen Gao"
      }
    ],
    "source": "semantic_scholar",
    "score": 179.69830580635454
  },
  {
    "paperId": "6709d5583f658f589ae6a2184805933aceb18849",
    "url": "https://www.semanticscholar.org/paper/6709d5583f658f589ae6a2184805933aceb18849",
    "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers",
    "abstract": "Very recently, a variety of vision transformer architectures for dense prediction tasks have been proposed and they show that the design of spatial attention is critical to their success in these tasks. In this work, we revisit the design of the spatial attention and demonstrate that a carefully-devised yet simple spatial attention mechanism performs favourably against the state-of-the-art schemes. As a result, we propose two vision transformer architectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures are highly-efficient and easy to implement, only involving matrix multiplications that are highly optimized in modern deep learning frameworks. More importantly, the proposed architectures achieve excellent performance on a wide range of visual tasks, including image level classification as well as dense detection and segmentation. The simplicity and strong performance suggest that our proposed architectures may serve as stronger backbones for many vision tasks. Our code is released at https://github.com/Meituan-AutoML/Twins .",
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "citationCount": 933,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-04-28",
    "authors": [
      {
        "authorId": "27628828",
        "name": "Xiangxiang Chu"
      },
      {
        "authorId": "2069520672",
        "name": "Zhi Tian"
      },
      {
        "authorId": "2108035065",
        "name": "Yuqing Wang"
      },
      {
        "authorId": null,
        "name": "Bo Zhang"
      },
      {
        "authorId": "1558541225",
        "name": "Haibing Ren"
      },
      {
        "authorId": "49141839",
        "name": "Xiaolin Wei"
      },
      {
        "authorId": "2065044626",
        "name": "Huaxia Xia"
      },
      {
        "authorId": "12459603",
        "name": "Chunhua Shen"
      }
    ],
    "source": "semantic_scholar",
    "score": 179.59214657343264
  },
  {
    "paperId": "36e30516683032634975c53e60f3737b6e35ff80",
    "url": "https://www.semanticscholar.org/paper/36e30516683032634975c53e60f3737b6e35ff80",
    "title": "Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting",
    "abstract": "Time series forecasting is an important problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. In this paper, we propose to tackle such forecasting problem with Transformer [1]. Although impressed by its performance in our preliminary study, we found its two major weaknesses: (1) locality-agnostics: the point-wise dot-product self-attention in canonical Transformer architecture is insensitive to local context, which can make the model prone to anomalies in time series; (2) memory bottleneck: space complexity of canonical Transformer grows quadratically with sequence length $L$, making directly modeling long time series infeasible. In order to solve these two issues, we first propose convolutional self-attention by producing queries and keys with causal convolution so that local context can be better incorporated into attention mechanism. Then, we propose LogSparse Transformer with only $O(L(\\log L)^{2})$ memory cost, improving forecasting accuracy for time series with fine granularity and strong long-term dependencies under constrained memory budget. Our experiments on both synthetic data and real-world datasets show that it compares favorably to the state-of-the-art.",
    "venue": "Neural Information Processing Systems",
    "year": 2019,
    "citationCount": 1269,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2019-06-29",
    "authors": [
      {
        "authorId": "50341591",
        "name": "SHIYANG LI"
      },
      {
        "authorId": "2149111126",
        "name": "Xiaoyong Jin"
      },
      {
        "authorId": "2067812849",
        "name": "Yao Xuan"
      },
      {
        "authorId": "3363593",
        "name": "Xiyou Zhou"
      },
      {
        "authorId": "2928777",
        "name": "Wenhu Chen"
      },
      {
        "authorId": "2040617",
        "name": "Yu-Xiang Wang"
      },
      {
        "authorId": "1740249",
        "name": "Xifeng Yan"
      }
    ],
    "source": "semantic_scholar",
    "score": 179.20158269178955
  },
  {
    "paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d",
    "url": "https://www.semanticscholar.org/paper/3fbf6339273c50b04e886fa9bd4ad18c952a683d",
    "title": "Rethinking Attention with Performers",
    "abstract": "We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.",
    "venue": "International Conference on Learning Representations",
    "year": 2020,
    "citationCount": 1394,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2020-09-30",
    "authors": [
      {
        "authorId": "1805203",
        "name": "K. Choromanski"
      },
      {
        "authorId": "52314889",
        "name": "Valerii Likhosherstov"
      },
      {
        "authorId": "35363891",
        "name": "David Dohan"
      },
      {
        "authorId": "32725720",
        "name": "Xingyou Song"
      },
      {
        "authorId": "3071104",
        "name": "Andreea Gane"
      },
      {
        "authorId": "2227764",
        "name": "Tamás Sarlós"
      },
      {
        "authorId": "2052793706",
        "name": "Peter Hawkins"
      },
      {
        "authorId": "29827891",
        "name": "Jared Davis"
      },
      {
        "authorId": "1579862074",
        "name": "Afroz Mohiuddin"
      },
      {
        "authorId": "40527594",
        "name": "Lukasz Kaiser"
      },
      {
        "authorId": "2636941",
        "name": "David Belanger"
      },
      {
        "authorId": "2654847",
        "name": "Lucy J. Colwell"
      },
      {
        "authorId": "145689461",
        "name": "Adrian Weller"
      }
    ],
    "source": "semantic_scholar",
    "score": 178.609745413832
  },
  {
    "paperId": "37179bfc4836890a32950ea2fb74795823284362",
    "url": "https://www.semanticscholar.org/paper/37179bfc4836890a32950ea2fb74795823284362",
    "title": "Systematic design of program analysis frameworks",
    "abstract": "Semantic analysis of programs is essential in optimizing compilers and program verification systems. It encompasses data flow analysis, data type determination, generation of approximate invariant assertions, etc. Several recent papers (among others Cousot & Cousot[77a], Graham & Wegman[76], Kam & Ullman[76], Kildall[73], Rosen[78], Tarjan[76], Wegbreit[75]) have introduced abstract approaches to program analysis which are tantamount to the use of a program analysis framework (A,t,ã) where A is a lattice of (approximate) assertions, t is an (approximate) predicate transformer and ã is an often implicit function specifying the meaning of the elements of A. This paper is devoted to the systematic and correct design of program analysis frameworks with respect to a formal semantics. Preliminary definitions are given in Section 2 concerning the merge over all paths and (least) fixpoint program-wide analysis methods. In Section 3 we briefly define the (forward and backward) deductive semantics of programs which is later used as a formal basis in order to prove the correctness of the approximate program analysis frameworks. Section 4 very shortly recall the main elements of the lattice theoretic approach to approximate semantic analysis of programs. The design of a space of approximate assertions A is studied in Section 5. We first justify the very reasonable assumption that A must be chosen such that the exact invariant assertions of any program must have an upper approximation in A and that the approximate analysis of any program must be performed using a deterministic process. These assumptions are shown to imply that A is a Moore family, that the approximation operator (wich defines the least upper approximation of any assertion) is an upper closure operator and that A is necessarily a complete lattice. We next show that the connection between a space of approximate assertions and a computer representation is naturally made using a pair of isotone adjoined functions. This type of connection between two complete lattices is related to Galois connections thus making available classical mathematical results. Additional results are proved, they hold when no two approximate assertions have the same meaning. In Section 6 we study and examplify various methods which can be used in order to define a space of approximate assertions or equivalently an approximation function. They include the characterization of the least Moore family containing an arbitrary set of assertions, the construction of the least closure operator greater than or equal to an arbitrary approximation function, the definition of closure operators by composition, the definition of a space of approximate assertions by means of a complete join congruence relation or by means of a family of principal ideals. Section 7 is dedicated to the design of the approximate predicate transformer induced by a space of approximate assertions. First we look for a reasonable definition of the correctness of approximate predicate transformers and show that a local correctness condition can be given which has to be verified for every type of elementary statement. This local correctness condition ensures that the (merge over all paths or fixpoint) global analysis of any program is correct. Since isotony is not required for approximate predicate transformers to be correct it is shown that non-isotone program analysis frameworks are manageable although it is later argued that the isotony hypothesis is natural. We next show that among all possible approximate predicate transformers which can be used with a given space of approximate assertions there exists a best one which provides the maximum information relative to a program-wide analysis method. The best approximate predicate transformer induced by a space of approximate assertions turns out to be isotone. Some interesting consequences of the existence of a best predicate transformer are examined. One is that we have in hand a formal specification of the programs which have to be written in order to implement a program analysis framework once a representation of the space of approximate assertions has been chosen. Examples are given, including ones where the semantics of programs is formalized using Hoare[78]'s sets of traces. In Section 8 we show that a hierarchy of approximate analyses can be defined according to the fineness of the approximations specified by a program analysis framework. Some elements of the hierarchy are shortly exhibited and related to the relevant literature. In Section 9 we consider global program analysis methods. The distinction between \"distributive\" and \"non-distributive\" program analysis frameworks is studied. It is shown that when the best approximate predicate transformer is considered the coincidence or not of the merge over all paths and least fixpoint global analyses of programs is a consequence of the choice of the space of approximate assertions. It is shown that the space of approximate assertions can always be refined so that the merge over all paths analysis of a program can be defined by means of a least fixpoint of isotone equations. Section 10 is devoted to the combination of program analysis frameworks. We study and examplify how to perform the \"sum\", \"product\" and \"power\" of program analysis frameworks. It is shown that combined analyses lead to more accurate information than the conjunction of the corresponding separate analyses but this can only be achieved by a new design of the approximate predicate transformer induced by the combined program analysis frameworks.",
    "venue": "ACM-SIGACT Symposium on Principles of Programming Languages",
    "year": 1979,
    "citationCount": 1808,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/567752.567778",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "1979-01-01",
    "authors": [
      {
        "authorId": "1743057",
        "name": "P. Cousot"
      },
      {
        "authorId": "1758667",
        "name": "R. Cousot"
      }
    ],
    "source": "semantic_scholar",
    "score": 178.50794228092943
  },
  {
    "paperId": "f4061bd225b3be5b3f5b18eb1a229ce991efefeb",
    "url": "https://www.semanticscholar.org/paper/f4061bd225b3be5b3f5b18eb1a229ce991efefeb",
    "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization",
    "abstract": "Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.",
    "venue": "International Conference on Machine Learning",
    "year": 2019,
    "citationCount": 1907,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2019-12-18",
    "authors": [
      {
        "authorId": "47540100",
        "name": "Jingqing Zhang"
      },
      {
        "authorId": "2143397386",
        "name": "Yao Zhao"
      },
      {
        "authorId": "144413479",
        "name": "Mohammad Saleh"
      },
      {
        "authorId": "35025299",
        "name": "Peter J. Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 178.30716278012346
  },
  {
    "paperId": "690edf44e8739fd80bdfb76f40c9a4a222f3bba8",
    "url": "https://www.semanticscholar.org/paper/690edf44e8739fd80bdfb76f40c9a4a222f3bba8",
    "title": "BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer",
    "abstract": "Modeling users' dynamic preferences from their historical behaviors is challenging and crucial for recommendation systems. Previous methods employ sequential neural networks to encode users' historical interactions from left to right into hidden representations for making recommendations. Despite their effectiveness, we argue that such left-to-right unidirectional models are sub-optimal due to the limitations including: \\begin enumerate* [label=series\\itshape\\alph*\\upshape)] \\item unidirectional architectures restrict the power of hidden representation in users' behavior sequences; \\item they often assume a rigidly ordered sequence which is not always practical. \\end enumerate* To address these limitations, we proposed a sequential recommendation model called BERT4Rec, which employs the deep bidirectional self-attention to model user behavior sequences. To avoid the information leakage and efficiently train the bidirectional model, we adopt the Cloze objective to sequential recommendation, predicting the random masked items in the sequence by jointly conditioning on their left and right context. In this way, we learn a bidirectional representation model to make recommendations by allowing each item in user historical behaviors to fuse information from both left and right sides. Extensive experiments on four benchmark datasets show that our model outperforms various state-of-the-art sequential models consistently.",
    "venue": "International Conference on Information and Knowledge Management",
    "year": 2019,
    "citationCount": 1889,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Book",
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2019-04-14",
    "authors": [
      {
        "authorId": "143770118",
        "name": "Fei Sun"
      },
      {
        "authorId": "46700431",
        "name": "Jun Liu"
      },
      {
        "authorId": "2115903429",
        "name": "Jian Wu"
      },
      {
        "authorId": "3438562",
        "name": "Changhua Pei"
      },
      {
        "authorId": "2117690202",
        "name": "Xiao Lin"
      },
      {
        "authorId": "10336865",
        "name": "Wenwu Ou"
      },
      {
        "authorId": "2061280682",
        "name": "Peng Jiang"
      }
    ],
    "source": "semantic_scholar",
    "score": 178.16498162080532
  },
  {
    "paperId": "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325",
    "url": "https://www.semanticscholar.org/paper/b364cdb02d18b9d9a3c097f5ea446f7e9ab10325",
    "title": "Going deeper with Image Transformers",
    "abstract": "Transformers have been recently adapted for large scale image classification, achieving high scores shaking up the long supremacy of convolutional neural networks. However the optimization of vision transformers has been little studied so far. In this work, we build and optimize deeper transformer networks for image classification. In particular, we investigate the interplay of architecture and optimization of such dedicated transformers. We make two architecture changes that significantly improve the accuracy of deep transformers. This leads us to produce models whose performance does not saturate early with more depth, for in-stance we obtain 86.5% top-1 accuracy on Imagenet when training with no external data, we thus attain the current sate of the art with less floating-point operations and parameters. Our best model establishes the new state of the art on Imagenet with Reassessed labels and Imagenet-V2 / match frequency, in the setting with no additional training data. We share our code and models1.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 905,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.17239",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-03-31",
    "authors": [
      {
        "authorId": "2113243762",
        "name": "Hugo Touvron"
      },
      {
        "authorId": "51021910",
        "name": "M. Cord"
      },
      {
        "authorId": "3469062",
        "name": "Alexandre Sablayrolles"
      },
      {
        "authorId": "2282478",
        "name": "Gabriel Synnaeve"
      },
      {
        "authorId": "2065248680",
        "name": "Herv'e J'egou"
      }
    ],
    "source": "semantic_scholar",
    "score": 178.1355895906447
  },
  {
    "paperId": "39b492db00faead70bc3f4fb4b0364d94398ffdb",
    "url": "https://www.semanticscholar.org/paper/39b492db00faead70bc3f4fb4b0364d94398ffdb",
    "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
    "abstract": "Convolutional neural networks (CNNs) have so far been the de-facto model for visual data. Recent work has shown that (Vision) Transformer models (ViT) can achieve comparable or even superior performance on image classification tasks. This raises a central question: how are Vision Transformers solving these tasks? Are they acting like convolutional networks, or learning entirely different visual representations? Analyzing the internal representation structure of ViTs and CNNs on image classification benchmarks, we find striking differences between the two architectures, such as ViT having more uniform representations across all layers. We explore how these differences arise, finding crucial roles played by self-attention, which enables early aggregation of global information, and ViT residual connections, which strongly propagate features from lower to higher layers. We study the ramifications for spatial localization, demonstrating ViTs successfully preserve input spatial information, with noticeable effects from different classification methods. Finally, we study the effect of (pretraining) dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to new architectures such as the MLP-Mixer.",
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "citationCount": 836,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-08-19",
    "authors": [
      {
        "authorId": "40297238",
        "name": "M. Raghu"
      },
      {
        "authorId": "2465270",
        "name": "Thomas Unterthiner"
      },
      {
        "authorId": "40464924",
        "name": "Simon Kornblith"
      },
      {
        "authorId": "2145179541",
        "name": "Chiyuan Zhang"
      },
      {
        "authorId": "2841331",
        "name": "Alexey Dosovitskiy"
      }
    ],
    "source": "semantic_scholar",
    "score": 177.94736105734214
  },
  {
    "paperId": "94eae578e6af3382f6449506965639f18aab3fa0",
    "url": "https://www.semanticscholar.org/paper/94eae578e6af3382f6449506965639f18aab3fa0",
    "title": "Video Swin Transformer",
    "abstract": "The vision community is witnessing a modeling shift from CNNs to Transformers, where pure Transformer architectures have attained top accuracy on the major video recognition benchmarks. These video models are all built on Transformer layers that globally connect patches across the spatial and temporal dimensions. In this paper, we instead advocate an inductive bias of locality in video Transformers, which leads to a better speed-accuracy trade-off compared to previous approaches which compute self-attention globally even with spatial-temporal factorization. The locality of the proposed video architecture is realized by adapting the Swin Transformer designed for the image domain, while continuing to leverage the power of pre-trained image models. Our approach achieves state-of-the-art accuracy on a broad range of video recognition benchmarks, including on action recognition (84.9 top-l accuracy on Kinetics-400 and 85.9 top-l accuracy on Kinetics-600 with ~20× less pre-training data and ~3× smaller model size) and temporal modeling (69.6 top-l accuracy on Something-Something v2).",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "citationCount": 1305,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-06-24",
    "authors": [
      {
        "authorId": "2109371439",
        "name": "Ze Liu"
      },
      {
        "authorId": "2136771175",
        "name": "Jia Ning"
      },
      {
        "authorId": "2112823372",
        "name": "Yue Cao"
      },
      {
        "authorId": "2107995927",
        "name": "Yixuan Wei"
      },
      {
        "authorId": "2148904543",
        "name": "Zheng Zhang"
      },
      {
        "authorId": "145676588",
        "name": "Stephen Lin"
      },
      {
        "authorId": "1823518756",
        "name": "Han Hu"
      }
    ],
    "source": "semantic_scholar",
    "score": 177.62086464754566
  },
  {
    "paperId": "5aec474c31a2f4b74703c6f786c0a8ff85c450da",
    "url": "https://www.semanticscholar.org/paper/5aec474c31a2f4b74703c6f786c0a8ff85c450da",
    "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language",
    "abstract": "We propose VisualBERT, a simple and flexible framework for modeling a broad range of vision-and-language tasks. VisualBERT consists of a stack of Transformer layers that implicitly align elements of an input text and regions in an associated input image with self-attention. We further propose two visually-grounded language model objectives for pre-training VisualBERT on image caption data. Experiments on four vision-and-language tasks including VQA, VCR, NLVR2, and Flickr30K show that VisualBERT outperforms or rivals with state-of-the-art models while being significantly simpler. Further analysis demonstrates that VisualBERT can ground elements of language to image regions without any explicit supervision and is even sensitive to syntactic relationships, tracking, for example, associations between verbs and image regions corresponding to their arguments.",
    "venue": "arXiv.org",
    "year": 2019,
    "citationCount": 1804,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2019-08-09",
    "authors": [
      {
        "authorId": "32562635",
        "name": "Liunian Harold Li"
      },
      {
        "authorId": "2064210",
        "name": "Mark Yatskar"
      },
      {
        "authorId": "144508458",
        "name": "Da Yin"
      },
      {
        "authorId": "1793529",
        "name": "Cho-Jui Hsieh"
      },
      {
        "authorId": "2782886",
        "name": "Kai-Wei Chang"
      }
    ],
    "source": "semantic_scholar",
    "score": 177.4747380615047
  },
  {
    "paperId": "0cbf97173391b0430140117027edcaf1a37968c7",
    "url": "https://www.semanticscholar.org/paper/0cbf97173391b0430140117027edcaf1a37968c7",
    "title": "TinyBERT: Distilling BERT for Natural Language Understanding",
    "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large “teacher” BERT can be effectively transferred to a small “student” TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT4 with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERT-Base on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only ~28% parameters and ~31% inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERT-Base.",
    "venue": "Findings",
    "year": 2019,
    "citationCount": 1711,
    "openAccessPdf": {
      "url": "https://www.aclweb.org/anthology/2020.findings-emnlp.372.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2019-09-23",
    "authors": [
      {
        "authorId": "39706649",
        "name": "Xiaoqi Jiao"
      },
      {
        "authorId": "1384668226",
        "name": "Yichun Yin"
      },
      {
        "authorId": "50812138",
        "name": "Lifeng Shang"
      },
      {
        "authorId": "145820291",
        "name": "Xin Jiang"
      },
      {
        "authorId": "2117025507",
        "name": "Xiao Chen"
      },
      {
        "authorId": "2111818678",
        "name": "Linlin Li"
      },
      {
        "authorId": "49451193",
        "name": "F. Wang"
      },
      {
        "authorId": "1688015",
        "name": "Qun Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 176.6812633505253
  },
  {
    "paperId": "2835951fabf12804e17d5a525b2be2bee70e7910",
    "url": "https://www.semanticscholar.org/paper/2835951fabf12804e17d5a525b2be2bee70e7910",
    "title": "Uformer: A General U-Shaped Transformer for Image Restoration",
    "abstract": "In this paper, we present Uformer, an effective and efficient Transformer-based architecture for image restoration, in which we build a hierarchical encoder-decoder network using the Transformer block. In Uformer, there are two core designs. First, we introduce a novel locally-enhanced window (LeWin) Transformer block, which performs non-overlapping window-based self-attention instead of global self-attention. It significantly reduces the computational complexity on high resolution feature map while capturing local context. Second, we propose a learnable multi-scale restoration modulator in the form of a multi-scale spatial bias to adjust features in multiple layers of the Uformer decoder. Our modulator demonstrates superior capability for restoring details for various image restoration tasks while introducing marginal extra parameters and computational cost. Powered by these two designs, Uformer enjoys a high capability for capturing both local and global dependencies for image restoration. To evaluate our approach, extensive experiments are conducted on several image restoration tasks, including image denoising, motion deblurring, defocus deblurring and deraining. Without bells and whistles, our Uformer achieves superior or comparable performance compared with the state-of-the-art algorithms. The code and models are available at https://github.com/ZhendongWang6/Uformer.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "citationCount": 1174,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2106.03106",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-06-06",
    "authors": [
      {
        "authorId": "2108160372",
        "name": "Zhendong Wang"
      },
      {
        "authorId": "30176430",
        "name": "Xiaodong Cun"
      },
      {
        "authorId": "9324504",
        "name": "Jianmin Bao"
      },
      {
        "authorId": "2144167531",
        "name": "Jianzhuang Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 176.0353513986739
  },
  {
    "paperId": "bd9866d9230b72fde302fccdd6992bd048a12f12",
    "url": "https://www.semanticscholar.org/paper/bd9866d9230b72fde302fccdd6992bd048a12f12",
    "title": "Radial distribution test feeders",
    "abstract": "Many computer programs are available for the analysis of radial distribution feeders. In 1992 a paper was published that presented the complete data for three four-wire wye and one three-wire delta radial distribution test feeders. The purpose of publishing the data was to make available a common set of data that could be used by program developers and users to verify the correctness of their solutions. This paper presents an updated version of the same test feeders along with a simple system that can be used to test three-phase transformer models.",
    "venue": "2001 IEEE Power Engineering Society Winter Meeting. Conference Proceedings (Cat. No.01CH37194)",
    "year": 1991,
    "citationCount": 2121,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "1991-08-01",
    "authors": [
      {
        "authorId": "34579883",
        "name": "W. Kersting"
      }
    ],
    "source": "semantic_scholar",
    "score": 175.9017147876089
  },
  {
    "paperId": "acf87283fa8ae426f1a4987b345b401bf2913f61",
    "url": "https://www.semanticscholar.org/paper/acf87283fa8ae426f1a4987b345b401bf2913f61",
    "title": "Do Transformers Really Perform Badly for Graph Representation?",
    "abstract": "The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer. The code and models of Graphormer will be made publicly available at https://github.com/Microsoft/Graphormer .",
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "citationCount": 728,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2051552141",
        "name": "Chengxuan Ying"
      },
      {
        "authorId": "123970124",
        "name": "Tianle Cai"
      },
      {
        "authorId": "2108801920",
        "name": "Shengjie Luo"
      },
      {
        "authorId": "150311931",
        "name": "Shuxin Zheng"
      },
      {
        "authorId": "35286545",
        "name": "Guolin Ke"
      },
      {
        "authorId": "2266036459",
        "name": "Di He"
      },
      {
        "authorId": "2266126249",
        "name": "Yanming Shen"
      },
      {
        "authorId": "2266182896",
        "name": "Tie-Yan Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 175.8751059801299
  },
  {
    "paperId": "c95383f251a62c63217586059c67f63507c3e839",
    "url": "https://www.semanticscholar.org/paper/c95383f251a62c63217586059c67f63507c3e839",
    "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing",
    "abstract": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.",
    "venue": "arXiv.org",
    "year": 2019,
    "citationCount": 1618,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2019-10-09",
    "authors": [
      {
        "authorId": "2257007291",
        "name": "Thomas Wolf"
      },
      {
        "authorId": "1380459402",
        "name": "Lysandre Debut"
      },
      {
        "authorId": "51918868",
        "name": "Victor Sanh"
      },
      {
        "authorId": "40811585",
        "name": "Julien Chaumond"
      },
      {
        "authorId": "40899333",
        "name": "Clement Delangue"
      },
      {
        "authorId": "1382164294",
        "name": "Anthony Moi"
      },
      {
        "authorId": "1382164165",
        "name": "Pierric Cistac"
      },
      {
        "authorId": "1382164170",
        "name": "Tim Rault"
      },
      {
        "authorId": "2185329",
        "name": "Rémi Louf"
      },
      {
        "authorId": "97662964",
        "name": "Morgan Funtowicz"
      },
      {
        "authorId": "48776237",
        "name": "Joe Davison"
      },
      {
        "authorId": "88728159",
        "name": "Sam Shleifer"
      },
      {
        "authorId": "138609838",
        "name": "Patrick von Platen"
      },
      {
        "authorId": "2257128341",
        "name": "Clara Ma"
      },
      {
        "authorId": "2268491803",
        "name": "Yacine Jernite"
      },
      {
        "authorId": "3008389",
        "name": "J. Plu"
      },
      {
        "authorId": "2257127518",
        "name": "Canwen Xu"
      },
      {
        "authorId": "1379806208",
        "name": "Teven Le Scao"
      },
      {
        "authorId": "103682620",
        "name": "Sylvain Gugger"
      },
      {
        "authorId": "2125818054",
        "name": "Mariama Drame"
      },
      {
        "authorId": "2113836945",
        "name": "Quentin Lhoest"
      },
      {
        "authorId": "2260132137",
        "name": "Alexander M. Rush"
      }
    ],
    "source": "semantic_scholar",
    "score": 175.84345930516452
  },
  {
    "paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
    "url": "https://www.semanticscholar.org/paper/8323c591e119eb09b28b29fd6c7bc76bd889df7a",
    "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
    "abstract": "Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",
    "venue": "arXiv.org",
    "year": 2019,
    "citationCount": 1603,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2019-09-17",
    "authors": [
      {
        "authorId": "1911755",
        "name": "M. Shoeybi"
      },
      {
        "authorId": "66870756",
        "name": "M. Patwary"
      },
      {
        "authorId": "41158993",
        "name": "Raul Puri"
      },
      {
        "authorId": "3081566",
        "name": "P. LeGresley"
      },
      {
        "authorId": "48991386",
        "name": "J. Casper"
      },
      {
        "authorId": "2301680",
        "name": "Bryan Catanzaro"
      }
    ],
    "source": "semantic_scholar",
    "score": 175.7038368263969
  },
  {
    "paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329",
    "url": "https://www.semanticscholar.org/paper/1db9bd18681b96473f3c82b21edc9240b44dc329",
    "title": "Image Transformer",
    "abstract": "Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. We propose another extension of self-attention allowing it to efficiently take advantage of the two-dimensional nature of images. While conceptually simple, our generative models trained on two image data sets are competitive with or significantly outperform the current state of the art in autoregressive image generation on two different data sets, CIFAR-10 and ImageNet. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we show that our super-resolution models improve significantly over previously published autoregressive super-resolution models. Images they generate fool human observers three times more often than the previous state of the art.",
    "venue": "International Conference on Machine Learning",
    "year": 2018,
    "citationCount": 1583,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2018-02-15",
    "authors": [
      {
        "authorId": "3877127",
        "name": "Niki Parmar"
      },
      {
        "authorId": "40348417",
        "name": "Ashish Vaswani"
      },
      {
        "authorId": "39328010",
        "name": "Jakob Uszkoreit"
      },
      {
        "authorId": "40527594",
        "name": "Lukasz Kaiser"
      },
      {
        "authorId": "1846258",
        "name": "Noam M. Shazeer"
      },
      {
        "authorId": "31702389",
        "name": "Alexander Ku"
      },
      {
        "authorId": "47497262",
        "name": "Dustin Tran"
      }
    ],
    "source": "semantic_scholar",
    "score": 175.51562858561556
  },
  {
    "paperId": "7b664a306b7d2f68dd816ea1d6586cf3472d75c1",
    "url": "https://www.semanticscholar.org/paper/7b664a306b7d2f68dd816ea1d6586cf3472d75c1",
    "title": "Early Convolutions Help Transformers See Better",
    "abstract": "Vision transformer (ViT) models exhibit substandard optimizability. In particular, they are sensitive to the choice of optimizer (AdamW vs. SGD), optimizer hyperparameters, and training schedule length. In comparison, modern convolutional neural networks are easier to optimize. Why is this the case? In this work, we conjecture that the issue lies with the patchify stem of ViT models, which is implemented by a stride-p p*p convolution (p=16 by default) applied to the input image. This large-kernel plus large-stride convolution runs counter to typical design choices of convolutional layers in neural networks. To test whether this atypical design choice causes an issue, we analyze the optimization behavior of ViT models with their original patchify stem versus a simple counterpart where we replace the ViT stem by a small number of stacked stride-two 3*3 convolutions. While the vast majority of computation in the two ViT designs is identical, we find that this small change in early visual processing results in markedly different training behavior in terms of the sensitivity to optimization settings as well as the final model accuracy. Using a convolutional stem in ViT dramatically increases optimization stability and also improves peak performance (by ~1-2% top-1 accuracy on ImageNet-1k), while maintaining flops and runtime. The improvement can be observed across the wide spectrum of model complexities (from 1G to 36G flops) and dataset scales (from ImageNet-1k to ImageNet-21k). These findings lead us to recommend using a standard, lightweight convolutional stem for ViT models in this regime as a more robust architectural choice compared to the original ViT model design.",
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "citationCount": 700,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-06-28",
    "authors": [
      {
        "authorId": "15727192",
        "name": "Tete Xiao"
      },
      {
        "authorId": "152964870",
        "name": "Mannat Singh"
      },
      {
        "authorId": "13131689",
        "name": "Eric Mintun"
      },
      {
        "authorId": "1753210",
        "name": "Trevor Darrell"
      },
      {
        "authorId": "3127283",
        "name": "Piotr Dollár"
      },
      {
        "authorId": "2983898",
        "name": "Ross B. Girshick"
      }
    ],
    "source": "semantic_scholar",
    "score": 175.28761830551883
  },
  {
    "paperId": "0ca7d8c3250d43d14fdde46bf6fc299654d861ef",
    "url": "https://www.semanticscholar.org/paper/0ca7d8c3250d43d14fdde46bf6fc299654d861ef",
    "title": "Heterogeneous Graph Transformer",
    "abstract": "Recent years have witnessed the emerging success of graph neural networks (GNNs) for modeling structured data. However, most GNNs are designed for homogeneous graphs, in which all nodes and edges belong to the same types, making it infeasible to represent heterogeneous structures. In this paper, we present the Heterogeneous Graph Transformer (HGT) architecture for modeling Web-scale heterogeneous graphs. To model heterogeneity, we design node- and edge-type dependent parameters to characterize the heterogeneous attention over each edge, empowering HGT to maintain dedicated representations for different types of nodes and edges. To handle Web-scale graph data, we design the heterogeneous mini-batch graph sampling algorithm—HGSampling—for efficient and scalable training. Extensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges show that the proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 9–21 on various downstream tasks. The dataset and source code of HGT are publicly available at https://github.com/acbull/pyHGT.",
    "venue": "The Web Conference",
    "year": 2020,
    "citationCount": 1087,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3366423.3380027",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference"
    ],
    "publicationDate": "2020-03-03",
    "authors": [
      {
        "authorId": "3407296",
        "name": "Ziniu Hu"
      },
      {
        "authorId": "2047998",
        "name": "Yuxiao Dong"
      },
      {
        "authorId": "1748169",
        "name": "Kuansan Wang"
      },
      {
        "authorId": "2109461904",
        "name": "Yizhou Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 174.8814464112383
  },
  {
    "paperId": "b4ce7f92a8b987b5e76d580bf5076e2495f06883",
    "url": "https://www.semanticscholar.org/paper/b4ce7f92a8b987b5e76d580bf5076e2495f06883",
    "title": "TransReID: Transformer-based Object Re-Identification",
    "abstract": "Extracting robust feature representation is one of the key challenges in object re-identification (ReID). Although convolution neural network (CNN)-based methods have achieved great success, they only process one local neighborhood at a time and suffer from information loss on details caused by convolution and downsampling operators (e.g. pooling and strided convolution). To overcome these limitations, we propose a pure transformer-based object ReID framework named TransReID. Specifically, we first encode an image as a sequence of patches and build a transformer-based strong baseline with a few critical improvements, which achieves competitive results on several ReID benchmarks with CNN-based methods. To further enhance the robust feature learning in the context of transformers, two novel modules are carefully designed. (i) The jigsaw patch module (JPM) is proposed to rearrange the patch embeddings via shift and patch shuffle operations which generates robust features with improved discrimination ability and more diversified coverage. (ii) The side information embeddings (SIE) is introduced to mitigate feature bias towards camera/view variations by plugging in learnable embeddings to incorporate these non-visual clues. To the best of our knowledge, this is the first work to adopt a pure transformer for ReID research. Experimental results of TransReID are superior promising, which achieve state-of-the-art performance on both person and vehicle ReID benchmarks. Code is available at https://github.com/heshuting555/TransReID.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 717,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2102.04378",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-02-08",
    "authors": [
      {
        "authorId": "2115300944",
        "name": "Shuting He"
      },
      {
        "authorId": "2110564218",
        "name": "Haowen Luo"
      },
      {
        "authorId": "8120382",
        "name": "Pichao Wang"
      },
      {
        "authorId": "1716453",
        "name": "F. Wang"
      },
      {
        "authorId": "144966714",
        "name": "Hao Li"
      },
      {
        "authorId": "1753694704",
        "name": "Wei Jiang"
      }
    ],
    "source": "semantic_scholar",
    "score": 174.64704353572336
  },
  {
    "paperId": "563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06",
    "url": "https://www.semanticscholar.org/paper/563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06",
    "title": "FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting",
    "abstract": "Although Transformer-based methods have significantly improved state-of-the-art results for long-term series forecasting, they are not only computationally expensive but more importantly, are unable to capture the global view of time series (e.g. overall trend). To address these problems, we propose to combine Transformer with the seasonal-trend decomposition method, in which the decomposition method captures the global profile of time series while Transformers capture more detailed structures. To further enhance the performance of Transformer for long-term prediction, we exploit the fact that most time series tend to have a sparse representation in well-known basis such as Fourier transform, and develop a frequency enhanced Transformer. Besides being more effective, the proposed method, termed as Frequency Enhanced Decomposed Transformer ({\\bf FEDformer}), is more efficient than standard Transformer with a linear complexity to the sequence length. Our empirical studies with six benchmark datasets show that compared with state-of-the-art methods, FEDformer can reduce prediction error by $14.8\\%$ and $22.6\\%$ for multivariate and univariate time series, respectively. Code is publicly available at https://github.com/MAZiqing/FEDformer.",
    "venue": "International Conference on Machine Learning",
    "year": 2022,
    "citationCount": 1055,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-01-30",
    "authors": [
      {
        "authorId": "50018188",
        "name": "Tian Zhou"
      },
      {
        "authorId": "1387898803",
        "name": "Ziqing Ma"
      },
      {
        "authorId": "3308963",
        "name": "Qingsong Wen"
      },
      {
        "authorId": "2118294665",
        "name": "Xue Wang"
      },
      {
        "authorId": "2110940896",
        "name": "Liang Sun"
      },
      {
        "authorId": "2152101871",
        "name": "Rong Jin"
      }
    ],
    "source": "semantic_scholar",
    "score": 174.4336519639931
  },
  {
    "paperId": "003326a15fc4a8833785a47a741d7712474fa256",
    "url": "https://www.semanticscholar.org/paper/003326a15fc4a8833785a47a741d7712474fa256",
    "title": "LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference",
    "abstract": "We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers.As a result, we propose LeViT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at https://github.com/facebookresearch/LeViT.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 693,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2104.01136",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-04-02",
    "authors": [
      {
        "authorId": "143853801",
        "name": "Benjamin Graham"
      },
      {
        "authorId": "1388811741",
        "name": "Alaaeldin El-Nouby"
      },
      {
        "authorId": "2113243762",
        "name": "Hugo Touvron"
      },
      {
        "authorId": "37502184",
        "name": "Pierre Stock"
      },
      {
        "authorId": "2319608",
        "name": "Armand Joulin"
      },
      {
        "authorId": "2065248680",
        "name": "Herv'e J'egou"
      },
      {
        "authorId": "3271933",
        "name": "Matthijs Douze"
      }
    ],
    "source": "semantic_scholar",
    "score": 174.13707940760207
  },
  {
    "paperId": "388e2fcdcefbe0834e153ab2a0be127092f9674d",
    "url": "https://www.semanticscholar.org/paper/388e2fcdcefbe0834e153ab2a0be127092f9674d",
    "title": "DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation",
    "abstract": "We present a large, tunable neural conversational response generation model, DIALOGPT (dialogue generative pre-trained transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems that leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent open-domain dialogue systems.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "citationCount": 1441,
    "openAccessPdf": {
      "url": "https://www.aclweb.org/anthology/2020.acl-demos.30.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2019-11-01",
    "authors": [
      {
        "authorId": "3272356",
        "name": "Yizhe Zhang"
      },
      {
        "authorId": "2109508754",
        "name": "Siqi Sun"
      },
      {
        "authorId": "1947267",
        "name": "Michel Galley"
      },
      {
        "authorId": "2378902",
        "name": "Yen-Chun Chen"
      },
      {
        "authorId": "3125776",
        "name": "Chris Brockett"
      },
      {
        "authorId": "71886367",
        "name": "Xiang Gao"
      },
      {
        "authorId": "1800422",
        "name": "Jianfeng Gao"
      },
      {
        "authorId": "46700348",
        "name": "Jingjing Liu"
      },
      {
        "authorId": "83415753",
        "name": "W. Dolan"
      }
    ],
    "source": "semantic_scholar",
    "score": 174.10679476767342
  },
  {
    "paperId": "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e",
    "url": "https://www.semanticscholar.org/paper/da74a10824193be9d3889ce0d6ed4c6f8ee48b9e",
    "title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer",
    "abstract": "Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets",
    "venue": "International Conference on Learning Representations",
    "year": 2021,
    "citationCount": 1027,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-10-05",
    "authors": [
      {
        "authorId": "144839857",
        "name": "Sachin Mehta"
      },
      {
        "authorId": "143887493",
        "name": "Mohammad Rastegari"
      }
    ],
    "source": "semantic_scholar",
    "score": 174.03055669022666
  },
  {
    "paperId": "bd20069f5cac3e63083ecf6479abc1799db33ce0",
    "url": "https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0",
    "title": "A Primer in BERTology: What We Know About How BERT Works",
    "abstract": "Abstract Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2020,
    "citationCount": 1386,
    "openAccessPdf": {
      "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2020-02-27",
    "authors": [
      {
        "authorId": "145046059",
        "name": "Anna Rogers"
      },
      {
        "authorId": "152176221",
        "name": "Olga Kovaleva"
      },
      {
        "authorId": "1681193",
        "name": "Anna Rumshisky"
      }
    ],
    "source": "semantic_scholar",
    "score": 173.52347630472246
  },
  {
    "paperId": "6075091294a0fe0fe5c6c7b7a0df9029b6a965cb",
    "url": "https://www.semanticscholar.org/paper/6075091294a0fe0fe5c6c7b7a0df9029b6a965cb",
    "title": "SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks",
    "abstract": "We introduce the SE(3)-Transformer, a variant of the self-attention module for 3D point clouds, which is equivariant under continuous 3D roto-translations. Equivariance is important to ensure stable and predictable performance in the presence of nuisance transformations of the data input. A positive corollary of equivariance is increased weight-tying within the model, leading to fewer trainable parameters and thus decreased sample complexity (i.e. we need less training data). The SE(3)-Transformer leverages the benefits of self-attention to operate on large point clouds with varying number of points, while guaranteeing SE(3)-equivariance for robustness. We evaluate our model on a toy $N$-body particle simulation dataset, showcasing the robustness of the predictions under rotations of the input. We further achieve competitive performance on two real-world datasets, ScanObjectNN and QM9. In all cases, our model outperforms a strong, non-equivariant attention baseline and an equivariant model without attention.",
    "venue": "Neural Information Processing Systems",
    "year": 2020,
    "citationCount": 619,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2020-06-18",
    "authors": [
      {
        "authorId": "38593960",
        "name": "F. Fuchs"
      },
      {
        "authorId": "3471551",
        "name": "Daniel E. Worrall"
      },
      {
        "authorId": "2144816511",
        "name": "Volker Fischer"
      },
      {
        "authorId": "1678311",
        "name": "M. Welling"
      }
    ],
    "source": "semantic_scholar",
    "score": 173.44579217058708
  },
  {
    "paperId": "63748e59f4e106cbda6b65939b77589f40e48fcb",
    "url": "https://www.semanticscholar.org/paper/63748e59f4e106cbda6b65939b77589f40e48fcb",
    "title": "Text Summarization with Pretrained Encoders",
    "abstract": "Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2019,
    "citationCount": 1363,
    "openAccessPdf": {
      "url": "https://www.aclweb.org/anthology/D19-1387.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2019-08-01",
    "authors": [
      {
        "authorId": "39798499",
        "name": "Yang Liu"
      },
      {
        "authorId": "1747893",
        "name": "Mirella Lapata"
      }
    ],
    "source": "semantic_scholar",
    "score": 173.27265257605112
  },
  {
    "paperId": "b91de7d12ec1103f6ef9eb0720d697a9e7ecc9fe",
    "url": "https://www.semanticscholar.org/paper/b91de7d12ec1103f6ef9eb0720d697a9e7ecc9fe",
    "title": "LoFTR: Detector-Free Local Feature Matching with Transformers",
    "abstract": "We present a novel method for local image feature matching. Instead of performing image feature detection, description, and matching sequentially, we propose to first establish pixel-wise dense matches at a coarse level and later refine the good matches at a fine level. In contrast to dense methods that use a cost volume to search correspondences, we use self and cross attention layers in Transformer to obtain feature descriptors that are conditioned on both images. The global receptive field provided by Transformer enables our method to produce dense matches in low-texture areas, where feature detectors usually struggle to produce repeatable interest points. The experiments on indoor and outdoor datasets show that LoFTR outperforms state-of-the-art methods by a large margin. LoFTR also ranks first on two public benchmarks of visual localization among the published methods. Code is available at our project page: https://zju3dv.github.io/loftr/.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "citationCount": 959,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2104.00680",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-04-01",
    "authors": [
      {
        "authorId": "153552118",
        "name": "Jiaming Sun"
      },
      {
        "authorId": "1384523019",
        "name": "Zehong Shen"
      },
      {
        "authorId": "37075603",
        "name": "Yuang Wang"
      },
      {
        "authorId": "1679542",
        "name": "H. Bao"
      },
      {
        "authorId": "145453113",
        "name": "Xiaowei Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 173.00399926692825
  },
  {
    "paperId": "2a805d0e1b067444a554c5169d189fa1f649f411",
    "url": "https://www.semanticscholar.org/paper/2a805d0e1b067444a554c5169d189fa1f649f411",
    "title": "Scaling Vision Transformers",
    "abstract": "Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10 examples per class.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "citationCount": 950,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2106.04560",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-06-08",
    "authors": [
      {
        "authorId": "2743563",
        "name": "Xiaohua Zhai"
      },
      {
        "authorId": "144629422",
        "name": "Alexander Kolesnikov"
      },
      {
        "authorId": "2815290",
        "name": "N. Houlsby"
      },
      {
        "authorId": "39611591",
        "name": "Lucas Beyer"
      }
    ],
    "source": "semantic_scholar",
    "score": 172.86271093818084
  },
  {
    "paperId": "72af9b2e03d3668e09edd0ec413b0b20cbce8f9c",
    "url": "https://www.semanticscholar.org/paper/72af9b2e03d3668e09edd0ec413b0b20cbce8f9c",
    "title": "Learning Spatio-Temporal Transformer for Visual Tracking",
    "abstract": "In this paper, we present a new tracking architecture with an encoder-decoder transformer as the key component. The encoder models the global spatio-temporal feature dependencies between target objects and search regions, while the decoder learns a query embedding to predict the spatial positions of the target objects. Our method casts object tracking as a direct bounding box prediction problem, without using any proposals or predefined anchors. With the encoder-decoder transformer, the prediction of objects just uses a simple fully-convolutional network, which estimates the corners of objects directly. The whole method is end-to-end, does not need any postprocessing steps such as cosine window and bounding box smoothing, thus largely simplifying existing tracking pipelines. The proposed tracker achieves state-of-the-art performance on multiple challenging short-term and long-term benchmarks, while running at real-time speed, being 6× faster than Siam R-CNN [54]. Code and models are open-sourced at https://github.com/researchmm/Stark.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 633,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.17154",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-03-31",
    "authors": [
      {
        "authorId": "47713338",
        "name": "Bin Yan"
      },
      {
        "authorId": "2484788",
        "name": "Houwen Peng"
      },
      {
        "authorId": "3247966",
        "name": "Jianlong Fu"
      },
      {
        "authorId": "143987328",
        "name": "Dong Wang"
      },
      {
        "authorId": "153176123",
        "name": "Huchuan Lu"
      }
    ],
    "source": "semantic_scholar",
    "score": 172.78073431655838
  },
  {
    "paperId": "16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78",
    "url": "https://www.semanticscholar.org/paper/16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78",
    "title": "Bottleneck Transformers for Visual Recognition",
    "abstract": "We present BoTNet, a conceptually simple yet powerful backbone architecture that incorporates self-attention for multiple computer vision tasks including image classification, object detection and instance segmentation. By just replacing the spatial convolutions with global self-attention in the final three bottleneck blocks of a ResNet and no other changes, our approach improves upon the baselines significantly on instance segmentation and object detection while also reducing the parameters, with minimal overhead in latency. Through the design of BoTNet, we also point out how ResNet bottleneck blocks with self-attention can be viewed as Transformer blocks. Without any bells and whistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO Instance Segmentation benchmark using the Mask R-CNN framework; surpassing the previous best published single model and single scale results of ResNeSt [67] evaluated on the COCO validation set. Finally, we present a simple adaptation of the BoTNet design for image classification, resulting in models that achieve a strong performance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to 1.64x faster in \"compute\"1 time than the popular EfficientNet models on TPU-v3 hardware. We hope our simple and effective approach will serve as a strong baseline for future research in self-attention models for vision.2",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "citationCount": 922,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2101.11605",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-01-27",
    "authors": [
      {
        "authorId": "41207614",
        "name": "A. Srinivas"
      },
      {
        "authorId": "33493200",
        "name": "Tsung-Yi Lin"
      },
      {
        "authorId": "3877127",
        "name": "Niki Parmar"
      },
      {
        "authorId": "1789737",
        "name": "Jonathon Shlens"
      },
      {
        "authorId": "1689992",
        "name": "P. Abbeel"
      },
      {
        "authorId": "1630664874",
        "name": "Ashish Vaswani"
      }
    ],
    "source": "semantic_scholar",
    "score": 172.41443851754278
  },
  {
    "paperId": "9144938e4ff665638218f29f19cd95894b9c5e74",
    "url": "https://www.semanticscholar.org/paper/9144938e4ff665638218f29f19cd95894b9c5e74",
    "title": "Zero Duality Gap in Optimal Power Flow Problem",
    "abstract": "The optimal power flow (OPF) problem is nonconvex and generally hard to solve. In this paper, we propose a semidefinite programming (SDP) optimization, which is the dual of an equivalent form of the OPF problem. A global optimum solution to the OPF problem can be retrieved from a solution of this convex dual problem whenever the duality gap is zero. A necessary and sufficient condition is provided in this paper to guarantee the existence of no duality gap for the OPF problem. This condition is satisfied by the standard IEEE benchmark systems with 14, 30, 57, 118, and 300 buses as well as several randomly generated systems. Since this condition is hard to study, a sufficient zero-duality-gap condition is also derived. This sufficient condition holds for IEEE systems after small resistance (10-5 per unit) is added to every transformer that originally assumes zero resistance. We investigate this sufficient condition and justify that it holds widely in practice. The main underlying reason for the successful convexification of the OPF problem can be traced back to the modeling of transformers and transmission lines as well as the non-negativity of physical quantities such as resistance and inductance.",
    "venue": "IEEE Transactions on Power Systems",
    "year": 2012,
    "citationCount": 1201,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Mathematics"
    ],
    "publicationTypes": null,
    "publicationDate": "2012-02-01",
    "authors": [
      {
        "authorId": "1688041",
        "name": "J. Lavaei"
      },
      {
        "authorId": "36255406",
        "name": "S. Low"
      }
    ],
    "source": "semantic_scholar",
    "score": 172.37613172642727
  },
  {
    "paperId": "216e8f12492bd11ad7047de6080d6bcd6d195aa0",
    "url": "https://www.semanticscholar.org/paper/216e8f12492bd11ad7047de6080d6bcd6d195aa0",
    "title": "Use of negative capacitance to provide voltage amplification for low power nanoscale devices.",
    "abstract": "It is well-known that conventional field effect transistors (FETs) require a change in the channel potential of at least 60 mV at 300 K to effect a change in the current by a factor of 10, and this minimum subthreshold slope S puts a fundamental lower limit on the operating voltage and hence the power dissipation in standard FET-based switches. Here, we suggest that by replacing the standard insulator with a ferroelectric insulator of the right thickness it should be possible to implement a step-up voltage transformer that will amplify the gate voltage thus leading to values of S lower than 60 mV/decade and enabling low voltage/low power operation. The voltage transformer action can be understood intuitively as the result of an effective negative capacitance provided by the ferroelectric capacitor that arises from an internal positive feedback that in principle could be obtained from other microscopic mechanisms as well. Unlike other proposals to reduce S, this involves no change in the basic physics of the FET and thus does not affect its current drive or impose other restrictions.",
    "venue": "Nano letters (Print)",
    "year": 2008,
    "citationCount": 1721,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Physics",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2008-02-01",
    "authors": [
      {
        "authorId": "2233742",
        "name": "S. Salahuddin"
      },
      {
        "authorId": "143784017",
        "name": "S. Datta"
      }
    ],
    "source": "semantic_scholar",
    "score": 171.76862527481512
  },
  {
    "paperId": "002c256d30d6be4b23d365a8de8ae0e67e4c9641",
    "url": "https://www.semanticscholar.org/paper/002c256d30d6be4b23d365a8de8ae0e67e4c9641",
    "title": "Improving language models by retrieving from trillions of tokens",
    "abstract": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",
    "venue": "International Conference on Machine Learning",
    "year": 2021,
    "citationCount": 880,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-12-08",
    "authors": [
      {
        "authorId": "148016269",
        "name": "Sebastian Borgeaud"
      },
      {
        "authorId": "1697879",
        "name": "A. Mensch"
      },
      {
        "authorId": "46616544",
        "name": "Jordan Hoffmann"
      },
      {
        "authorId": "2072572294",
        "name": "Trevor Cai"
      },
      {
        "authorId": "2143538252",
        "name": "Eliza Rutherford"
      },
      {
        "authorId": "2143434227",
        "name": "Katie Millican"
      },
      {
        "authorId": "47568983",
        "name": "George van den Driessche"
      },
      {
        "authorId": "143783339",
        "name": "Jean-Baptiste Lespiau"
      },
      {
        "authorId": "2143374656",
        "name": "Bogdan Damoc"
      },
      {
        "authorId": "31993415",
        "name": "Aidan Clark"
      },
      {
        "authorId": "40550616",
        "name": "Diego de Las Casas"
      },
      {
        "authorId": "40895205",
        "name": "Aurelia Guy"
      },
      {
        "authorId": "10698483",
        "name": "Jacob Menick"
      },
      {
        "authorId": "81387328",
        "name": "Roman Ring"
      },
      {
        "authorId": "4629007",
        "name": "T. Hennigan"
      },
      {
        "authorId": "2148653469",
        "name": "Saffron Huang"
      },
      {
        "authorId": "108173905",
        "name": "Lorenzo Maggiore"
      },
      {
        "authorId": "2115601070",
        "name": "Chris Jones"
      },
      {
        "authorId": "51042571",
        "name": "Albin Cassirer"
      },
      {
        "authorId": "2065040422",
        "name": "Andy Brock"
      },
      {
        "authorId": "35550664",
        "name": "Michela Paganini"
      },
      {
        "authorId": "2060655766",
        "name": "G. Irving"
      },
      {
        "authorId": "1689108",
        "name": "O. Vinyals"
      },
      {
        "authorId": "2217144",
        "name": "Simon Osindero"
      },
      {
        "authorId": "34838386",
        "name": "K. Simonyan"
      },
      {
        "authorId": "34269227",
        "name": "Jack W. Rae"
      },
      {
        "authorId": "152585800",
        "name": "Erich Elsen"
      },
      {
        "authorId": "2175946",
        "name": "L. Sifre"
      }
    ],
    "source": "semantic_scholar",
    "score": 171.71586438904268
  },
  {
    "paperId": "1359d2ef45f1550941e22bf046026c89f6edf315",
    "url": "https://www.semanticscholar.org/paper/1359d2ef45f1550941e22bf046026c89f6edf315",
    "title": "AraBERT: Transformer-based Model for Arabic Language Understanding",
    "abstract": "The Arabic language is a morphologically rich language with relatively few resources and a less explored syntax compared to English. Given these limitations, Arabic Natural Language Processing (NLP) tasks like Sentiment Analysis (SA), Named Entity Recognition (NER), and Question Answering (QA), have proven to be very challenging to tackle. Recently, with the surge of transformers based models, language-specific BERT based models have proven to be very efficient at language understanding, provided they are pre-trained on a very large corpus. Such models were able to set new standards and achieve state-of-the-art results for most NLP tasks. In this paper, we pre-trained BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language. The performance of AraBERT is compared to multilingual BERT from Google and other state-of-the-art approaches. The results showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks. The pretrained araBERT models are publicly available on https://github.com/aub-mind/araBERT hoping to encourage research and applications for Arabic NLP.",
    "venue": "OSACT",
    "year": 2020,
    "citationCount": 873,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2020-02-01",
    "authors": [
      {
        "authorId": "51040671",
        "name": "Wissam Antoun"
      },
      {
        "authorId": "1412947005",
        "name": "Fady Baly"
      },
      {
        "authorId": "145336745",
        "name": "Hazem M. Hajj"
      }
    ],
    "source": "semantic_scholar",
    "score": 171.59620563483304
  },
  {
    "paperId": "f0524b3005720bcff886bcb0227f7f0dd924ff07",
    "url": "https://www.semanticscholar.org/paper/f0524b3005720bcff886bcb0227f7f0dd924ff07",
    "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text",
    "abstract": "We present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, our Video-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks. We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval. Furthermore, we study a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities. We show that the convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream tasks. Especially, VATT's vision Transformer achieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600, 72.7% on Kinetics-700, and 41.1% on Moments in Time, new records while avoiding supervised pre-training. Transferring to image classification leads to 78.7% top-1 accuracy on ImageNet compared to 64.7% by training the same Transformer from scratch, showing the generalizability of our model despite the domain gap between videos and images. VATT's audio Transformer also sets a new record on waveform-based audio event recognition by achieving the mAP of 39.4% on AudioSet without any supervised pre-training. VATT's source code is publicly available.",
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "citationCount": 539,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-04-22",
    "authors": [
      {
        "authorId": "153769937",
        "name": "Hassan Akbari"
      },
      {
        "authorId": null,
        "name": "Linagzhe Yuan"
      },
      {
        "authorId": "47519958",
        "name": "Rui Qian"
      },
      {
        "authorId": "1764944",
        "name": "Wei-Hong Chuang"
      },
      {
        "authorId": "9546964",
        "name": "Shih-Fu Chang"
      },
      {
        "authorId": "50355189",
        "name": "Yin Cui"
      },
      {
        "authorId": "40206014",
        "name": "Boqing Gong"
      }
    ],
    "source": "semantic_scholar",
    "score": 171.3735370933748
  },
  {
    "paperId": "800cfb3d23115cdcd4d114234b65bbdf2080f798",
    "url": "https://www.semanticscholar.org/paper/800cfb3d23115cdcd4d114234b65bbdf2080f798",
    "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows",
    "abstract": "We present CSWin Transformer, an efficient and effective Transformer-based backbone for general-purpose vision tasks. A challenging issue in Transformer design is that global self-attention is very expensive to compute whereas local self-attention often limits the field of interactions of each token. To address this issue, we develop the Cross-Shaped Window self-attention mechanism for computing self-attention in the horizontal and vertical stripes in parallel that form a cross-shaped window, with each stripe obtained by splitting the input feature into stripes of equal width. We provide a mathematical analysis of the effect of the stripe width and vary the stripe width for different layers of the Transformer network which achieves strong modeling capability while limiting the computation cost. We also introduce Locally-enhanced Positional Encoding (LePE), which handles the local positional information better than existing encoding schemes. LePE naturally supports arbitrary input resolutions, and is thus especially effective and friendly for downstream tasks. Incorporated with these designs and a hierarchical structure, CSWin Transformer demonstrates competitive performance on common vision tasks. Specifically, it achieves 85.4% Top-1 accuracy on ImageNet-1K without any extra training data or label, 53.9 box AP and 46.4 mask AP on the COCO detection task, and 52.2 mIOU on the ADE20K semantic segmentation task, surpassing previous state-of-the-art Swin Transformer backbone by +1.2, +2.0, +1.4, and +2.0 respectively under the similar FLOPs setting. By further pretraining on the larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K and high segmentation performance on ADE20K with 55.7 mIoU. 11Code and pretrain model is available at https://github.com/microsoft/CSWin-Transformer",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "citationCount": 856,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2107.00652",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-07-01",
    "authors": [
      {
        "authorId": "5620602",
        "name": "Xiaoyi Dong"
      },
      {
        "authorId": "9324504",
        "name": "Jianmin Bao"
      },
      {
        "authorId": "49025801",
        "name": "Dongdong Chen"
      },
      {
        "authorId": "47528018",
        "name": "Weiming Zhang"
      },
      {
        "authorId": "1708598",
        "name": "Nenghai Yu"
      },
      {
        "authorId": "145347147",
        "name": "Lu Yuan"
      },
      {
        "authorId": "47514557",
        "name": "Dong Chen"
      },
      {
        "authorId": "2261753424",
        "name": "B. Guo"
      }
    ],
    "source": "semantic_scholar",
    "score": 171.3015687789667
  },
  {
    "paperId": "7c3ce1b3ad598a282546e03e2dc8b52c338caed6",
    "url": "https://www.semanticscholar.org/paper/7c3ce1b3ad598a282546e03e2dc8b52c338caed6",
    "title": "Transformer Tracking",
    "abstract": "Correlation acts as a critical role in the tracking field, especially in recent popular Siamese-based trackers. The correlation operation is a simple fusion manner to consider the similarity between the template and the search region. However, the correlation operation itself is a local linear matching process, leading to lose semantic information and fall into local optimum easily, which may be the bottleneck of designing high-accuracy tracking algorithms. Is there any better feature fusion method than correlation? To address this issue, inspired by Transformer, this work presents a novel attention-based feature fusion network, which effectively combines the template and search region features solely using attention. Specifically, the proposed method includes an ego-context augment module based on self-attention and a cross-feature augment module based on cross-attention. Finally, we present a Transformer tracking (named TransT) method based on the Siamese-like feature extraction backbone, the designed attention-based fusion mechanism, and the classification and regression head. Experiments show that our TransT achieves very promising results on six challenging datasets, especially on large-scale LaSOT, TrackingNet, and GOT-10k benchmarks. Our tracker runs at approximatively 50 fps on GPU. Code and models are available at https://github.com/chenxin-dlut/TransT.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "citationCount": 854,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-03-29",
    "authors": [
      {
        "authorId": "2145230123",
        "name": "Xin Chen"
      },
      {
        "authorId": "47713338",
        "name": "Bin Yan"
      },
      {
        "authorId": "2109445463",
        "name": "Jiawen Zhu"
      },
      {
        "authorId": "143987328",
        "name": "Dong Wang"
      },
      {
        "authorId": "50031004",
        "name": "Xiaoyun Yang"
      },
      {
        "authorId": "153176123",
        "name": "Huchuan Lu"
      }
    ],
    "source": "semantic_scholar",
    "score": 171.2665220340514
  },
  {
    "paperId": "75acc731bdd2b626edc74672a30da3bc51010ae8",
    "url": "https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8",
    "title": "CTRL: A Conditional Transformer Language Model for Controllable Generation",
    "abstract": "Large-scale language models show promising text generation capabilities, but users cannot easily control particular aspects of the generated text. We release CTRL, a 1.63 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-specific behavior. Control codes were derived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while providing more explicit control over text generation. These codes also allow CTRL to predict which parts of the training data are most likely given a sequence. This provides a potential method for analyzing large amounts of data via model-based source attribution. We have released multiple full-sized, pretrained versions of CTRL at this https URL.",
    "venue": "arXiv.org",
    "year": 2019,
    "citationCount": 1166,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2019-09-11",
    "authors": [
      {
        "authorId": "2844898",
        "name": "N. Keskar"
      },
      {
        "authorId": "143775536",
        "name": "Bryan McCann"
      },
      {
        "authorId": "1697944",
        "name": "L. Varshney"
      },
      {
        "authorId": "2228109",
        "name": "Caiming Xiong"
      },
      {
        "authorId": "2166511",
        "name": "R. Socher"
      }
    ],
    "source": "semantic_scholar",
    "score": 170.93287448429834
  },
  {
    "paperId": "2fe2f849b94cf08b559226bc9d78adcaef5ef186",
    "url": "https://www.semanticscholar.org/paper/2fe2f849b94cf08b559226bc9d78adcaef5ef186",
    "title": "AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition",
    "abstract": "Pretraining Vision Transformers (ViTs) has achieved great success in visual recognition. A following scenario is to adapt a ViT to various image and video recognition tasks. The adaptation is challenging because of heavy computation and memory storage. Each model needs an independent and complete finetuning process to adapt to different tasks, which limits its transferability to different visual domains. To address this challenge, we propose an effective adaptation approach for Transformer, namely AdaptFormer, which can adapt the pre-trained ViTs into many different image and video tasks efficiently. It possesses several benefits more appealing than prior arts. Firstly, AdaptFormer introduces lightweight modules that only add less than 2% extra parameters to a ViT, while it is able to increase the ViT's transferability without updating its original pre-trained parameters, significantly outperforming the existing 100\\% fully fine-tuned models on action recognition benchmarks. Secondly, it can be plug-and-play in different Transformers and scalable to many visual tasks. Thirdly, extensive experiments on five image and video datasets show that AdaptFormer largely improves ViTs in the target domains. For example, when updating just 1.5% extra parameters, it achieves about 10% and 19% relative improvement compared to the fully fine-tuned models on Something-Something~v2 and HMDB51, respectively. Code is available at https://github.com/ShoufaChen/AdaptFormer.",
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "citationCount": 518,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2205.13535",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-05-26",
    "authors": [
      {
        "authorId": "2107977968",
        "name": "Shoufa Chen"
      },
      {
        "authorId": "102482926",
        "name": "Chongjian Ge"
      },
      {
        "authorId": "2143681543",
        "name": "Zhan Tong"
      },
      {
        "authorId": "46584859",
        "name": "Jiangliu Wang"
      },
      {
        "authorId": "2255687",
        "name": "Yibing Song"
      },
      {
        "authorId": "2167482071",
        "name": "Jue Wang"
      },
      {
        "authorId": "2143481782",
        "name": "Ping Luo"
      }
    ],
    "source": "semantic_scholar",
    "score": 170.77855824748832
  },
  {
    "paperId": "949fef650da4c41afe6049a183b504b3cc91f4bd",
    "url": "https://www.semanticscholar.org/paper/949fef650da4c41afe6049a183b504b3cc91f4bd",
    "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences",
    "abstract": "Human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist: 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. In addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2019,
    "citationCount": 1137,
    "openAccessPdf": {
      "url": "https://www.aclweb.org/anthology/P19-1656.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2019-06-01",
    "authors": [
      {
        "authorId": "145639633",
        "name": "Yao-Hung Hubert Tsai"
      },
      {
        "authorId": "35836381",
        "name": "Shaojie Bai"
      },
      {
        "authorId": "28130078",
        "name": "Paul Pu Liang"
      },
      {
        "authorId": "145116464",
        "name": "J. Z. Kolter"
      },
      {
        "authorId": "49933077",
        "name": "Louis-Philippe Morency"
      },
      {
        "authorId": "145124475",
        "name": "R. Salakhutdinov"
      }
    ],
    "source": "semantic_scholar",
    "score": 170.55541422029415
  },
  {
    "paperId": "1197ae4a62f0e0e4e3f3fb70396b5ff06ef371aa",
    "url": "https://www.semanticscholar.org/paper/1197ae4a62f0e0e4e3f3fb70396b5ff06ef371aa",
    "title": "CogView: Mastering Text-to-Image Generation via Transformers",
    "abstract": "Text-to-Image generation in the general domain has long been an open problem, which requires both a powerful generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E.",
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "citationCount": 687,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-05-26",
    "authors": [
      {
        "authorId": "2055623340",
        "name": "Ming Ding"
      },
      {
        "authorId": "2109506541",
        "name": "Zhuoyi Yang"
      },
      {
        "authorId": "2105844599",
        "name": "Wenyi Hong"
      },
      {
        "authorId": "31253908",
        "name": "Wendi Zheng"
      },
      {
        "authorId": "144161025",
        "name": "Chang Zhou"
      },
      {
        "authorId": "2075292864",
        "name": "Da Yin"
      },
      {
        "authorId": "35996608",
        "name": "Junyang Lin"
      },
      {
        "authorId": "46927790",
        "name": "Xu Zou"
      },
      {
        "authorId": "1740580186",
        "name": "Zhou Shao"
      },
      {
        "authorId": "38385080",
        "name": "Hongxia Yang"
      },
      {
        "authorId": "2109541439",
        "name": "Jie Tang"
      }
    ],
    "source": "semantic_scholar",
    "score": 170.00683256900015
  },
  {
    "paperId": "57150ca7d793d6f784cf82da1c349edf7beb6bc2",
    "url": "https://www.semanticscholar.org/paper/57150ca7d793d6f784cf82da1c349edf7beb6bc2",
    "title": "MetaFormer is Actually What You Need for Vision",
    "abstract": "Transformers have shown great potential in computer vision tasks. A common belief is their attention-based token mixer module contributes most to their competence. However, recent works show the attention-based module in transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this observation, we hypothesize that the general architecture of the transformers, instead of the specific token mixer module, is more essential to the model's performance. To verify this, we deliberately replace the attention module in transformers with an embarrassingly simple spatial pooling operator to conduct only basic token mixing. Surprisingly, we observe that the derived model, termed as PoolFormer, achieves competitive performance on multiple computer vision tasks. For example, on ImageNet-1K, PoolFormer achieves 82.1 % top-1 accuracy, surpassing well-tuned vision transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy with 35%/52% fewer parameters and 49%/61% fewer MACs. The effectiveness of Pool-Former verifies our hypothesis and urges us to initiate the concept of “MetaFormer”, a general architecture abstracted from transformers without specifying the token mixer. Based on the extensive experiments, we argue that MetaFormer is the key player in achieving superior results for recent transformer and MLP-like models on vision tasks. This work calls for more future research dedicated to improving MetaFormer instead of focusing on the token mixer modules. Additionally, our proposed PoolFormer could serve as a starting baseline for future MetaFormer architecture design.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "citationCount": 776,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2111.11418",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-11-22",
    "authors": [
      {
        "authorId": "23476952",
        "name": "Weihao Yu"
      },
      {
        "authorId": "2109495772",
        "name": "Mi Luo"
      },
      {
        "authorId": "2109890021",
        "name": "Pan Zhou"
      },
      {
        "authorId": "39927579",
        "name": "Chenyang Si"
      },
      {
        "authorId": "46432827",
        "name": "Yichen Zhou"
      },
      {
        "authorId": "48631088",
        "name": "Xinchao Wang"
      },
      {
        "authorId": "1698982",
        "name": "Jiashi Feng"
      },
      {
        "authorId": "143653681",
        "name": "Shuicheng Yan"
      }
    ],
    "source": "semantic_scholar",
    "score": 169.83160525551472
  },
  {
    "paperId": "40f4d7fe800810288a80f84cdb357a8f4c28e880",
    "url": "https://www.semanticscholar.org/paper/40f4d7fe800810288a80f84cdb357a8f4c28e880",
    "title": "Rethinking Spatial Dimensions of Vision Transformers",
    "abstract": "Vision Transformer (ViT) extends the application range of transformers from language processing to computer vision tasks as being an alternative architecture against the existing convolutional neural networks (CNN). Since the transformer-based architecture has been innovative for computer vision modeling, the design convention towards an effective architecture has been less studied yet. From the successful design principles of CNN, we investigate the role of spatial dimension conversion and its effectiveness on transformer-based architecture. We particularly attend to the dimension reduction principle of CNNs; as the depth increases, a conventional CNN increases channel dimension and decreases spatial dimensions. We empirically show that such a spatial dimension reduction is beneficial to a transformer architecture as well, and propose a novel Pooling-based Vision Transformer (PiT) upon the original ViT model. We show that PiT achieves the improved model capability and generalization performance against ViT. Throughout the extensive experiments, we further show PiT outperforms the baseline on several tasks such as image classification, object detection, and robustness evaluation. Source codes and ImageNet models are available at https://github.com/naver-ai/pit.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 516,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.16302",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-03-30",
    "authors": [
      {
        "authorId": "3086596",
        "name": "Byeongho Heo"
      },
      {
        "authorId": "2151587",
        "name": "Sangdoo Yun"
      },
      {
        "authorId": "2086576",
        "name": "Dongyoon Han"
      },
      {
        "authorId": "2647582",
        "name": "Sanghyuk Chun"
      },
      {
        "authorId": "3338475",
        "name": "Junsuk Choe"
      },
      {
        "authorId": "2390510",
        "name": "Seong Joon Oh"
      }
    ],
    "source": "semantic_scholar",
    "score": 169.72064311762642
  },
  {
    "paperId": "cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0",
    "url": "https://www.semanticscholar.org/paper/cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0",
    "title": "VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts",
    "abstract": "We present a unified Vision-Language pretrained Model (VLMo) that jointly learns a dual encoder and a fusion encoder with a modular Transformer network. Specifically, we introduce Mixture-of-Modality-Experts (MoME) Transformer, where each block contains a pool of modality-specific experts and a shared self-attention layer. Because of the modeling flexibility of MoME, pretrained VLMo can be fine-tuned as a fusion encoder for vision-language classification tasks, or used as a dual encoder for efficient image-text retrieval. Moreover, we propose a stagewise pre-training strategy, which effectively leverages large-scale image-only and text-only data besides image-text pairs. Experimental results show that VLMo achieves state-of-the-art results on various vision-language tasks, including VQA, NLVR2 and image-text retrieval. The code and pretrained models are available at https://aka.ms/vlmo.",
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "citationCount": 476,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-11-03",
    "authors": [
      {
        "authorId": "51456429",
        "name": "Wenhui Wang"
      },
      {
        "authorId": "10699417",
        "name": "Hangbo Bao"
      },
      {
        "authorId": "145307652",
        "name": "Li Dong"
      },
      {
        "authorId": "49807919",
        "name": "Furu Wei"
      }
    ],
    "source": "semantic_scholar",
    "score": 169.5127473633251
  },
  {
    "paperId": "7fff8018bf625447df837c2fda5c58a705fbc038",
    "url": "https://www.semanticscholar.org/paper/7fff8018bf625447df837c2fda5c58a705fbc038",
    "title": "XCiT: Cross-Covariance Image Transformers",
    "abstract": "Following their success in natural language processing, transformers have recently shown much promise for computer vision. The self-attention operation underlying transformers yields global interactions between all tokens ,i.e. words or image patches, and enables flexible modelling of image data beyond the local interactions of convolutions. This flexibility, however, comes with a quadratic complexity in time and memory, hindering application to long sequences and high-resolution images. We propose a\"transposed\"version of self-attention that operates across feature channels rather than tokens, where the interactions are based on the cross-covariance matrix between keys and queries. The resulting cross-covariance attention (XCA) has linear complexity in the number of tokens, and allows efficient processing of high-resolution images. Our cross-covariance image transformer (XCiT) is built upon XCA. It combines the accuracy of conventional transformers with the scalability of convolutional architectures. We validate the effectiveness and generality of XCiT by reporting excellent results on multiple vision benchmarks, including image classification and self-supervised feature learning on ImageNet-1k, object detection and instance segmentation on COCO, and semantic segmentation on ADE20k.",
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "citationCount": 463,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-06-17",
    "authors": [
      {
        "authorId": "1388811741",
        "name": "Alaaeldin El-Nouby"
      },
      {
        "authorId": "2113243762",
        "name": "Hugo Touvron"
      },
      {
        "authorId": "2062862676",
        "name": "Mathilde Caron"
      },
      {
        "authorId": "2329288",
        "name": "Piotr Bojanowski"
      },
      {
        "authorId": "3271933",
        "name": "Matthijs Douze"
      },
      {
        "authorId": "2319608",
        "name": "Armand Joulin"
      },
      {
        "authorId": "143991676",
        "name": "I. Laptev"
      },
      {
        "authorId": "2759569",
        "name": "N. Neverova"
      },
      {
        "authorId": "2282478",
        "name": "Gabriel Synnaeve"
      },
      {
        "authorId": "34602236",
        "name": "Jakob Verbeek"
      },
      {
        "authorId": "1681054",
        "name": "H. Jégou"
      }
    ],
    "source": "semantic_scholar",
    "score": 169.09826828339385
  },
  {
    "paperId": "0e2d8b8d81092037f9866c1ceddcebb87318e38b",
    "url": "https://www.semanticscholar.org/paper/0e2d8b8d81092037f9866c1ceddcebb87318e38b",
    "title": "AST: Audio Spectrogram Transformer",
    "abstract": "In the past decade, convolutional neural networks (CNNs) have been widely adopted as the main building block for end-to-end audio classification models, which aim to learn a direct mapping from audio spectrograms to corresponding labels. To better capture long-range global context, a recent trend is to add a self-attention mechanism on top of the CNN, forming a CNN-attention hybrid model. However, it is unclear whether the reliance on a CNN is necessary, and if neural networks purely based on attention are sufficient to obtain good performance in audio classification. In this paper, we answer the question by introducing the Audio Spectrogram Transformer (AST), the first convolution-free, purely attention-based model for audio classification. We evaluate AST on various audio classification benchmarks, where it achieves new state-of-the-art results of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50, and 98.1% accuracy on Speech Commands V2.",
    "venue": "Interspeech",
    "year": 2021,
    "citationCount": 731,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2104.01778",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-04-05",
    "authors": [
      {
        "authorId": "145802952",
        "name": "Yuan Gong"
      },
      {
        "authorId": "2815804",
        "name": "Yu-An Chung"
      },
      {
        "authorId": "145898106",
        "name": "James R. Glass"
      }
    ],
    "source": "semantic_scholar",
    "score": 168.93670770941966
  },
  {
    "paperId": "5863d7b35ea317c19f707376978ef1cc53e3534c",
    "url": "https://www.semanticscholar.org/paper/5863d7b35ea317c19f707376978ef1cc53e3534c",
    "title": "Rethinking Graph Transformers with Spectral Attention",
    "abstract": "In recent years, the Transformer architecture has proven to be very successful in sequence processing, but its application to other data structures, such as graphs, has remained limited due to the difficulty of properly defining positions. Here, we present the $\\textit{Spectral Attention Network}$ (SAN), which uses a learned positional encoding (LPE) that can take advantage of the full Laplacian spectrum to learn the position of each node in a given graph. This LPE is then added to the node features of the graph and passed to a fully-connected Transformer. By leveraging the full spectrum of the Laplacian, our model is theoretically powerful in distinguishing graphs, and can better detect similar sub-structures from their resonance. Further, by fully connecting the graph, the Transformer does not suffer from over-squashing, an information bottleneck of most GNNs, and enables better modeling of physical phenomenons such as heat transfer and electric interaction. When tested empirically on a set of 4 standard datasets, our model performs on par or better than state-of-the-art GNNs, and outperforms any attention-based model by a wide margin, becoming the first fully-connected architecture to perform well on graph benchmarks.",
    "venue": "Neural Information Processing Systems",
    "year": 2021,
    "citationCount": 455,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-06-07",
    "authors": [
      {
        "authorId": "2107709516",
        "name": "Devin Kreuzer"
      },
      {
        "authorId": "51034451",
        "name": "D. Beaini"
      },
      {
        "authorId": "2057555930",
        "name": "William L. Hamilton"
      },
      {
        "authorId": "2067158294",
        "name": "Vincent L'etourneau"
      },
      {
        "authorId": "12611623",
        "name": "Prudencio Tossou"
      }
    ],
    "source": "semantic_scholar",
    "score": 168.8373921427158
  },
  {
    "paperId": "51c9d4d2f50ac5707c1f889aa97f08350d549132",
    "url": "https://www.semanticscholar.org/paper/51c9d4d2f50ac5707c1f889aa97f08350d549132",
    "title": "Attention Is All You Need In Speech Separation",
    "abstract": "Recurrent Neural Networks (RNNs) have long been the dominant architecture in sequence-to-sequence learning. RNNs, however, are inherently sequential models that do not allow parallelization of their computations. Transformers are emerging as a natural alternative to standard RNNs, replacing recurrent computations with a multi-head attention mechanism.In this paper, we propose the SepFormer, a novel RNN-free Transformer-based neural network for speech separation. The Sep-Former learns short and long-term dependencies with a multi-scale approach that employs transformers. The proposed model achieves state-of-the-art (SOTA) performance on the standard WSJ0-2/3mix datasets. It reaches an SI-SNRi of 22.3 dB on WSJ0-2mix and an SI-SNRi of 19.5 dB on WSJ0-3mix. The SepFormer inherits the parallelization advantages of Transformers and achieves a competitive performance even when downsampling the encoded representation by a factor of 8. It is thus significantly faster and it is less memory-demanding than the latest speech separation systems with comparable performance.",
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year": 2020,
    "citationCount": 483,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2010.13154",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2020-10-25",
    "authors": [
      {
        "authorId": "1389581270",
        "name": "Cem Subakan"
      },
      {
        "authorId": "34924498",
        "name": "M. Ravanelli"
      },
      {
        "authorId": "1381145668",
        "name": "Samuele Cornell"
      },
      {
        "authorId": "1760944",
        "name": "Mirko Bronzi"
      },
      {
        "authorId": "2693593",
        "name": "Jianyuan Zhong"
      }
    ],
    "source": "semantic_scholar",
    "score": 168.7312736007495
  },
  {
    "paperId": "277dd73bfeb5c46513ce305136b0e71fcd2a311c",
    "url": "https://www.semanticscholar.org/paper/277dd73bfeb5c46513ce305136b0e71fcd2a311c",
    "title": "Recipe for a General, Powerful, Scalable Graph Transformer",
    "abstract": "We propose a recipe on how to build a general, powerful, scalable (GPS) graph Transformer with linear complexity and state-of-the-art results on a diverse set of benchmarks. Graph Transformers (GTs) have gained popularity in the field of graph representation learning with a variety of recent publications but they lack a common foundation about what constitutes a good positional or structural encoding, and what differentiates them. In this paper, we summarize the different types of encodings with a clearer definition and categorize them as being $\\textit{local}$, $\\textit{global}$ or $\\textit{relative}$. The prior GTs are constrained to small graphs with a few hundred nodes, here we propose the first architecture with a complexity linear in the number of nodes and edges $O(N+E)$ by decoupling the local real-edge aggregation from the fully-connected Transformer. We argue that this decoupling does not negatively affect the expressivity, with our architecture being a universal function approximator on graphs. Our GPS recipe consists of choosing 3 main ingredients: (i) positional/structural encoding, (ii) local message-passing mechanism, and (iii) global attention mechanism. We provide a modular framework $\\textit{GraphGPS}$ that supports multiple types of encodings and that provides efficiency and scalability both in small and large graphs. We test our architecture on 16 benchmarks and show highly competitive results in all of them, show-casing the empirical benefits gained by the modularity and the combination of different strategies.",
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "citationCount": 448,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2205.12454",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-05-25",
    "authors": [
      {
        "authorId": "2125913",
        "name": "Ladislav Rampášek"
      },
      {
        "authorId": "2066369448",
        "name": "Mikhail Galkin"
      },
      {
        "authorId": "51235219",
        "name": "Vijay Prakash Dwivedi"
      },
      {
        "authorId": "1755919",
        "name": "A. Luu"
      },
      {
        "authorId": "2683398",
        "name": "Guy Wolf"
      },
      {
        "authorId": "51034451",
        "name": "D. Beaini"
      }
    ],
    "source": "semantic_scholar",
    "score": 168.60534331613383
  },
  {
    "paperId": "d532a98f64f243d08c2532e5ccc9f7c6b3669fe8",
    "url": "https://www.semanticscholar.org/paper/d532a98f64f243d08c2532e5ccc9f7c6b3669fe8",
    "title": "Spectral–Spatial Feature Tokenization Transformer for Hyperspectral Image Classification",
    "abstract": "In hyperspectral image (HSI) classification, each pixel sample is assigned to a land-cover category. In the recent past, convolutional neural network (CNN)-based HSI classification methods have greatly improved performance due to their superior ability to represent features. However, these methods have limited ability to obtain deep semantic features, and as the layer’s number increases, computational costs rise significantly. The transformer framework can represent high-level semantic features well. In this article, a spectral–spatial feature tokenization transformer (SSFTT) method is proposed to capture spectral–spatial features and high-level semantic features. First, a spectral–spatial feature extraction module is built to extract low-level features. This module is composed of a 3-D convolution layer and a 2-D convolution layer, which are used to extract the shallow spectral and spatial features. Second, a Gaussian weighted feature tokenizer is introduced for features transformation. Third, the transformed features are input into the transformer encoder module for feature representation and learning. Finally, a linear layer is used to identify the first learnable token to obtain the sample label. Using three standard datasets, experimental analysis confirms that the computation time is less than other deep learning methods and the performance of the classification outperforms several current state-of-the-art methods. The code of this work is available at https://github.com/zgr6010/HSI_SSFTT for the sake of reproducibility.",
    "venue": "IEEE Transactions on Geoscience and Remote Sensing",
    "year": 2022,
    "citationCount": 365,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": null,
        "name": "Le Sun"
      },
      {
        "authorId": "2152138893",
        "name": "Guangrui Zhao"
      },
      {
        "authorId": "46323585",
        "name": "Yuhui Zheng"
      },
      {
        "authorId": "134909078",
        "name": "Zebin Wu"
      }
    ],
    "source": "semantic_scholar",
    "score": 168.53950000102049
  },
  {
    "paperId": "1d986a5f18735c6b142a0ed48ebfe95fdf8f339a",
    "url": "https://www.semanticscholar.org/paper/1d986a5f18735c6b142a0ed48ebfe95fdf8f339a",
    "title": "Power System Analysis",
    "abstract": "early as possible for new large industrial systems. These studies are an excellent tool for system planning. These studies should be used to confirm selected switchgear, transformer, and cable sizing. These studies should also be used to confirm adequate voltage profiles during different operating conditions, such as heavily loaded and lightly loaded system conditions. Load flow studies can be used to determine the optimum size and location of capacitors for power factor correction. The results of load flow studies are also starting points for other system studies.",
    "venue": "",
    "year": 2018,
    "citationCount": 1354,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2018-01-03",
    "authors": [
      {
        "authorId": "2335603416",
        "name": "Ismail Kasikci"
      }
    ],
    "source": "semantic_scholar",
    "score": 168.17335099970703
  },
  {
    "paperId": "76a9f336481b39515d6cea2920696f11fb686451",
    "url": "https://www.semanticscholar.org/paper/76a9f336481b39515d6cea2920696f11fb686451",
    "title": "Quantifying Attention Flow in Transformers",
    "abstract": "In the Transformer model, “self-attention” combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "citationCount": 694,
    "openAccessPdf": {
      "url": "https://www.aclweb.org/anthology/2020.acl-main.385.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2020-05-01",
    "authors": [
      {
        "authorId": "2786352",
        "name": "Samira Abnar"
      },
      {
        "authorId": "83390207",
        "name": "W. Zuidema"
      }
    ],
    "source": "semantic_scholar",
    "score": 168.15867768347186
  },
  {
    "paperId": "fdcad86866ca22d8417599deb41b54fe01487ce8",
    "url": "https://www.semanticscholar.org/paper/fdcad86866ca22d8417599deb41b54fe01487ce8",
    "title": "ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation",
    "abstract": "Although no specific domain knowledge is considered in the design, plain vision transformers have shown excellent performance in visual recognition tasks. However, little effort has been made to reveal the potential of such simple structures for pose estimation tasks. In this paper, we show the surprisingly good capabilities of plain vision transformers for pose estimation from various aspects, namely simplicity in model structure, scalability in model size, flexibility in training paradigm, and transferability of knowledge between models, through a simple baseline model called ViTPose. Specifically, ViTPose employs plain and non-hierarchical vision transformers as backbones to extract features for a given person instance and a lightweight decoder for pose estimation. It can be scaled up from 100M to 1B parameters by taking the advantages of the scalable model capacity and high parallelism of transformers, setting a new Pareto front between throughput and performance. Besides, ViTPose is very flexible regarding the attention type, input resolution, pre-training and finetuning strategy, as well as dealing with multiple pose tasks. We also empirically demonstrate that the knowledge of large ViTPose models can be easily transferred to small ones via a simple knowledge token. Experimental results show that our basic ViTPose model outperforms representative methods on the challenging MS COCO Keypoint Detection benchmark, while the largest model sets a new state-of-the-art. The code and models are available at https://github.com/ViTAE-Transformer/ViTPose.",
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "citationCount": 432,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2204.12484",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-04-26",
    "authors": [
      {
        "authorId": "48615515",
        "name": "Yufei Xu"
      },
      {
        "authorId": "1519070643",
        "name": "Jing Zhang"
      },
      {
        "authorId": "151714237",
        "name": "Qiming Zhang"
      },
      {
        "authorId": "2075330732",
        "name": "Dacheng Tao"
      }
    ],
    "source": "semantic_scholar",
    "score": 168.06106592003735
  },
  {
    "paperId": "a09cbcaac305884f043810afc4fa4053099b5970",
    "url": "https://www.semanticscholar.org/paper/a09cbcaac305884f043810afc4fa4053099b5970",
    "title": "Exploring Plain Vision Transformer Backbones for Object Detection",
    "abstract": "We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP_box on the COCO dataset using only ImageNet-1K pre-training. We hope our study will draw attention to research on plain-backbone detectors. Code for ViTDet is available in Detectron2.",
    "venue": "European Conference on Computer Vision",
    "year": 2022,
    "citationCount": 687,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2203.16527",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-03-30",
    "authors": [
      {
        "authorId": "3128506",
        "name": "Yanghao Li"
      },
      {
        "authorId": "2053590350",
        "name": "Hanzi Mao"
      },
      {
        "authorId": "2983898",
        "name": "Ross B. Girshick"
      },
      {
        "authorId": "2058350112",
        "name": "Kaiming He"
      }
    ],
    "source": "semantic_scholar",
    "score": 168.00683256900015
  },
  {
    "paperId": "0357156aef567fb5b709222894ddea1ce5d4e721",
    "url": "https://www.semanticscholar.org/paper/0357156aef567fb5b709222894ddea1ce5d4e721",
    "title": "TrackFormer: Multi-Object Tracking with Transformers",
    "abstract": "The challenging task of multi-object tracking (MOT) requires simultaneous reasoning about track initialization, identity, and spatio-temporal trajectories. We formulate this task as a frame-to-frame set prediction problem and introduce TrackFormer, an end-to-end trainable MOT approach based on an encoder-decoder Transformer architecture. Our model achieves data association between frames via attention by evolving a set of track predictions through a video sequence. The Transformer decoder initializes new tracks from static object queries and autoregressively follows existing tracks in space and time with the conceptually new and identity preserving track queries. Both query types benefit from self- and encoder-decoder attention on global frame-level features, thereby omitting any additional graph optimization or modeling of motion and/or appearance. TrackFormer introduces a new tracking-by-attention paradigm and while simple in its design is able to achieve state-of-the-art performance on the task of multi-object tracking (MOT17) and segmentation (MOTS20). The code is available at https://github.com/timmeinhardt/TrackFormer",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "citationCount": 682,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2101.02702",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-01-07",
    "authors": [
      {
        "authorId": "10683576",
        "name": "Tim Meinhardt"
      },
      {
        "authorId": "2064802835",
        "name": "A. Kirillov"
      },
      {
        "authorId": "1388407684",
        "name": "L. Leal-Taixé"
      },
      {
        "authorId": "2322150",
        "name": "Christoph Feichtenhofer"
      }
    ],
    "source": "semantic_scholar",
    "score": 167.89742289356184
  },
  {
    "paperId": "e3d06054af531ee2f42270d43100b309c28546ef",
    "url": "https://www.semanticscholar.org/paper/e3d06054af531ee2f42270d43100b309c28546ef",
    "title": "MUSIQ: Multi-scale Image Quality Transformer",
    "abstract": "Image quality assessment (IQA) is an important research topic for understanding and improving visual experience. The current state-of-the-art IQA methods are based on convolutional neural networks (CNNs). The performance of CNN-based models is often compromised by the fixed shape constraint in batch training. To accommodate this, the input images are usually resized and cropped to a fixed shape, causing image quality degradation. To address this, we design a multi-scale image quality Transformer (MUSIQ) to process native resolution images with varying sizes and aspect ratios. With a multi-scale image representation, our proposed method can capture image quality at different granularities. Furthermore, a novel hash-based 2D spatial embedding and a scale embedding is proposed to support the positional embedding in the multi-scale representation. Experimental results verify that our method can achieve state-of-the-art performance on multiple large scale IQA datasets such as PaQ-2-PiQ [41], SPAQ [11], and KonIQ-10k [16]. 1",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 454,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2108.05997",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-08-12",
    "authors": [
      {
        "authorId": "49287230",
        "name": "Junjie Ke"
      },
      {
        "authorId": "2230525",
        "name": "Qifei Wang"
      },
      {
        "authorId": "2108103589",
        "name": "Yilin Wang"
      },
      {
        "authorId": "1718280",
        "name": "P. Milanfar"
      },
      {
        "authorId": "1454990616",
        "name": "Feng Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 167.80446128426425
  },
  {
    "paperId": "849b88ddc8f8cabc6d4246479b275a1ee65d0647",
    "url": "https://www.semanticscholar.org/paper/849b88ddc8f8cabc6d4246479b275a1ee65d0647",
    "title": "A Generalization of Transformer Networks to Graphs",
    "abstract": "We propose a generalization of transformer neural network architecture for arbitrary graphs. The original transformer was designed for Natural Language Processing (NLP), which operates on fully connected graphs representing all connections between the words in a sequence. Such architecture does not leverage the graph connectivity inductive bias, and can perform poorly when the graph topology is important and has not been encoded into the node features. We introduce a graph transformer with four new properties compared to the standard model. First, the attention mechanism is a function of the neighborhood connectivity for each node in the graph. Second, the positional encoding is represented by the Laplacian eigenvectors, which naturally generalize the sinusoidal positional encodings often used in NLP. Third, the layer normalization is replaced by a batch normalization layer, which provides faster training and better generalization performance. Finally, the architecture is extended to edge feature representation, which can be critical to tasks s.a. chemistry (bond type) or link prediction (entity relationship in knowledge graphs). Numerical experiments on a graph benchmark demonstrate the performance of the proposed graph transformer architecture. This work closes the gap between the original transformer, which was designed for the limited case of line graphs, and graph neural networks, that can work with arbitrary graphs. As our architecture is simple and generic, we believe it can be used as a black box for future applications that wish to consider transformer and graphs.",
    "venue": "arXiv.org",
    "year": 2020,
    "citationCount": 661,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2020-12-17",
    "authors": [
      {
        "authorId": "51235219",
        "name": "Vijay Prakash Dwivedi"
      },
      {
        "authorId": "2549032",
        "name": "X. Bresson"
      }
    ],
    "source": "semantic_scholar",
    "score": 167.42898333905512
  },
  {
    "paperId": "03a2befad038a9f29859295fdfcdbfa52c564622",
    "url": "https://www.semanticscholar.org/paper/03a2befad038a9f29859295fdfcdbfa52c564622",
    "title": "An End-to-End Transformer Model for 3D Object Detection",
    "abstract": "We propose 3DETR, an end-to-end Transformer based object detection model for 3D point clouds. Compared to existing detection methods that employ a number of 3D-specific inductive biases, 3DETR requires minimal modifications to the vanilla Transformer block. Specifically, we find that a standard Transformer with non-parametric queries and Fourier positional embeddings is competitive with specialized architectures that employ libraries of 3D-specific operators with hand-tuned hyperparameters. Nevertheless, 3DETR is conceptually simple and easy to implement, enabling further improvements by incorporating 3D domain knowledge. Through extensive experiments, we show 3DETR outperforms the well-established and highly optimized VoteNet baselines on the challenging ScanNetV2 dataset by 9.5%. Furthermore, we show 3DETR is applicable to 3D tasks beyond detection, and can serve as a building block for future research.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 439,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2109.08141",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-09-16",
    "authors": [
      {
        "authorId": "1806773",
        "name": "Ishan Misra"
      },
      {
        "authorId": "3102850",
        "name": "Rohit Girdhar"
      },
      {
        "authorId": "2319608",
        "name": "Armand Joulin"
      }
    ],
    "source": "semantic_scholar",
    "score": 167.30162090368458
  },
  {
    "paperId": "2984ab83ade26639c3a82d29628d0d9e4abbebb0",
    "url": "https://www.semanticscholar.org/paper/2984ab83ade26639c3a82d29628d0d9e4abbebb0",
    "title": "Incorporating Convolution Designs into Visual Transformers",
    "abstract": "Motivated by the success of Transformers in natural language processing (NLP) tasks, there emerge some attempts (e.g., ViT and DeiT) to apply Transformers to the vision domain. However, pure Transformer architectures often require a large amount of training data or extra supervision to obtain comparable performance with convolutional neural networks (CNNs). To overcome these limitations, we analyze the potential drawbacks when directly borrowing Transformer architectures from NLP. Then we propose a new Convolution-enhanced image Transformer (CeiT) which combines the advantages of CNNs in extracting low-level features, strengthening locality, and the advantages of Transformers in establishing long-range dependencies. Three modifications are made to the original Transformer: 1) instead of the straightforward tokenization from raw input images, we design an Image-to-Tokens (I2T) module that extracts patches from generated low-level features; 2) the feed-froward network in each encoder block is replaced with a Locally-enhanced Feed-Forward (LeFF) layer that promotes the correlation among neighboring tokens in the spatial dimension; 3) a Layer-wise Class token Attention (LCA) is attached at the top of the Transformer that utilizes the multi-level representations.Experimental results on ImageNet and seven down-stream tasks show the effectiveness and generalization ability of CeiT compared with previous Transformers and state- of-the-art CNNs, without requiring a large amount of training data and extra CNN teachers. Besides, CeiT models demonstrate better convergence with 3× fewer training iterations, which can reduce the training cost significantly 1.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 438,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.11816",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-03-22",
    "authors": [
      {
        "authorId": "50492964",
        "name": "Kun Yuan"
      },
      {
        "authorId": "2119111668",
        "name": "Shaopeng Guo"
      },
      {
        "authorId": "2145253401",
        "name": "Ziwei Liu"
      },
      {
        "authorId": "9548994",
        "name": "Aojun Zhou"
      },
      {
        "authorId": "5237666",
        "name": "F. Yu"
      },
      {
        "authorId": "2118256875",
        "name": "Wei Wu"
      }
    ],
    "source": "semantic_scholar",
    "score": 167.26749119612757
  },
  {
    "paperId": "f6e670d23e4830e7801352098cb21b72250985ca",
    "url": "https://www.semanticscholar.org/paper/f6e670d23e4830e7801352098cb21b72250985ca",
    "title": "Recognition",
    "abstract": ": In LaTeX text recognition using Transformer-based architectures, this paper identifies certain ”bias” issues. For instance, e − t is frequently misrecognized as e − t . This bias stems from the inherent characteristics of the dataset. To mitigate this bias, we propose a LaTeX printed text recognition model trained on a mixed dataset of pseudo-formulas and pseudo-text. The model employs a Swin Transformer as the encoder and a RoBERTa model as the decoder. Experimental results demonstrate that this approach reduces ”bias”, enhancing the accuracy and robustness of text recognition. For clear images, the model strictly adheres to the image content; for blurred images, it integrates both image and contextual information to produce reasonable recognition results.",
    "venue": "",
    "year": 2003,
    "citationCount": 1273,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": "2003-09-01",
    "authors": [
      {
        "authorId": "2333825457",
        "name": "Dr. Joanna Timmins"
      },
      {
        "authorId": "2333832110",
        "name": "Dr. Jean-Luc Ravanat"
      },
      {
        "authorId": "2333832101",
        "name": "Prof. Dr. Maurizio Battino"
      }
    ],
    "source": "semantic_scholar",
    "score": 167.24875254198162
  },
  {
    "paperId": "ed4087f6e8d77452810979f58246c5b2ad846cf8",
    "url": "https://www.semanticscholar.org/paper/ed4087f6e8d77452810979f58246c5b2ad846cf8",
    "title": "Transformers in Time Series: A Survey",
    "abstract": "Transformers have achieved superior performances in many tasks in natural language processing and computer vision, which also triggered great interest in the time series community. Among multiple advantages of Transformers, the ability to capture long-range dependencies and interactions is especially attractive for time series modeling, leading to exciting progress in various time series applications. In this paper, we systematically review Transformer schemes for time series modeling by highlighting their strengths as well as limitations. In particular, we examine the development of time series Transformers in two perspectives. From the perspective of network structure, we summarize the adaptations and modifications that have been made to Transformers in order to accommodate the challenges in time series analysis. From the perspective of applications, we categorize time series Transformers based on common tasks including forecasting, anomaly detection, and classification. Empirically, we perform robust analysis, model size analysis, and seasonal-trend decomposition analysis to study how Transformers perform in time series. Finally, we discuss and suggest future directions to provide useful research guidance.",
    "venue": "International Joint Conference on Artificial Intelligence",
    "year": 2022,
    "citationCount": 649,
    "openAccessPdf": {
      "url": "https://www.ijcai.org/proceedings/2023/0759.pdf",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference",
      "Review"
    ],
    "publicationDate": "2022-02-15",
    "authors": [
      {
        "authorId": "3308963",
        "name": "Qingsong Wen"
      },
      {
        "authorId": "50018188",
        "name": "Tian Zhou"
      },
      {
        "authorId": "144677904",
        "name": "Chao Zhang"
      },
      {
        "authorId": "2109780096",
        "name": "Weiqiu Chen"
      },
      {
        "authorId": "1387898803",
        "name": "Ziqing Ma"
      },
      {
        "authorId": "3063894",
        "name": "Junchi Yan"
      },
      {
        "authorId": "2110940896",
        "name": "Liang Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 167.15458544334524
  },
  {
    "paperId": "7e9ff94476f41041c75e253e84f487db00e9c861",
    "url": "https://www.semanticscholar.org/paper/7e9ff94476f41041c75e253e84f487db00e9c861",
    "title": "Long Range Arena: A Benchmark for Efficient Transformers",
    "abstract": "Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from $1K$ to $16K$ tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. LRA paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle. Our benchmark code will be released at this https URL.",
    "venue": "International Conference on Learning Representations",
    "year": 2020,
    "citationCount": 642,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2020-11-08",
    "authors": [
      {
        "authorId": "144447820",
        "name": "Yi Tay"
      },
      {
        "authorId": "3226635",
        "name": "Mostafa Dehghani"
      },
      {
        "authorId": "2786352",
        "name": "Samira Abnar"
      },
      {
        "authorId": "2714199",
        "name": "Yikang Shen"
      },
      {
        "authorId": "2119725651",
        "name": "Dara Bahri"
      },
      {
        "authorId": "38552691",
        "name": "Philip Pham"
      },
      {
        "authorId": "30586030",
        "name": "J. Rao"
      },
      {
        "authorId": "2119062135",
        "name": "Liu Yang"
      },
      {
        "authorId": "2884561",
        "name": "Sebastian Ruder"
      },
      {
        "authorId": "1680617",
        "name": "Donald Metzler"
      }
    ],
    "source": "semantic_scholar",
    "score": 166.9921708635643
  },
  {
    "paperId": "6648b4db5f12c30941ea78c695e77aded19672bb",
    "url": "https://www.semanticscholar.org/paper/6648b4db5f12c30941ea78c695e77aded19672bb",
    "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA",
    "abstract": "This paper presents a unified Vision-Language Pre-training (VLP) model. The model is unified in that (1) it can be fine-tuned for either vision-language generation (e.g., image captioning) or understanding (e.g., visual question answering) tasks, and (2) it uses a shared multi-layer transformer network for both encoding and decoding, which differs from many existing methods where the encoder and decoder are implemented using separate models. The unified VLP model is pre-trained on a large amount of image-text pairs using the unsupervised learning objectives of two tasks: bidirectional and sequence-to-sequence (seq2seq) masked vision-language prediction. The two tasks differ solely in what context the prediction conditions on. This is controlled by utilizing specific self-attention masks for the shared transformer network. To the best of our knowledge, VLP is the first reported model that achieves state-of-the-art results on both vision-language generation and understanding tasks, as disparate as image captioning and visual question answering, across three challenging benchmark datasets: COCO Captions, Flickr30k Captions, and VQA 2.0. The code and the pre-trained models are available at https://github.com/LuoweiZhou/VLP.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2019,
    "citationCount": 895,
    "openAccessPdf": {
      "url": "https://ojs.aaai.org/index.php/AAAI/article/download/7005/6859",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2019-09-24",
    "authors": [
      {
        "authorId": "2116644664",
        "name": "Luowei Zhou"
      },
      {
        "authorId": "2542427",
        "name": "Hamid Palangi"
      },
      {
        "authorId": "39089563",
        "name": "Lei Zhang"
      },
      {
        "authorId": "35431603",
        "name": "Houdong Hu"
      },
      {
        "authorId": "3587688",
        "name": "Jason J. Corso"
      },
      {
        "authorId": "1800422",
        "name": "Jianfeng Gao"
      }
    ],
    "source": "semantic_scholar",
    "score": 166.96910619462398
  },
  {
    "paperId": "eedf2748a9a1ba2779cde95fd8bad9c2260d5317",
    "url": "https://www.semanticscholar.org/paper/eedf2748a9a1ba2779cde95fd8bad9c2260d5317",
    "title": "LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention",
    "abstract": "Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at this https URL.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "citationCount": 635,
    "openAccessPdf": {
      "url": "https://www.aclweb.org/anthology/2020.emnlp-main.523.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2020-10-02",
    "authors": [
      {
        "authorId": "2303128",
        "name": "Ikuya Yamada"
      },
      {
        "authorId": "35584853",
        "name": "Akari Asai"
      },
      {
        "authorId": "73467110",
        "name": "Hiroyuki Shindo"
      },
      {
        "authorId": "46966356",
        "name": "Hideaki Takeda"
      },
      {
        "authorId": "1681502",
        "name": "Yuji Matsumoto"
      }
    ],
    "source": "semantic_scholar",
    "score": 166.82797845010185
  },
  {
    "paperId": "622428f5122ad12a40229e1768ecb929fd747ee7",
    "url": "https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7",
    "title": "Multimodal Learning With Transformers: A Survey",
    "abstract": "Transformer is a promising neural network learner, and has achieved great success in various machine learning tasks. Thanks to the recent prevalence of multimodal applications and Big Data, Transformer-based multimodal learning has become a hot topic in AI research. This paper presents a comprehensive survey of Transformer techniques oriented at multimodal data. The main contents of this survey include: (1) a background of multimodal learning, Transformer ecosystem, and the multimodal Big Data era, (2) a systematic review of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective, (3) a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks, (4) a summary of the common challenges and designs shared by the multimodal Transformer models and applications, and (5) a discussion of open problems and potential research directions for the community.",
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "year": 2022,
    "citationCount": 422,
    "openAccessPdf": {
      "url": "https://ieeexplore.ieee.org/ielx7/34/4359286/10123038.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2022-06-13",
    "authors": [
      {
        "authorId": "2087575115",
        "name": "P. Xu"
      },
      {
        "authorId": "2310779361",
        "name": "Xiatian Zhu"
      },
      {
        "authorId": "31799453",
        "name": "D. Clifton"
      }
    ],
    "source": "semantic_scholar",
    "score": 166.71058268569416
  },
  {
    "paperId": "ea7cfe7f2340584cbe653da6077ee7c213e49b92",
    "url": "https://www.semanticscholar.org/paper/ea7cfe7f2340584cbe653da6077ee7c213e49b92",
    "title": "Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation",
    "abstract": null,
    "venue": "ECCV Workshops",
    "year": 2021,
    "citationCount": 2325,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-05-12",
    "authors": [
      {
        "authorId": "40223253",
        "name": "Hu Cao"
      },
      {
        "authorId": "2115849834",
        "name": "Yueyue Wang"
      },
      {
        "authorId": "90972805",
        "name": "Jieneng Chen"
      },
      {
        "authorId": "49731273",
        "name": "Dongsheng Jiang"
      },
      {
        "authorId": "2108250420",
        "name": "Xiaopeng Zhang"
      },
      {
        "authorId": "1400120070",
        "name": "Qi Tian"
      },
      {
        "authorId": "40532896",
        "name": "Manning Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 166.27857999617913
  },
  {
    "paperId": "7623a054b85bed188ebd608915ee7a15fc947f7f",
    "url": "https://www.semanticscholar.org/paper/7623a054b85bed188ebd608915ee7a15fc947f7f",
    "title": "UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-wise Perspective with Transformer",
    "abstract": "Most recent semantic segmentation methods adopt a U-Net framework with an encoder-decoder architecture. It is still challenging for U-Net with a simple skip connection scheme to model the global multi-scale context: 1) Not each skip connection setting is effective due to the issue of incompatible feature sets of encoder and decoder stage, even some skip connection negatively influence the segmentation performance; 2) The original U-Net is worse than the one without any skip connection on some datasets. Based on our findings, we propose a new segmentation framework, named UCTransNet (with a proposed CTrans module in U-Net), from the channel perspective with attention mechanism. Specifically, the CTrans (Channel Transformer) module is an alternate of the U-Net skip connections, which consists of a sub-module to conduct the multi-scale Channel Cross fusion with Transformer (named CCT) and a sub-module Channel-wise Cross-Attention (named CCA) to guide the fused multi-scale channel-wise information to effectively connect to the decoder features for eliminating the ambiguity. Hence, the proposed connection consisting of the CCT and CCA is able to replace the original skip connection to solve the semantic gaps for an accurate automatic medical image segmentation. The experimental results suggest that our UCTransNet produces more precise segmentation performance and achieves consistent improvements over the state-of-the-art for semantic segmentation across different datasets and conventional architectures involving transformer or U-shaped framework. Code: https://github.com/McGregorWwww/UCTransNet.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2021,
    "citationCount": 602,
    "openAccessPdf": {
      "url": "https://ojs.aaai.org/index.php/AAAI/article/download/20144/19903",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-09-09",
    "authors": [
      {
        "authorId": "30490097",
        "name": "Haonan Wang"
      },
      {
        "authorId": "2135554417",
        "name": "Peng Cao"
      },
      {
        "authorId": null,
        "name": "Jiaqi Wang"
      },
      {
        "authorId": "1683380",
        "name": "Osmar R Zaiane"
      }
    ],
    "source": "semantic_scholar",
    "score": 166.0287579509078
  },
  {
    "paperId": "021bd56b6c95d0196f3b8e818c948ea0702b0836",
    "url": "https://www.semanticscholar.org/paper/021bd56b6c95d0196f3b8e818c948ea0702b0836",
    "title": "AI Choreographer: Music Conditioned 3D Dance Generation with AIST++",
    "abstract": "We present AIST++, a new multi-modal dataset of 3D dance motion and music, along with FACT, a Full-Attention Cross-modal Transformer network for generating 3D dance motion conditioned on music. The proposed AIST++ dataset contains 5.2 hours of 3D dance motion in 1408 sequences, covering 10 dance genres with multi-view videos with known camera poses—the largest dataset of this kind to our knowledge. We show that naively applying sequence models such as transformers to this dataset for the task of music conditioned 3D motion generation does not produce satisfactory 3D motion that is well correlated with the input music. We overcome these shortcomings by introducing key changes in its architecture design and supervision: FACT model involves a deep cross-modal transformer block with full-attention that is trained to predict N future motions. We empirically show that these changes are key factors in generating long sequences of realistic dance motion that are well-attuned to the input music. We conduct extensive experiments on AIST++ with user studies, where our method outperforms recent state-of-the-art methods both qualitatively and quantitatively. The code and the dataset can be found at: https://google.github.io/aichoreographer.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 399,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2101.08779",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-01-21",
    "authors": [
      {
        "authorId": "51104559",
        "name": "Ruilong Li"
      },
      {
        "authorId": "2115300260",
        "name": "Sha Yang"
      },
      {
        "authorId": "144711958",
        "name": "David A. Ross"
      },
      {
        "authorId": "20615377",
        "name": "Angjoo Kanazawa"
      }
    ],
    "source": "semantic_scholar",
    "score": 165.87196820661973
  },
  {
    "paperId": "8a66099266512166d8c2481ddf73a81916d18dec",
    "url": "https://www.semanticscholar.org/paper/8a66099266512166d8c2481ddf73a81916d18dec",
    "title": "Electricity Theft Detection in AMI Using Customers’ Consumption Patterns",
    "abstract": "As one of the key components of the smart grid, advanced metering infrastructure brings many potential advantages such as load management and demand response. However, computerizing the metering system also introduces numerous new vectors for energy theft. In this paper, we present a novel consumption pattern-based energy theft detector, which leverages the predictability property of customers' normal and malicious consumption patterns. Using distribution transformer meters, areas with a high probability of energy theft are short listed, and by monitoring abnormalities in consumption patterns, suspicious customers are identified. Application of appropriate classification and clustering techniques, as well as concurrent use of transformer meters and anomaly detectors, make the algorithm robust against nonmalicious changes in usage pattern, and provide a high and adjustable performance with a low-sampling rate. Therefore, the proposed method does not invade customers' privacy. Extensive experiments on a real dataset of 5000 customers show a high performance for the proposed method.",
    "venue": "IEEE Transactions on Smart Grid",
    "year": 2016,
    "citationCount": 557,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "3143704",
        "name": "Paria Jokar"
      },
      {
        "authorId": "2198502",
        "name": "Nasim Arianpoo"
      },
      {
        "authorId": "143698682",
        "name": "Victor C. M. Leung"
      }
    ],
    "source": "semantic_scholar",
    "score": 165.86538443571965
  },
  {
    "paperId": "7507603da9711c0e43e29f3094f33d9337e7dfbd",
    "url": "https://www.semanticscholar.org/paper/7507603da9711c0e43e29f3094f33d9337e7dfbd",
    "title": "3D Human Pose Estimation with Spatial and Temporal Transformers",
    "abstract": "Transformer architectures have become the model of choice in natural language processing and are now being introduced into computer vision tasks such as image classification, object detection, and semantic segmentation. However, in the field of human pose estimation, convolutional architectures still remain dominant. In this work, we present PoseFormer, a purely transformer-based approach for 3D human pose estimation in videos without convolutional architectures involved. Inspired by recent developments in vision transformers, we design a spatial-temporal transformer structure to comprehensively model the human joint relations within each frame as well as the temporal correlations across frames, then output an accurate 3D human pose of the center frame. We quantitatively and qualitatively evaluate our method on two popular and standard benchmark datasets: Human3.6M and MPI-INF-3DHP. Extensive experiments show that PoseFormer achieves state-of-the-art performance on both datasets. Code is available at https://github.com/zczcwh/PoseFormer",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 391,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.10455",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-03-18",
    "authors": [
      {
        "authorId": "2113919899",
        "name": "Ce Zheng"
      },
      {
        "authorId": "9385903",
        "name": "Sijie Zhu"
      },
      {
        "authorId": "1422036273",
        "name": "Mat'ias Mendieta"
      },
      {
        "authorId": "1390892946",
        "name": "Taojiannan Yang"
      },
      {
        "authorId": "2141809453",
        "name": "Chen Chen"
      },
      {
        "authorId": "2788685",
        "name": "Zhengming Ding"
      }
    ],
    "source": "semantic_scholar",
    "score": 165.56892759685695
  },
  {
    "paperId": "37246e26163cdd0f5ddd1ea47a5a5019dead8abb",
    "url": "https://www.semanticscholar.org/paper/37246e26163cdd0f5ddd1ea47a5a5019dead8abb",
    "title": "Vision Transformers for Single Image Dehazing",
    "abstract": "Image dehazing is a representative low-level vision task that estimates latent haze-free images from hazy images. In recent years, convolutional neural network-based methods have dominated image dehazing. However, vision Transformers, which has recently made a breakthrough in high-level vision tasks, has not brought new dimensions to image dehazing. We start with the popular Swin Transformer and find that several of its key designs are unsuitable for image dehazing. To this end, we propose DehazeFormer, which consists of various improvements, such as the modified normalization layer, activation function, and spatial information aggregation scheme. We train multiple variants of DehazeFormer on various datasets to demonstrate its effectiveness. Specifically, on the most frequently used SOTS indoor set, our small model outperforms FFA-Net with only 25% #Param and 5% computational cost. To the best of our knowledge, our large model is the first method with the PSNR over 40 dB on the SOTS indoor set, dramatically outperforming the previous state-of-the-art methods. We also collect a large-scale realistic remote sensing dehazing dataset for evaluating the method’s capability to remove highly non-homogeneous haze. We share our code and dataset at https://github.com/IDKiro/DehazeFormer.",
    "venue": "IEEE Transactions on Image Processing",
    "year": 2022,
    "citationCount": 390,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2204.03883",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-04-08",
    "authors": [
      {
        "authorId": "152241734",
        "name": "Yuda Song"
      },
      {
        "authorId": "2115934613",
        "name": "Zhuqing He"
      },
      {
        "authorId": "2112125907",
        "name": "Hui Qian"
      },
      {
        "authorId": "2114714362",
        "name": "Xin Du"
      }
    ],
    "source": "semantic_scholar",
    "score": 165.5306133997805
  },
  {
    "paperId": "ef3b15260a610473c95662f5df2c195ac19f64d6",
    "url": "https://www.semanticscholar.org/paper/ef3b15260a610473c95662f5df2c195ac19f64d6",
    "title": "A Transformer-Based Siamese Network for Change Detection",
    "abstract": "This paper presents a transformer-based Siamese network architecture (abbreviated by ChangeFormer) for Change Detection (CD) from a pair of co-registered remote sensing images. Different from recent CD frameworks, which are based on fully convolutional networks (ConvNets), the proposed method unifies hierarchically structured transformer encoder with Multi-Layer Perception (MLP) decoder in a Siamese network architecture to efficiently render multi-scale long-range details required for accurate CD. Experiments on two CD datasets show that the proposed end-to-end trainable ChangeFormer architecture achieves better CD performance than previous counterparts. Our code and pre-trained models are available at github.com/wgcban/ChangeFormer.",
    "venue": "IEEE International Geoscience and Remote Sensing Symposium",
    "year": 2022,
    "citationCount": 413,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2201.01293",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-01-04",
    "authors": [
      {
        "authorId": "151389742",
        "name": "W. G. C. Bandara"
      },
      {
        "authorId": "1741177",
        "name": "Vishal M. Patel"
      }
    ],
    "source": "semantic_scholar",
    "score": 165.38798960737972
  },
  {
    "paperId": "cf5e6e3c50a798d87033e0e108e88b3647738bbe",
    "url": "https://www.semanticscholar.org/paper/cf5e6e3c50a798d87033e0e108e88b3647738bbe",
    "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
    "abstract": "Vision Transformers (ViT) have been shown to attain highly competitive performance for a wide range of vision applications, such as image classification, object detection and semantic image segmentation. In comparison to convolutional neural networks, the Vision Transformer's weaker inductive bias is generally found to cause an increased reliance on model regularization or data augmentation (\"AugReg\"for short) when training on smaller training datasets. We conduct a systematic empirical study in order to better understand the interplay between the amount of training data, AugReg, model size and compute budget. As one result of this study we find that the combination of increased compute and AugReg can yield models with the same performance as models trained on an order of magnitude more training data: we train ViT models of various sizes on the public ImageNet-21k dataset which either match or outperform their counterparts trained on the larger, but not publicly available JFT-300M dataset.",
    "venue": "Trans. Mach. Learn. Res.",
    "year": 2021,
    "citationCount": 568,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-06-18",
    "authors": [
      {
        "authorId": "2079614268",
        "name": "A. Steiner"
      },
      {
        "authorId": "144629422",
        "name": "Alexander Kolesnikov"
      },
      {
        "authorId": "2743563",
        "name": "Xiaohua Zhai"
      },
      {
        "authorId": "2113839396",
        "name": "Ross Wightman"
      },
      {
        "authorId": "39328010",
        "name": "Jakob Uszkoreit"
      },
      {
        "authorId": "39611591",
        "name": "Lucas Beyer"
      }
    ],
    "source": "semantic_scholar",
    "score": 165.15820651189495
  },
  {
    "paperId": "6102a12b22ec3e44f9bbdae76185ca6e8a358f83",
    "url": "https://www.semanticscholar.org/paper/6102a12b22ec3e44f9bbdae76185ca6e8a358f83",
    "title": "End-to-End Human Pose and Mesh Reconstruction with Transformers",
    "abstract": "We present a new method, called MEsh TRansfOrmer (METRO), to reconstruct 3D human pose and mesh vertices from a single image. Our method uses a transformer encoder to jointly model vertex-vertex and vertex-joint interactions, and outputs 3D joint coordinates and mesh vertices simultaneously. Compared to existing techniques that regress pose and shape parameters, METRO does not rely on any parametric mesh models like SMPL, thus it can be easily extended to other objects such as hands. We further relax the mesh topology and allow the transformer self-attention mechanism to freely attend between any two vertices, making it possible to learn non-local relationships among mesh vertices and joints. With the proposed masked vertex modeling, our method is more robust and effective in handling challenging situations like partial occlusions. METRO generates new state-of-the-art results for human mesh reconstruction on the public Human3.6M and 3DPW datasets. Moreover, we demonstrate the generalizability of METRO to 3D hand reconstruction in the wild, outperforming existing state-of-the-art methods on FreiHAND dataset.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2020,
    "citationCount": 568,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2012.09760",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2020-12-17",
    "authors": [
      {
        "authorId": "143786724",
        "name": "Kevin Lin"
      },
      {
        "authorId": "29957038",
        "name": "Lijuan Wang"
      },
      {
        "authorId": "2145253136",
        "name": "Zicheng Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 165.15820651189495
  },
  {
    "paperId": "0b036cd5dfc49d835d0c759c8ca31d89f2410e65",
    "url": "https://www.semanticscholar.org/paper/0b036cd5dfc49d835d0c759c8ca31d89f2410e65",
    "title": "CMT: Convolutional Neural Networks Meet Vision Transformers",
    "abstract": "Vision transformers have been successfully applied to image recognition tasks due to their ability to capture long-range dependencies within an image. However, there are still gaps in both performance and computational cost between transformers and existing convolutional neural networks (CNNs). In this paper, we aim to address this issue and develop a network that can outperform not only the canonical transformers, but also the high-performance convolutional models. We propose a new transformer based hybrid network by taking advantage of transformers to capture long-range dependencies, and of CNNs to extract local information. Furthermore, we scale it to obtain a family of models, called CMTs, obtaining much better trade-off for accuracy and efficiency than previous CNN-based and transformer-based models. In particular, our CMT-S achieves 83.5% top-1 accuracy on ImageNet, while being 14x and 2x smaller on FLOPs than the existing DeiT and EfficientNet, respectively. The proposed CMT-S also generalizes well on CIFAR10 (99.2%), CIFAR100 (91.7%), Flowers (98.7%), and other challenging vision datasets such as COCO (44.3% mAP), with considerably less computational cost.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "citationCount": 567,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2107.06263",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-07-13",
    "authors": [
      {
        "authorId": "2148899357",
        "name": "Jianyuan Guo"
      },
      {
        "authorId": "3826388",
        "name": "Kai Han"
      },
      {
        "authorId": "2109295256",
        "name": "Han Wu"
      },
      {
        "authorId": "9196284",
        "name": "Chang Xu"
      },
      {
        "authorId": "103603255",
        "name": "Yehui Tang"
      },
      {
        "authorId": "1691522",
        "name": "Chunjing Xu"
      },
      {
        "authorId": "2108702980",
        "name": "Yunhe Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 165.1318212808173
  },
  {
    "paperId": "2c953a3c378b40dadf2e3fb486713c8608b8e282",
    "url": "https://www.semanticscholar.org/paper/2c953a3c378b40dadf2e3fb486713c8608b8e282",
    "title": "Pretrained Transformers for Text Ranking: BERT and Beyond",
    "abstract": "The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query for a particular task. Although the most common formulation of text ranking is search, instances of the task can also be found in many text processing applications. This tutorial provides an overview of text ranking with neural network architectures known as transformers, of which BERT (Bidirectional Encoder Representations from Transformers) is the best-known example. These models produce high quality results across many domains, tasks, and settings. This tutorial, which is based on the preprint of a forthcoming book to be published by Morgan and & Claypool under the Synthesis Lectures on Human Language Technologies series, provides an overview of existing work as a single point of entry for practitioners who wish to deploy transformers for text ranking in real-world applications and researchers who wish to pursue work in this area. We cover a wide range of techniques, grouped into two categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that perform ranking directly.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2020,
    "citationCount": 564,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3437963.3441667",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Book",
      "Conference",
      "Review"
    ],
    "publicationDate": "2020-10-13",
    "authors": [
      {
        "authorId": "145580839",
        "name": "Jimmy J. Lin"
      },
      {
        "authorId": "143744603",
        "name": "Rodrigo Nogueira"
      },
      {
        "authorId": "144115896",
        "name": "Andrew Yates"
      }
    ],
    "source": "semantic_scholar",
    "score": 165.0523859671966
  },
  {
    "paperId": "80da612e1831b8c11539180871843cff6dfaac90",
    "url": "https://www.semanticscholar.org/paper/80da612e1831b8c11539180871843cff6dfaac90",
    "title": "PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers",
    "abstract": "Point clouds captured in real-world applications are of-ten incomplete due to the limited sensor resolution, single viewpoint, and occlusion. Therefore, recovering the complete point clouds from partial ones becomes an indispensable task in many practical applications. In this paper, we present a new method that reformulates point cloud completion as a set-to-set translation problem and design a new model, called PoinTr that adopts a transformer encoder-decoder architecture for point cloud completion. By rep-resenting the point cloud as a set of unordered groups of points with position embeddings, we convert the point cloud to a sequence of point proxies and employ the transformers for point cloud generation. To facilitate transformers to better leverage the inductive bias about 3D geometric structures of point clouds, we further devise a geometry-aware block that models the local geometric relationships explicitly. The migration of transformers enables our model to better learn structural knowledge and preserve detailed information for point cloud completion. Furthermore, we propose two more challenging benchmarks with more diverse incomplete point clouds that can better reflect the real-world scenarios to promote future research. Experimental results show that our method outperforms state-of-the-art methods by a large margin on both the new bench-marks and the existing ones. Code is available at https://github.com/yuxumin/PoinTr.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 377,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2108.08839",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-08-19",
    "authors": [
      {
        "authorId": "2116330410",
        "name": "Xumin Yu"
      },
      {
        "authorId": "39358728",
        "name": "Yongming Rao"
      },
      {
        "authorId": "2142663191",
        "name": "Ziyi Wang"
      },
      {
        "authorId": "2124814824",
        "name": "Zuyan Liu"
      },
      {
        "authorId": "1697700",
        "name": "Jiwen Lu"
      },
      {
        "authorId": "2108485135",
        "name": "Jie Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 165.02341293429382
  },
  {
    "paperId": "dd2819016c6bf244c39b3e6707b60389bbdbcd21",
    "url": "https://www.semanticscholar.org/paper/dd2819016c6bf244c39b3e6707b60389bbdbcd21",
    "title": "Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling",
    "abstract": "We present Point-BERT, a new paradigm for learning Transformers to generalize the concept of BERT [8] to 3D point cloud. Inspired by BERT, we devise a Masked Point Modeling (MPM) task to pre-train point cloud Transformers. Specifically, we first divide a point cloud into several local point patches, and a point cloud Tokenizer with a discrete Variational AutoEncoder (dVAE) is designed to generate discrete point tokens containing meaningful local information. Then, we randomly mask out some patches of input point clouds and feed them into the backbone Transformers. The pre-training objective is to recover the original point tokens at the masked locations under the supervision of point tokens obtained by the Tokenizer. Extensive experiments demonstrate that the proposed BERT-style pre-training strategy significantly improves the performance of standard point cloud Transformers. Equipped with our pre-training strategy, we show that a pure Transformer architecture attains 93.8% accuracy on ModelNet40 and 83.1% accuracy on the hardest setting of ScanObjectNN, surpassing carefully designed point cloud models with much fewer hand-made designs. We also demonstrate that the representations learned by Point-BERT transfer well to new tasks and domains, where our models largely advance the state-of-the-art of few-shot point cloud classification task. The code and pre-trained models are available at https://github.com/lulutang0608/Point-BERT.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "citationCount": 558,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2111.14819",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-11-29",
    "authors": [
      {
        "authorId": "2116329737",
        "name": "Xumin Yu"
      },
      {
        "authorId": "145016965",
        "name": "Lulu Tang"
      },
      {
        "authorId": "2052552620",
        "name": "Yongming Rao"
      },
      {
        "authorId": "34097174",
        "name": "Tiejun Huang"
      },
      {
        "authorId": "48128428",
        "name": "Jie Zhou"
      },
      {
        "authorId": "1697700",
        "name": "Jiwen Lu"
      }
    ],
    "source": "semantic_scholar",
    "score": 164.8922420973265
  },
  {
    "paperId": "63812f583caac3ac32bbfb64f66ba69e57c1e90a",
    "url": "https://www.semanticscholar.org/paper/63812f583caac3ac32bbfb64f66ba69e57c1e90a",
    "title": "Conditional Positional Encodings for Vision Transformers",
    "abstract": "We propose a conditional positional encoding (CPE) scheme for vision Transformers. Unlike previous fixed or learnable positional encodings, which are pre-defined and independent of input tokens, CPE is dynamically generated and conditioned on the local neighborhood of the input tokens. As a result, CPE can easily generalize to the input sequences that are longer than what the model has ever seen during training. Besides, CPE can keep the desired translation-invariance in the image classification task, resulting in improved performance. We implement CPE with a simple Position Encoding Generator (PEG) to get seamlessly incorporated into the current Transformer framework. Built on PEG, we present Conditional Position encoding Vision Transformer (CPVT). We demonstrate that CPVT has visually similar attention maps compared to those with learned positional encodings and delivers outperforming results. Our code is available at https://github.com/Meituan-AutoML/CPVT .",
    "venue": "International Conference on Learning Representations",
    "year": 2021,
    "citationCount": 551,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-02-22",
    "authors": [
      {
        "authorId": "27628828",
        "name": "Xiangxiang Chu"
      },
      {
        "authorId": "2069520672",
        "name": "Zhi Tian"
      },
      {
        "authorId": "49846372",
        "name": "Bo Zhang"
      },
      {
        "authorId": "51316629",
        "name": "Xinlong Wang"
      },
      {
        "authorId": "12459603",
        "name": "Chunhua Shen"
      }
    ],
    "source": "semantic_scholar",
    "score": 164.7032206941564
  },
  {
    "paperId": "657329c633709dd1ac34a30d57341b186b1a47c2",
    "url": "https://www.semanticscholar.org/paper/657329c633709dd1ac34a30d57341b186b1a47c2",
    "title": "Efficient Content-Based Sparse Attention with Routing Transformers",
    "abstract": "Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2020,
    "citationCount": 550,
    "openAccessPdf": {
      "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00353/1923932/tacl_a_00353.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2020-03-12",
    "authors": [
      {
        "authorId": "39788470",
        "name": "Aurko Roy"
      },
      {
        "authorId": "2814161",
        "name": "M. Saffar"
      },
      {
        "authorId": "40348417",
        "name": "Ashish Vaswani"
      },
      {
        "authorId": "2529182",
        "name": "David Grangier"
      }
    ],
    "source": "semantic_scholar",
    "score": 164.67602213729373
  },
  {
    "paperId": "813b03e123d448d53d93a087a2d34a04dfe70c5c",
    "url": "https://www.semanticscholar.org/paper/813b03e123d448d53d93a087a2d34a04dfe70c5c",
    "title": "Voxel Transformer for 3D Object Detection",
    "abstract": "We present Voxel Transformer (VoTr), a novel and effective voxel-based Transformer backbone for 3D object detection from point clouds. Conventional 3D convolutional backbones in voxel-based 3D detectors cannot efficiently capture large context information, which is crucial for object recognition and localization, owing to the limited receptive fields. In this paper, we resolve the problem by introducing a Transformer-based architecture that enables long-range relationships between voxels by self-attention. Given the fact that non-empty voxels are naturally sparse but numerous, directly applying standard Transformer on voxels is non-trivial. To this end, we propose the sparse voxel module and the submanifold voxel module, which can operate on the empty and non-empty voxel positions effectively. To further enlarge the attention range while maintaining comparable computational overhead to the convolutional counterparts, we propose two attention mechanisms for multi-head attention in those two modules: Local Attention and Dilated Attention, and we further propose Fast Voxel Query to accelerate the querying process in multi-head attention. VoTr contains a series of sparse and submanifold voxel modules, and can be applied in most voxel-based detectors. Our proposed VoTr shows consistent improvement over the convolutional baselines while maintaining computational efficiency on the KITTI dataset and the Waymo Open dataset.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 367,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2109.02497",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-09-06",
    "authors": [
      {
        "authorId": "87581594",
        "name": "Jiageng Mao"
      },
      {
        "authorId": "2087093334",
        "name": "Yujing Xue"
      },
      {
        "authorId": "40896269",
        "name": "Minzhe Niu"
      },
      {
        "authorId": "1930797",
        "name": "Haoyue Bai"
      },
      {
        "authorId": "1698982",
        "name": "Jiashi Feng"
      },
      {
        "authorId": "40250403",
        "name": "Xiaodan Liang"
      },
      {
        "authorId": "2143534132",
        "name": "Hang Xu"
      },
      {
        "authorId": "1691522",
        "name": "Chunjing Xu"
      }
    ],
    "source": "semantic_scholar",
    "score": 164.62124407253395
  },
  {
    "paperId": "2ad12a7be5eaf339a98c4defd8669e11fe726acc",
    "url": "https://www.semanticscholar.org/paper/2ad12a7be5eaf339a98c4defd8669e11fe726acc",
    "title": "MaxViT: Multi-Axis Vision Transformer",
    "abstract": "Transformers have recently gained significant attention in the computer vision community. However, the lack of scalability of self-attention mechanisms with respect to image size has limited their wide adoption in state-of-the-art vision backbones. In this paper we introduce an efficient and scalable attention model we call multi-axis attention, which consists of two aspects: blocked local and dilated global attention. These design choices allow global-local spatial interactions on arbitrary input resolutions with only linear complexity. We also present a new architectural element by effectively blending our proposed attention model with convolutions, and accordingly propose a simple hierarchical vision backbone, dubbed MaxViT, by simply repeating the basic building block over multiple stages. Notably, MaxViT is able to ''see'' globally throughout the entire network, even in earlier, high-resolution stages. We demonstrate the effectiveness of our model on a broad spectrum of vision tasks. On image classification, MaxViT achieves state-of-the-art performance under various settings: without extra data, MaxViT attains 86.5% ImageNet-1K top-1 accuracy; with ImageNet-21K pre-training, our model achieves 88.7% top-1 accuracy. For downstream tasks, MaxViT as a backbone delivers favorable performance on object detection as well as visual aesthetic assessment. We also show that our proposed model expresses strong generative modeling capability on ImageNet, demonstrating the superior potential of MaxViT blocks as a universal vision module. The source code and trained models will be available at https://github.com/google-research/maxvit.",
    "venue": "European Conference on Computer Vision",
    "year": 2022,
    "citationCount": 534,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2204.01697",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-04-04",
    "authors": [
      {
        "authorId": "40992714",
        "name": "Zhengzhong Tu"
      },
      {
        "authorId": "1980498754",
        "name": "Hossein Talebi"
      },
      {
        "authorId": "2119077146",
        "name": "Han Zhang"
      },
      {
        "authorId": "1454990616",
        "name": "Feng Yang"
      },
      {
        "authorId": "1718280",
        "name": "P. Milanfar"
      },
      {
        "authorId": "1747569",
        "name": "A. Bovik"
      },
      {
        "authorId": "2110412307",
        "name": "Yinxiao Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 164.2340012034401
  },
  {
    "paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e",
    "url": "https://www.semanticscholar.org/paper/ac4dafdef1d2b685b7f28a11837414573d39ff4e",
    "title": "Universal Transformers",
    "abstract": "Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.",
    "venue": "International Conference on Learning Representations",
    "year": 2018,
    "citationCount": 718,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2018-07-10",
    "authors": [
      {
        "authorId": "3226635",
        "name": "Mostafa Dehghani"
      },
      {
        "authorId": "2776283",
        "name": "Stephan Gouws"
      },
      {
        "authorId": "1689108",
        "name": "O. Vinyals"
      },
      {
        "authorId": "39328010",
        "name": "Jakob Uszkoreit"
      },
      {
        "authorId": "40527594",
        "name": "Lukasz Kaiser"
      }
    ],
    "source": "semantic_scholar",
    "score": 163.6679203658157
  },
  {
    "paperId": "31426bfcfbf477acc230bcb82d32e2b7e4e546eb",
    "url": "https://www.semanticscholar.org/paper/31426bfcfbf477acc230bcb82d32e2b7e4e546eb",
    "title": "Swin Transformer Embedding UNet for Remote Sensing Image Semantic Segmentation",
    "abstract": "Global context information is essential for the semantic segmentation of remote sensing (RS) images. However, most existing methods rely on a convolutional neural network (CNN), which is challenging to directly obtain the global context due to the locality of the convolution operation. Inspired by the Swin transformer with powerful global modeling capabilities, we propose a novel semantic segmentation framework for RS images called ST-U-shaped network (UNet), which embeds the Swin transformer into the classical CNN-based UNet. ST-UNet constitutes a novel dual encoder structure of the Swin transformer and CNN in parallel. First, we propose a spatial interaction module (SIM), which encodes spatial information in the Swin transformer block by establishing pixel-level correlation to enhance the feature representation ability of occluded objects. Second, we construct a feature compression module (FCM) to reduce the loss of detailed information and condense more small-scale features in patch token downsampling of the Swin transformer, which improves the segmentation accuracy of small-scale ground objects. Finally, as a bridge between dual encoders, a relational aggregation module (RAM) is designed to integrate global dependencies from the Swin transformer into the features from CNN hierarchically. Our ST-UNet brings significant improvement on the ISPRS-Vaihingen and Potsdam datasets, respectively. The code will be available at https://github.com/XinnHe/ST-UNet.",
    "venue": "IEEE Transactions on Geoscience and Remote Sensing",
    "year": 2022,
    "citationCount": 263,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2116553369",
        "name": "Xingwei He"
      },
      {
        "authorId": "145815308",
        "name": "Yong Zhou"
      },
      {
        "authorId": "48019101",
        "name": "Jiaqi Zhao"
      },
      {
        "authorId": "113087438",
        "name": "Di Zhang"
      },
      {
        "authorId": "145786594",
        "name": "Rui Yao"
      },
      {
        "authorId": "2114921908",
        "name": "Yang Xue"
      }
    ],
    "source": "semantic_scholar",
    "score": 163.63923654719474
  },
  {
    "paperId": "7c597874535c1537d7ddff3b3723015b4dc79d30",
    "url": "https://www.semanticscholar.org/paper/7c597874535c1537d7ddff3b3723015b4dc79d30",
    "title": "MaskGIT: Masked Generative Image Transformer",
    "abstract": "Generative transformers have experienced rapid popularity growth in the computer vision community in synthesizing high-fidelity and high-resolution images. The best generative transformer models so far, however, still treat an image naively as a sequence of tokens, and decode an image sequentially following the raster scan ordering (i.e. line-by-line). We find this strategy neither optimal nor efficient. This paper proposes a novel image synthesis paradigm using a bidirectional transformer decoder, which we term MaskGIT. During training, MaskGIT learns to predict randomly masked tokens by attending to tokens in all directions. At inference time, the model begins with generating all tokens of an image simultaneously, and then refines the image iteratively conditioned on the previous generation. Our experiments demonstrate that MaskGIT significantly outperforms the state-of-the-art transformer model on the ImageNet dataset, and accelerates autoregressive decoding by up to 48x. Besides, we illustrate that MaskGIT can be easily extended to various image editing tasks, such as inpainting, extrapolation, and image manipulation. Project page: masked-generative-image-transformer.github.io.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2022,
    "citationCount": 503,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2202.04200",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-02-08",
    "authors": [
      {
        "authorId": "2914394",
        "name": "Huiwen Chang"
      },
      {
        "authorId": "2119079641",
        "name": "Han Zhang"
      },
      {
        "authorId": "39978626",
        "name": "Lu Jiang"
      },
      {
        "authorId": "2107890439",
        "name": "Ce Liu"
      },
      {
        "authorId": "1768236",
        "name": "W. Freeman"
      }
    ],
    "source": "semantic_scholar",
    "score": 163.33864402107054
  },
  {
    "paperId": "787119e3c3f819244c82b7d97779473773e60696",
    "url": "https://www.semanticscholar.org/paper/787119e3c3f819244c82b7d97779473773e60696",
    "title": "MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers",
    "abstract": "We present MaX-DeepLab, the first end-to-end model for panoptic segmentation. Our approach simplifies the current pipeline that depends heavily on surrogate sub-tasks and hand-designed components, such as box detection, non-maximum suppression, thing-stuff merging, etc. Although these sub-tasks are tackled by area experts, they fail to comprehensively solve the target task. By contrast, our MaX-DeepLab directly predicts class-labeled masks with a mask transformer, and is trained with a panoptic quality inspired loss via bipartite matching. Our mask transformer employs a dual-path architecture that introduces a global memory path in addition to a CNN path, allowing direct communication with any CNN layers. As a result, MaX-DeepLab shows a significant 7.1% PQ gain in the box-free regime on the challenging COCO dataset, closing the gap between box-based and box-free methods for the first time. A small variant of MaX-DeepLab improves 3.0% PQ over DETR with similar parameters and M-Adds. Furthermore, MaX-DeepLab, without test time augmentation, achieves new state-of-the-art 51.3% PQ on COCO test-dev set.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2020,
    "citationCount": 502,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2012.00759",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2020-12-01",
    "authors": [
      {
        "authorId": "1587922010",
        "name": "Huiyu Wang"
      },
      {
        "authorId": "1844940337",
        "name": "Yukun Zhu"
      },
      {
        "authorId": "2595180",
        "name": "Hartwig Adam"
      },
      {
        "authorId": "145081362",
        "name": "A. Yuille"
      },
      {
        "authorId": "34192119",
        "name": "Liang-Chieh Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 163.3088525514961
  },
  {
    "paperId": "f6fbb6809374ca57205bd2cf1421d4f4fa04f975",
    "url": "https://www.semanticscholar.org/paper/f6fbb6809374ca57205bd2cf1421d4f4fa04f975",
    "title": "Linguistic Knowledge and Transferability of Contextual Representations",
    "abstract": "Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of sixteen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2019,
    "citationCount": 700,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/1903.08855",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2019-03-01",
    "authors": [
      {
        "authorId": "22243769",
        "name": "Nelson F. Liu"
      },
      {
        "authorId": "40642935",
        "name": "Matt Gardner"
      },
      {
        "authorId": "2083259",
        "name": "Yonatan Belinkov"
      },
      {
        "authorId": "39139825",
        "name": "Matthew E. Peters"
      },
      {
        "authorId": "144365875",
        "name": "Noah A. Smith"
      }
    ],
    "source": "semantic_scholar",
    "score": 163.28761830551883
  },
  {
    "paperId": "71a1503732902d925993e09d8af9f209932ba978",
    "url": "https://www.semanticscholar.org/paper/71a1503732902d925993e09d8af9f209932ba978",
    "title": "Point Transformer V2: Grouped Vector Attention and Partition-based Pooling",
    "abstract": "As a pioneering work exploring transformer architecture for 3D point cloud understanding, Point Transformer achieves impressive results on multiple highly competitive benchmarks. In this work, we analyze the limitations of the Point Transformer and propose our powerful and efficient Point Transformer V2 model with novel designs that overcome the limitations of previous work. In particular, we first propose group vector attention, which is more effective than the previous version of vector attention. Inheriting the advantages of both learnable weight encoding and multi-head attention, we present a highly effective implementation of grouped vector attention with a novel grouped weight encoding layer. We also strengthen the position information for attention by an additional position encoding multiplier. Furthermore, we design novel and lightweight partition-based pooling methods which enable better spatial alignment and more efficient sampling. Extensive experiments show that our model achieves better performance than its predecessor and achieves state-of-the-art on several challenging 3D point cloud understanding benchmarks, including 3D point cloud segmentation on ScanNet v2 and S3DIS and 3D point cloud classification on ModelNet40. Our code will be available at https://github.com/Gofinge/PointTransformerV2.",
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "citationCount": 303,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2210.05666",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-10-11",
    "authors": [
      {
        "authorId": "2257364811",
        "name": "Xiaoyang Wu"
      },
      {
        "authorId": "35582047",
        "name": "Yixing Lao"
      },
      {
        "authorId": "2108810714",
        "name": "Li Jiang"
      },
      {
        "authorId": "46522599",
        "name": "Xihui Liu"
      },
      {
        "authorId": "3459894",
        "name": "Hengshuang Zhao"
      }
    ],
    "source": "semantic_scholar",
    "score": 162.7554155210933
  },
  {
    "paperId": "75284d5e4dfe1cd8a9ce69085210319e14fcfa3d",
    "url": "https://www.semanticscholar.org/paper/75284d5e4dfe1cd8a9ce69085210319e14fcfa3d",
    "title": "Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking",
    "abstract": "In video object tracking, there exist rich temporal contexts among successive frames, which have been largely overlooked in existing trackers. In this work, we bridge the individual video frames and explore the temporal contexts across them via a transformer architecture for robust object tracking. Different from classic usage of the transformer in natural language processing tasks, we separate its encoder and decoder into two parallel branches and carefully design them within the Siamese-like tracking pipelines. The transformer encoder promotes the target templates via attention-based feature reinforcement, which benefits the high-quality tracking model generation. The transformer decoder propagates the tracking cues from previous templates to the current frame, which facilitates the object searching process. Our transformer-assisted tracking framework is neat and trained in an end-to-end manner. With the proposed transformer, a simple Siamese matching approach is able to outperform the current top-performing trackers. By combining our transformer with the recent discriminative tracking pipeline, our method sets several new state-of-the-art records on prevalent tracking benchmarks.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "citationCount": 477,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.11681",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-03-22",
    "authors": [
      {
        "authorId": "2152169749",
        "name": "Ning Wang"
      },
      {
        "authorId": "38272296",
        "name": "Wen-gang Zhou"
      },
      {
        "authorId": "2146044028",
        "name": "Jie Wang"
      },
      {
        "authorId": "2144406784",
        "name": "Houqiang Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 162.54416098737184
  },
  {
    "paperId": "751b71158b7dcd2a7949e72a6ad8fb13657a401c",
    "url": "https://www.semanticscholar.org/paper/751b71158b7dcd2a7949e72a6ad8fb13657a401c",
    "title": "Visual Saliency Transformer",
    "abstract": "Existing state-of-the-art saliency detection methods heavily rely on CNN-based architectures. Alternatively, we rethink this task from a convolution-free sequence-to-sequence perspective and predict saliency by modeling long-range dependencies, which can not be achieved by convolution. Specifically, we develop a novel unified model based on a pure transformer, namely, Visual Saliency Transformer (VST), for both RGB and RGB-D salient object detection (SOD). It takes image patches as inputs and leverages the transformer to propagate global contexts among image patches. Unlike conventional architectures used in Vision Transformer (ViT), we leverage multi-level token fusion and propose a new token upsampling method under the transformer framework to get high-resolution detection results. We also develop a token-based multi-task decoder to simultaneously perform saliency and boundary detection by introducing task-related tokens and a novel patch-task-attention mechanism. Experimental results show that our model outperforms existing methods on both RGB and RGB-D SOD benchmark datasets. Most importantly, our whole framework not only provides a new perspective for the SOD field but also shows a new paradigm for transformer-based dense prediction models. Code is available at https://github.com/nnizhang/VST.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 319,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2104.12099",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-04-25",
    "authors": [
      {
        "authorId": "2027424343",
        "name": "Nian Liu"
      },
      {
        "authorId": "2153012406",
        "name": "Ni Zhang"
      },
      {
        "authorId": "2084554385",
        "name": "Kaiyuan Wan"
      },
      {
        "authorId": "7181955",
        "name": "Junwei Han"
      },
      {
        "authorId": "144082425",
        "name": "Ling Shao"
      }
    ],
    "source": "semantic_scholar",
    "score": 162.52481493690658
  },
  {
    "paperId": "a3e4ceb42cbcd2c807d53aff90a8cb1f5ee3f031",
    "url": "https://www.semanticscholar.org/paper/a3e4ceb42cbcd2c807d53aff90a8cb1f5ee3f031",
    "title": "SPECTER: Document-level Representation Learning using Citation-informed Transformers",
    "abstract": "Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, accurate embeddings of documents are a necessity. We propose SPECTER, a new method to generate document-level embedding of scientific papers based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, Specter can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that Specter outperforms a variety of competitive baselines on the benchmark.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "citationCount": 476,
    "openAccessPdf": {
      "url": "https://www.aclweb.org/anthology/2020.acl-main.207.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2020-04-15",
    "authors": [
      {
        "authorId": "2527954",
        "name": "Arman Cohan"
      },
      {
        "authorId": "46411828",
        "name": "Sergey Feldman"
      },
      {
        "authorId": "46181066",
        "name": "Iz Beltagy"
      },
      {
        "authorId": "145612610",
        "name": "Doug Downey"
      },
      {
        "authorId": "1780531",
        "name": "Daniel S. Weld"
      }
    ],
    "source": "semantic_scholar",
    "score": 162.5127473633251
  },
  {
    "paperId": "dd1139cfc609c2f3263d02e97176d5275caebc0a",
    "url": "https://www.semanticscholar.org/paper/dd1139cfc609c2f3263d02e97176d5275caebc0a",
    "title": "EfficientFormer: Vision Transformers at MobileNet Speed",
    "abstract": "Vision Transformers (ViT) have shown rapid progress in computer vision tasks, achieving promising results on various benchmarks. However, due to the massive number of parameters and model design, \\textit{e.g.}, attention mechanism, ViT-based models are generally times slower than lightweight convolutional networks. Therefore, the deployment of ViT for real-time applications is particularly challenging, especially on resource-constrained hardware such as mobile devices. Recent efforts try to reduce the computation complexity of ViT through network architecture search or hybrid design with MobileNet block, yet the inference speed is still unsatisfactory. This leads to an important question: can transformers run as fast as MobileNet while obtaining high performance? To answer this, we first revisit the network architecture and operators used in ViT-based models and identify inefficient designs. Then we introduce a dimension-consistent pure transformer (without MobileNet blocks) as a design paradigm. Finally, we perform latency-driven slimming to get a series of final models dubbed EfficientFormer. Extensive experiments show the superiority of EfficientFormer in performance and speed on mobile devices. Our fastest model, EfficientFormer-L1, achieves $79.2\\%$ top-1 accuracy on ImageNet-1K with only $1.6$ ms inference latency on iPhone 12 (compiled with CoreML), which runs as fast as MobileNetV2$\\times 1.4$ ($1.6$ ms, $74.7\\%$ top-1), and our largest model, EfficientFormer-L7, obtains $83.3\\%$ accuracy with only $7.0$ ms latency. Our work proves that properly designed transformers can reach extremely low latency on mobile devices while maintaining high performance.",
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "citationCount": 298,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2206.01191",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-06-02",
    "authors": [
      {
        "authorId": "1527091497",
        "name": "Yanyu Li"
      },
      {
        "authorId": "9347641",
        "name": "Geng Yuan"
      },
      {
        "authorId": "2167854921",
        "name": "Yang Wen"
      },
      {
        "authorId": "2167582358",
        "name": "Eric Hu"
      },
      {
        "authorId": "143998839",
        "name": "Georgios Evangelidis"
      },
      {
        "authorId": "145582202",
        "name": "S. Tulyakov"
      },
      {
        "authorId": "2136922252",
        "name": "Yanzhi Wang"
      },
      {
        "authorId": "2111473627",
        "name": "Jian Ren"
      }
    ],
    "source": "semantic_scholar",
    "score": 162.50665360086032
  },
  {
    "paperId": "96da196d6f8c947db03d13759f030642f8234abf",
    "url": "https://www.semanticscholar.org/paper/96da196d6f8c947db03d13759f030642f8234abf",
    "title": "DeepViT: Towards Deeper Vision Transformer",
    "abstract": "Vision transformers (ViTs) have been successfully applied in image classification tasks recently. In this paper, we show that, unlike convolution neural networks (CNNs)that can be improved by stacking more convolutional layers, the performance of ViTs saturate fast when scaled to be deeper. More specifically, we empirically observe that such scaling difficulty is caused by the attention collapse issue: as the transformer goes deeper, the attention maps gradually become similar and even much the same after certain layers. In other words, the feature maps tend to be identical in the top layers of deep ViT models. This fact demonstrates that in deeper layers of ViTs, the self-attention mechanism fails to learn effective concepts for representation learning and hinders the model from getting expected performance gain. Based on above observation, we propose a simple yet effective method, named Re-attention, to re-generate the attention maps to increase their diversity at different layers with negligible computation and memory cost. The pro-posed method makes it feasible to train deeper ViT models with consistent performance improvements via minor modification to existing ViT models. Notably, when training a deep ViT model with 32 transformer blocks, the Top-1 classification accuracy can be improved by 1.6% on ImageNet. Code is publicly available at https://github.com/zhoudaquan/dvit_repo.",
    "venue": "arXiv.org",
    "year": 2021,
    "citationCount": 475,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-03-22",
    "authors": [
      {
        "authorId": "18119920",
        "name": "Daquan Zhou"
      },
      {
        "authorId": "2064621368",
        "name": "Bingyi Kang"
      },
      {
        "authorId": "2103483",
        "name": "Xiaojie Jin"
      },
      {
        "authorId": "9145768",
        "name": "Linjie Yang"
      },
      {
        "authorId": "5964529",
        "name": "Xiaochen Lian"
      },
      {
        "authorId": "3298532",
        "name": "Qibin Hou"
      },
      {
        "authorId": "1698982",
        "name": "Jiashi Feng"
      }
    ],
    "source": "semantic_scholar",
    "score": 162.4812678134713
  },
  {
    "paperId": "deee48c5e0ac0407a1e002905caaf2b174bdb0e6",
    "url": "https://www.semanticscholar.org/paper/deee48c5e0ac0407a1e002905caaf2b174bdb0e6",
    "title": "MSA Transformer",
    "abstract": "Unsupervised protein language models trained across millions of diverse sequences learn structure and function of proteins. Protein language models studied to date have been trained to perform inference from individual sequences. The longstanding approach in computational biology has been to make inferences from a family of evo lutionarily related sequences by fitting a model to each family independently. In this work we combine the two paradigms. We introduce a protein language model which takes as input a set of sequences in the form of a multiple sequence alignment. The model interleaves row and column attention across the input sequences and is trained with a variant of the masked language modeling objective across many protein families. The performance of the model surpasses current state-of-the-art unsupervised structure learning methods by a wide margin, with far greater parameter efficiency than prior state-of-the-art protein language models.",
    "venue": "bioRxiv",
    "year": 2021,
    "citationCount": 471,
    "openAccessPdf": {
      "url": "https://www.biorxiv.org/content/biorxiv/early/2021/08/27/2021.02.12.430858.full.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Biology"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-02-13",
    "authors": [
      {
        "authorId": "47127382",
        "name": "Roshan Rao"
      },
      {
        "authorId": "2119081469",
        "name": "Jason Liu"
      },
      {
        "authorId": "1393004790",
        "name": "Robert Verkuil"
      },
      {
        "authorId": "49628465",
        "name": "Joshua Meier"
      },
      {
        "authorId": "1729041",
        "name": "J. Canny"
      },
      {
        "authorId": "1689992",
        "name": "P. Abbeel"
      },
      {
        "authorId": "2500466",
        "name": "Tom Sercu"
      },
      {
        "authorId": "119589314",
        "name": "Alexander Rives"
      }
    ],
    "source": "semantic_scholar",
    "score": 162.35468478378334
  },
  {
    "paperId": "1f133158a8973fb33fea188f20517cd7e69bfe7f",
    "url": "https://www.semanticscholar.org/paper/1f133158a8973fb33fea188f20517cd7e69bfe7f",
    "title": "FNet: Mixing Tokens with Fourier Transforms",
    "abstract": "We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that “mix” input tokens. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the “efficient Transformers” on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.",
    "venue": "North American Chapter of the Association for Computational Linguistics",
    "year": 2021,
    "citationCount": 471,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2022.naacl-main.319.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-05-09",
    "authors": [
      {
        "authorId": "1405626394",
        "name": "J. Lee-Thorp"
      },
      {
        "authorId": "1643737606",
        "name": "J. Ainslie"
      },
      {
        "authorId": "2085636189",
        "name": "Ilya Eckstein"
      },
      {
        "authorId": "1722671",
        "name": "Santiago Ontañón"
      }
    ],
    "source": "semantic_scholar",
    "score": 162.35468478378334
  },
  {
    "paperId": "f7c974321aa689102536228efcbcde3166acf1c9",
    "url": "https://www.semanticscholar.org/paper/f7c974321aa689102536228efcbcde3166acf1c9",
    "title": "SwinSUNet: Pure Transformer Network for Remote Sensing Image Change Detection",
    "abstract": "Convolutional neural network (CNN) can extract effective semantic features, so it was widely used for remote sensing image change detection (CD) in the latest years. CNN has acquired great achievements in the field of CD, but due to the intrinsic locality of convolution operation, it could not capture global information in space-time. The Transformer was proposed in recent years and it can effectively extract global information, so it was used to solve computer vision (CV) tasks and achieved amazing success. In this paper, we design a pure Transformer network with siamese U-shaped structure to solve CD problems, and name it SwinSUNet. SwinSUNet contains encoder, fusion and decoder, and all of them use Swin Transformer blocks as basic units. Encoder has a siamese structure based on hierarchical Swin Transformer, so encoder can process bi-temporal images in parallel and extract their multi-scale features. Fusion is mainly responsible for the merge operation of the bi-temporal features generated by encoder. Like encoder, decoder also based on hierarchical Swin Transformer. Different from encoder, decoder uses up-sampling and merging (UM) block and Swin Transformer blocks to recover the details of the change information. The encoder uses patch merging and Swin Transformer blocks to generate effective semantic features. After the sequential process of these three modules, SwinSUNet will output the change maps. We did expensive experiments on four CD datasets, in these experiments SwinSUNet achieved better results than other related methods.",
    "venue": "IEEE Transactions on Geoscience and Remote Sensing",
    "year": 2022,
    "citationCount": 240,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2159211528",
        "name": "Cui Zhang"
      },
      {
        "authorId": "120692716",
        "name": "Liejun Wang"
      },
      {
        "authorId": "148024293",
        "name": "Shuli Cheng"
      },
      {
        "authorId": "2111175250",
        "name": "Yongming Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 162.27195400235982
  },
  {
    "paperId": "c431408780586268e8bcf2483b01a80728d10960",
    "url": "https://www.semanticscholar.org/paper/c431408780586268e8bcf2483b01a80728d10960",
    "title": "Vision Transformer Adapter for Dense Predictions",
    "abstract": "This work investigates a simple yet powerful dense prediction task adapter for Vision Transformer (ViT). Unlike recently advanced variants that incorporate vision-specific inductive biases into their architectures, the plain ViT suffers inferior performance on dense predictions due to weak prior assumptions. To address this issue, we propose the ViT-Adapter, which allows plain ViT to achieve comparable performance to vision-specific transformers. Specifically, the backbone in our framework is a plain ViT that can learn powerful representations from large-scale multi-modal data. When transferring to downstream tasks, a pre-training-free adapter is used to introduce the image-related inductive biases into the model, making it suitable for these tasks. We verify ViT-Adapter on multiple dense prediction tasks, including object detection, instance segmentation, and semantic segmentation. Notably, without using extra detection data, our ViT-Adapter-L yields state-of-the-art 60.9 box AP and 53.0 mask AP on COCO test-dev. We hope that the ViT-Adapter could serve as an alternative for vision-specific transformers and facilitate future research. The code and models will be released at https://github.com/czczup/ViT-Adapter.",
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "citationCount": 465,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2205.08534",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-05-17",
    "authors": [
      {
        "authorId": "66350249",
        "name": "Zhe Chen"
      },
      {
        "authorId": "2111768400",
        "name": "Yuchen Duan"
      },
      {
        "authorId": "71074736",
        "name": "Wenhai Wang"
      },
      {
        "authorId": "1720735918",
        "name": "Junjun He"
      },
      {
        "authorId": "2115137018",
        "name": "Tong Lu"
      },
      {
        "authorId": "3304536",
        "name": "Jifeng Dai"
      },
      {
        "authorId": "145858545",
        "name": "Y. Qiao"
      }
    ],
    "source": "semantic_scholar",
    "score": 162.1627845118847
  },
  {
    "paperId": "2bbffec6395d3a8f81f5fe9137f7e078bd0f0336",
    "url": "https://www.semanticscholar.org/paper/2bbffec6395d3a8f81f5fe9137f7e078bd0f0336",
    "title": "Activating More Pixels in Image Super-Resolution Transformer",
    "abstract": "Transformer-based methods have shown impressive performance in low-level vision tasks, such as image super-resolution. However, we find that these networks can only utilize a limited spatial range of input information through attribution analysis. This implies that the potential of Transformer is still not fully exploited in existing networks. In order to activate more input pixels for better reconstruction, we propose a novel Hybrid Attention Transformer (HAT). It combines both channel attention and window-based self-attention schemes, thus making use of their complementary advantages of being able to utilize global statistics and strong local fitting capability. Moreover, to better aggregate the cross-window information, we introduce an overlapping cross-attention module to enhance the interaction between neighboring window features. In the training stage, we additionally adopt a same-task pre-training strategy to exploit the potential of the model for further improvement. Extensive experiments show the effectiveness of the proposed modules, and we further scale up the model to demonstrate that the performance of this task can be greatly improved. Our overall method significantly outperforms the state-of-the-art methods by more than 1dB.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2022,
    "citationCount": 464,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2205.04437",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-05-09",
    "authors": [
      {
        "authorId": "2143735670",
        "name": "Xiangyu Chen"
      },
      {
        "authorId": null,
        "name": "Xintao Wang"
      },
      {
        "authorId": "1735685",
        "name": "Jiantao Zhou"
      },
      {
        "authorId": "30459277",
        "name": "Chao Dong"
      }
    ],
    "source": "semantic_scholar",
    "score": 162.13056108381033
  },
  {
    "paperId": "60ee030773ba1b68eb222a265b052ca028353362",
    "url": "https://www.semanticscholar.org/paper/60ee030773ba1b68eb222a265b052ca028353362",
    "title": "GIT: A Generative Image-to-text Transformer for Vision and Language",
    "abstract": "In this paper, we design and train a Generative Image-to-text Transformer, GIT, to unify vision-language tasks such as image/video captioning and question answering. While generative models provide a consistent network architecture between pre-training and fine-tuning, existing work typically contains complex structures (uni/multi-modal encoder/decoder) and depends on external modules such as object detectors/taggers and optical character recognition (OCR). In GIT, we simplify the architecture as one image encoder and one text decoder under a single language modeling task. We also scale up the pre-training data and the model size to boost the model performance. Without bells and whistles, our GIT establishes new state of the arts on 12 challenging benchmarks with a large margin. For instance, our model surpasses the human performance for the first time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a new scheme of generation-based image classification and scene text recognition, achieving decent performance on standard benchmarks. Codes are released at \\url{https://github.com/microsoft/GenerativeImage2Text}.",
    "venue": "Trans. Mach. Learn. Res.",
    "year": 2022,
    "citationCount": 464,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2205.14100",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-05-27",
    "authors": [
      {
        "authorId": "2124948371",
        "name": "Jianfeng Wang"
      },
      {
        "authorId": "2149231840",
        "name": "Zhengyuan Yang"
      },
      {
        "authorId": "2148941781",
        "name": "Xiaowei Hu"
      },
      {
        "authorId": "50703697",
        "name": "Linjie Li"
      },
      {
        "authorId": "143786724",
        "name": "Kevin Lin"
      },
      {
        "authorId": "144702900",
        "name": "Zhe Gan"
      },
      {
        "authorId": "2145253136",
        "name": "Zicheng Liu"
      },
      {
        "authorId": "2107890439",
        "name": "Ce Liu"
      },
      {
        "authorId": "29957038",
        "name": "Lijuan Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 162.13056108381033
  },
  {
    "paperId": "33172567ab1dff9ca32c8d995d88bbff466f3236",
    "url": "https://www.semanticscholar.org/paper/33172567ab1dff9ca32c8d995d88bbff466f3236",
    "title": "Integrating Multimodal Information in Large Pretrained Transformers",
    "abstract": "Recent Transformer-based contextual word representations, including BERT and XLNet, have shown state-of-the-art performance in multiple disciplines within NLP. Fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream. While fine-tuning these pre-trained models is straightforward for lexical applications (applications with only language modality), it is not trivial for multimodal language (a growing area in NLP focused on modeling face-to-face communication). More specifically, this is due to the fact that pre-trained models don’t have the necessary components to accept two extra modalities of vision and acoustic. In this paper, we proposed an attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG). MAG allows BERT and XLNet to accept multimodal nonverbal data during fine-tuning. It does so by generating a shift to internal representation of BERT and XLNet; a shift that is conditioned on the visual and acoustic modalities. In our experiments, we study the commonly used CMU-MOSI and CMU-MOSEI datasets for multimodal sentiment analysis. Fine-tuning MAG-BERT and MAG-XLNet significantly boosts the sentiment analysis performance over previous baselines as well as language-only fine-tuning of BERT and XLNet. On the CMU-MOSI dataset, MAG-XLNet achieves human-level multimodal sentiment analysis performance for the first time in the NLP community.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2020,
    "citationCount": 462,
    "openAccessPdf": {
      "url": "https://www.aclweb.org/anthology/2020.acl-main.214.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Medicine",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2020-07-01",
    "authors": [
      {
        "authorId": "153515321",
        "name": "Wasifur Rahman"
      },
      {
        "authorId": "2811524",
        "name": "M. Hasan"
      },
      {
        "authorId": "1459934320",
        "name": "Sangwu Lee"
      },
      {
        "authorId": "144802290",
        "name": "Amir Zadeh"
      },
      {
        "authorId": "1429197894",
        "name": "Chengfeng Mao"
      },
      {
        "authorId": "49933077",
        "name": "Louis-Philippe Morency"
      },
      {
        "authorId": "1491348598",
        "name": "E. Hoque"
      }
    ],
    "source": "semantic_scholar",
    "score": 162.0659058112935
  },
  {
    "paperId": "f4b11a696aa5a03fed1bfc47e65fdb7eb0e529c1",
    "url": "https://www.semanticscholar.org/paper/f4b11a696aa5a03fed1bfc47e65fdb7eb0e529c1",
    "title": "UniFormer: Unifying Convolution and Self-Attention for Visual Recognition",
    "abstract": "It is a challenging task to learn discriminative representation from images and videos, due to large local redundancy and complex global dependency in these visual data. Convolution neural networks (CNNs) and vision transformers (ViTs) have been two dominant frameworks in the past few years. Though CNNs can efficiently decrease local redundancy by convolution within a small neighborhood, the limited receptive field makes it hard to capture global dependency. Alternatively, ViTs can effectively capture long-range dependency via self-attention, while blind similarity comparisons among all the tokens lead to high redundancy. To resolve these problems, we propose a novel Unified transFormer (UniFormer), which can seamlessly integrate the merits of convolution and self-attention in a concise transformer format. Different from the typical transformer blocks, the relation aggregators in our UniFormer block are equipped with local and global token affinity respectively in shallow and deep layers, allowing tackling both redundancy and dependency for efficient and effective representation learning. Finally, we flexibly stack our blocks into a new powerful backbone, and adopt it for various vision tasks from image to video domain, from classification to dense prediction. Without any extra training data, our UniFormer achieves <italic>86.3</italic> top-1 accuracy on ImageNet-1 K classification task. With only ImageNet-1 K pre-training, it can simply achieve state-of-the-art performance in a broad range of downstream tasks. It obtains <italic>82.9/84.8</italic> top-1 accuracy on Kinetics-400/600, <italic>60.9/71.2</italic> top-1 accuracy on Something-Something V1/V2 video classification tasks, <italic>53.8</italic> box AP and <italic>46.4</italic> mask AP on COCO object detection task, <italic>50.8</italic> mIoU on ADE20 K semantic segmentation task, and <italic>77.4</italic> AP on COCO pose estimation task. Moreover, we build an efficient UniFormer with a concise hourglass design of token shrinking and recovering, which achieves <bold>2-4</bold><inline-formula><tex-math notation=\"LaTeX\">$\\bm {\\times }$</tex-math><alternatives><mml:math><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href=\"li-ieq1-3282631.gif\"/></alternatives></inline-formula> higher throughput than the recent lightweight models.",
    "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "year": 2022,
    "citationCount": 309,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2201.09450",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-01-24",
    "authors": [
      {
        "authorId": "2115468491",
        "name": "Kunchang Li"
      },
      {
        "authorId": "47903936",
        "name": "Yali Wang"
      },
      {
        "authorId": "2108047877",
        "name": "Junhao Zhang"
      },
      {
        "authorId": "144740494",
        "name": "Peng Gao"
      },
      {
        "authorId": "12920342",
        "name": "Guanglu Song"
      },
      {
        "authorId": "2146400394",
        "name": "Yu Liu"
      },
      {
        "authorId": "47893312",
        "name": "Hongsheng Li"
      },
      {
        "authorId": "2059129841",
        "name": "Y. Qiao"
      }
    ],
    "source": "semantic_scholar",
    "score": 162.04858446218788
  },
  {
    "paperId": "076a8e778f2e9efb3c2fd45fed534ae9e6035f1b",
    "url": "https://www.semanticscholar.org/paper/076a8e778f2e9efb3c2fd45fed534ae9e6035f1b",
    "title": "Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis",
    "abstract": "Vision Transformers (ViT)s have shown great performance in self-supervised learning of global and local representations that can be transferred to downstream applications. Inspired by these results, we introduce a novel self-supervised learning framework with tailored proxy tasks for medical image analysis. Specifically, we propose: (i) a new 3D transformer-based model, dubbed Swin UNEt TRansformers (Swin UNETR), with a hierarchical encoder for self-supervised pretraining; (ii) tailored proxy tasks for learning the underlying pattern of human anatomy. We demonstrate successful pre-training of the proposed model on 5,050 publicly available computed tomography (CT) images from various body organs. The effectiveness of our approach is validated by fine-tuning the pre-trained models on the Beyond the Cranial Vault (BTCV) Segmentation Challenge with 13 abdominal organs and segmentation tasks from the Medical Segmentation Decathlon (MSD) dataset. Our model is currently the state-of-the-art on the public test leaderboards of both MSD11https://decathlon-10.grand-challenge.org/evaluation/challenge/leaderboard/ and BTCV 22https://www.synapse.org/#!Synapse:syn3193805/wiki/217785/ datasets. Code: https://monai.io/research/swin-unetr.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "citationCount": 457,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2111.14791",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-11-29",
    "authors": [
      {
        "authorId": "46556781",
        "name": "Yucheng Tang"
      },
      {
        "authorId": "144041873",
        "name": "Dong Yang"
      },
      {
        "authorId": "2108730532",
        "name": "Wenqi Li"
      },
      {
        "authorId": "144531567",
        "name": "H. Roth"
      },
      {
        "authorId": "1699344",
        "name": "B. Landman"
      },
      {
        "authorId": "3262394",
        "name": "Daguang Xu"
      },
      {
        "authorId": "10751841",
        "name": "V. Nath"
      },
      {
        "authorId": "31374559",
        "name": "Ali Hatamizadeh"
      }
    ],
    "source": "semantic_scholar",
    "score": 161.90303776171277
  },
  {
    "paperId": "e28a054b11b0b861b5055595bbc69a8d51cac0e4",
    "url": "https://www.semanticscholar.org/paper/e28a054b11b0b861b5055595bbc69a8d51cac0e4",
    "title": "Few-Shot Learning via Embedding Adaptation With Set-to-Set Functions",
    "abstract": "Learning with limited data is a key challenge for visual recognition. Many few-shot learning methods address this challenge by learning an instance embedding function from seen classes and apply the function to instances from unseen classes with limited labels. This style of transfer learning is task-agnostic: the embedding function is not learned optimally discriminative with respect to the unseen classes, where discerning among them leads to the target task. In this paper, we propose a novel approach to adapt the instance embeddings to the target classification task with a set-to-set function, yielding embeddings that are task-specific and are discriminative. We empirically investigated various instantiations of such set-to-set functions and observed the Transformer is most effective --- as it naturally satisfies key properties of our desired model. We denote this model as FEAT (few-shot embedding adaptation w/ Transformer) and validate it on both the standard few-shot classification benchmark and four extended few-shot learning settings with essential use cases, i.e., cross-domain, transductive, generalized few-shot learning, and low-shot learning. It archived consistent improvements over baseline models as well as previous methods, and established the new state-of-the-art results on two benchmarks.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2018,
    "citationCount": 618,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/1812.03664",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2018-12-10",
    "authors": [
      {
        "authorId": "2151459740",
        "name": "Han-Jia Ye"
      },
      {
        "authorId": "2804000",
        "name": "Hexiang Hu"
      },
      {
        "authorId": "1721819",
        "name": "De-chuan Zhan"
      },
      {
        "authorId": "145757665",
        "name": "Fei Sha"
      }
    ],
    "source": "semantic_scholar",
    "score": 161.42157909026895
  },
  {
    "paperId": "ef89d5899eff8d5e62f85018b3f11889d920a1aa",
    "url": "https://www.semanticscholar.org/paper/ef89d5899eff8d5e62f85018b3f11889d920a1aa",
    "title": "TransVG: End-to-End Visual Grounding with Transformers",
    "abstract": "In this paper, we present a neat yet effective transformer-based framework for visual grounding, namely TransVG, to address the task of grounding a language query to the corresponding region onto an image. The state-of-the-art methods, including two-stage or one-stage ones, rely on a complex module with manually-designed mechanisms to perform the query reasoning and multi-modal fusion. However, the involvement of certain mechanisms in fusion module design, such as query decomposition and image scene graph, makes the models easily overfit to datasets with specific scenarios, and limits the plenitudinous interaction between the visual-linguistic context. To avoid this caveat, we propose to establish the multi-modal correspondence by leveraging transformers, and empirically show that the complex fusion modules (e.g., modular attention network, dynamic graph, and multi-modal tree) can be replaced by a simple stack of transformer encoder layers with higher performance. Moreover, we re-formulate the visual grounding as a direct coordinates regression problem and avoid making predictions out of a set of candidates (i.e., region proposals or anchor boxes). Extensive experiments are conducted on five widely used datasets, and a series of state-of-the-art records are set by our TransVG. We build the benchmark of transformer-based visual grounding framework and make the code available at https://github.com/djiajunustc/TransVG.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2021,
    "citationCount": 296,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2104.08541",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-04-17",
    "authors": [
      {
        "authorId": "2087218319",
        "name": "Jiajun Deng"
      },
      {
        "authorId": "2149231840",
        "name": "Zhengyuan Yang"
      },
      {
        "authorId": "7934161",
        "name": "Tianlang Chen"
      },
      {
        "authorId": "38272296",
        "name": "Wen-gang Zhou"
      },
      {
        "authorId": "2108508109",
        "name": "Houqiang Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 161.4059820820405
  },
  {
    "paperId": "05dcdfece56d1869895f53ed581d8ad64118c05f",
    "url": "https://www.semanticscholar.org/paper/05dcdfece56d1869895f53ed581d8ad64118c05f",
    "title": "Sign Language Transformers: Joint End-to-End Sign Language Recognition and Translation",
    "abstract": "Prior work on Sign Language Translation has shown that having a mid-level sign gloss representation (effectively recognizing the individual signs) improves the translation performance drastically. In fact, the current state-of-the-art in translation requires gloss level tokenization in order to work. We introduce a novel transformer based architecture that jointly learns Continuous Sign Language Recognition and Translation while being trainable in an end-to-end manner. This is achieved by using a Connectionist Temporal Classification (CTC) loss to bind the recognition and translation problems into a single unified architecture. This joint approach does not require any ground-truth timing information, simultaneously solving two co-dependant sequence-to-sequence learning problems and leads to significant performance gains. We evaluate the recognition and translation performances of our approaches on the challenging RWTH-PHOENIX-Weather-2014T (PHOENIX14T) dataset. We report state-of-the-art sign language recognition and translation results achieved by our Sign Language Transformers. Our translation networks outperform both sign video to spoken language and gloss to spoken language translation models, in some cases more than doubling the performance (9.58 vs. 21.80 BLEU-4 Score). We also share new baseline translation results using transformer networks for several other text-to-text sign language translation tasks.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2020,
    "citationCount": 442,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2003.13830",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2020-03-30",
    "authors": [
      {
        "authorId": "2808158",
        "name": "Necati Cihan Camgöz"
      },
      {
        "authorId": "47285696",
        "name": "Oscar Koller"
      },
      {
        "authorId": "2417546",
        "name": "Simon Hadfield"
      },
      {
        "authorId": "145398628",
        "name": "R. Bowden"
      }
    ],
    "source": "semantic_scholar",
    "score": 161.40354655067705
  },
  {
    "paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
    "url": "https://www.semanticscholar.org/paper/66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
    "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
    "abstract": null,
    "venue": "Neurocomputing",
    "year": 2021,
    "citationCount": 1655,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2104.09864",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-04-20",
    "authors": [
      {
        "authorId": "51111230",
        "name": "Jianlin Su"
      },
      {
        "authorId": "2140045110",
        "name": "Yu Lu"
      },
      {
        "authorId": "1382633722",
        "name": "Shengfeng Pan"
      },
      {
        "authorId": "2079396269",
        "name": "Bo Wen"
      },
      {
        "authorId": "1807486863",
        "name": "Yunfeng Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 161.18240502417808
  },
  {
    "paperId": "026b3396a63ed5772329708b7580d633bb86bec9",
    "url": "https://www.semanticscholar.org/paper/026b3396a63ed5772329708b7580d633bb86bec9",
    "title": "RWKV: Reinventing RNNs for the Transformer Era",
    "abstract": "Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "citationCount": 434,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.findings-emnlp.936.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-22",
    "authors": [
      {
        "authorId": "145560079",
        "name": "Bo Peng"
      },
      {
        "authorId": "79046907",
        "name": "Eric Alcaide"
      },
      {
        "authorId": "1404060481",
        "name": "Quentin G. Anthony"
      },
      {
        "authorId": "2044198106",
        "name": "Alon Albalak"
      },
      {
        "authorId": "2322500848",
        "name": "Samuel Arcadinho"
      },
      {
        "authorId": "103476203",
        "name": "Stella Biderman"
      },
      {
        "authorId": "47709883",
        "name": "Huanqi Cao"
      },
      {
        "authorId": "2193630544",
        "name": "Xin Cheng"
      },
      {
        "authorId": "2218291950",
        "name": "Michael Chung"
      },
      {
        "authorId": "2425906",
        "name": "Matteo Grella"
      },
      {
        "authorId": "101433524",
        "name": "G. Kranthikiran"
      },
      {
        "authorId": "46214809",
        "name": "Xingjian Du"
      },
      {
        "authorId": "33913193",
        "name": "Xuming He"
      },
      {
        "authorId": "9397636",
        "name": "Haowen Hou"
      },
      {
        "authorId": "1724788",
        "name": "Przemyslaw Kazienko"
      },
      {
        "authorId": "2905929",
        "name": "Jan Kocoń"
      },
      {
        "authorId": "35171548",
        "name": "Jiaming Kong"
      },
      {
        "authorId": "2208962106",
        "name": "Bartlomiej Koptyra"
      },
      {
        "authorId": "1598440603",
        "name": "Hayden Lau"
      },
      {
        "authorId": "2209207087",
        "name": "Krishna Sri Ipsit Mantri"
      },
      {
        "authorId": "2218334866",
        "name": "Ferdinand Mom"
      },
      {
        "authorId": "2186861874",
        "name": "Atsushi Saito"
      },
      {
        "authorId": "47274259",
        "name": "Xiangru Tang"
      },
      {
        "authorId": "2153213619",
        "name": "Bolun Wang"
      },
      {
        "authorId": "1388112865",
        "name": "J. S. Wind"
      },
      {
        "authorId": "2218377273",
        "name": "Stansilaw Wozniak"
      },
      {
        "authorId": "2218987422",
        "name": "Ruichong Zhang"
      },
      {
        "authorId": "2144400204",
        "name": "Zhenyuan Zhang"
      },
      {
        "authorId": "2110483969",
        "name": "Qihang Zhao"
      },
      {
        "authorId": "1994721202",
        "name": "P. Zhou"
      },
      {
        "authorId": "144549416",
        "name": "Jian Zhu"
      },
      {
        "authorId": "144649570",
        "name": "Rui Zhu"
      }
    ],
    "source": "semantic_scholar",
    "score": 161.13019046633025
  },
  {
    "paperId": "7d97c17a75beb89f938eaac1d3ca60ac2245fb2e",
    "url": "https://www.semanticscholar.org/paper/7d97c17a75beb89f938eaac1d3ca60ac2245fb2e",
    "title": "Faith and Fate: Limits of Transformers on Compositionality",
    "abstract": "Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with\\,increased\\,task\\,complexity.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 271,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-29",
    "authors": [
      {
        "authorId": "46217681",
        "name": "Nouha Dziri"
      },
      {
        "authorId": "50085131",
        "name": "Ximing Lu"
      },
      {
        "authorId": "1947172233",
        "name": "Melanie Sclar"
      },
      {
        "authorId": "1737850",
        "name": "Xiang Lorraine Li"
      },
      {
        "authorId": "2218495662",
        "name": "Liwei Jian"
      },
      {
        "authorId": "51583409",
        "name": "Bill Yuchen Lin"
      },
      {
        "authorId": "119659229",
        "name": "Peter West"
      },
      {
        "authorId": "1857797",
        "name": "Chandra Bhagavatula"
      },
      {
        "authorId": "39227408",
        "name": "Ronan Le Bras"
      },
      {
        "authorId": "2012510",
        "name": "Jena D. Hwang"
      },
      {
        "authorId": "3313909",
        "name": "Soumya Sanyal"
      },
      {
        "authorId": "2129663",
        "name": "S. Welleck"
      },
      {
        "authorId": "145201124",
        "name": "Xiang Ren"
      },
      {
        "authorId": "37907837",
        "name": "Allyson Ettinger"
      },
      {
        "authorId": "1753355",
        "name": "Zaïd Harchaoui"
      },
      {
        "authorId": "1699545",
        "name": "Yejin Choi"
      }
    ],
    "source": "semantic_scholar",
    "score": 161.08703099443997
  },
  {
    "paperId": "707bd332d2c21dc5eb1f02a52d4a0506199aae76",
    "url": "https://www.semanticscholar.org/paper/707bd332d2c21dc5eb1f02a52d4a0506199aae76",
    "title": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers",
    "abstract": "Large-scale pretrained transformers have created milestones in text (GPT-3) and text-to-image (DALL-E and CogView) generation. Its application to video generation is still facing many challenges: The potential huge computation cost makes the training from scratch unaffordable; The scarcity and weak relevance of text-video datasets hinder the model understanding complex movement semantics. In this work, we present 9B-parameter transformer CogVideo, trained by inheriting a pretrained text-to-image model, CogView2. We also propose multi-frame-rate hierarchical training strategy to better align text and video clips. As (probably) the first open-source large-scale pretrained text-to-video model, CogVideo outperforms all publicly available models at a large margin in machine and human evaluations.",
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "citationCount": 425,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2205.15868",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-05-29",
    "authors": [
      {
        "authorId": "2105844599",
        "name": "Wenyi Hong"
      },
      {
        "authorId": "2055623340",
        "name": "Ming Ding"
      },
      {
        "authorId": "2163967642",
        "name": "Wendi Zheng"
      },
      {
        "authorId": "46522721",
        "name": "Xinghan Liu"
      },
      {
        "authorId": "2148911990",
        "name": "Jie Tang"
      }
    ],
    "source": "semantic_scholar",
    "score": 160.81659019404054
  },
  {
    "paperId": "47ae807cd511b35e78a2cd4e198283dea6dafd41",
    "url": "https://www.semanticscholar.org/paper/47ae807cd511b35e78a2cd4e198283dea6dafd41",
    "title": "Do Transformers Really Perform Bad for Graph Representation?",
    "abstract": "The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer.",
    "venue": "arXiv.org",
    "year": 2021,
    "citationCount": 425,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-06-09",
    "authors": [
      {
        "authorId": "2051552141",
        "name": "Chengxuan Ying"
      },
      {
        "authorId": "123970124",
        "name": "Tianle Cai"
      },
      {
        "authorId": "2108801920",
        "name": "Shengjie Luo"
      },
      {
        "authorId": "150311931",
        "name": "Shuxin Zheng"
      },
      {
        "authorId": "35286545",
        "name": "Guolin Ke"
      },
      {
        "authorId": "1391126980",
        "name": "Di He"
      },
      {
        "authorId": "2115437382",
        "name": "Yanming Shen"
      },
      {
        "authorId": "2110264337",
        "name": "Tie-Yan Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 160.81659019404054
  },
  {
    "paperId": "2d9ae4c167510ed78803735fc57ea67c3cc55a35",
    "url": "https://www.semanticscholar.org/paper/2d9ae4c167510ed78803735fc57ea67c3cc55a35",
    "title": "VideoGPT: Video Generation using VQ-VAE and Transformers",
    "abstract": "We present VideoGPT: a conceptually simple architecture for scaling likelihood based generative modeling to natural videos. VideoGPT uses VQ-VAE that learns downsampled discrete latent representations of a raw video by employing 3D convolutions and axial self-attention. A simple GPT-like architecture is then used to autoregressively model the discrete latents using spatio-temporal position encodings. Despite the simplicity in formulation and ease of training, our architecture is able to generate samples competitive with state-of-the-art GAN models for video generation on the BAIR Robot dataset, and generate high fidelity natural videos from UCF-101 and Tumbler GIF Dataset (TGIF). We hope our proposed architecture serves as a reproducible reference for a minimalistic implementation of transformer based video generation models. Samples and code are available at https://wilson1yan.github.io/videogpt/index.html",
    "venue": "arXiv.org",
    "year": 2021,
    "citationCount": 424,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-04-20",
    "authors": [
      {
        "authorId": "46705049",
        "name": "Wilson Yan"
      },
      {
        "authorId": "49890268",
        "name": "Yunzhi Zhang"
      },
      {
        "authorId": "1689992",
        "name": "P. Abbeel"
      },
      {
        "authorId": "41207614",
        "name": "A. Srinivas"
      }
    ],
    "source": "semantic_scholar",
    "score": 160.78133753386626
  },
  {
    "paperId": "a66686e60a3eda0c606e036403cf0a07a5962595",
    "url": "https://www.semanticscholar.org/paper/a66686e60a3eda0c606e036403cf0a07a5962595",
    "title": "Mobile-Former: Bridging MobileNet and Transformer",
    "abstract": "We present Mobile-Former, a parallel design of MobileNet and transformer with a two-way bridge in between. This structure leverages the advantages of MobileNet at local processing and transformer at global interaction. And the bridge enables bidirectional fusion of local and global features. Different from recent works on vision transformer, the transformer in Mobile-Former contains very few tokens (e.g. 6 or fewer tokens) that are randomly initialized to learn global priors, resulting in low computational cost. Combining with the proposed light-weight cross attention to model the bridge, Mobile-Former is not only computationally efficient, but also has more representation power. It outperforms MobileNetV3 at low FLOP regime from 25M to 500M FLOPs on ImageNet classification. For instance, Mobile-Former achieves 77.9% top-1 accuracy at 294M FLOPs, gaining 1.3% over MobileNetV3 but saving 17% of computations. When transferring to object detection, Mobile-Former outperforms MobileNetV3 by 8.6 AP in RetinaNet framework. Furthermore, we build an efficient end-to-end detector by replacing backbone, encoder and decoder in DETR with Mobile-Former, which outperforms DETR by 1.3 AP but saves 52% of computational cost and 36% of parameters. Code will be released at https://github.com/aaboys/mobileformer.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "citationCount": 415,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2108.05895",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-08-12",
    "authors": [
      {
        "authorId": "2109306087",
        "name": "Yinpeng Chen"
      },
      {
        "authorId": "3386593",
        "name": "Xiyang Dai"
      },
      {
        "authorId": "49025801",
        "name": "Dongdong Chen"
      },
      {
        "authorId": "2152968847",
        "name": "Mengchen Liu"
      },
      {
        "authorId": "5620602",
        "name": "Xiaoyi Dong"
      },
      {
        "authorId": "145347147",
        "name": "Lu Yuan"
      },
      {
        "authorId": "2145253136",
        "name": "Zicheng Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 160.46027890391895
  },
  {
    "paperId": "19adf1af8daa9551328226fc6c0140e955bf5689",
    "url": "https://www.semanticscholar.org/paper/19adf1af8daa9551328226fc6c0140e955bf5689",
    "title": "Generating Radiology Reports via Memory-driven Transformer",
    "abstract": "Medical imaging is frequently used in clinical practice and trials for diagnosis and treatment. Writing imaging reports is time-consuming and can be error-prone for inexperienced radiologists. Therefore, automatically generating radiology reports is highly desired to lighten the workload of radiologists and accordingly promote clinical automation, which is an essential task to apply artificial intelligence to the medical domain. In this paper, we propose to generate radiology reports with memory-driven Transformer, where a relational memory is designed to record key information of the generation process and a memory-driven conditional layer normalization is applied to incorporating the memory into the decoder of Transformer. Experimental results on two prevailing radiology report datasets, IU X-Ray and MIMIC-CXR, show that our proposed approach outperforms previous models with respect to both language generation metrics and clinical evaluations. Particularly, this is the first work reporting the generation results on MIMIC-CXR to the best of our knowledge. Further analyses also demonstrate that our approach is able to generate long reports with necessary medical terms as well as meaningful image-text attention mappings.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2020,
    "citationCount": 411,
    "openAccessPdf": {
      "url": "https://www.aclweb.org/anthology/2020.emnlp-main.112.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2020-10-30",
    "authors": [
      {
        "authorId": "46843171",
        "name": "Zhihong Chen"
      },
      {
        "authorId": "1922182598",
        "name": "Yan Song"
      },
      {
        "authorId": "2678812",
        "name": "Tsung-Hui Chang"
      },
      {
        "authorId": "2005096276",
        "name": "Xiang Wan"
      }
    ],
    "source": "semantic_scholar",
    "score": 160.3153502402429
  },
  {
    "paperId": "48418b285a92376a38daafa664a2dd07d42e3fe3",
    "url": "https://www.semanticscholar.org/paper/48418b285a92376a38daafa664a2dd07d42e3fe3",
    "title": "Focal Self-attention for Local-Global Interactions in Vision Transformers",
    "abstract": "Recently, Vision Transformer and its variants have shown great promise on various computer vision tasks. The ability of capturing short- and long-range visual dependencies through self-attention is arguably the main source for the success. But it also brings challenges due to quadratic computational overhead, especially for the high-resolution vision tasks (e.g., object detection). In this paper, we present focal self-attention, a new mechanism that incorporates both fine-grained local and coarse-grained global interactions. Using this new mechanism, each token attends the closest surrounding tokens at fine granularity but the tokens far away at coarse granularity, and thus can capture both short- and long-range visual dependencies efficiently and effectively. With focal self-attention, we propose a new variant of Vision Transformer models, called Focal Transformer, which achieves superior performance over the state-of-the-art vision Transformers on a range of public image classification and object detection benchmarks. In particular, our Focal Transformer models with a moderate size of 51.1M and a larger size of 89.8M achieve 83.5 and 83.8 Top-1 accuracy, respectively, on ImageNet classification at 224x224 resolution. Using Focal Transformers as the backbones, we obtain consistent and substantial improvements over the current state-of-the-art Swin Transformers for 6 different object detection methods trained with standard 1x and 3x schedules. Our largest Focal Transformer yields 58.7/58.9 box mAPs and 50.9/51.3 mask mAPs on COCO mini-val/test-dev, and 55.4 mIoU on ADE20K for semantic segmentation, creating new SoTA on three of the most challenging computer vision tasks.",
    "venue": "arXiv.org",
    "year": 2021,
    "citationCount": 404,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-07-01",
    "authors": [
      {
        "authorId": "120157163",
        "name": "Jianwei Yang"
      },
      {
        "authorId": "2109737569",
        "name": "Chunyuan Li"
      },
      {
        "authorId": "9325940",
        "name": "Pengchuan Zhang"
      },
      {
        "authorId": "3386593",
        "name": "Xiyang Dai"
      },
      {
        "authorId": "2054421528",
        "name": "Bin Xiao"
      },
      {
        "authorId": "145347147",
        "name": "Lu Yuan"
      },
      {
        "authorId": "48441311",
        "name": "Jianfeng Gao"
      }
    ],
    "source": "semantic_scholar",
    "score": 160.0583060065981
  },
  {
    "paperId": "60c8d0619481eaafdd1189af610d0e636271fed5",
    "url": "https://www.semanticscholar.org/paper/60c8d0619481eaafdd1189af610d0e636271fed5",
    "title": "Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation",
    "abstract": "Transformers have revolutionized vision and natural language processing with their ability to scale with large datasets. But in robotic manipulation, data is both limited and expensive. Can manipulation still benefit from Transformers with the right problem formulation? We investigate this question with PerAct, a language-conditioned behavior-cloning agent for multi-task 6-DoF manipulation. PerAct encodes language goals and RGB-D voxel observations with a Perceiver Transformer, and outputs discretized actions by ``detecting the next best voxel action''. Unlike frameworks that operate on 2D images, the voxelized 3D observation and action space provides a strong structural prior for efficiently learning 6-DoF actions. With this formulation, we train a single multi-task Transformer for 18 RLBench tasks (with 249 variations) and 7 real-world tasks (with 18 variations) from just a few demonstrations per task. Our results show that PerAct significantly outperforms unstructured image-to-action agents and 3D ConvNet baselines for a wide range of tabletop tasks.",
    "venue": "Conference on Robot Learning",
    "year": 2022,
    "citationCount": 403,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2209.05451",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-09-12",
    "authors": [
      {
        "authorId": "33516562",
        "name": "Mohit Shridhar"
      },
      {
        "authorId": "2033958",
        "name": "Lucas Manuelli"
      },
      {
        "authorId": "145197953",
        "name": "D. Fox"
      }
    ],
    "source": "semantic_scholar",
    "score": 160.02122316941725
  },
  {
    "paperId": "c90a99eeb57019732a6cc996bb9eaf13faedf00f",
    "url": "https://www.semanticscholar.org/paper/c90a99eeb57019732a6cc996bb9eaf13faedf00f",
    "title": "In-context Learning and Induction Heads",
    "abstract": "“Induction heads” are attention heads that implement a simple algorithm to complete token sequences like [A][B] ... [A] → [B]. In this work, we present preliminary and indirect evidence for a hypothesis that induction heads might constitute the mechanism for the majority of all “incontext learning” in large transformer models (i.e. decreasing loss at increasing token indices). We find that induction heads develop at precisely the same point as a sudden sharp increase in incontext learning ability, visible as a bump in the training loss. We present six complementary lines of evidence, arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size. For small attention-only models, we present strong, causal evidence; for larger models with MLPs, we present correlational evidence. We recommend reading this paper as an HTML article. As Transformer generative models continue to scale and gain increasing real world use, addressing their associated safety problems becomes increasingly important. Mechanistic interpretability – attempting to reverse engineer the detailed computations performed by the model – offers one possible avenue for addressing these safety issues. If we can understand the internal structures that cause Transformer models to produce the outputs they do, then we may be able to address current safety problems more systematically, as well as anticipating safety problems in future more powerful models. [1, 2, 3, 4, 5]",
    "venue": "arXiv.org",
    "year": 2022,
    "citationCount": 401,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2209.11895",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-09-24",
    "authors": [
      {
        "authorId": "2061321863",
        "name": "Catherine Olsson"
      },
      {
        "authorId": "2866708",
        "name": "Nelson Elhage"
      },
      {
        "authorId": "2051128902",
        "name": "Neel Nanda"
      },
      {
        "authorId": "2117706920",
        "name": "Nicholas Joseph"
      },
      {
        "authorId": "2142833890",
        "name": "Nova Dassarma"
      },
      {
        "authorId": "103143311",
        "name": "T. Henighan"
      },
      {
        "authorId": "2056658938",
        "name": "Benjamin Mann"
      },
      {
        "authorId": "119609682",
        "name": "Amanda Askell"
      },
      {
        "authorId": "1486307451",
        "name": "Yuntao Bai"
      },
      {
        "authorId": "2111073313",
        "name": "Anna Chen"
      },
      {
        "authorId": "2154608209",
        "name": "Tom Conerly"
      },
      {
        "authorId": "1943097969",
        "name": "Dawn Drain"
      },
      {
        "authorId": "2081806483",
        "name": "Deep Ganguli"
      },
      {
        "authorId": "1573482302",
        "name": "Zac Hatfield-Dodds"
      },
      {
        "authorId": "39182747",
        "name": "Danny Hernandez"
      },
      {
        "authorId": "2154610174",
        "name": "Scott Johnston"
      },
      {
        "authorId": "2149890773",
        "name": "Andy Jones"
      },
      {
        "authorId": "1583434563",
        "name": "John Kernion"
      },
      {
        "authorId": "2154608229",
        "name": "Liane Lovitt"
      },
      {
        "authorId": "1978097132",
        "name": "Kamal Ndousse"
      },
      {
        "authorId": "2698777",
        "name": "Dario Amodei"
      },
      {
        "authorId": "31035595",
        "name": "Tom B. Brown"
      },
      {
        "authorId": "2115193883",
        "name": "Jack Clark"
      },
      {
        "authorId": "2053807409",
        "name": "Jared Kaplan"
      },
      {
        "authorId": "52238703",
        "name": "Sam McCandlish"
      },
      {
        "authorId": "37232298",
        "name": "C. Olah"
      }
    ],
    "source": "semantic_scholar",
    "score": 159.94678132928533
  },
  {
    "paperId": "a46b06a4b8b4deecf96a4e42cd19b4696f999e66",
    "url": "https://www.semanticscholar.org/paper/a46b06a4b8b4deecf96a4e42cd19b4696f999e66",
    "title": "Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy",
    "abstract": "Unsupervised detection of anomaly points in time series is a challenging problem, which requires the model to derive a distinguishable criterion. Previous methods tackle the problem mainly through learning pointwise representation or pairwise association, however, neither is sufficient to reason about the intricate dynamics. Recently, Transformers have shown great power in unified modeling of pointwise representation and pairwise association, and we find that the self-attention weight distribution of each time point can embody rich association with the whole series. Our key observation is that due to the rarity of anomalies, it is extremely difficult to build nontrivial associations from abnormal points to the whole series, thereby, the anomalies' associations shall mainly concentrate on their adjacent time points. This adjacent-concentration bias implies an association-based criterion inherently distinguishable between normal and abnormal points, which we highlight through the \\emph{Association Discrepancy}. Technically, we propose the \\emph{Anomaly Transformer} with a new \\emph{Anomaly-Attention} mechanism to compute the association discrepancy. A minimax strategy is devised to amplify the normal-abnormal distinguishability of the association discrepancy. The Anomaly Transformer achieves state-of-the-art results on six unsupervised time series anomaly detection benchmarks of three applications: service monitoring, space&earth exploration, and water treatment.",
    "venue": "International Conference on Learning Representations",
    "year": 2021,
    "citationCount": 400,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-10-06",
    "authors": [
      {
        "authorId": "2111064536",
        "name": "Jiehui Xu"
      },
      {
        "authorId": "2051867856",
        "name": "Haixu Wu"
      },
      {
        "authorId": "2144499343",
        "name": "Jianmin Wang"
      },
      {
        "authorId": "2054275000",
        "name": "Mingsheng Long"
      }
    ],
    "source": "semantic_scholar",
    "score": 159.90942140959854
  },
  {
    "paperId": "a601e71127d95bdae30fd818d2a0cc34b80b13f7",
    "url": "https://www.semanticscholar.org/paper/a601e71127d95bdae30fd818d2a0cc34b80b13f7",
    "title": "Masked Autoencoders for Point Cloud Self-supervised Learning",
    "abstract": "As a promising scheme of self-supervised learning, masked autoencoding has significantly advanced natural language processing and computer vision. Inspired by this, we propose a neat scheme of masked autoencoders for point cloud self-supervised learning, addressing the challenges posed by point cloud's properties, including leakage of location information and uneven information density. Concretely, we divide the input point cloud into irregular point patches and randomly mask them at a high ratio. Then, a standard Transformer based autoencoder, with an asymmetric design and a shifting mask tokens operation, learns high-level latent features from unmasked point patches, aiming to reconstruct the masked point patches. Extensive experiments show that our approach is efficient during pre-training and generalizes well on various downstream tasks. Specifically, our pre-trained models achieve 85.18% accuracy on ScanObjectNN and 94.04% accuracy on ModelNet40, outperforming all the other self-supervised learning methods. We show with our scheme, a simple architecture entirely based on standard Transformers can surpass dedicated Transformer models from supervised learning. Our approach also advances state-of-the-art accuracies by 1.5%-2.3% in the few-shot object classification. Furthermore, our work inspires the feasibility of applying unified architectures from languages and images to the point cloud.",
    "venue": "European Conference on Computer Vision",
    "year": 2022,
    "citationCount": 391,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2203.06604",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-03-13",
    "authors": [
      {
        "authorId": "2158817999",
        "name": "Yatian Pang"
      },
      {
        "authorId": "47825356",
        "name": "Wenxiao Wang"
      },
      {
        "authorId": "40983412",
        "name": "Francis E. H. Tay"
      },
      {
        "authorId": "46641573",
        "name": "W. Liu"
      },
      {
        "authorId": "2115729952",
        "name": "Yonghong Tian"
      },
      {
        "authorId": "2152193547",
        "name": "Liuliang Yuan"
      }
    ],
    "source": "semantic_scholar",
    "score": 159.56892759685695
  },
  {
    "paperId": "7aa801b907b59b8ee4cfb1296d9dac22c5164c5d",
    "url": "https://www.semanticscholar.org/paper/7aa801b907b59b8ee4cfb1296d9dac22c5164c5d",
    "title": "What learning algorithm is in-context learning? Investigations with linear models",
    "abstract": "Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples $(x, f(x))$ presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context. Using linear regression as a prototypical problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noise vary, and converging to Bayesian estimators for large widths and depths. Third, we present preliminary evidence that in-context learners share algorithmic features with these predictors: learners' late layers non-linearly encode weight vectors and moment matrices. These results suggest that in-context learning is understandable in algorithmic terms, and that (at least in the linear case) learners may rediscover standard estimation algorithms. Code and reference implementations are released at https://github.com/ekinakyurek/google-research/blob/master/incontext.",
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "citationCount": 390,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2211.15661",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-11-28",
    "authors": [
      {
        "authorId": "1992708068",
        "name": "Ekin Akyürek"
      },
      {
        "authorId": "50319359",
        "name": "D. Schuurmans"
      },
      {
        "authorId": "2112400",
        "name": "Jacob Andreas"
      },
      {
        "authorId": "2114186424",
        "name": "Tengyu Ma"
      },
      {
        "authorId": "65855107",
        "name": "Denny Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 159.5306133997805
  },
  {
    "paperId": "ff50b46b4e1cc0fd9beb832fc3468785b635a824",
    "url": "https://www.semanticscholar.org/paper/ff50b46b4e1cc0fd9beb832fc3468785b635a824",
    "title": "PCT: Point cloud transformer",
    "abstract": null,
    "venue": "Computational Visual Media",
    "year": 2020,
    "citationCount": 1447,
    "openAccessPdf": {
      "url": "https://link.springer.com/content/pdf/10.1007/s41095-021-0229-5.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2020-12-17",
    "authors": [
      {
        "authorId": "2088775481",
        "name": "Meng-Hao Guo"
      },
      {
        "authorId": "67011150",
        "name": "Junxiong Cai"
      },
      {
        "authorId": "79305819",
        "name": "Zheng-Ning Liu"
      },
      {
        "authorId": "31471368",
        "name": "Tai-Jiang Mu"
      },
      {
        "authorId": "2404014",
        "name": "Ralph Robert Martin"
      },
      {
        "authorId": "145140922",
        "name": "Shimin Hu"
      }
    ],
    "source": "semantic_scholar",
    "score": 159.16907859418492
  },
  {
    "paperId": "e5cb26148791b57bfd36aa26ce2401e231d01b57",
    "url": "https://www.semanticscholar.org/paper/e5cb26148791b57bfd36aa26ce2401e231d01b57",
    "title": "Vision Transformer with Deformable Attention",
    "abstract": "Transformers have recently shown superior performances on various vision tasks. The large, sometimes even global, receptive field endows Transformer models with higher representation power over their CNN counterparts. Nevertheless, simply enlarging receptive field also gives rise to several concerns. On the one hand, using dense attention e.g., in ViT, leads to excessive memory and computational cost, and features can be influenced by irrelevant parts which are beyond the region of interests. On the other hand, the sparse attention adopted in PVT or Swin Transformer is data agnostic and may limit the ability to model long range relations. To mitigate these issues, we propose a novel deformable selfattention module, where the positions of key and value pairs in selfattention are selected in a data-dependent way. This flexible scheme enables the self-attention module to focus on relevant re-gions and capture more informative features. On this basis, we present Deformable Attention Transformer, a general backbone model with deformable attention for both image classification and dense prediction tasks. Extensive experi-ments show that our models achieve consistently improved results on comprehensive benchmarks. Code is available at https://github.com/LeapLabTHU/DAT.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2022,
    "citationCount": 379,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2201.00520",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-01-03",
    "authors": [
      {
        "authorId": "2039921875",
        "name": "Zhuofan Xia"
      },
      {
        "authorId": "51170295",
        "name": "Xuran Pan"
      },
      {
        "authorId": "30619669",
        "name": "S. Song"
      },
      {
        "authorId": "2156057522",
        "name": "Li Erran Li"
      },
      {
        "authorId": "2115218570",
        "name": "Gao Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 159.10256879080646
  },
  {
    "paperId": "3425495ee3b6ead009f35aeb70edeac4e6eb2d10",
    "url": "https://www.semanticscholar.org/paper/3425495ee3b6ead009f35aeb70edeac4e6eb2d10",
    "title": "Patches Are All You Need?",
    "abstract": "Although convolutional networks have been the dominant architecture for vision tasks for many years, recent experiments have shown that Transformer-based models, most notably the Vision Transformer (ViT), may exceed their performance in some settings. However, due to the quadratic runtime of the self-attention layers in Transformers, ViTs require the use of patch embeddings, which group together small regions of the image into single input features, in order to be applied to larger image sizes. This raises a question: Is the performance of ViTs due to the inherently-more-powerful Transformer architecture, or is it at least partly due to using patches as the input representation? In this paper, we present some evidence for the latter: specifically, we propose the ConvMixer, an extremely simple model that is similar in spirit to the ViT and the even-more-basic MLP-Mixer in that it operates directly on patches as input, separates the mixing of spatial and channel dimensions, and maintains equal size and resolution throughout the network. In contrast, however, the ConvMixer uses only standard convolutions to achieve the mixing steps. Despite its simplicity, we show that the ConvMixer outperforms the ViT, MLP-Mixer, and some of their variants for similar parameter counts and data set sizes, in addition to outperforming classical vision models such as the ResNet. Our code is available at https://github.com/locuslab/convmixer.",
    "venue": "Trans. Mach. Learn. Res.",
    "year": 2022,
    "citationCount": 376,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-01-24",
    "authors": [
      {
        "authorId": "50820040",
        "name": "Asher Trockman"
      },
      {
        "authorId": "145116464",
        "name": "J. Z. Kolter"
      }
    ],
    "source": "semantic_scholar",
    "score": 158.98367781172016
  },
  {
    "paperId": "67040b931c1a384426c44ae73f9553e97f08cf6a",
    "url": "https://www.semanticscholar.org/paper/67040b931c1a384426c44ae73f9553e97f08cf6a",
    "title": "PVT v2: Improved baselines with Pyramid Vision Transformer",
    "abstract": null,
    "venue": "Computational Visual Media",
    "year": 2021,
    "citationCount": 1419,
    "openAccessPdf": {
      "url": "https://link.springer.com/content/pdf/10.1007/s41095-022-0274-8.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-06-25",
    "authors": [
      {
        "authorId": "71074736",
        "name": "Wenhai Wang"
      },
      {
        "authorId": "41020000",
        "name": "Enze Xie"
      },
      {
        "authorId": "2144439048",
        "name": "Xiang Li"
      },
      {
        "authorId": "23999143",
        "name": "Deng-Ping Fan"
      },
      {
        "authorId": "50982078",
        "name": "Kaitao Song"
      },
      {
        "authorId": "152335674",
        "name": "Ding Liang"
      },
      {
        "authorId": "2115137018",
        "name": "Tong Lu"
      },
      {
        "authorId": "144389940",
        "name": "P. Luo"
      },
      {
        "authorId": "2067609498",
        "name": "Ling Shao"
      }
    ],
    "source": "semantic_scholar",
    "score": 158.8761822589296
  },
  {
    "paperId": "2f4d8f3c016ec53380b376ae7ac516f9c0f07a0d",
    "url": "https://www.semanticscholar.org/paper/2f4d8f3c016ec53380b376ae7ac516f9c0f07a0d",
    "title": "BiFormer: Vision Transformer with Bi-Level Routing Attention",
    "abstract": "As the core building block of vision transformers, attention is a powerful tool to capture long-range dependency. However, such power comes at a cost: it incurs a huge computation burden and heavy memory footprint as pairwise token interaction across all spatial locations is computed. A series of works attempt to alleviate this problem by introducing handcrafted and content-agnostic sparsity into attention, such as restricting the attention operation to be inside local windows, axial stripes, or dilated windows. In contrast to these approaches, we propose a novel dynamic sparse attention via bi-level routing to enable a more flexible allocation of computations with content awareness. Specifically, for a query, irrelevant key-value pairs are first filtered out at a coarse region level, and then fine-grained token-to-token attention is applied in the union of remaining candidate regions (i.e., routed regions). We provide a simple yet effective implementation of the proposed bilevel routing attention, which utilizes the sparsity to save both computation and memory while involving only GPU-friendly dense matrix multiplications. Built with the proposed bi-level routing attention, a new general vision transformer, named BiFormer, is then presented. As BiFormer attends to a small subset of relevant tokens in a query adaptive manner without distraction from other irrelevant ones, it enjoys both good performance and high computational efficiency, especially in dense prediction tasks. Empirical results across several computer vision tasks such as image classification, object detection, and semantic segmentation verify the effectiveness of our design. Code is available at https://github.com/rayleizhu/BiFormer.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2023,
    "citationCount": 372,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2303.08810",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-03-15",
    "authors": [
      {
        "authorId": "2112583610",
        "name": "Lei Zhu"
      },
      {
        "authorId": "48630464",
        "name": "Xinjiang Wang"
      },
      {
        "authorId": "152984116",
        "name": "Zhanghan Ke"
      },
      {
        "authorId": "1726357",
        "name": "Wayne Zhang"
      },
      {
        "authorId": "1726262",
        "name": "Rynson W. H. Lau"
      }
    ],
    "source": "semantic_scholar",
    "score": 158.82367629465722
  },
  {
    "paperId": "62635347c2dfcced8869790d24313f35c34be68b",
    "url": "https://www.semanticscholar.org/paper/62635347c2dfcced8869790d24313f35c34be68b",
    "title": "HTS-AT: A Hierarchical Token-Semantic Audio Transformer for Sound Classification and Detection",
    "abstract": "Audio classification is an important task of mapping audio samples into their corresponding labels. Recently, the transformer model with self-attention mechanisms has been adopted in this field. However, existing audio transformers require large GPU memories and long training time, meanwhile relying on pretrained vision models to achieve high performance, which limits the model’s scalability in audio tasks. To combat these problems, we introduce HTS-AT: an audio transformer with a hierarchical structure to reduce the model size and training time. It is further combined with a token-semantic module to map final outputs into class featuremaps, thus enabling the model for the audio event detection (i.e. localization in time). We evaluate HTS-AT on three datasets of audio classification where it achieves new state-of-the-art (SOTA) results on AudioSet and ESC50, and equals the SOTA on Speech Command V2. It also achieves better performance in event localization than the previous CNN-based models. Moreover, HTS-AT requires only 35% model parameters and 15% training time of the previous audio transformer. These results demonstrate the high performance and high efficiency of HTS-AT.",
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year": 2022,
    "citationCount": 236,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2202.00874",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-02-02",
    "authors": [
      {
        "authorId": "2152953901",
        "name": "Ke Chen"
      },
      {
        "authorId": "46214809",
        "name": "Xingjian Du"
      },
      {
        "authorId": "1842814",
        "name": "Bilei Zhu"
      },
      {
        "authorId": "2919563",
        "name": "Zejun Ma"
      },
      {
        "authorId": "1400419309",
        "name": "Taylor Berg-Kirkpatrick"
      },
      {
        "authorId": "2204186",
        "name": "S. Dubnov"
      }
    ],
    "source": "semantic_scholar",
    "score": 158.020902117027
  },
  {
    "paperId": "b4ce19f3b3819accb160acffabffa849f18f4758",
    "url": "https://www.semanticscholar.org/paper/b4ce19f3b3819accb160acffabffa849f18f4758",
    "title": "Anchor DETR: Query Design for Transformer-Based Detector",
    "abstract": "In this paper, we propose a novel query design for the transformer-based object detection. \nIn previous transformer-based detectors, the object queries are a set of learned embeddings.\nHowever, each learned embedding does not have an explicit physical meaning and we cannot explain where it will focus on.\nIt is difficult to optimize as the prediction slot of each object query does not have a specific mode. \nIn other words, each object query will not focus on a specific region.\nTo solve these problems, in our query design, object queries are based on anchor points, which are widely used in CNN-based detectors. \nSo each object query focuses on the objects near the anchor point. \nMoreover, our query design can predict multiple objects at one position to solve the difficulty: ``one region, multiple objects''.\nIn addition, we design an attention variant, which can reduce the memory cost while achieving similar or better performance than the standard attention in DETR.\nThanks to the query design and the attention variant, the proposed detector that we called Anchor DETR, can achieve better performance and run faster than the DETR with 10x fewer training epochs.\nFor example, it achieves 44.2 AP with 19 FPS on the MSCOCO dataset when using the ResNet50-DC5 feature for training 50 epochs.\nExtensive experiments on the MSCOCO benchmark prove the effectiveness of the proposed methods.\nCode is available at https://github.com/megvii-research/AnchorDETR.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2022,
    "citationCount": 347,
    "openAccessPdf": {
      "url": "https://ojs.aaai.org/index.php/AAAI/article/download/20158/19917",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-06-28",
    "authors": [
      {
        "authorId": "2107945710",
        "name": "Yingming Wang"
      },
      {
        "authorId": "1771551",
        "name": "X. Zhang"
      },
      {
        "authorId": "2109350833",
        "name": "Tong Yang"
      },
      {
        "authorId": "2032184078",
        "name": "Jian Sun"
      }
    ],
    "source": "semantic_scholar",
    "score": 157.7830371966171
  },
  {
    "paperId": "860e24025c67487b9dd87b442c7b44e5bbf5a054",
    "url": "https://www.semanticscholar.org/paper/860e24025c67487b9dd87b442c7b44e5bbf5a054",
    "title": "TransFG: A Transformer Architecture for Fine-grained Recognition",
    "abstract": "Fine-grained visual classification (FGVC) which aims at recognizing objects from subcategories is a very challenging task due to the inherently subtle inter-class differences. Most existing works mainly tackle this problem by reusing the backbone network to extract features of detected discriminative regions. However, this strategy inevitably complicates the pipeline and pushes the proposed regions to contain most parts of the objects thus fails to locate the really important parts. Recently, vision transformer (ViT) shows its strong performance in the traditional classification task. The self-attention mechanism of the transformer links every patch token to the classification token. In this work, we first evaluate the effectiveness of the ViT framework in the fine-grained recognition setting. Then motivated by the strength of the attention link can be intuitively considered as an indicator of the importance of tokens, we further propose a novel Part Selection Module that can be applied to most of the transformer architectures where we integrate all raw attention weights of the transformer into an attention map for guiding the network to effectively and accurately select discriminative image patches and compute their relations. A contrastive loss is applied to enlarge the distance between feature representations of confusing classes. We name the augmented transformer-based model TransFG and demonstrate the value of it by conducting experiments on five popular fine-grained benchmarks where we achieve state-of-the-art performance. Qualitative results are presented for better understanding of our model.",
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2021,
    "citationCount": 344,
    "openAccessPdf": {
      "url": "https://ojs.aaai.org/index.php/AAAI/article/download/19967/19726",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-03-14",
    "authors": [
      {
        "authorId": "153146760",
        "name": "Ju He"
      },
      {
        "authorId": "90972805",
        "name": "Jieneng Chen"
      },
      {
        "authorId": "2108592195",
        "name": "Shuai Liu"
      },
      {
        "authorId": "2780587",
        "name": "Adam Kortylewski"
      },
      {
        "authorId": "2154170341",
        "name": "Cheng Yang"
      },
      {
        "authorId": "48442730",
        "name": "Yutong Bai"
      },
      {
        "authorId": "1906061249",
        "name": "Changhu Wang"
      },
      {
        "authorId": "145081362",
        "name": "A. Yuille"
      }
    ],
    "source": "semantic_scholar",
    "score": 157.6531662554704
  },
  {
    "paperId": "94ff111c4d81bd03f159321728ceec8b4711c89d",
    "url": "https://www.semanticscholar.org/paper/94ff111c4d81bd03f159321728ceec8b4711c89d",
    "title": "An Empirical Study of Training End-to-End Vision-and-Language Transformers",
    "abstract": "Vision-and-language (VL) pre-training has proven to be highly effective on various VL downstream tasks. While recent work has shown that fully transformer-based VL models can be more efficient than previous region-feature-based methods, their performance on downstream tasks often degrades significantly. In this paper, we present Meter, a Multimodal End-to-end TransformER framework, through which we investigate how to design and pre-train a fully transformer-based VL model in an end-to-end manner. Specifically, we dissect the model designs along multiple dimensions: vision encoders (e.g., CLIP-ViT, Swin transformer), text encoders (e.g., RoBERTa, De-BERTa), multimodal fusion module (e.g., merged attention vs. co-attention), architectural design (e.g., encoder-only vs. encoder-decoder), and pre-training objectives (e.g., masked image modeling). We conduct comprehensive experiments and provide insights on how to train a performant VL transformer. Meterachieves an accuracy of 77.64% on the VQAv2 test-std set using only 4M images for pre-training, surpassing the state-of-the-art region-feature-based model by 1.04%, and outperforming the previous best fully transformer-based model by 1.6%. Notably, when further scaled up, our best VQA model achieves an accuracy of 80.54%. Code and pre-trained models are released at https://github.com/zdou0830/METER.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "citationCount": 337,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2111.02387",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-11-03",
    "authors": [
      {
        "authorId": "14199369",
        "name": "Zi-Yi Dou"
      },
      {
        "authorId": "2110197273",
        "name": "Yichong Xu"
      },
      {
        "authorId": "144702900",
        "name": "Zhe Gan"
      },
      {
        "authorId": "2124948371",
        "name": "Jianfeng Wang"
      },
      {
        "authorId": "2992833",
        "name": "Shuohang Wang"
      },
      {
        "authorId": "29957038",
        "name": "Lijuan Wang"
      },
      {
        "authorId": "1456009348",
        "name": "Chenguang Zhu"
      },
      {
        "authorId": "3157053",
        "name": "Nanyun Peng"
      },
      {
        "authorId": "2145253136",
        "name": "Zicheng Liu"
      },
      {
        "authorId": "48262024",
        "name": "Michael Zeng"
      }
    ],
    "source": "semantic_scholar",
    "score": 157.34568843224528
  },
  {
    "paperId": "75bb9eda70751c63fc54dbe63377c673b7dbdb15",
    "url": "https://www.semanticscholar.org/paper/75bb9eda70751c63fc54dbe63377c673b7dbdb15",
    "title": "CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers",
    "abstract": "The development of the transformer-based text-to-image models are impeded by its slow generation and complexity for high-resolution images. In this work, we put forward a solution based on hierarchical transformers and local parallel auto-regressive generation. We pretrain a 6B-parameter transformer with a simple and flexible self-supervised task, Cross-modal general language model (CogLM), and finetune it for fast super-resolution. The new text-to-image system, CogView2, shows very competitive generation compared to concurrent state-of-the-art DALL-E-2, and naturally supports interactive text-guided editing on images.",
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "citationCount": 284,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2204.14217",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-04-28",
    "authors": [
      {
        "authorId": "145573466",
        "name": "Ming Ding"
      },
      {
        "authorId": "2163967642",
        "name": "Wendi Zheng"
      },
      {
        "authorId": "2105844599",
        "name": "Wenyi Hong"
      },
      {
        "authorId": "2109541439",
        "name": "Jie Tang"
      }
    ],
    "source": "semantic_scholar",
    "score": 156.78733770402977
  },
  {
    "paperId": "52c7dc185708962d58367fd8c710e3f34e26d9dd",
    "url": "https://www.semanticscholar.org/paper/52c7dc185708962d58367fd8c710e3f34e26d9dd",
    "title": "VRT: A Video Restoration Transformer",
    "abstract": "Video restoration aims to restore high-quality frames from low-quality frames. Different from single image restoration, video restoration generally requires to utilize temporal information from multiple adjacent but usually misaligned video frames. Existing deep methods generally tackle with this by exploiting a sliding window strategy or a recurrent architecture, which are restricted by frame-by-frame restoration. In this paper, we propose a Video Restoration Transformer (VRT) with parallel frame prediction ability. More specifically, VRT is composed of multiple scales, each of which consists of two kinds of modules: temporal reciprocal self attention (TRSA) and parallel warping. TRSA divides the video into small clips, on which reciprocal attention is applied for joint motion estimation, feature alignment and feature fusion, while self attention is used for feature extraction. To enable cross-clip interactions, the video sequence is shifted for every other layer. Besides, parallel warping is used to further fuse information from neighboring frames by parallel feature warping. Experimental results on five tasks, including video super-resolution, video deblurring, video denoising, video frame interpolation and space-time video super-resolution, demonstrate that VRT outperforms the state-of-the-art methods by large margins (up to 2.16dB) on fourteen benchmark datasets. The codes are available at https://github.com/JingyunLiang/VRT.",
    "venue": "IEEE Transactions on Image Processing",
    "year": 2022,
    "citationCount": 212,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2201.12288",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-01-28",
    "authors": [
      {
        "authorId": "145270228",
        "name": "Jingyun Liang"
      },
      {
        "authorId": "32879676",
        "name": "Jiezhang Cao"
      },
      {
        "authorId": "143668321",
        "name": "Yuchen Fan"
      },
      {
        "authorId": "144110274",
        "name": "K. Zhang"
      },
      {
        "authorId": "2067240083",
        "name": "Rakesh Ranjan"
      },
      {
        "authorId": "2110414511",
        "name": "Yawei Li"
      },
      {
        "authorId": "1732855",
        "name": "R. Timofte"
      },
      {
        "authorId": "1681236",
        "name": "L. Gool"
      }
    ],
    "source": "semantic_scholar",
    "score": 156.4193824856414
  },
  {
    "paperId": "22ad3545d78f2acfe7de1b2c38ec72efc9faa0d6",
    "url": "https://www.semanticscholar.org/paper/22ad3545d78f2acfe7de1b2c38ec72efc9faa0d6",
    "title": "Multimodal Motion Prediction with Stacked Transformers",
    "abstract": "Predicting multiple plausible future trajectories of the nearby vehicles is crucial for the safety of autonomous driving. Recent motion prediction approaches attempt to achieve such multimodal motion prediction by implicitly regularizing the feature or explicitly generating multiple candidate proposals. However, it remains challenging since the latent features may concentrate on the most frequent mode of the data while the proposal-based methods depend largely on the prior knowledge to generate and select the proposals. In this work, we propose a novel transformer framework for multimodal motion prediction, termed as mmTransformer. A novel network architecture based on stacked transformers is designed to model the multimodality at feature level with a set of fixed independent proposals. A region-based training strategy is then developed to induce the multimodality of the generated proposals. Experiments on Argoverse dataset show that the proposed model achieves the state-of-the-art performance on motion prediction, substantially improving the diversity and the accuracy of the predicted trajectories. Demo video and code are available at https://decisionforce.github.io/mmTransformer.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "citationCount": 314,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.11624",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-03-22",
    "authors": [
      {
        "authorId": "2433821",
        "name": "Yicheng Liu"
      },
      {
        "authorId": "2142750773",
        "name": "Jinghuai Zhang"
      },
      {
        "authorId": "40901159",
        "name": "Liangji Fang"
      },
      {
        "authorId": "1617905616",
        "name": "Qinhong Jiang"
      },
      {
        "authorId": "145291669",
        "name": "Bolei Zhou"
      }
    ],
    "source": "semantic_scholar",
    "score": 156.2885895823845
  },
  {
    "paperId": "2fe1ac0b09cc0f50eb83eef6c7c6b45ac8b12413",
    "url": "https://www.semanticscholar.org/paper/2fe1ac0b09cc0f50eb83eef6c7c6b45ac8b12413",
    "title": "Mass-Editing Memory in a Transformer",
    "abstract": "Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by orders of magnitude. Our code and data are at https://memit.baulab.info.",
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "citationCount": 435,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2210.07229",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-10-13",
    "authors": [
      {
        "authorId": "153615419",
        "name": "Kevin Meng"
      },
      {
        "authorId": "1429844787",
        "name": "Arnab Sen Sharma"
      },
      {
        "authorId": "50112310",
        "name": "A. Andonian"
      },
      {
        "authorId": "2083259",
        "name": "Yonatan Belinkov"
      },
      {
        "authorId": "144159726",
        "name": "David Bau"
      }
    ],
    "source": "semantic_scholar",
    "score": 156.16463365023552
  },
  {
    "paperId": "fb45d31cc89207aec392dbac8908cc24db2df871",
    "url": "https://www.semanticscholar.org/paper/fb45d31cc89207aec392dbac8908cc24db2df871",
    "title": "Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting",
    "abstract": "Recently many deep models have been proposed for multivariate time series (MTS) forecasting. In particular, Transformer-based models have shown great potential because they can capture long-term dependency. However, existing Transformer-based models mainly focus on modeling the temporal dependency (cross-time dependency) yet often omit the dependency among different variables (cross-dimension dependency), which is critical for MTS forecasting. To ﬁll the gap, we propose Crossformer, a Transformer-based model utilizing cross-dimension dependency for MTS forecasting. In Crossformer, the input MTS is embedded into a 2D vector array through the Dimension-Segment-Wise (DSW) embedding to preserve time and dimension information. Then the Two-Stage Attention (TSA) layer is proposed to efﬁciently capture the cross-time and cross-dimension dependency. Utilizing DSW embedding and TSA layer, Crossformer establishes a Hierarchical Encoder-Decoder (HED) to use the information at different scales for the ﬁnal forecasting. Extensive experimental results on six real-world datasets show the effectiveness of Crossformer against previous state-of-the-arts.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 301,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2130520888",
        "name": "Yunhao Zhang"
      },
      {
        "authorId": "3063894",
        "name": "Junchi Yan"
      }
    ],
    "source": "semantic_scholar",
    "score": 155.65640526062305
  },
  {
    "paperId": "f51497f463566581874c941353dd9d80069c5b77",
    "url": "https://www.semanticscholar.org/paper/f51497f463566581874c941353dd9d80069c5b77",
    "title": "Compressive Transformers for Long-Range Sequence Modelling",
    "abstract": "We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.",
    "venue": "International Conference on Learning Representations",
    "year": 2019,
    "citationCount": 576,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2019-11-13",
    "authors": [
      {
        "authorId": "34269227",
        "name": "Jack W. Rae"
      },
      {
        "authorId": "13759734",
        "name": "Anna Potapenko"
      },
      {
        "authorId": "35880964",
        "name": "Siddhant M. Jayakumar"
      },
      {
        "authorId": "2542999",
        "name": "T. Lillicrap"
      }
    ],
    "source": "semantic_scholar",
    "score": 155.3676339976215
  },
  {
    "paperId": "cf36236015c9f93f15bfafbf282f69e08bdc9c16",
    "url": "https://www.semanticscholar.org/paper/cf36236015c9f93f15bfafbf282f69e08bdc9c16",
    "title": "Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space",
    "abstract": "Transformer-based language models (LMs) are at the core of modern NLP, but their internal prediction construction process is opaque and largely not understood. In this work, we make a substantial step towards unveiling this underlying prediction process, by reverse-engineering the operation of the feed-forward network (FFN) layers, one of the building blocks of transformer models. We view the token representation as a changing distribution over the vocabulary, and the output from each FFN layer as an additive update to that distribution. Then, we analyze the FFN updates in the vocabulary space, showing that each update can be decomposed to sub-updates corresponding to single FFN parameter vectors, each promoting concepts that are often human-interpretable. We then leverage these findings for controlling LM predictions, where we reduce the toxicity of GPT2 by almost 50%, and for improving computation efficiency with a simple early exit rule, saving 20% of computation on average.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "citationCount": 286,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2203.14680",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-03-28",
    "authors": [
      {
        "authorId": "22245981",
        "name": "Mor Geva"
      },
      {
        "authorId": "27743758",
        "name": "Avi Caciularu"
      },
      {
        "authorId": "1664850594",
        "name": "Ke Wang"
      },
      {
        "authorId": "79775260",
        "name": "Yoav Goldberg"
      }
    ],
    "source": "semantic_scholar",
    "score": 154.8922332363943
  },
  {
    "paperId": "ba26df992c6fc271c92eb6866e2b3c09eb327c69",
    "url": "https://www.semanticscholar.org/paper/ba26df992c6fc271c92eb6866e2b3c09eb327c69",
    "title": "Dissolved gas analysis: It can save your transformer",
    "abstract": "The use of dissolved gas analysis (DGA) to monitor the in-service behavior of transformers is discussed. Sampling techniques are briefly considered, and two commercial hydrogen-in-oil detectors are described. The first allows the hydrogen concentration to be measured at intervals of a few hours by a portable gas collector that can be connected to semipermeable tubes. Continuous remote monitoring from the substation is possible with the second model, which uses a fuel-cell-type detector. The use of DGA for fault diagnosis is examined, and acceptable gas levels are indicated. The use of expert systems to facilitate decision making on the basis of DGA results is discussed, as is international cooperation in sharing data and experience and reaching agreement on methods of analysis and interpretation. Further applications of DGA are indicated.<<ETX>>",
    "venue": "IEEE Electrical Insulation Magazine",
    "year": 1989,
    "citationCount": 370,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1989-11-01",
    "authors": [
      {
        "authorId": "2260269731",
        "name": "Michel Duval"
      }
    ],
    "source": "semantic_scholar",
    "score": 154.74303093911152
  },
  {
    "paperId": "8110e72b9ff8bc9a2fcd30a8775a1fe82b0fe455",
    "url": "https://www.semanticscholar.org/paper/8110e72b9ff8bc9a2fcd30a8775a1fe82b0fe455",
    "title": "A Power Electronic-Based Distribution Transformer",
    "abstract": "The distribution transformer has been in use by utilities throughout the twentieth century. Until now, it has consisted of a configuration of iron or steel cores and copper/aluminum coils, with mineral oil serving as both coolant and dielectric medium. Inherent in this type of construction are regulation, significant weight, losses, environmental concerns, and power quality issues. A new kind of distribution transformer is proposed for the twenty-first century, one that can be made self-regulating, oil-free, and able to correct power quality problems. A power electronic transformer has been analyzed, simulated, prototyped, and tested. Results of this effort, as well as the novel features of this new type of transformer, are discussed herein.",
    "venue": "IEEE Power Engineering Review",
    "year": 2002,
    "citationCount": 364,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2002-08-07",
    "authors": [
      {
        "authorId": "145625637",
        "name": "E. R. Ronan"
      },
      {
        "authorId": "30918788",
        "name": "S. Sudhoff"
      },
      {
        "authorId": "2248541453",
        "name": "S. F. Glover"
      },
      {
        "authorId": "153191782",
        "name": "D. L. Galloway"
      }
    ],
    "source": "semantic_scholar",
    "score": 154.49846030373737
  },
  {
    "paperId": "3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
    "url": "https://www.semanticscholar.org/paper/3dfb1f50f2a34a699c339dabaa6f9b3a977973de",
    "title": "LongT5: Efficient Text-To-Text Transformer for Long Sequences",
    "abstract": "Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we explore the effects of scaling both the input length and model size at the same time. Specifically, we integrated attention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global} (TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on question answering tasks.",
    "venue": "NAACL-HLT",
    "year": 2021,
    "citationCount": 278,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2022.findings-naacl.55.pdf",
      "status": "HYBRID"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-12-15",
    "authors": [
      {
        "authorId": "51150315",
        "name": "Mandy Guo"
      },
      {
        "authorId": "1643737606",
        "name": "J. Ainslie"
      },
      {
        "authorId": "3046959",
        "name": "David C. Uthus"
      },
      {
        "authorId": "1722671",
        "name": "Santiago Ontañón"
      },
      {
        "authorId": "2148023",
        "name": "Jianmo Ni"
      },
      {
        "authorId": "2305450",
        "name": "Yun-Hsuan Sung"
      },
      {
        "authorId": "2118771180",
        "name": "Yinfei Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 154.4681767273205
  },
  {
    "paperId": "f7a3d9bcf052f2b4ef7d59dcca4013ea11081d0f",
    "url": "https://www.semanticscholar.org/paper/f7a3d9bcf052f2b4ef7d59dcca4013ea11081d0f",
    "title": "Long Range Graph Benchmark",
    "abstract": "Graph Neural Networks (GNNs) that are based on the message passing (MP) paradigm generally exchange information between 1-hop neighbors to build node representations at each layer. In principle, such networks are not able to capture long-range interactions (LRI) that may be desired or necessary for learning a given task on graphs. Recently, there has been an increasing interest in development of Transformer-based methods for graphs that can consider full node connectivity beyond the original sparse structure, thus enabling the modeling of LRI. However, MP-GNNs that simply rely on 1-hop message passing often fare better in several existing graph benchmarks when combined with positional feature representations, among other innovations, hence limiting the perceived utility and ranking of Transformer-like architectures. Here, we present the Long Range Graph Benchmark (LRGB) with 5 graph learning datasets: PascalVOC-SP, COCO-SP, PCQM-Contact, Peptides-func and Peptides-struct that arguably require LRI reasoning to achieve strong performance in a given task. We benchmark both baseline GNNs and Graph Transformer networks to verify that the models which capture long-range dependencies perform significantly better on these tasks. Therefore, these datasets are suitable for benchmarking and exploration of MP-GNNs and Graph Transformer architectures that are intended to capture LRI.",
    "venue": "Neural Information Processing Systems",
    "year": 2022,
    "citationCount": 173,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2206.08164",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-06-16",
    "authors": [
      {
        "authorId": "51235219",
        "name": "Vijay Prakash Dwivedi"
      },
      {
        "authorId": "2125913",
        "name": "Ladislav Rampášek"
      },
      {
        "authorId": "2066369448",
        "name": "Mikhail Galkin"
      },
      {
        "authorId": "84094194",
        "name": "Alipanah Parviz"
      },
      {
        "authorId": "2683398",
        "name": "Guy Wolf"
      },
      {
        "authorId": "1755919",
        "name": "A. Luu"
      },
      {
        "authorId": "51034451",
        "name": "D. Beaini"
      }
    ],
    "source": "semantic_scholar",
    "score": 154.38582948821795
  },
  {
    "paperId": "afeeb8f5018eebb1a1d334b94dbbfc48d167efef",
    "url": "https://www.semanticscholar.org/paper/afeeb8f5018eebb1a1d334b94dbbfc48d167efef",
    "title": "iTransformer: Inverted Transformers Are Effective for Time Series Forecasting",
    "abstract": "The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp. However, Transformers are challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. Besides, the embedding for each temporal token fuses multiple variates that represent potential delayed events and distinct physical measurements, which may fail in learning variate-centric representations and result in meaningless attention maps. In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any modification to the basic components. We propose iTransformer that simply applies the attention and feed-forward network on the inverted dimensions. Specifically, the time points of individual series are embedded into variate tokens which are utilized by the attention mechanism to capture multivariate correlations; meanwhile, the feed-forward network is applied for each variate token to learn nonlinear representations. The iTransformer model achieves state-of-the-art on challenging real-world datasets, which further empowers the Transformer family with promoted performance, generalization ability across different variates, and better utilization of arbitrary lookback windows, making it a nice alternative as the fundamental backbone of time series forecasting. Code is available at this repository: https://github.com/thuml/iTransformer.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 274,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.06625",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-10",
    "authors": [
      {
        "authorId": "2257375992",
        "name": "Yong Liu"
      },
      {
        "authorId": "2203368846",
        "name": "Tengge Hu"
      },
      {
        "authorId": "2257340579",
        "name": "Haoran Zhang"
      },
      {
        "authorId": "2051867856",
        "name": "Haixu Wu"
      },
      {
        "authorId": "2255363760",
        "name": "Shiyu Wang"
      },
      {
        "authorId": "2253908414",
        "name": "Lintao Ma"
      },
      {
        "authorId": "2054275000",
        "name": "Mingsheng Long"
      }
    ],
    "source": "semantic_scholar",
    "score": 154.25156646499858
  },
  {
    "paperId": "31444a27bb09e64cf252be9fa2349a5178307012",
    "url": "https://www.semanticscholar.org/paper/31444a27bb09e64cf252be9fa2349a5178307012",
    "title": "nnFormer: Interleaved Transformer for Volumetric Segmentation",
    "abstract": ". Transformers, the default model of choices in natural language processing, have drawn scant attention from the medical imaging community. Given the ability to exploit long-term dependencies, transformers are promising to help atypical convolutional neural networks (convnets) to overcome its inherent shortcomings of spatial inductive bias. However, most of recently proposed transformer-based segmentation approaches simply treated transformers as assisted modules to help encode global context into convolutional representations without investigating how to optimally combine self-attention (i.e., the core of transformers) with convolution. To address this issue, in this paper, we introduce nnFormer (i.e., n ot-a n other trans Former ), a powerful segmentation model with an interleaved architecture based on empirical combination of self-attention and convolution. In practice, nnFormer learns volumetric representations from 3D local volumes. Compared to the naive voxel-level self-attention implementation, such volume-based operations help to reduce the computational complexity by approximate 98% and 99.5% on Synapse and ACDC datasets, respectively. In comparison to prior-art network conﬁgurations, nnFormer achieves tremendous improvements over previous transformer-based methods on two commonly used datasets Synapse and ACDC. For instance, nnFormer outperforms Swin-UNet by over 7 percents on Synapse. Even when compared to nnUNet, currently the best performing fully-convolutional medical segmentation network, nnFormer still provides slightly better performance on Synapse and ACDC. Codes and models are available at https://github.com/282857341/nnFormer .",
    "venue": "arXiv.org",
    "year": 2021,
    "citationCount": 267,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2157473801",
        "name": "Hong-Yu Zhou"
      },
      {
        "authorId": "2157893559",
        "name": "Jiansen Guo"
      },
      {
        "authorId": "2254832557",
        "name": "Yinghao Zhang"
      },
      {
        "authorId": "2254896046",
        "name": "Lequan Yu"
      },
      {
        "authorId": "2254332860",
        "name": "Liansheng Wang"
      },
      {
        "authorId": "2244641432",
        "name": "Yizhou Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 153.86480470766284
  },
  {
    "paperId": "f2736077d96cccdc8e95ac92fdc3dd3916885d29",
    "url": "https://www.semanticscholar.org/paper/f2736077d96cccdc8e95ac92fdc3dd3916885d29",
    "title": "MAT: Mask-Aware Transformer for Large Hole Image Inpainting",
    "abstract": "Recent studies have shown the importance of modeling long-range interactions in the inpainting problem. To achieve this goal, existing approaches exploit either standalone attention techniques or transformers, but usually under a low resolution in consideration of computational cost. In this paper, we present a novel transformer-based model for large hole inpainting, which unifies the merits of transformers and convolutions to efficiently process high-resolution images. We carefully design each component of our framework to guarantee the high fidelity and diversity of recovered images. Specifically, we customize an inpainting-oriented transformer block, where the attention module aggregates non-local information only from partial valid tokens, indicated by a dynamic mask. Extensive experiments demonstrate the state-of-the-art performance of the new model on multiple benchmark datasets. Code is released at https://github.com/fenglinglwb/MAT.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2022,
    "citationCount": 264,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2203.15270",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-03-29",
    "authors": [
      {
        "authorId": "2116112934",
        "name": "Wenbo Li"
      },
      {
        "authorId": "2112754809",
        "name": "Zhe Lin"
      },
      {
        "authorId": "2075360355",
        "name": "Kun Zhou"
      },
      {
        "authorId": "49231213",
        "name": "Lu Qi"
      },
      {
        "authorId": "2154458079",
        "name": "Yi Wang"
      },
      {
        "authorId": "1729056",
        "name": "Jiaya Jia"
      }
    ],
    "source": "semantic_scholar",
    "score": 153.69594738979333
  },
  {
    "paperId": "ca1d5aa8f63707931692bf6a62642becf928c1fa",
    "url": "https://www.semanticscholar.org/paper/ca1d5aa8f63707931692bf6a62642becf928c1fa",
    "title": "LAVT: Language-Aware Vision Transformer for Referring Image Segmentation",
    "abstract": "Referring image segmentation is a fundamental vision-language task that aims to segment out an object referred to by a natural language expression from an image. One of the key challenges behind this task is leveraging the referring expression for highlighting relevant positions in the image. A paradigm for tackling this problem is to leverage a powerful vision-language (“cross-madal”) decoder to fuse features independently extracted from a vision encoder and a language encoder. Recent methods have made remarkable advancements in this paradigm by exploiting Transformers as cross-modal decoders, concurrent to the Transformer's overwhelming success in many other vision-language tasks. Adopting a different approach in this work, we show that significantly better cross-modal alignments can be achieved through the early fusion of linguistic and visual features in intermediate layers of a vision Transformer encoder network. By conducting cross-modal feature fusion in the visual feature encoding stage, we can leverage the well-proven correlation modeling power of a Transformer encoder for excavating helpful multi-modal context. This way, accurate segmentation results are readily harvested with a light-weight mask predictor. Without bells and whistles, our method surpasses the previous state-of-the-art methods on Ref CoCo, RefCOCO+, and G-Ref by large margins.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2021,
    "citationCount": 263,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2112.02244",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2021-12-04",
    "authors": [
      {
        "authorId": "2116370920",
        "name": "Zhao Yang"
      },
      {
        "authorId": null,
        "name": "Jiaqi Wang"
      },
      {
        "authorId": "35299091",
        "name": "Yansong Tang"
      },
      {
        "authorId": "152568027",
        "name": "Kai Chen"
      },
      {
        "authorId": "3459894",
        "name": "Hengshuang Zhao"
      },
      {
        "authorId": "143635540",
        "name": "Philip H. S. Torr"
      }
    ],
    "source": "semantic_scholar",
    "score": 153.63923654719474
  },
  {
    "paperId": "a77f27f6181aeb1d772cd92a5dd767d1afc579ca",
    "url": "https://www.semanticscholar.org/paper/a77f27f6181aeb1d772cd92a5dd767d1afc579ca",
    "title": "Transfer Function Method to Diagnose Axial Displacement and Radial Deformation of Transformer Winding",
    "abstract": "Short circuit currents or forces during transport can cause mechanical displacements of transformer windings. The transfer function method is presented as a tool to detect these displacements. In order to be able to evaluate the measurements, the correlation between the characteristics of transfer functions and possible damages must be known. Axial displacement and radial deformation of transformer windings have been studied-in this research work using two test transformers. The primary winding of the first transformer for axial displacement has 31 double disk coils (1.3 MVA, 10 kV) and the secondary winding is a four-layer winding. The second transformer for the study of radial deformation has 30 double disk coils (1.2 MVA, 10 kV) as primary winding and a one-layer winding as secondary winding. The detailed mathematical models were developed for the test objects and a comparison was carried out between measured and calculated results. It is shown that this model can present the behavior of the transformer windings in the frequency domain in the case of sound and displaced conditions.",
    "venue": "IEEE Power Engineering Review",
    "year": 2002,
    "citationCount": 343,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2002-08-01",
    "authors": [
      {
        "authorId": "3451051",
        "name": "E. Rahimpour"
      },
      {
        "authorId": "2285622964",
        "name": "Jochen Christian"
      },
      {
        "authorId": "2285609971",
        "name": "Kurt Feser"
      },
      {
        "authorId": "2285547803",
        "name": "Hossein Mohseni"
      }
    ],
    "source": "semantic_scholar",
    "score": 153.60962486060097
  },
  {
    "paperId": "88bfc532e97723d909709e81a4a88112034de26a",
    "url": "https://www.semanticscholar.org/paper/88bfc532e97723d909709e81a4a88112034de26a",
    "title": "A 1 kW, 500 kHz front-end converter for a distributed power supply system",
    "abstract": "The analysis, design, and performance are discussed of a prototype high power-density converter suitable for use in the front-end of a distributed power supply system. The system delivers 1 kW to a regulated 40 V distribution bus from the rectified utility line. Its switching frequency is 500 kHz, and it uses a phase-shifted pulse-width modulation technique to avoid primary-side switching losses. The converter's topology is a standard power MOSFET H-bridge that drives a transformer. The output of this transformer is rectified by a full bridge of Schottky diodes. The switches of this forward converter are operated in a fixed-frequency PWM mode. The dominant parasitic elements are the transformer's leakage inductor, the MOSFETs' output capacitors, and the rectifiers' junction capacitors. Of these three groups of parasitic elements, only the leakage inductors do not result in a direct switching loss. To avoid MOSFET switching losses, the converter is controlled with a special gate-drive pattern that permits full recovery of the MOSFETs' capacitive energy. At the same time this drive scheme gives zero-voltage switching for the MOSFETs. The converter's efficiency at full load approaches 90%.<<ETX>>",
    "venue": "Proceedings, Fourth Annual IEEE Applied Power Electronics Conference and Exposition",
    "year": 1989,
    "citationCount": 339,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "1989-03-13",
    "authors": [
      {
        "authorId": "70659214",
        "name": "L. H. Mweene"
      },
      {
        "authorId": "2285725543",
        "name": "Chris A. Wright"
      },
      {
        "authorId": "14026420",
        "name": "M. Schlecht"
      }
    ],
    "source": "semantic_scholar",
    "score": 153.43418426415312
  },
  {
    "paperId": "c5e6f0c52c1f91086879f46120efa79e96158eba",
    "url": "https://www.semanticscholar.org/paper/c5e6f0c52c1f91086879f46120efa79e96158eba",
    "title": "Cross-view Transformers for real-time Map-view Semantic Segmentation",
    "abstract": "We present cross-view transformers, an efficient attention-based model for map-view semantic segmentation from multiple cameras. Our architecture implicitly learns a mapping from individual camera views into a canonical map-view representation using a camera-aware cross-view attention mechanism. Each camera uses positional embeddings that depend on its intrinsic and extrinsic calibration. These embeddings allow a transformer to learn the mapping across different views without ever explicitly modeling it geometrically. The architecture consists of a convolutional image encoder for each view and cross-view transformer layers to infer a map-view semantic segmentation. Our model is simple, easily parallelizable, and runs in realtime. The presented architecture performs at state-of-the-art on the nuScenes dataset, with 4x faster inference speeds. Code is available at https://github.com/bradyz/cross_view_transformers.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2022,
    "citationCount": 245,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2205.02833",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-05-05",
    "authors": [
      {
        "authorId": "144576462",
        "name": "Brady Zhou"
      },
      {
        "authorId": "2305682524",
        "name": "Philipp Krahenbuhl"
      }
    ],
    "source": "semantic_scholar",
    "score": 152.57997303898543
  },
  {
    "paperId": "0aacc78fdf041ca386e7387f1b640964f358cd4c",
    "url": "https://www.semanticscholar.org/paper/0aacc78fdf041ca386e7387f1b640964f358cd4c",
    "title": "A fundamental approach to transformer thermal modeling. I. Theory and equivalent circuit",
    "abstract": "A simple equivalent circuit to represent the thermal heat flow equations for power transformers is presented. Key features are the use of a current source analogy to represent heat input due to losses, and a nonlinear resistor analogy to represent the effect of air or oil cooling convection currents. The effect was first quantified in 1817. It is shown that the idea of \"exponential response\" is not the best way to think of the dynamics of the situation. It is also shown that one can consider ambient temperature to be a variable input to the system, and that it is properly represented as an ideal voltage source.",
    "venue": "",
    "year": 2001,
    "citationCount": 460,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2001-04-01",
    "authors": [
      {
        "authorId": "48207973",
        "name": "G. Swift"
      },
      {
        "authorId": "31269306",
        "name": "T. Molinski"
      },
      {
        "authorId": "3211737",
        "name": "W. Lehn"
      }
    ],
    "source": "semantic_scholar",
    "score": 152.00097064494975
  },
  {
    "paperId": "7bffc157b3b3626a3912a3b0ef74ce5904630fce",
    "url": "https://www.semanticscholar.org/paper/7bffc157b3b3626a3912a3b0ef74ce5904630fce",
    "title": "FlowFormer: A Transformer Architecture for Optical Flow",
    "abstract": "We introduce optical Flow transFormer, dubbed as FlowFormer, a transformer-based neural network architecture for learning optical flow. FlowFormer tokenizes the 4D cost volume built from an image pair, encodes the cost tokens into a cost memory with alternate-group transformer (AGT) layers in a novel latent space, and decodes the cost memory via a recurrent transformer decoder with dynamic positional cost queries. On the Sintel benchmark, FlowFormer achieves 1.159 and 2.088 average end-point-error (AEPE) on the clean and final pass, a 16.5% and 15.5% error reduction from the best published result (1.388 and 2.47). Besides, FlowFormer also achieves strong generalization performance. Without being trained on Sintel, FlowFormer achieves 1.01 AEPE on the clean pass of Sintel training set, outperforming the best published result (1.29) by 21.7%.",
    "venue": "European Conference on Computer Vision",
    "year": 2022,
    "citationCount": 235,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2203.16194",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-03-30",
    "authors": [
      {
        "authorId": "1830448350",
        "name": "Zhaoyang Huang"
      },
      {
        "authorId": "2119204728",
        "name": "Xiaoyu Shi"
      },
      {
        "authorId": "2152738870",
        "name": "Chao Zhang"
      },
      {
        "authorId": "2224668365",
        "name": "Qiang Wang"
      },
      {
        "authorId": "2067676799",
        "name": "K. Cheung"
      },
      {
        "authorId": "46636770",
        "name": "Hongwei Qin"
      },
      {
        "authorId": "3304536",
        "name": "Jifeng Dai"
      },
      {
        "authorId": "47893312",
        "name": "Hongsheng Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 151.95747707538416
  },
  {
    "paperId": "367f7f64ded5d18528c1013db9dfa01b075db484",
    "url": "https://www.semanticscholar.org/paper/367f7f64ded5d18528c1013db9dfa01b075db484",
    "title": "Medical Transformer: Gated Axial-Attention for Medical Image Segmentation",
    "abstract": null,
    "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
    "year": 2021,
    "citationCount": 883,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-02-21",
    "authors": [
      {
        "authorId": "144958260",
        "name": "Jeya Maria Jose Valanarasu"
      },
      {
        "authorId": "144477698",
        "name": "Poojan Oza"
      },
      {
        "authorId": "1836588",
        "name": "I. Hacihaliloglu"
      },
      {
        "authorId": "1741177",
        "name": "Vishal M. Patel"
      }
    ],
    "source": "semantic_scholar",
    "score": 151.76685593956466
  },
  {
    "paperId": "f1cbd20a3f38aef454020ba31289aa270dcd7fbb",
    "url": "https://www.semanticscholar.org/paper/f1cbd20a3f38aef454020ba31289aa270dcd7fbb",
    "title": "Ultra-low-Voltage high-performance CMOS VCOs using transformer feedback",
    "abstract": "A transformer-feedback voltage-controlled oscillator (TF-VCO) is proposed to achieve low-phase-noise and low-power designs even at a supply below the threshold voltage. The advantages of the proposed TF-VCO are described together with its detailed analysis and its cyclo-stationary characteristic. Two prototypes using the proposed TF-VCO techniques are demonstrated in a standard 0.18-/spl mu/m CMOS process. The first design using two single-ended transformers is operated at 1.4 GHz at a 0.35-V supply using PMOS transistors whose threshold voltage is around 0.52 V. The power consumption is 1.46 mW while the measured phase noise is -128.6 dBc/Hz at 1-MHz frequency offset. Using an optimum differential transformer to maximize quality factor and to minimize the chip area, the second design is operated at 3.8 GHz at a 0.5-V supply with power consumption of 570 /spl mu/W and a measured phase noise of -119 dBc/Hz at 1-MHz frequency offset. The figures of merits are comparable or better to that of other state-of-the-art VCO designs operating at much higher supply voltage.",
    "venue": "IEEE Journal of Solid-State Circuits",
    "year": 2005,
    "citationCount": 299,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2005-03-21",
    "authors": [
      {
        "authorId": "2223400",
        "name": "K. Kwok"
      },
      {
        "authorId": "145076490",
        "name": "H. Luong"
      }
    ],
    "source": "semantic_scholar",
    "score": 151.556737119843
  },
  {
    "paperId": "70c3d5ab03a54281be91709b19e3f50a2e4be0e3",
    "url": "https://www.semanticscholar.org/paper/70c3d5ab03a54281be91709b19e3f50a2e4be0e3",
    "title": "Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection",
    "abstract": "Neural sequence models based on the transformer architecture have demonstrated remarkable \\emph{in-context learning} (ICL) abilities, where they can perform new tasks when prompted with training and test examples, without any parameter update to the model. This work first provides a comprehensive statistical theory for transformers to perform ICL. Concretely, we show that transformers can implement a broad class of standard machine learning algorithms in context, such as least squares, ridge regression, Lasso, learning generalized linear models, and gradient descent on two-layer neural networks, with near-optimal predictive power on various in-context data distributions. Using an efficient implementation of in-context gradient descent as the underlying mechanism, our transformer constructions admit mild size bounds, and can be learned with polynomially many pretraining sequences. Building on these ``base'' ICL algorithms, intriguingly, we show that transformers can implement more complex ICL procedures involving \\emph{in-context algorithm selection}, akin to what a statistician can do in real life -- A \\emph{single} transformer can adaptively select different base ICL algorithms -- or even perform qualitatively different tasks -- on different input sequences, without any explicit prompting of the right algorithm or task. We both establish this in theory by explicit constructions, and also observe this phenomenon experimentally. In theory, we construct two general mechanisms for algorithm selection with concrete examples: pre-ICL testing, and post-ICL validation. As an example, we use the post-ICL validation mechanism to construct a transformer that can perform nearly Bayes-optimal ICL on a challenging task -- noisy linear models with mixed noise levels. Experimentally, we demonstrate the strong in-context algorithm selection capabilities of standard transformer architectures.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 142,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2306.04637",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-07",
    "authors": [
      {
        "authorId": "8681368",
        "name": "Yu Bai"
      },
      {
        "authorId": "103244045",
        "name": "Fan Chen"
      },
      {
        "authorId": "46507194",
        "name": "Haiquan Wang"
      },
      {
        "authorId": "2054594326",
        "name": "Caiming Xiong"
      },
      {
        "authorId": "2068869988",
        "name": "Song Mei"
      }
    ],
    "source": "semantic_scholar",
    "score": 151.4426694538986
  },
  {
    "paperId": "379e42895f6d40ab9e9559609f505aba89145a5d",
    "url": "https://www.semanticscholar.org/paper/379e42895f6d40ab9e9559609f505aba89145a5d",
    "title": "Efficiently Scaling Transformer Inference",
    "abstract": "We study the problem of efficient generative inference for Transformer models, in one of its most challenging settings: large deep models, with tight latency targets and long sequence lengths. Better understanding of the engineering tradeoffs for inference for large Transformer-based models is important as use cases of these models are growing rapidly throughout application areas. We develop a simple analytical model for inference efficiency to select the best multi-dimensional partitioning techniques optimized for TPU v4 slices based on the application requirements. We combine these with a suite of low-level optimizations to achieve a new Pareto frontier on the latency and model FLOPS utilization (MFU) tradeoffs on 500B+ parameter models that outperforms the FasterTransformer suite of benchmarks. We further show that with appropriate partitioning, the lower memory requirements of multiquery attention (i.e. multiple query heads share single key/value head) enables scaling up to 32x larger context lengths. Finally, we achieve a low-batch-size latency of 29ms per token during generation (using int8 weight quantization) and a 76% MFU during large-batch-size processing of input tokens, while supporting a long 2048-token context length on the PaLM 540B parameter model.",
    "venue": "Conference on Machine Learning and Systems",
    "year": 2022,
    "citationCount": 227,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2211.05102",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-11-09",
    "authors": [
      {
        "authorId": "2161431901",
        "name": "Reiner Pope"
      },
      {
        "authorId": "2269733876",
        "name": "Sholto Douglas"
      },
      {
        "authorId": "2841893",
        "name": "Aakanksha Chowdhery"
      },
      {
        "authorId": "39172707",
        "name": "Jacob Devlin"
      },
      {
        "authorId": "2065251344",
        "name": "James Bradbury"
      },
      {
        "authorId": "6639036",
        "name": "Anselm Levskaya"
      },
      {
        "authorId": "151488492",
        "name": "J. Heek"
      },
      {
        "authorId": "8867471",
        "name": "Kefan Xiao"
      },
      {
        "authorId": "3504647",
        "name": "Shivani Agrawal"
      },
      {
        "authorId": "48448318",
        "name": "J. Dean"
      }
    ],
    "source": "semantic_scholar",
    "score": 151.44018443431662
  },
  {
    "paperId": "5e49652206669975f0d70c25a2267ed1dc8d9c13",
    "url": "https://www.semanticscholar.org/paper/5e49652206669975f0d70c25a2267ed1dc8d9c13",
    "title": "Control of doublesex alternative splicing by transformer and transformer-2 in Drosophila",
    "abstract": "Sex-specific alternative processing of doublesex (dsx) precursor messenger RNA (pre-mRNA) regulates somatic sexual differentiation in Drosophila melanogaster. Cotransfection analyses in which the dsx gene and the female-specific transformer (tra) and transformer-2 (tra-2) complementary DNAs were expressed in Drosophila Kc cells revealed that female-specific splicing of the dsx transcript was positively regulated by the products of the tra and tra-2 genes. Furthermore, analyses of mutant constructs of dsx showed that a portion of the female-specific exon sequence was required for regulation of dsx pre-messenger RNA splicing.",
    "venue": "Science",
    "year": 1991,
    "citationCount": 311,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Biology",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "1991-05-10",
    "authors": [
      {
        "authorId": "2253683005",
        "name": "Kazuyuki Hoshijima"
      },
      {
        "authorId": "2253132028",
        "name": "Kunio Inoue"
      },
      {
        "authorId": "2073545494",
        "name": "Ikuko Higuchi"
      },
      {
        "authorId": "153005057",
        "name": "H. Sakamoto"
      },
      {
        "authorId": "2237346049",
        "name": "Yoshiro Shimura"
      }
    ],
    "source": "semantic_scholar",
    "score": 151.14504781714226
  },
  {
    "paperId": "36169ad27b5ef7a7c664bf9b01d334a4fc0bdf52",
    "url": "https://www.semanticscholar.org/paper/36169ad27b5ef7a7c664bf9b01d334a4fc0bdf52",
    "title": "Image Dehazing Transformer with Transmission-Aware 3D Position Embedding",
    "abstract": "Despite single image dehazing has been made promising progress with Convolutional Neural Networks (CNNs), the inherent equivariance and locality of convolution still bottleneck deharing performance. Though Transformer has occupied various computer vision tasks, directly leveraging Transformer for image dehazing is challenging: 1) it tends to result in ambiguous and coarse details that are undesired for image reconstruction; 2) previous position embedding of Transformer is provided in logic or spatial position order that neglects the variational haze densities, which results in the sub-optimal dehazlng performance. The key insight of this study is to investigate how to combine CNN and Transformer for image dehazing. To solve the feature inconsistency issue between Transformer and CNN, we propose to modulate CNN features via learning modulation matrices (i.e., coefficient matrix and bias matrix) conditioned on Transformer features instead of simple feature addition or concatenation. The feature modulation naturally inherits the global context modeling capability of Transformer and the local representation capability of CNN. We bring a haze density-related prior into Trans-former via a novel transmission-aware 3D position embedding module, which not only provides the relative position but also suggests the haze density of different spatial regions. Extensive experiments demonstrate that our method, DeHamer, attains state-of-the-art performance on several image dehazing benchmarks.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2022,
    "citationCount": 221,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-06-01",
    "authors": [
      {
        "authorId": "18158517",
        "name": "Chunle Guo"
      },
      {
        "authorId": "2072780927",
        "name": "Qixin Yan"
      },
      {
        "authorId": "49053414",
        "name": "Saeed Anwar"
      },
      {
        "authorId": "3409475",
        "name": "Runmin Cong"
      },
      {
        "authorId": "144850642",
        "name": "Wenqi Ren"
      },
      {
        "authorId": "2698424",
        "name": "Chongyi Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 151.04016072808417
  },
  {
    "paperId": "6f6e2e0311589a9af045f6acd00b7dee6d19fce4",
    "url": "https://www.semanticscholar.org/paper/6f6e2e0311589a9af045f6acd00b7dee6d19fce4",
    "title": "The Impact of Positional Encoding on Length Generalization in Transformers",
    "abstract": "Length generalization, the ability to generalize from small training context sizes to larger ones, is a critical challenge in the development of Transformer-based language models. Positional encoding (PE) has been identified as a major factor influencing length generalization, but the exact impact of different PE schemes on extrapolation in downstream tasks remains unclear. In this paper, we conduct a systematic empirical study comparing the length generalization performance of decoder-only Transformers with five different position encoding approaches including Absolute Position Embedding (APE), T5's Relative PE, ALiBi, and Rotary, in addition to Transformers without positional encoding (NoPE). Our evaluation encompasses a battery of reasoning and mathematical tasks. Our findings reveal that the most commonly used positional encoding methods, such as ALiBi, Rotary, and APE, are not well suited for length generalization in downstream tasks. More importantly, NoPE outperforms other explicit positional encoding methods while requiring no additional computation. We theoretically demonstrate that NoPE can represent both absolute and relative PEs, but when trained with SGD, it mostly resembles T5's relative PE attention patterns. Finally, we find that scratchpad is not always helpful to solve length generalization and its format highly impacts the model's performance. Overall, our work suggests that explicit position embeddings are not essential for decoder-only Transformers to generalize well to longer sequences.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 136,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.19466",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-05-31",
    "authors": [
      {
        "authorId": "1754452702",
        "name": "Amirhossein Kazemnejad"
      },
      {
        "authorId": "8350409",
        "name": "Inkit Padhi"
      },
      {
        "authorId": "1704263",
        "name": "K. Ramamurthy"
      },
      {
        "authorId": "1730372",
        "name": "Payel Das"
      },
      {
        "authorId": "145732771",
        "name": "Siva Reddy"
      }
    ],
    "source": "semantic_scholar",
    "score": 150.7997138874219
  },
  {
    "paperId": "ea0e4a9778e33b7f8e7b3246d63071330950995a",
    "url": "https://www.semanticscholar.org/paper/ea0e4a9778e33b7f8e7b3246d63071330950995a",
    "title": "Structure-Aware Transformer for Graph Representation Learning",
    "abstract": "The Transformer architecture has gained growing attention in graph representation learning recently, as it naturally overcomes several limitations of graph neural networks (GNNs) by avoiding their strict structural inductive biases and instead only encoding the graph structure via positional encoding. Here, we show that the node representations generated by the Transformer with positional encoding do not necessarily capture structural similarity between them. To address this issue, we propose the Structure-Aware Transformer, a class of simple and flexible graph Transformers built upon a new self-attention mechanism. This new self-attention incorporates structural information into the original self-attention by extracting a subgraph representation rooted at each node before computing the attention. We propose several methods for automatically generating the subgraph representation and show theoretically that the resulting representations are at least as expressive as the subgraph representations. Empirically, our method achieves state-of-the-art performance on five graph prediction benchmarks. Our structure-aware framework can leverage any existing GNN to extract the subgraph representation, and we show that it systematically improves performance relative to the base GNN model, successfully combining the advantages of GNNs and Transformers. Our code is available at https://github.com/BorgwardtLab/SAT.",
    "venue": "International Conference on Machine Learning",
    "year": 2022,
    "citationCount": 217,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-02-07",
    "authors": [
      {
        "authorId": "81299639",
        "name": "Dexiong Chen"
      },
      {
        "authorId": "2007782619",
        "name": "Leslie O’Bray"
      },
      {
        "authorId": "1704422",
        "name": "Karsten M. Borgwardt"
      }
    ],
    "source": "semantic_scholar",
    "score": 150.76742594183634
  },
  {
    "paperId": "c8831d0629f0eaf7f723317d71bbd60b8eb3c39f",
    "url": "https://www.semanticscholar.org/paper/c8831d0629f0eaf7f723317d71bbd60b8eb3c39f",
    "title": "UniFormer: Unified Transformer for Efficient Spatiotemporal Representation Learning",
    "abstract": "It is a challenging task to learn rich and multi-scale spatiotemporal semantics from high-dimensional videos, due to large local redundancy and complex global dependency between video frames. The recent advances in this research have been mainly driven by 3D convolutional neural networks and vision transformers. Although 3D convolution can efficiently aggregate local context to suppress local redundancy from a small 3D neighborhood, it lacks the capability to capture global dependency because of the limited receptive field. Alternatively, vision transformers can effectively capture long-range dependency by self-attention mechanism, while having the limitation on reducing local redundancy with blind similarity comparison among all the tokens in each layer. Based on these observations, we propose a novel Unified transFormer (UniFormer) which seamlessly integrates merits of 3D convolution and spatiotemporal self-attention in a concise transformer format, and achieves a preferable balance between computation and accuracy. Different from traditional transformers, our relation aggregator can tackle both spatiotemporal redundancy and dependency, by learning local and global token affinity respectively in shallow and deep layers. We conduct extensive experiments on the popular video benchmarks, e.g., Kinetics-400, Kinetics-600, and Something-Something V1&V2. With only ImageNet-1K pretraining, our UniFormer achieves 82.9%/84.8% top-1 accuracy on Kinetics-400/Kinetics-600, while requiring 10x fewer GFLOPs than other state-of-the-art methods. For Something-Something V1 and V2, our UniFormer achieves new state-of-the-art performances of 60.9% and 71.2% top-1 accuracy respectively. Code is available at https://github.com/Sense-X/UniFormer.",
    "venue": "International Conference on Learning Representations",
    "year": 2022,
    "citationCount": 214,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2022-01-12",
    "authors": [
      {
        "authorId": "2115468491",
        "name": "Kunchang Li"
      },
      {
        "authorId": "47903936",
        "name": "Yali Wang"
      },
      {
        "authorId": "144740494",
        "name": "Peng Gao"
      },
      {
        "authorId": "12920342",
        "name": "Guanglu Song"
      },
      {
        "authorId": "2146400394",
        "name": "Yu Liu"
      },
      {
        "authorId": "47893312",
        "name": "Hongsheng Li"
      },
      {
        "authorId": "2059129841",
        "name": "Y. Qiao"
      }
    ],
    "source": "semantic_scholar",
    "score": 150.55957042191494
  },
  {
    "paperId": "10bd38673951f5d7729568284093cbd80482ab16",
    "url": "https://www.semanticscholar.org/paper/10bd38673951f5d7729568284093cbd80482ab16",
    "title": "Vision Transformers Need Registers",
    "abstract": "Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 209,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.16588",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-09-28",
    "authors": [
      {
        "authorId": "2214523349",
        "name": "Timothée Darcet"
      },
      {
        "authorId": "2248163611",
        "name": "Maxime Oquab"
      },
      {
        "authorId": "2599292",
        "name": "J. Mairal"
      },
      {
        "authorId": "2329288",
        "name": "Piotr Bojanowski"
      }
    ],
    "source": "semantic_scholar",
    "score": 150.20661296076202
  },
  {
    "paperId": "761148bc27f0bb00645876869ada15248abd7acb",
    "url": "https://www.semanticscholar.org/paper/761148bc27f0bb00645876869ada15248abd7acb",
    "title": "Transformer tank vibration modeling as a method of detecting winding deformations-part I: theoretical foundation",
    "abstract": "In this paper, a model developed for a transformer monitoring system to estimate transformer tank vibration is presented. The model calculates vibration on the transformer tank starting from some input variables that can be easily measured on the transformer. Tank vibration is also measured, showing a good concordance between estimated and measured values if the transformer is healthy. In case of a winding deformation winding vibration and, consequently, that of the tank, changes and a big difference between estimated and measured vibration appear. To estimate tank vibration, the model takes into account the main physical phenomena generating vibrations in the different transformer elements and how these vibrations are superposed and transmitted to the tank. The model has been tested experimentally on a test transformer fitted with internal and external accelerometers. A deformation has been provoked in the test transformer winding with the aim of testing the model's ability to detect it. The model has been also tested on several in-service grid transformers. The results of the experimental validation are shown in Part II of the paper.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2006,
    "citationCount": 260,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2248523791",
        "name": "Belén García"
      },
      {
        "authorId": "39177274",
        "name": "J. Burgos"
      },
      {
        "authorId": "47642155",
        "name": "A. Alonso"
      }
    ],
    "source": "semantic_scholar",
    "score": 149.4678061098404
  },
  {
    "paperId": "f5e9337477d7a9eb6267d0310549fdefafbb7fe2",
    "url": "https://www.semanticscholar.org/paper/f5e9337477d7a9eb6267d0310549fdefafbb7fe2",
    "title": "Transformers learn to implement preconditioned gradient descent for in-context learning",
    "abstract": "Several recent works demonstrate that transformers can implement algorithms like gradient descent. By a careful construction of weights, these works show that multiple layers of transformers are expressive enough to simulate iterations of gradient descent. Going beyond the question of expressivity, we ask: Can transformers learn to implement such algorithms by training over random problem instances? To our knowledge, we make the first theoretical progress on this question via an analysis of the loss landscape for linear transformers trained over random instances of linear regression. For a single attention layer, we prove the global minimum of the training objective implements a single iteration of preconditioned gradient descent. Notably, the preconditioning matrix not only adapts to the input distribution but also to the variance induced by data inadequacy. For a transformer with $L$ attention layers, we prove certain critical points of the training objective implement $L$ iterations of preconditioned gradient descent. Our results call for future theoretical studies on learning algorithms by training transformers.",
    "venue": "Neural Information Processing Systems",
    "year": 2023,
    "citationCount": 123,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.00297",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-01",
    "authors": [
      {
        "authorId": "9036928",
        "name": "Kwangjun Ahn"
      },
      {
        "authorId": "2149478480",
        "name": "Xiang Cheng"
      },
      {
        "authorId": "1764651",
        "name": "Hadi Daneshmand"
      },
      {
        "authorId": "3072326",
        "name": "S. Sra"
      }
    ],
    "source": "semantic_scholar",
    "score": 149.30422348407555
  },
  {
    "paperId": "9338c021c98ab017e11b865510b59b9a99cf1755",
    "url": "https://www.semanticscholar.org/paper/9338c021c98ab017e11b865510b59b9a99cf1755",
    "title": "Computer Methods and Programs in Biomedicine",
    "abstract": "Background and objective: COVID-19 is a serious threat to human health. Traditional convolutional neural networks (CNNs) can realize medical image segmentation, whilst transformers can be used to perform machine vision tasks, because they have a better ability to capture long-range relationships than CNNs. The combination of CNN and transformers to complete the task of semantic segmentation has attracted intense research. Currently, it is challenging to segment medical images on limited data sets like that on COVID-19. Methods: This study proposes a lightweight transformer+CNN model",
    "venue": "",
    "year": 2023,
    "citationCount": 274,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2108828331",
        "name": "Yuan Yang"
      },
      {
        "authorId": "2143838630",
        "name": "Lin Zhang"
      },
      {
        "authorId": "2277237929",
        "name": "Lei Ren"
      },
      {
        "authorId": "2133169767",
        "name": "Xiaohan Wang"
      },
      {
        "authorId": "2345405793",
        "name": "Cam"
      },
      {
        "authorId": "2345405642",
        "name": "L. I. Split"
      },
      {
        "authorId": "2345405640",
        "name": "Concat"
      }
    ],
    "source": "semantic_scholar",
    "score": 149.25156646499858
  },
  {
    "paperId": "642c1b4a9da95ea4239708afc5929a5007a1870d",
    "url": "https://www.semanticscholar.org/paper/642c1b4a9da95ea4239708afc5929a5007a1870d",
    "title": "Tensor2Tensor for Neural Machine Translation",
    "abstract": "Tensor2Tensor is a library for deep learning models that is well-suited for neural machine translation and includes the reference implementation of the state-of-the-art Transformer model.",
    "venue": "Conference of the Association for Machine Translation in the Americas",
    "year": 2018,
    "citationCount": 521,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2018-03-16",
    "authors": [
      {
        "authorId": "40348417",
        "name": "Ashish Vaswani"
      },
      {
        "authorId": "1751569",
        "name": "Samy Bengio"
      },
      {
        "authorId": "2445241",
        "name": "E. Brevdo"
      },
      {
        "authorId": "1565641737",
        "name": "François Chollet"
      },
      {
        "authorId": "19177000",
        "name": "Aidan N. Gomez"
      },
      {
        "authorId": "2776283",
        "name": "Stephan Gouws"
      },
      {
        "authorId": "145024664",
        "name": "Llion Jones"
      },
      {
        "authorId": "40527594",
        "name": "Lukasz Kaiser"
      },
      {
        "authorId": "2583391",
        "name": "Nal Kalchbrenner"
      },
      {
        "authorId": "3877127",
        "name": "Niki Parmar"
      },
      {
        "authorId": "35474601",
        "name": "Ryan Sepassi"
      },
      {
        "authorId": "1846258",
        "name": "Noam M. Shazeer"
      },
      {
        "authorId": "39328010",
        "name": "Jakob Uszkoreit"
      }
    ],
    "source": "semantic_scholar",
    "score": 148.8650138182396
  },
  {
    "paperId": "6787381b9c5d51c36ed63dafbf5efc60edbede84",
    "url": "https://www.semanticscholar.org/paper/6787381b9c5d51c36ed63dafbf5efc60edbede84",
    "title": "A Novel Approach to the Classification of the Transient Phenomena in Power Transformers Using Combined Wavelet Transform and Neural Network",
    "abstract": "The wavelet transform is a powerful tool in the analysis of the power transformer transient phenomena because of its ability to extract information from the transient signals simultaneously in both the time and frequency domain. This paper presents a novel technique for accurate discrimination between an intemal fault and a magnetizing inrush current in the power transformer by combining wavelet transforms with neural networks. The wavelet transform is first applied to decompose the differential current signals of the power transformer into a series of detailed wavelet components. The spectral energies of the wavelet components are calculated and then employed to train a neural network to discriminate an intemal fault from the magnetizing inrush current. The simulated results presented clearly show that the proposed technique can accurately discriminate between an intemal fault and a magnetizing inrush current in power transformer protection.",
    "venue": "IEEE Power Engineering Review",
    "year": 2001,
    "citationCount": 241,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2001-07-01",
    "authors": [
      {
        "authorId": "97236973",
        "name": "P. Mao"
      },
      {
        "authorId": "30393665",
        "name": "R. Aggarwal"
      }
    ],
    "source": "semantic_scholar",
    "score": 148.33406589235028
  },
  {
    "paperId": "3a81a988050627f0cb768bdb3f2eec7f31881a0b",
    "url": "https://www.semanticscholar.org/paper/3a81a988050627f0cb768bdb3f2eec7f31881a0b",
    "title": "Electric Power Transformer Engineering",
    "abstract": "Theory and Principles, D.J. Allan and H. Moore Power Transformers, H.J. Sim and S.H. Digby Distribution Transformers, D.L. Galloway, D. Mulkey, and A.L. Wilks Phase-Shifting Transformers, G. Preininger Rectifier Transformers, S.P. Kennedy Dry-Type Transformers, C.W. Johnson, Jr. Instrument Transformers, R. Mullikin Step-Voltage Regulators, C.A. Colopy Constant-Voltage Transformers, A. Maitra, A. Gaikwad, A. Mansoor, D. Dorr, and R. Ferraro Transformers for Wind Turbine Generators and Photovoltaic Applications, D.E. Buckmaster and H. Shertukde Reactors, R.F. Dudley, M. Sharp, A. Castanheira, and B.B. Biglar Insulating Media, T.A. Prevost, D.L. Hanson, L.J. Savio, and T. Haupert Electrical Bushings, L.B. Wagenaar Tap Changers and Smart Intelligent Controls, D. Dohnal, A. Kraemer, and K. Viereck Loading and Thermal Performance, R.F. Tillman, Jr. and D.A. Duckett Transformer Connections, S. Shull and D.D. Perco Transformer Testing, S.P. Mehta and W.R. Henning Load-Tap-Change Control and Transformer Paralleling, J.H. Harlow Power Transformer Protection, A. Guzman, H.J. Altuve, and G. Benmouyal Causes and Effects of Transformer Sound Levels, J.L. Puri Transient-Voltage Response of Coils and Windings, R.C. Degeneff Transformer Installation and Maintenance, T.D. Kabrich Problem and Failure Investigation, W.B. Binder and H. Moore On-Line Monitoring of Liquid-Immersed Transformers, G.R. Hoffman U.S. Power Transformer Equipment Standards and Processes, P.J. Hopkinson",
    "venue": "",
    "year": 2003,
    "citationCount": 353,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2003-08-15",
    "authors": [
      {
        "authorId": "6076005",
        "name": "J. Harlow"
      }
    ],
    "source": "semantic_scholar",
    "score": 148.0394536970066
  },
  {
    "paperId": "0b569c3bc1c535f0b0fb57227098dae5ad7c8f63",
    "url": "https://www.semanticscholar.org/paper/0b569c3bc1c535f0b0fb57227098dae5ad7c8f63",
    "title": "Frequency response of oil impregnated pressboard and paper samples for estimating moisture in transformer insulation",
    "abstract": "Knowledge about moisture content in oil impregnated paper insulation is essential when estimating remaining lifetime of power transformers. Direct evaluation of moisture content is rarely possible due to inaccessibility of the internal insulation system in transformers. Therefore, various indirect estimation techniques are utilized. Frequency domain spectroscopy (FDS) measurements of transformer insulation belong to this group. To perform high quality interpretation of results of FDS measurements a good knowledge on dielectric responses in oil impregnated pressboard and paper is required, especially as it refers to their variation with water content and temperature. The aim of this paper is to provide an open access to the frequency domain spectra of oil impregnated paper and pressboard samples, which can then be used in modeling of the results of diagnostic measurements in power transformers.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2006,
    "citationCount": 233,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2006-06-26",
    "authors": [
      {
        "authorId": "2239497570",
        "name": "Chandima Ekanayake"
      },
      {
        "authorId": "2239497600",
        "name": "Stanislaw Gubanski"
      },
      {
        "authorId": "2076382757",
        "name": "A. Graczkowski"
      },
      {
        "authorId": "2239500813",
        "name": "Krzysztof Walczak"
      }
    ],
    "source": "semantic_scholar",
    "score": 147.8298167303655
  },
  {
    "paperId": "d2654ca28e831f8a7395bc358053aa5d09e447c6",
    "url": "https://www.semanticscholar.org/paper/d2654ca28e831f8a7395bc358053aa5d09e447c6",
    "title": "Transformer modeling for low- and mid-frequency transients - a review",
    "abstract": "One of the weakest components of modern transient simulation software is the transformer model. Many opportunities exist to improve the simulation of its complicated behaviors, which include magnetic saturation of the core, frequency-dependency, capacitive coupling, and topologically correctness of core and coil structure. This paper presents a review of transformer models for simulation of low- and mid-frequency transients. Salient points of key references are presented and discussed in order to give an accessible overview of development, implementation and limitations of the most useful models proposed to date. Techniques for representation of the most important behaviors are examined, with the intent of providing the needed foundation for continued development and improvement of transformer models.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2005,
    "citationCount": 232,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": "2005-04-04",
    "authors": [
      {
        "authorId": "2249525327",
        "name": "Juan A. Martinez"
      },
      {
        "authorId": "2259167939",
        "name": "B. Mork"
      }
    ],
    "source": "semantic_scholar",
    "score": 147.7655768034855
  },
  {
    "paperId": "1e14fe0de505d73b28c7b73adb2e24b74f3befbb",
    "url": "https://www.semanticscholar.org/paper/1e14fe0de505d73b28c7b73adb2e24b74f3befbb",
    "title": "A fuzzy dissolved gas analysis method for the diagnosis of multiple incipient faults in a transformer",
    "abstract": "Dissolved gas analysis (DGA) of transformer oil has been one of the most useful techniques to detect the incipient faults. Various methods, such as the IEC codes, have been developed to interpret DGA results directly obtained from a chromatographer. Although these methods are widely used in the world, they sometimes fail to diagnose, especially when more than one fault exists in a transformer. This paper presents a fuzzy logic technique which can diagnose multiple faults in a transformer and quantitatively indicates the likelihood/severity of each fault. Insulation deterioration at each fault location can then be monitored closely according to its trend, which is important for a transformer in critical situation. Tests using this technique on a number of transformers have given promising results.",
    "venue": "",
    "year": 2000,
    "citationCount": 332,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "31348659",
        "name": "Q. Su"
      },
      {
        "authorId": "47896237",
        "name": "L. Lai"
      },
      {
        "authorId": "143841755",
        "name": "P. Austin"
      }
    ],
    "source": "semantic_scholar",
    "score": 147.12213734970666
  },
  {
    "paperId": "2e61c7cbd5a54de5a3279b865ed419f106dd21ed",
    "url": "https://www.semanticscholar.org/paper/2e61c7cbd5a54de5a3279b865ed419f106dd21ed",
    "title": "Charge equalization for series connected battery strings",
    "abstract": "A simple technique that provides charge equalization for a series string of battery cells is presented. The advantages of accurate charge equalization are very substantial and include reduced damage to battery cells in the stack, and a dramatic increase in battery life. The basic technique utilizes a single isolated DC/DC converter with a capacitive output filter along with a multiple co-axial winding transformer (CWT). The co-axial winding transformer is known for its low and controlled leakage inductance. The transformer leakage inductance is used as the main driving impedance to control the total charging current for the individual cells. The use of one low power converter to obtain charge equalization for the entire battery string is very attractive and leads to a low cost implementation.<<ETX>>",
    "venue": "Proceedings of 1994 IEEE Industry Applications Society Annual Meeting",
    "year": 1994,
    "citationCount": 222,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Physics",
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1994-10-02",
    "authors": [
      {
        "authorId": "2245473656",
        "name": "Nasser H. Kutkut"
      },
      {
        "authorId": "2245473699",
        "name": "D. M. Divan"
      },
      {
        "authorId": "2245474254",
        "name": "Donald W. Novotny"
      }
    ],
    "source": "semantic_scholar",
    "score": 147.1075765719018
  },
  {
    "paperId": "961d9f7e055cb099d0c31858ddb852d8bb2da337",
    "url": "https://www.semanticscholar.org/paper/961d9f7e055cb099d0c31858ddb852d8bb2da337",
    "title": "Deriving an equivalent circuit of transformers insulation for understanding the dielectric response measurements",
    "abstract": "Preventive diagnosis and maintenance of transformers have become more and more popular in recent times in order to improve the reliability of electric power systems. Dielectric testing techniques such as return voltage measurement (RVM) and polarization-depolarization current (PDC) measurement are being investigated as potential tools for condition assessment of transformer insulation. A better understanding and analysis of the dielectric test results are only possible with a clear understanding of the physical behavior of the insulation system in response to moisture and aging. A circuit model, which describes the dielectric behavior of the transformer's main insulation system, has been parameterized in this paper. The values of the parameters of the model have been identified from the dielectric tests. A correlation has been developed between the physical condition of the insulation and the equivalent model parameters that enable a clear and transparent interpretation of the dielectric test results.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2005,
    "citationCount": 219,
    "openAccessPdf": {
      "url": "http://espace.library.uq.edu.au/view/UQ:9899/saha-raj-muller-.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Materials Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2005-02-14",
    "authors": [
      {
        "authorId": "143956512",
        "name": "T. Saha"
      },
      {
        "authorId": "1772565",
        "name": "P. Purkait"
      },
      {
        "authorId": "152969045",
        "name": "F. Muller"
      }
    ],
    "source": "semantic_scholar",
    "score": 146.90441319528543
  },
  {
    "paperId": "f41c3fafa5c14f73984161551a9ca314effbe731",
    "url": "https://www.semanticscholar.org/paper/f41c3fafa5c14f73984161551a9ca314effbe731",
    "title": "Dual Aggregation Transformer for Image Super-Resolution",
    "abstract": "Transformer has recently gained considerable popularity in low-level vision tasks, including image super-resolution (SR). These networks utilize self-attention along different dimensions, spatial or channel, and achieve impressive performance. This inspires us to combine the two dimensions in Transformer for a more powerful representation capability. Based on the above idea, we propose a novel Transformer model, Dual Aggregation Transformer (DAT), for image SR. Our DAT aggregates features across spatial and channel dimensions, in the inter-block and intra-block dual manner. Specifically, we alternately apply spatial and channel self-attention in consecutive Transformer blocks. The alternate strategy enables DAT to capture the global context and realize inter-block feature aggregation. Furthermore, we propose the adaptive interaction module (AIM) and the spatial-gate feed-forward network (SGFN) to achieve intra-block feature aggregation. AIM complements two self-attention mechanisms from corresponding dimensions. Meanwhile, SGFN introduces additional non-linear spatial information in the feed-forward network. Extensive experiments show that our DAT surpasses current methods. Code and models are obtainable at https://github.com/zhengchen1999/DAT.",
    "venue": "IEEE International Conference on Computer Vision",
    "year": 2023,
    "citationCount": 111,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-08-07",
    "authors": [
      {
        "authorId": "150068358",
        "name": "Zheng Chen"
      },
      {
        "authorId": "2410227",
        "name": "Yulun Zhang"
      },
      {
        "authorId": "4398255",
        "name": "Jinjin Gu"
      },
      {
        "authorId": "3254296",
        "name": "L. Kong"
      },
      {
        "authorId": "2145266405",
        "name": "Xiaokang Yang"
      },
      {
        "authorId": "1807197",
        "name": "F. Yu"
      }
    ],
    "source": "semantic_scholar",
    "score": 146.7774830694264
  },
  {
    "paperId": "ecd6c5e558e3bb6514b940e4d1ba251a21165e5a",
    "url": "https://www.semanticscholar.org/paper/ecd6c5e558e3bb6514b940e4d1ba251a21165e5a",
    "title": "Analysis, Design, and Control of a Transcutaneous Power Regulator for Artificial Hearts",
    "abstract": "Based on a generic transcutaneous transformer model, a remote power supply using a resonant topology for use in artificial hearts is analyzed and designed for easy controllability and high efficiency. The primary and secondary windings of the transcutaneous transformer are positioned outside and inside the human body, respectively. In such a transformer, the alignment and gap may change with external positioning. As a result, the coupling coefficient of the transcutaneous transformer is also varying, and so are the two large leakage inductances and the mutual inductance. Resonant-tank circuits with varying resonant-frequency are formed from the transformer inductors and external capacitors. For a given range of coupling coefficients, an operating frequency corresponding to a particular coupling coefficient can be found, for which the voltage transfer function is insensitive to load. Prior works have used frequency modulation to regulate the output voltage under varying load and transformer coupling. The use of frequency modulation may require a wide control frequency range which may extend well above the load insensitive frequency. In this paper, study of the input-to-output voltage transfer function is carried out, and a control method is proposed to lock the switching frequency at just above the load insensitive frequency for optimized efficiency at heavy loads. Specifically, operation at above resonant of the resonant circuits is maintained under varying coupling-coefficient. Using a digital-phase-lock-loop (PLL), zero-voltage switching is achieved in a full-bridge converter which is also programmed to provide output voltage regulation via pulsewidth modulation (PWM). A prototype transcutaneous power regulator is built and found to to perform excellently with high efficiency and tight regulation under variations of the alignment or gap of the transcutaneous transformer, load and input voltage.",
    "venue": "IEEE Transactions on Biomedical Circuits and Systems",
    "year": 2009,
    "citationCount": 216,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2009-01-20",
    "authors": [
      {
        "authorId": "3191194",
        "name": "Qianhong Chen"
      },
      {
        "authorId": "2695906",
        "name": "Siu‐Chung Wong"
      },
      {
        "authorId": "145557245",
        "name": "C. Tse"
      },
      {
        "authorId": "144383707",
        "name": "X. Ruan"
      }
    ],
    "source": "semantic_scholar",
    "score": 146.6984603031069
  },
  {
    "paperId": "229177177ac796d39bf3ead1adbbbc30f3f65647",
    "url": "https://www.semanticscholar.org/paper/229177177ac796d39bf3ead1adbbbc30f3f65647",
    "title": "A Novel Structure for Three-Phase Four-Wire Distribution System Utilizing Unified Power Quality Conditioner (UPQC)",
    "abstract": "This paper presents a novel structure for a three-phase four-wire (3P4W) distribution system utilizing unified power quality conditioner (UPQC). The 3P4W system is realized from a three-phase three-wire system where the neutral of series transformer used in series part UPQC is considered as the fourth wire for the 3P4W system. A new control strategy to balance the unbalanced load currents is also presented in this paper. The neutral current that may flow toward transformer neutral point is compensated by using a four-leg voltage source inverter topology for shunt part. Thus, the series transformer neutral will be at virtual zero potential during all operating conditions. The simulation results based on MATLAB/Simulink are presented to show the effectiveness of the proposed UPQC-based 3P4W distribution system.",
    "venue": "IEEE transactions on industry applications",
    "year": 2006,
    "citationCount": 216,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2006-12-01",
    "authors": [
      {
        "authorId": "2260198688",
        "name": "Vinod Khadkikar"
      },
      {
        "authorId": "2259425150",
        "name": "Ambrish Chandra"
      }
    ],
    "source": "semantic_scholar",
    "score": 146.6984603031069
  },
  {
    "paperId": "147164a3905f41a7a5a10f732d086a621c9c5862",
    "url": "https://www.semanticscholar.org/paper/147164a3905f41a7a5a10f732d086a621c9c5862",
    "title": "TransBTS: Multimodal Brain Tumor Segmentation Using Transformer",
    "abstract": null,
    "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
    "year": 2021,
    "citationCount": 627,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-03-07",
    "authors": [
      {
        "authorId": "152201640",
        "name": "Wenxuan Wang"
      },
      {
        "authorId": "2127378914",
        "name": "Chen Chen"
      },
      {
        "authorId": "2055623871",
        "name": "Meng Ding"
      },
      {
        "authorId": "2269804555",
        "name": "Jiangyun Li"
      },
      {
        "authorId": "2119120552",
        "name": "Hong Yu"
      },
      {
        "authorId": "1381221385",
        "name": "Sen Zha"
      }
    ],
    "source": "semantic_scholar",
    "score": 146.63810249702297
  },
  {
    "paperId": "8096c057a8d6dcc2cec2119f0c2de2bae27cecb8",
    "url": "https://www.semanticscholar.org/paper/8096c057a8d6dcc2cec2119f0c2de2bae27cecb8",
    "title": "Wide band modeling of power transformers",
    "abstract": "This paper describes the measurement setup and modeling technique used for obtaining a linear wide band frequency-dependent black box model of a two-winding power transformer, for the purpose of calculation of electromagnetic transients in power systems. The measurement setup is based on a network analyzer, shielded cables, and a connection board. The setup is demonstrated to give a consistent data set where the effect of the measurement cables can be eliminated. The accuracy of the data set is increased by using a combination of current measurements and voltage transfer measurements. A rational approximation of the admittance matrix is calculated in the frequency domain in the range of 50 Hz to 1 MHz and subjected to passivity enforcement, giving a stable model which can be included in electromagnetic transients program (EMTP)-type simulation programs. The accuracy is validated both in the frequency domain and in the time domain.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2004,
    "citationCount": 215,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2004-01-07",
    "authors": [
      {
        "authorId": "144633927",
        "name": "B. Gustavsen"
      }
    ],
    "source": "semantic_scholar",
    "score": 146.62917611526248
  },
  {
    "paperId": "6fdee901124c22565cf98ab521873f4129c19ac4",
    "url": "https://www.semanticscholar.org/paper/6fdee901124c22565cf98ab521873f4129c19ac4",
    "title": "A Multilevel Modular Converter for a Large, Light Weight Wind Turbine Generator",
    "abstract": "In an onshore horizontal axis wind turbine, generator and converter are usually in the nacelle on the top of the tower, while the grid step-up transformer is placed at the bottom. Electric power is transmitted down through flexible cables of high current rating which are expensive and can suffer from large I2 R loss. An offshore wind turbine usually has to include the step-up transformer in the nacelle. This adds significantly to the mechanical loading of the tower even new designs aim to reduce the transformer size and weight. In either case, a transformer-less, high voltage, high reliability generating unit for nacelle installation would be an attractive technology for large wind turbines. This study presents a power electronic solution based on a permanent magnet generator design. A multilevel cascaded voltage source converter is developed to synthesize a high sinusoidal output voltage. The dc link voltages of inverter modules are balanced by rectifiers fed from isolated generator coils while the inverter switching strategy equalizes the power sharing between the modules. The switching strategy also reduces the low order harmonics to constrain the sizing of the dc link capacitors. The modulating effect between the ac and dc sides of the inverter is taken into account. This paper describes the generator-converter arrangement, analyzes the inverter switching effects and derives the switching strategy which is verified by simulation and laboratory experiment.",
    "venue": "IEEE transactions on power electronics",
    "year": 2008,
    "citationCount": 214,
    "openAccessPdf": {
      "url": "https://dro.dur.ac.uk/6700/1/6700.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2008-05-02",
    "authors": [
      {
        "authorId": "2241764295",
        "name": "Chong H. Ng"
      },
      {
        "authorId": "2241723715",
        "name": "M. A. Parker"
      },
      {
        "authorId": "145378466",
        "name": "L. Ran"
      },
      {
        "authorId": "35139769",
        "name": "P. Tavner"
      },
      {
        "authorId": "9139076",
        "name": "J. Bumby"
      },
      {
        "authorId": "2241792762",
        "name": "Ed Spooner"
      }
    ],
    "source": "semantic_scholar",
    "score": 146.55957042191494
  },
  {
    "paperId": "cba71abe2a43910589ec7956f0974aa3caf1654d",
    "url": "https://www.semanticscholar.org/paper/cba71abe2a43910589ec7956f0974aa3caf1654d",
    "title": "30-100-GHz inductors and transformers for millimeter-wave (Bi)CMOS integrated circuits",
    "abstract": "Silicon planar and three-dimensional inductors and transformers were designed and characterized on-wafer up to 100 GHz. Self-resonance frequencies (SRFs) beyond 100 GHz were obtained, demonstrating for the first time that spiral structures are suitable for applications such as 60-GHz wireless local area network and 77-GHz automotive RADAR. Minimizing area over substrate is critical to achieving high SRF. A stacked transformer is reported with S/sub 21/ of -2.5 dB at 50 GHz, and which offers improved performance and less area (30 /spl mu/m/spl times/30 /spl mu/m) than planar transformers or microstrip couplers. A compact inductor model is described, along with a methodology for extracting model parameters from simulated or measured y-parameters. Millimeter-wave SiGe BiCMOS mixer and voltage-controlled-oscillator circuits employing spiral inductors are presented with better or comparable performance to previously reported transmission-line-based circuits.",
    "venue": "IEEE transactions on microwave theory and techniques",
    "year": 2005,
    "citationCount": 214,
    "openAccessPdf": {
      "url": "https://tspace.library.utoronto.ca/bitstream/1807/10045/1/Voinigescu_11437_2763.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2005-01-17",
    "authors": [
      {
        "authorId": "9359686",
        "name": "T. Dickson"
      },
      {
        "authorId": "31172698",
        "name": "Marc-Andre LaCroix"
      },
      {
        "authorId": "3021498",
        "name": "S. Boret"
      },
      {
        "authorId": "145506779",
        "name": "D. Gloria"
      },
      {
        "authorId": "9358162",
        "name": "R. Beerkens"
      },
      {
        "authorId": "34949769",
        "name": "S. Voinigescu"
      }
    ],
    "source": "semantic_scholar",
    "score": 146.55957042191494
  },
  {
    "paperId": "a23a4b04c487585792ecf45bce4ea43aa71bac2f",
    "url": "https://www.semanticscholar.org/paper/a23a4b04c487585792ecf45bce4ea43aa71bac2f",
    "title": "Learned Image Compression with Mixed Transformer-CNN Architectures",
    "abstract": "Learned image compression (LIC) methods have exhibited promising progress and superior rate-distortion performance compared with classical image compression standards. Most existing LIC methods are Convolutional Neural Networks-based (CNN-based) or Transformer-based, which have different advantages. Exploiting both advantages is a point worth exploring, which has two challenges: 1) how to effectively fuse the two methods? 2) how to achieve higher performance with a suitable complexity? In this paper, we propose an efficient parallel Transformer-CNN Mixture (TCM) block with a controllable complexity to incorporate the local modeling ability of CNN and the non-local modeling ability of transformers to improve the overall architecture of image compression models. Besides, inspired by the recent progress of entropy estimation models and attention modules, we propose a channel-wise entropy model with parameter-efficient swin-transformer-based attention (SWAtten) modules by using channel squeezing. Experimental results demonstrate our proposed method achieves state-of-the-art rate-distortion performances on three different resolution datasets (i.e., Kodak, Tecnick, CLIC Professional Validation) compared to existing LIC methods. The code is at https://github.com/jmliu206/LIC_TCM.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2023,
    "citationCount": 163,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2303.14978",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-03-27",
    "authors": [
      {
        "authorId": "2140269898",
        "name": "Jinming Liu"
      },
      {
        "authorId": "3294614",
        "name": "Heming Sun"
      },
      {
        "authorId": "1788294",
        "name": "J. Katto"
      }
    ],
    "source": "semantic_scholar",
    "score": 146.49799641736297
  },
  {
    "paperId": "1472ef1ae548849e7fecfd78c35fa65acd1aa81e",
    "url": "https://www.semanticscholar.org/paper/1472ef1ae548849e7fecfd78c35fa65acd1aa81e",
    "title": "A Single-Stage Single-Phase Transformer-Less Doubly Grounded Grid-Connected PV Interface",
    "abstract": "A transformer provides galvanic isolation and grounding of the photovoltaic (PV) array in a PV-fed grid-connected inverter. Inclusion of the transformer, however, may increase the cost and/or bulk of the system. To overcome this drawback, a single-phase, single-stage [no extra converter for voltage boost or maximum power point tracking (MPPT)], doubly grounded, transformer-less PV interface, based on the buck-boost principle, is presented. The configuration is compact and uses lesser components. Only one (undivided) PV source and one buck-boost inductor are used and shared between the two half cycles, which prevents asymmetrical operation and parameter mismatch problems. Total harmonic distortion and DC component of the current supplied to the grid is low, compared to existing topologies and conform to standards like IEEE 1547. A brief review of the existing, transformer-less, grid-connected inverter topologies is also included. It is demonstrated that, as compared to the split PV source topology, the proposed configuration is more effective in MPPT and array utilization. Design and analysis of the inverter in discontinuous conduction mode is carried out. Simulation and experimental results are presented.",
    "venue": "IEEE transactions on energy conversion",
    "year": 2009,
    "citationCount": 211,
    "openAccessPdf": {
      "url": "http://dspace.library.iitb.ac.in/xmlui/bitstream/10054/8203/1/A%20Single%20Stage%20Single%20Phase%20Transformer%20.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": "2009-02-10",
    "authors": [
      {
        "authorId": "2113460294",
        "name": "H. Patel"
      },
      {
        "authorId": "145419812",
        "name": "V. Agarwal"
      }
    ],
    "source": "semantic_scholar",
    "score": 146.34879412008019
  },
  {
    "paperId": "6a5f3abbcdb97eb7d70d4a466e226f0208f17823",
    "url": "https://www.semanticscholar.org/paper/6a5f3abbcdb97eb7d70d4a466e226f0208f17823",
    "title": "Vibro-acoustic techniques to diagnose power transformers",
    "abstract": "This paper deals with new procedures based on vibro-acoustic techniques to diagnose power medium-voltage/low-voltage (MV/LV) transformers. The normal operating machines can be monitored and checked from a remote, dedicated diagnostic center where information is received directly from the field. The diagnostic method adopted and the experimental test results are reported. Tests have been performed either in the laboratory or directly in the field on transformers in normal operating conditions. The laboratory tests related the transformer vibrations to internal anomalies. The suggested monitoring and diagnostic system allows one to usefully schedule preventative maintenance, reduce costs, and improve the quality of power distribution.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2004,
    "citationCount": 210,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2004-01-07",
    "authors": [
      {
        "authorId": "145343258",
        "name": "C. Bartoletti"
      },
      {
        "authorId": "2037370015",
        "name": "M. Desiderio"
      },
      {
        "authorId": "2065561401",
        "name": "D. Di Carlo"
      },
      {
        "authorId": "32648389",
        "name": "G. Fazio"
      },
      {
        "authorId": "2592692",
        "name": "F. Muzi"
      },
      {
        "authorId": "46802223",
        "name": "G. Sacerdoti"
      },
      {
        "authorId": "2075889026",
        "name": "F. Salvatori"
      }
    ],
    "source": "semantic_scholar",
    "score": 146.277872002141
  },
  {
    "paperId": "21ec632156ee574dadbd69124976c71b66855792",
    "url": "https://www.semanticscholar.org/paper/21ec632156ee574dadbd69124976c71b66855792",
    "title": "Fundamental limits on energy transfer and circuit considerations for piezoelectric transformers",
    "abstract": "This work investigates fundamental limits on the electromechanical energy conversion capacity of piezoelectric transformers by considering a work cycle analysis. Fundamental limitations are imposed by maximum electric field strength, maximum surface charge density, maximum stress and maximum strain. The authors' analysis indicates that the mechanical stress limit is the effective constraint in typical PZT materials. For a specific PZT-5H sample considered, a mechanical stress-limited work cycle indicates that this material can handle 330 W/cm/sup 3/ at 100 kHz. The theory of operation of soft-switching resonant drive circuitry is then discussed. Experimental results on a soft-switching inverter incorporating no magnetic components are reported.",
    "venue": "PESC 98 Record. 29th Annual IEEE Power Electronics Specialists Conference (Cat. No.98CH36196)",
    "year": 1998,
    "citationCount": 207,
    "openAccessPdf": {
      "url": "http://power.eecs.berkeley.edu/publications/flynn_02.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "1998-05-17",
    "authors": [
      {
        "authorId": "97676680",
        "name": "Piezoelectric Transformers"
      },
      {
        "authorId": "2277464865",
        "name": "Anita M. Flynn"
      },
      {
        "authorId": "2257288157",
        "name": "Seth R. Sanders"
      }
    ],
    "source": "semantic_scholar",
    "score": 146.06307119551977
  },
  {
    "paperId": "4388c61f07580b76e42ddcc6da281b5d878527b9",
    "url": "https://www.semanticscholar.org/paper/4388c61f07580b76e42ddcc6da281b5d878527b9",
    "title": "An energy transmission system for an artificial heart using leakage inductance compensation of transcutaneous transformer",
    "abstract": "A power supply system using a transcutaneous transformer to power an artificial heart through intact skin has been designed and built. In order to realize both high voltage gain and minimum circulating current, compensation of leakage inductances on both sides of a transcutaneous transformer is proposed. A frequency region which realizes the robustness against coupling coefficient and load variation is identified. In this region, the converter has inherent advantages such as zero voltage switching (ZVS) or zero current switching (ZCS) of the switches, high voltage gain, minimum circulating current, and high efficiency.",
    "venue": "PESC Record. 27th Annual IEEE Power Electronics Specialists Conference",
    "year": 1996,
    "citationCount": 289,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "1996-06-23",
    "authors": [
      {
        "authorId": "2084288793",
        "name": "Gyu-Bum Joung"
      },
      {
        "authorId": "2254326243",
        "name": "Bo-Hyung Cho"
      }
    ],
    "source": "semantic_scholar",
    "score": 146.0482138447078
  },
  {
    "paperId": "fd3ac50b4849b8869e39682ad450ec0e6d93d028",
    "url": "https://www.semanticscholar.org/paper/fd3ac50b4849b8869e39682ad450ec0e6d93d028",
    "title": "Multilevel PWM inverters suitable for the use of stand-alone photovoltaic power systems",
    "abstract": "This paper presents a new multilevel pulse width-modulation (PWM) inverter scheme for the use of stand-alone photovoltaic systems. It consists of a PWM inverter, an assembly of LEVEL inverters, generating staircase output voltages, and cascaded transformers. To produce high-quality output voltage waves, it synthesizes a large number of output voltage levels using cascaded transformers, which have a series-connected secondary. By a suitable selection of the secondary turn-ratio of the transformer, the amplitude of an output voltage appears at the rate of an integer to an input dc source. Operational principles and analysis are illustrated in depth. The validity of the proposed system is verified through computer-aided simulations and experimental results using prototypes generating output voltages of an 11 level and a 29 level, respectively, and their results are compared with conventional counterparts.",
    "venue": "IEEE transactions on energy conversion",
    "year": 2005,
    "citationCount": 203,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2005-12-05",
    "authors": [
      {
        "authorId": "34783867",
        "name": "F. Kang"
      },
      {
        "authorId": "2262456513",
        "name": "Sung‐Jun Park"
      },
      {
        "authorId": "94849072",
        "name": "Su-Eog Cho"
      },
      {
        "authorId": "2185945683",
        "name": "Cheul-U. Kim"
      },
      {
        "authorId": "2289099091",
        "name": "Toshifumi Ise"
      }
    ],
    "source": "semantic_scholar",
    "score": 145.77179990766325
  },
  {
    "paperId": "58905f31d58c9a8896ea72b347b9d5f902d2aac2",
    "url": "https://www.semanticscholar.org/paper/58905f31d58c9a8896ea72b347b9d5f902d2aac2",
    "title": "A Comparative Study of Neural Network Efficiency in Power Transformers Diagnosis Using Dissolved Gas Analysis",
    "abstract": "This paper presents a comparative study of neural network (NN) efficiency for the detection of incipient faults in power transformers. The NN was trained according to five diagnosis criteria commonly used for dissolved gas analysis (DGA) in transformer insulating oil. These criteria are Doemenburg, modified Rogers, Rogers, IEC, and CSUS. Once trained, the NN was tested by using a new set of DGA results. Finally, NN diagnosis results were compared with those obtained by inspection and analysis. The study shows that the NN rate of successful diagnosis is dependant on the criterion under consideration, with values in the range of 87-100%.",
    "venue": "IEEE Power Engineering Review",
    "year": 2001,
    "citationCount": 199,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2001-07-01",
    "authors": [
      {
        "authorId": "13243942",
        "name": "J. L. Guardado"
      },
      {
        "authorId": "143749498",
        "name": "J. L. Naredo"
      },
      {
        "authorId": "2262251046",
        "name": "Pablo Moreno"
      },
      {
        "authorId": "50189560",
        "name": "C. Fuerte"
      }
    ],
    "source": "semantic_scholar",
    "score": 145.47476049822055
  },
  {
    "paperId": "f6a6e3bd9e0fcedd70df07a3a1c721f7f2ae6d3a",
    "url": "https://www.semanticscholar.org/paper/f6a6e3bd9e0fcedd70df07a3a1c721f7f2ae6d3a",
    "title": "A simple setup to simultaneously measure the resonant frequency and the absolute dissipation factor of a quartz crystal microbalance",
    "abstract": "An experimental setup is described that can simultaneously measure the absolute dissipation factor and the resonant frequency of a short‐circuited quartz crystal microbalance. The crystal is driven at approximately its resonant frequency by a signal generator which is intermittently disconnected by a relay, causing the crystal oscillation amplitude to decay exponentially. The decay is measured using a ferrite toroid transformer. One of the crystal leads is fed through the center of the ferrite toroid and thereby acts as the primary winding of the transformer. The secondary winding of the transformer is connected to a digitizing oscilloscope which records the decay of the crystal oscillation. From the recorded decay curve, the absolute dissipation factor (calculated from the decay time constant) and the series resonant frequency of the freely oscillating crystal are obtained. Alternatively, the dissipation factor and resonant frequency can be measured for the crystal oscillating under open‐circuit conditio...",
    "venue": "",
    "year": 1996,
    "citationCount": 297,
    "openAccessPdf": {
      "url": "https://pubs.aip.org/aip/rsi/article-pdf/67/9/3238/11217522/3238_1_online.pdf",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Physics"
    ],
    "publicationTypes": null,
    "publicationDate": "1996-09-01",
    "authors": [
      {
        "authorId": "6179976",
        "name": "M. Rodahl"
      },
      {
        "authorId": "4114037",
        "name": "B. Kasemo"
      }
    ],
    "source": "semantic_scholar",
    "score": 145.45640229758106
  },
  {
    "paperId": "9821022fc6ee9e4629fa0da731034d27dd384b05",
    "url": "https://www.semanticscholar.org/paper/9821022fc6ee9e4629fa0da731034d27dd384b05",
    "title": "Optimized transformer design: inclusive of high-frequency effects",
    "abstract": "Switching circuits, operating at high frequencies, have led to considerable reductions in the size of magnetic components and power supplies. Nonsinusoidal voltage and current waveforms and high-frequency skin and proximity effects contribute to power transformer losses. Traditionally, power transformer design has been based on sinusoidal voltage and current waveforms operating at low frequencies. The physical and electrical properties of the transformer form the basis of a new design methodology while taking full account of the current and voltage waveforms and high-frequency effects. Core selection is based on the optimum throughput of energy with minimum losses. The optimum core is found directly from the following transformer specifications: frequency; power output; and temperature rise. The design methodology is illustrated with a detailed design of a push-pull power converter.",
    "venue": "",
    "year": 1998,
    "citationCount": 294,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1998-07-01",
    "authors": [
      {
        "authorId": "2912114",
        "name": "W. G. Hurley"
      },
      {
        "authorId": "2238975736",
        "name": "W. H. Ẅolfle"
      },
      {
        "authorId": "1707342",
        "name": "J. Breslin"
      }
    ],
    "source": "semantic_scholar",
    "score": 145.3046303450973
  },
  {
    "paperId": "7628d330e43103246928a1ed38ab47f344443a1d",
    "url": "https://www.semanticscholar.org/paper/7628d330e43103246928a1ed38ab47f344443a1d",
    "title": "Complete transformer model for electromagnetic transients",
    "abstract": "A complete, three phase transformer model for the calculation of electromagnetic transients is presented. The model consists of a set of state equations solved with the trapezoidal rule of integration in order to obtain an equivalent Norton circuit at the transformer terminals. Thus the transformer model can be easily interfaced with an electromagnetic transients program. Its main features are: (a) the basic elements for the winding model are the turns; (b) the complete model includes the losses due to eddy currents in the windings and in the iron core; and (c) the solution of the state equations is obtained in decoupled iterations. For validation, the frequency response of the model is compared with tests on several transformers. Applications to the calculation of transients are given for illustration. >",
    "venue": "",
    "year": 1994,
    "citationCount": 293,
    "openAccessPdf": {
      "url": "https://tspace.library.utoronto.ca/bitstream/1807/9971/1/Semlyen_9842_2826.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "145821325",
        "name": "F. D. León"
      },
      {
        "authorId": "144406566",
        "name": "A. Semlyen"
      }
    ],
    "source": "semantic_scholar",
    "score": 145.2536965100802
  },
  {
    "paperId": "d214e6dc314a6674d4af2206dd989fca73128e56",
    "url": "https://www.semanticscholar.org/paper/d214e6dc314a6674d4af2206dd989fca73128e56",
    "title": "The Impact of Inrush Currents on the Mechanical Stress of High-Voltage Power Transformer Coils",
    "abstract": "From failure experience on power transformers very often it was suspected that inrush currents, occurring when energizing unloaded transformers, were the reason for damage. In this paper it was investigated how mechanical forces within the transformer coils build up under inrush compared to those occurring at short circuit. 2D and 3D computer modeling for a real 268 MVA, 525/17.75 kV three-legged step up transformer were employed. The results show that inrush current peaks of 70% of the rated short circuit current cause local forces in the same order of magnitude as those at short circuit. The resulting force summed up over the high voltage coil is even three times higher. Although inrush currents are normally smaller, the forces can have similar amplitudes as those at short circuit, with longer exposure time, however. Therefore, care has to be taken to avoid such high inrush currents. Today controlled switching offers an elegant and practical solution.",
    "venue": "IEEE Power Engineering Review",
    "year": 2001,
    "citationCount": 193,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2001-10-01",
    "authors": [
      {
        "authorId": "5843982",
        "name": "M. Steurer"
      },
      {
        "authorId": "31038736",
        "name": "K. Frohlich"
      }
    ],
    "source": "semantic_scholar",
    "score": 145.01787238594994
  },
  {
    "paperId": "4a7530bbaee7563ee244f3ffed6b706bd96f08a8",
    "url": "https://www.semanticscholar.org/paper/4a7530bbaee7563ee244f3ffed6b706bd96f08a8",
    "title": "Trained Transformers Learn Linear Models In-Context",
    "abstract": "Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares. Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this global minimum, when given a test prompt of labeled examples from a new prediction task, the transformer achieves prediction error competitive with the best linear predictor over the test prompt distribution. We additionally characterize the robustness of the trained transformer to a variety of distribution shifts and show that although a number of shifts are tolerated, shifts in the covariate distribution of the prompts are not. Motivated by this, we consider a generalized ICL setting where the covariate distributions can vary across prompts. We show that although gradient flow succeeds at finding a global minimum in this setting, the trained transformer is still brittle under mild covariate shifts. We complement this finding with experiments on large, nonlinear transformer architectures which we show are more robust under covariate shifts.",
    "venue": "Journal of machine learning research",
    "year": 2023,
    "citationCount": 147,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2306.09927",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-06-16",
    "authors": [
      {
        "authorId": null,
        "name": "Ruiqi Zhang"
      },
      {
        "authorId": "120443772",
        "name": "Spencer Frei"
      },
      {
        "authorId": "1745169",
        "name": "P. Bartlett"
      }
    ],
    "source": "semantic_scholar",
    "score": 144.95818410646172
  },
  {
    "paperId": "d7cebdefea4feca6d606dfd51f001d24b88edffa",
    "url": "https://www.semanticscholar.org/paper/d7cebdefea4feca6d606dfd51f001d24b88edffa",
    "title": "An Artificial Neural Network Approach to Transformer Fault Diagnosis",
    "abstract": "This paper presents an artificial neural network (ANN) approach to the diagnosis and detection of faults in oil-filled power transformers based on dissolved gas-in-oil analysis. A two-step ANN method is used to detect faults with or without cellulose involved. Good diagnosis accuracy is obtained with the proposed approach.",
    "venue": "IEEE Power Engineering Review",
    "year": 1996,
    "citationCount": 373,
    "openAccessPdf": {
      "url": "https://vtechworks.lib.vt.edu/bitstreams/9bce9f02-3dc2-4b68-914f-9576c7a6800a/download",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1996-10-01",
    "authors": [
      {
        "authorId": "49890707",
        "name": "Y. Zhang"
      },
      {
        "authorId": "2148783095",
        "name": "X. Ding"
      },
      {
        "authorId": "2144469541",
        "name": "Yilu Liu"
      },
      {
        "authorId": "144256078",
        "name": "P. J. Griffin"
      }
    ],
    "source": "semantic_scholar",
    "score": 144.863836961218
  },
  {
    "paperId": "d7cebdefea4feca6d606dfd51f001d24b88edffa",
    "url": "https://www.semanticscholar.org/paper/d7cebdefea4feca6d606dfd51f001d24b88edffa",
    "title": "An Artificial Neural Network Approach to Transformer Fault Diagnosis",
    "abstract": "This paper presents an artificial neural network (ANN) approach to the diagnosis and detection of faults in oil-filled power transformers based on dissolved gas-in-oil analysis. A two-step ANN method is used to detect faults with or without cellulose involved. Good diagnosis accuracy is obtained with the proposed approach.",
    "venue": "IEEE Power Engineering Review",
    "year": 1996,
    "citationCount": 373,
    "openAccessPdf": {
      "url": "https://vtechworks.lib.vt.edu/bitstreams/9bce9f02-3dc2-4b68-914f-9576c7a6800a/download",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1996-10-01",
    "authors": [
      {
        "authorId": "49890707",
        "name": "Y. Zhang"
      },
      {
        "authorId": "2148783095",
        "name": "X. Ding"
      },
      {
        "authorId": "2144469541",
        "name": "Yilu Liu"
      },
      {
        "authorId": "144256078",
        "name": "P. J. Griffin"
      }
    ],
    "source": "semantic_scholar",
    "score": 144.863836961218
  },
  {
    "paperId": "adeb1d6220b3bb505479b0a152a08ac7faebc686",
    "url": "https://www.semanticscholar.org/paper/adeb1d6220b3bb505479b0a152a08ac7faebc686",
    "title": "Geomagnetically Induced Current Effects on Transformers",
    "abstract": "Geomagnetically induced currents (GIC) can cause saturation of the magnetic circuit of transformers in a power system. This saturation can increase the Mvar absorption of the transformers leading to voltage control problems, generate significant harmonic currents and cause heating of the internal components of the transformer itself, leading to gas relay alarm/operation as well as possible damage. This paper sets out the methods used to examine these effects using a mathematical model explicitly incorporating both the electric and magnetic circuits, including the shunting effect of the tank to predict the current and flux waveforms. The model has been used to predict GIC effects for a variety of winding connections for single-, three-, and five-limb core type transformers connected to the National Grid Company plc transmission system in England and Wales. The size and form of the return limbs together with the tank shunting effect determine the magnitude and the often complex shape of the waveforms resulting from GIC. Field and factory dc injection tests on various types of transformers have been conducted to validate the model and gain an insight into the magnetic behavior of transformers. With the aid of finite element analysis techniques and a consideration of the various constructional arrangements of the core and coils in the tank, it is possible to evaluate the power losses and accompanying temperature rises of the core, structural components, windings and tank. Some guidance on the acceptable GIC current levels for various transformer types is given.",
    "venue": "IEEE Power Engineering Review",
    "year": 2002,
    "citationCount": 190,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2002-12-10",
    "authors": [
      {
        "authorId": "20806447",
        "name": "P. Price"
      }
    ],
    "source": "semantic_scholar",
    "score": 144.78410142069944
  },
  {
    "paperId": "cbaf689fd9ea9bc939510019d90535d6249b3367",
    "url": "https://www.semanticscholar.org/paper/cbaf689fd9ea9bc939510019d90535d6249b3367",
    "title": "Jamba: A Hybrid Transformer-Mamba Language Model",
    "abstract": "We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine Transformer and Mamba layers, and how to mix experts, and show that some of them are crucial in large scale modeling. We also describe several interesting properties of these architectures which the training and evaluation of Jamba have revealed, and plan to release checkpoints from various ablation runs, to encourage further exploration of this novel architecture. We make the weights of our implementation of Jamba publicly available under a permissive license.",
    "venue": "arXiv.org",
    "year": 2024,
    "citationCount": 145,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2024-03-28",
    "authors": [
      {
        "authorId": "2654727",
        "name": "Opher Lieber"
      },
      {
        "authorId": "1412384990",
        "name": "Barak Lenz"
      },
      {
        "authorId": "1753618003",
        "name": "Hofit Bata"
      },
      {
        "authorId": "2294173960",
        "name": "Gal Cohen"
      },
      {
        "authorId": "1679842019",
        "name": "Jhonathan Osin"
      },
      {
        "authorId": "1491822146",
        "name": "Itay Dalmedigos"
      },
      {
        "authorId": "2294173872",
        "name": "Erez Safahi"
      },
      {
        "authorId": "2166499826",
        "name": "S. Meirom"
      },
      {
        "authorId": "2083259",
        "name": "Yonatan Belinkov"
      },
      {
        "authorId": "1389955537",
        "name": "Shai Shalev-Shwartz"
      },
      {
        "authorId": "2769805",
        "name": "Omri Abend"
      },
      {
        "authorId": "2294173649",
        "name": "Raz Alon"
      },
      {
        "authorId": "2188707683",
        "name": "Tomer Asida"
      },
      {
        "authorId": "2294173925",
        "name": "Amir Bergman"
      },
      {
        "authorId": "2294173488",
        "name": "Roman Glozman"
      },
      {
        "authorId": "2294173786",
        "name": "Michael Gokhman"
      },
      {
        "authorId": "2186741185",
        "name": "Avshalom Manevich"
      },
      {
        "authorId": "2148471161",
        "name": "Nir Ratner"
      },
      {
        "authorId": "2123595984",
        "name": "N. Rozen"
      },
      {
        "authorId": "2294173705",
        "name": "Erez Shwartz"
      },
      {
        "authorId": "2294173866",
        "name": "Mor Zusman"
      },
      {
        "authorId": "1701353",
        "name": "Y. Shoham"
      }
    ],
    "source": "semantic_scholar",
    "score": 144.75409932562505
  },
  {
    "paperId": "3cefa52c40639cd3b032042fec8a743828485bac",
    "url": "https://www.semanticscholar.org/paper/3cefa52c40639cd3b032042fec8a743828485bac",
    "title": "Asset-Management of Transformers Based on Condition Monitoring and Standard Diagnosis [Feature Article]",
    "abstract": "In this paper, a methodology is developed to use data acquisition derived from condition monitoring and standard diagnosis for rehabilitation purposes of transformers. The interpretation and understanding of the test data are obtained from international test standards to determine the current condition of transformers. In an attempt to ascertain monitoring priorities, the effective test methods are selected for transformer diagnosis. In particular, the standardization of diagnostic and analytical techniques are being improved that will enable field personnel to more easily use the test results and will reduce the need for interpretation by experts. In addition, the advanced method has the potential to reduce the time greatly and increase the accuracy of diagnostics. The important aim of the standardization is to develop the multiple diagnostic models that combine results from the different tests and give an overall assessment of reliability and maintenance for transformers.",
    "venue": "IEEE Electrical Insulation Magazine",
    "year": 2008,
    "citationCount": 183,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2008-07-29",
    "authors": [
      {
        "authorId": "2239506648",
        "name": "Zhang Xiang"
      },
      {
        "authorId": "2239499479",
        "name": "Ernst Gockenbach"
      }
    ],
    "source": "semantic_scholar",
    "score": 144.2240363641348
  },
  {
    "paperId": "6f5f4d791c02615bdea097bca15dd37554a4dd79",
    "url": "https://www.semanticscholar.org/paper/6f5f4d791c02615bdea097bca15dd37554a4dd79",
    "title": "Modeling and protection of a three-phase power transformer using wavelet packet transform",
    "abstract": "This paper introduces a novel algorithm for differential protection of three-phase power transformers that is based on the wavelet packet transform. The wavelet packet transform is employed to extract certain features of the differential current to distinguish between the magnetizing inrush and different internal fault currents. The selection of the optimal wavelet analysis that includes selecting both the optimal mother wavelet and the optimal number of levels of resolution is carried out using the minimum description length (MDL) data criteria. The proposed algorithm is tested off-line using data collected from a prototype laboratory three-phase power transformer. The test results show reduced computational burden, high speed and high accuracy.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2005,
    "citationCount": 180,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Mathematics"
    ],
    "publicationTypes": null,
    "publicationDate": "2005-04-04",
    "authors": [
      {
        "authorId": "145722452",
        "name": "S. Saleh"
      },
      {
        "authorId": "2157555657",
        "name": "M. Rahman"
      }
    ],
    "source": "semantic_scholar",
    "score": 143.9774554689874
  },
  {
    "paperId": "e8281126dd8f490f76230c31b74339082f29c948",
    "url": "https://www.semanticscholar.org/paper/e8281126dd8f490f76230c31b74339082f29c948",
    "title": "A wavelet-based technique for discrimination between faults and magnetising inrush currents in transformers",
    "abstract": "Summary form only given, as follows. This paper presents the development of a wavelet-based scheme, for distinguishing between transformer inrush currents and power system fault currents, which proved to provide a reliable, fast and computationally efficient tool. The operating time of the scheme is less than half power frequency cycle (based on 5 kHz sampling rate). In this work, a wavelet transform concept is presented. Feature extraction and method of discrimination between transformer inrush and fault currents is derived. A 132/11 kV, transformer connected to a 132 kV power system were simulated using EMTP. The generated data were used by the MATLAB to test the performance of the technique as to its speed of response, computational burden and reliability. The proposed scheme proved to be reliable, accurate and fast.",
    "venue": "IEEE Power Engineering Society Summer Meeting,",
    "year": 2002,
    "citationCount": 178,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2002-06-01",
    "authors": [
      {
        "authorId": "153366882",
        "name": "O. Youssef"
      }
    ],
    "source": "semantic_scholar",
    "score": 143.81078708761132
  },
  {
    "paperId": "56b7fe7eb3ddf81760ea4dbdce43bb07b17701ef",
    "url": "https://www.semanticscholar.org/paper/56b7fe7eb3ddf81760ea4dbdce43bb07b17701ef",
    "title": "An Integrated Neural Fuzzy Approach for Fault Diagnosis of Transformers",
    "abstract": "This paper presents a new and efficient integrated neural fuzzy approach for transformer fault diagnosis using dissolved gas analysis. The proposed approach formulates the modeling problem of higher dimensions into lower dimensions by using the input feature selection based on competitive learning and neural fuzzy model. Then, the fuzzy rule base for the identification of fault is designed by applying the subtractive clustering method which is very good at handling the noisy input data. Verification of the proposed approach has been carried out by testing on standard and practical data. In comparison to the results obtained from the existing conventional and neural fuzzy techniques, the proposed method has been shown to possess superior performance in identifying the transformer fault type.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2008,
    "citationCount": 174,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2008-09-23",
    "authors": [
      {
        "authorId": "2248552404",
        "name": "Ram Naresh"
      },
      {
        "authorId": "2240745393",
        "name": "Veena Sharma"
      },
      {
        "authorId": "2121094120",
        "name": "M. Vashisth"
      }
    ],
    "source": "semantic_scholar",
    "score": 143.4717896088527
  },
  {
    "paperId": "bcdcdf7dac5c0cb678004e9727c4fde032646719",
    "url": "https://www.semanticscholar.org/paper/bcdcdf7dac5c0cb678004e9727c4fde032646719",
    "title": "Procedures for detecting winding displacements in power transformers by the transfer function method",
    "abstract": "The paper investigates three different ways of using the transfer function method for detecting mechanical winding displacements in power transformers. The most reliable approach is time-based comparison , which requires finger print data from a previous measurement. Such information is, however, usually not available. For multilegged transformers without zigzag-connected windings the results of separately tested legs can be used as mutual references (construction-based comparison ). A third approach is to compare the transfer functions with those obtained from an identically constructed transformer ( type-based comparison). However, for a transformer with given nominal specification data, the winding design may over time undergo changes which causes changes to the transfer function. It is proposed to solve this problem by calculating tolerance bands using transfer functions from a big group of the same-type transformers. A novel statistical algorithm for this purpose is presented. The approach is demonstrated for a set of 28 specified identically 200-MVA power transformers.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2004,
    "citationCount": 173,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2004-01-07",
    "authors": [
      {
        "authorId": "50827864",
        "name": "J. Christian"
      },
      {
        "authorId": "48784750",
        "name": "K. Feser"
      }
    ],
    "source": "semantic_scholar",
    "score": 143.38582948821795
  },
  {
    "paperId": "3567d8120e01aabb59805db1c3e3ba77c95ca590",
    "url": "https://www.semanticscholar.org/paper/3567d8120e01aabb59805db1c3e3ba77c95ca590",
    "title": "Analysis , Design , and Optimization of Spiral Inductors and Transformers for Si RF IC ’ s",
    "abstract": "Silicon integrated circuit spiral inductors and transformers are analyzed using electromagnetic analysis. With appropriate approximations, the calculations are reduced to electrostatic and magnetostatic calculations. The important effects of substrate loss are included in the analysis. Classic circuit analysis and network analysis techniques are used to derive two-port parameters from the circuits. From two-port measurements, loworder, frequency-independent lumped circuits are used to model the physical behavior over a broad-frequency range. The analysis is applied to traditional square and polygon inductors and transformer structures as well as to multilayer metal structures and coupled inductors. A custom computer-aided-design tool called ASITIC is described, which is used for the analysis, design, and optimization of these structures. Measurements taken over a frequency range from 100 MHz to 5 GHz show good agreement with theory.",
    "venue": "",
    "year": 1998,
    "citationCount": 258,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2868132",
        "name": "A. Niknejad"
      },
      {
        "authorId": "2723555",
        "name": "R. Meyer"
      }
    ],
    "source": "semantic_scholar",
    "score": 143.35242092549305
  },
  {
    "paperId": "75bb545aa5e029f03ddda8bf048af09cc231be87",
    "url": "https://www.semanticscholar.org/paper/75bb545aa5e029f03ddda8bf048af09cc231be87",
    "title": "A review of paper aging in power transformers",
    "abstract": "As part of a comprehensive study of transformer behaviour, the factors which control the aging processes in transformer paper have been reviewed experimentally. Three different types of paper have been used: 60/40 manilla/wood, all-wood and thermally upgraded paper. Aging has been done in vitro under oil in atmospheres of nitrogen and air with controlled water contents. The aging temperatures were in the range 110-140°C with aging periods up to 1.5 years. Paper condition was monitored by measuring its degree of polymerisation (DP) checked by burst strength. In most of the experiments normal BS 148 oil was used, but some experiments were done to examine the effect of oil preservatives on paper aging. Opportunity was taken to compare the furfuraldehyde contents of oils from the aging cells with paper condition. It is confirmed that paper aging is highly dependent on temperature and the presence of water and oxygen, although the aging of the thermally upgraded paper examined is virtually unaffected by water and only slightly by oxygen.",
    "venue": "",
    "year": 1985,
    "citationCount": 257,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Materials Science"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": "1985-11-01",
    "authors": [
      {
        "authorId": "94424009",
        "name": "D. Shroff"
      },
      {
        "authorId": "145168825",
        "name": "A. W. Stannett"
      }
    ],
    "source": "semantic_scholar",
    "score": 143.29439377382425
  },
  {
    "paperId": "81328e6a2370ae59aef52a62617f9701865e71bd",
    "url": "https://www.semanticscholar.org/paper/81328e6a2370ae59aef52a62617f9701865e71bd",
    "title": "A high frequency transformer model for the EMTP",
    "abstract": "A model to simulate the high-frequency behavior of a power transformer is presented. This model is based on the frequency characteristics of the transformer admittance matrix between its terminals over a given range of frequencies. The transformer admittance characteristics can be obtained from measurements or from detailed internal models based on the physical layout of the transformer. The elements of the nodal admittance matrix are approximated with rational functions consisting of real as well as complex conjugate poles and zeroes. These approximations are realized in the form of an RLC network in a format suitable for direct use with the ElectroMagnetics Transient Program (EMTP). The high-frequency transformer model can be used as a stand-alone linear model or as an add-on module of a more comprehensive model where iron core nonlinearities are represented in detail. >",
    "venue": "",
    "year": 1993,
    "citationCount": 256,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1993-07-01",
    "authors": [
      {
        "authorId": "12084982",
        "name": "A. Morched"
      },
      {
        "authorId": "144275135",
        "name": "L. Marti"
      },
      {
        "authorId": "94796085",
        "name": "J. Ottevangers"
      }
    ],
    "source": "semantic_scholar",
    "score": 143.2361412734283
  },
  {
    "paperId": "70667a97a6b42eb6aff5729219fab64b7ae6fef2",
    "url": "https://www.semanticscholar.org/paper/70667a97a6b42eb6aff5729219fab64b7ae6fef2",
    "title": "of Energy",
    "abstract": "Summary — In this paper, the developed adaptive linearly constrained minimum variance finite focus beamforming method is used to increase the accuracy and resolution of low-frequency transformer sound source localization. In the first step, simulations on vir - tual arrays are conducted to define the array size and compare the influence of three different beamforming algorithms on sound loca - lization accuracy and resolution. In the second step, measurements of the regulating transformer as a noise source are conducted in the semi-anechoic chamber. A large rectangular microphone array, with 90 microphone positions spaced 0.44 meters apart, is used for that purpose. The developed algorithm is compared with infinite and fi - nite focus beamforming for the localization of total noise and noise at specific noise harmonics.",
    "venue": "",
    "year": null,
    "citationCount": 491,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "23683639",
        "name": "Karlo Petrović"
      },
      {
        "authorId": "2289263924",
        "name": "Antonio Petošić"
      },
      {
        "authorId": "2281551537",
        "name": "Tomislav Župan"
      }
    ],
    "source": "semantic_scholar",
    "score": 142.97718074738464
  },
  {
    "paperId": "9a07d18a2bf1ce1f593e63ff2344f9e1937a2d25",
    "url": "https://www.semanticscholar.org/paper/9a07d18a2bf1ce1f593e63ff2344f9e1937a2d25",
    "title": "Transformer tank vibration modeling as a method of detecting winding deformations-part II: experimental verification",
    "abstract": "In Part I of the paper, a tank vibration model was proposed as a method to detect the winding deformations in power transformers. This model is incorporated in a model-based monitoring system for power transformers. In this paper, the experimental verification of the proposed model that calculates vibration on the transformer tank is reported. The model was validated in a 1500-kVA experimental transformer constructed as a reduced scale model of a 60-MVA 220-kV unit. In order to load the test transformer, the opposition method described in IEC 60076-2 Standard was used allowing to vary the load and power factor over a wide range. Sensors to measure vibrations and temperature were installed in the test transformer. The model was validated under different test transformer operating conditions. In order to verify the model's ability to detect failures, a deformation was provoked in the test transformer winding. Model predictions were compared with the measured vibration in that situation. The model has also been applied to four (30-40 MVA) grid transformers. Some results of this field validation are presented in this paper.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2006,
    "citationCount": 168,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "46889527",
        "name": "B. García"
      },
      {
        "authorId": "39177274",
        "name": "J. Burgos"
      },
      {
        "authorId": "47642155",
        "name": "A. Alonso"
      }
    ],
    "source": "semantic_scholar",
    "score": 142.9484807238461
  },
  {
    "paperId": "a7fa71dc6856ebef79f354597128d1c68b19b6e4",
    "url": "https://www.semanticscholar.org/paper/a7fa71dc6856ebef79f354597128d1c68b19b6e4",
    "title": "Transformers as Algorithms: Generalization and Stability in In-context Learning",
    "abstract": "In-context learning (ICL) is a type of prompting where a transformer model operates on a sequence of (input, output) examples and performs inference on-the-fly. In this work, we formalize in-context learning as an algorithm learning problem where a transformer model implicitly constructs a hypothesis function at inference-time. We first explore the statistical aspects of this abstraction through the lens of multitask learning: We obtain generalization bounds for ICL when the input prompt is (1) a sequence of i.i.d. (input, label) pairs or (2) a trajectory arising from a dynamical system. The crux of our analysis is relating the excess risk to the stability of the algorithm implemented by the transformer. We characterize when transformer/attention architecture provably obeys the stability condition and also provide empirical verification. For generalization on unseen tasks, we identify an inductive bias phenomenon in which the transfer learning risk is governed by the task complexity and the number of MTL tasks in a highly predictable manner. Finally, we provide numerical evaluations that (1) demonstrate transformers can indeed implement near-optimal algorithms on classical regression problems with i.i.d. and dynamic data, (2) provide insights on stability, and (3) verify our theoretical predictions.",
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "citationCount": 124,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-01-17",
    "authors": [
      {
        "authorId": "1527089987",
        "name": "Yingcong Li"
      },
      {
        "authorId": "46214352",
        "name": "M. E. Ildiz"
      },
      {
        "authorId": "1740595",
        "name": "Dimitris Papailiopoulos"
      },
      {
        "authorId": "3103394",
        "name": "Samet Oymak"
      }
    ],
    "source": "semantic_scholar",
    "score": 142.42470605953451
  },
  {
    "paperId": "6de7c6a0f5eef30394583a5c8d7e497931c51d24",
    "url": "https://www.semanticscholar.org/paper/6de7c6a0f5eef30394583a5c8d7e497931c51d24",
    "title": "Analysis of zig-zag transformer applying in the three-phase four-wire distribution power system",
    "abstract": "The load unbalance and the nonlinear loads result in a significant neutral current in the three-phase four-wire distribution power system. The Zig-Zag transformer has been proposed to attenuate the neutral current of the three-phase four-wire distribution power system. In this paper, an analysis is carried out and computer simulation is used to evaluate the performance of the Zig-Zag transformer under ideal and nonideal power conditions. The simulation results show that (a) the Zig-Zag transformer can effectively attenuate the neutral current and zero-sequence harmonic currents on the utility side under the balanced utility voltage, (b) the utility side neutral current becomes larger under the unbalanced utility voltage or the distorted utility voltage with zero sequence harmonic components after applying the Zig-Zag transformer, (c) the insertion of an inductor in the utility side of the neutral conductor can alleviate overloading of the neutral current caused by the unbalanced utility voltages and the distorted utility voltages with zero sequence harmonic components.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2005,
    "citationCount": 160,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2005-04-04",
    "authors": [
      {
        "authorId": "2608573",
        "name": "H. Jou"
      },
      {
        "authorId": "2537151",
        "name": "Jinn-Chang Wu"
      },
      {
        "authorId": "2878624",
        "name": "Kuen-Der Wu"
      },
      {
        "authorId": "34716100",
        "name": "Wen-Jung Chiang"
      },
      {
        "authorId": "2116613614",
        "name": "Yi-Hsun Chen"
      }
    ],
    "source": "semantic_scholar",
    "score": 142.22106547476693
  },
  {
    "paperId": "89e30fbaf61c538b8f7422345e57483c31bc9d90",
    "url": "https://www.semanticscholar.org/paper/89e30fbaf61c538b8f7422345e57483c31bc9d90",
    "title": "A new method for generating families of continuous distributions",
    "abstract": null,
    "venue": "",
    "year": 2013,
    "citationCount": 905,
    "openAccessPdf": {
      "url": "https://link.springer.com/content/pdf/10.1007/s40300-013-0007-y.pdf",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Mathematics"
    ],
    "publicationTypes": null,
    "publicationDate": "2013-04-10",
    "authors": [
      {
        "authorId": "1971190",
        "name": "A. Alzaatreh"
      },
      {
        "authorId": "2108956872",
        "name": "Carl Lee"
      },
      {
        "authorId": "1790079",
        "name": "F. Famoye"
      }
    ],
    "source": "semantic_scholar",
    "score": 142.1355895906447
  },
  {
    "paperId": "3d21828a9507e1303dd0175f65df896014a60c4e",
    "url": "https://www.semanticscholar.org/paper/3d21828a9507e1303dd0175f65df896014a60c4e",
    "title": "Transformer Design and Optimization: A Literature Survey",
    "abstract": "With the fast-paced changing technologies in the power industry, new references addressing new technologies are coming to the market. Based on this fact, there is an urgent need to keep track of international experiences and activities taking place in the field of modern transformer design. The complexity of transformer design demands reliable and rigorous solution methods. A survey of current research reveals the continued interest in application of advanced techniques for transformer design optimization. This paper conducts a literature survey and reveals general backgrounds of research and developments in the field of transformer design and optimization for the past 35 years, based on more than 420 published articles, 50 transformer books, and 65 standards.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2009,
    "citationCount": 158,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": "2009-09-22",
    "authors": [
      {
        "authorId": "34613730",
        "name": "E. Amoiralis"
      },
      {
        "authorId": "35140669",
        "name": "M. Tsili"
      },
      {
        "authorId": "1783620",
        "name": "A. Kladas"
      }
    ],
    "source": "semantic_scholar",
    "score": 142.03356303330347
  },
  {
    "paperId": "c06188ee348bb2e58740e61edeadea97272250ec",
    "url": "https://www.semanticscholar.org/paper/c06188ee348bb2e58740e61edeadea97272250ec",
    "title": "Hybrid Transformer Model for Transient Simulation—Part I: Development and Parameters",
    "abstract": "A new topologically correct hybrid transformer model is developed for low and midfrequency transient simulations. Power transformers have a conceptually simple design, but behaviors can be very complex. The selection of the most suitable representation for a given behavior depends on the type of transformer to be simulated, the frequency range, and other factors, such as the internal design of the transformer and available parameters or design data. Here, a modular model suitable for frequencies up to 3-5 kHz is developed, utilizing a duality-based lumped-parameter saturable core, matrix descriptions of leakage and capacitive effects, and frequency-dependent coil resistance. Implementation and testing of this model was done for 15-kVA 208Delta-120Y 3-legged and 150-kVA 12,470Y-208Y 5-legged transformers. The basis and development of the model is presented, along with a discussion of necessary parameters and the approaches for obtaining them",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2007,
    "citationCount": 157,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2007-06-24",
    "authors": [
      {
        "authorId": "1977120",
        "name": "B. Mork"
      },
      {
        "authorId": "144396438",
        "name": "F. González"
      },
      {
        "authorId": "3133492",
        "name": "D. Ishchenko"
      },
      {
        "authorId": "97915568",
        "name": "D. Stuehm"
      },
      {
        "authorId": "2169222930",
        "name": "J. Mitra"
      }
    ],
    "source": "semantic_scholar",
    "score": 141.9389254954045
  },
  {
    "paperId": "d52d0f1ab3bbf358ed7bcefa43bacc7fba292469",
    "url": "https://www.semanticscholar.org/paper/d52d0f1ab3bbf358ed7bcefa43bacc7fba292469",
    "title": "Power Supply for a High-Voltage Application",
    "abstract": "In this paper, the guidelines to design a high-voltage power converter based on the hybrid series parallel resonant topology, PRC-LCC, with a capacitor as output filter are established. As a consequence of the selection of this topology, transformer ratio, and therefore secondary volume, is reduced. The mathematical analysis provides an original equivalent circuit for the steady-state and dynamical behavior of the topology. A new way to construct high-voltage transformers is also proposed, pointing out the advantages and establishing an original method to evaluate the stray components of the transformer before construction. The way to make compatible the characteristics of both, topology and transformer is illustrated in the frame of a practical application. To demonstrate the feasibility of this solution, a high-voltage, high-power prototype is assembled and tested with good performance and similar behavior to the one predicted by the models. Experimental results are shown on this particular.",
    "venue": "IEEE transactions on power electronics",
    "year": 2008,
    "citationCount": 157,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2008-07-09",
    "authors": [
      {
        "authorId": "1398409172",
        "name": "J. Martín-Ramos"
      },
      {
        "authorId": "144740030",
        "name": "A. Pernía"
      },
      {
        "authorId": "2107140521",
        "name": "J. Díaz"
      },
      {
        "authorId": "144227410",
        "name": "F. Nuño"
      },
      {
        "authorId": "121751405",
        "name": "J.A. Martinez"
      }
    ],
    "source": "semantic_scholar",
    "score": 141.9389254954045
  },
  {
    "paperId": "40278531da7b90aaa454cf53046439174bf40510",
    "url": "https://www.semanticscholar.org/paper/40278531da7b90aaa454cf53046439174bf40510",
    "title": "Mathematical models for current, voltage, and coupling capacitor voltage transformers",
    "abstract": "This paper reviews a number of mathematical models used to represent the nonlinear behavior of the magnetic core of instrument transformers. Models of instrument transformers using these core representations are presented. The transient response of the instrument transformer is compared to actual test results recorded in the laboratory. The paper provides practical guidelines as to which of the physical elements of instrument transformers are important to model during transient studies and which elements could be ignored without sacrificing the accuracy of the simulation results. The electromagnetic transients program (EMTP) data files used to generate the models are also provided in the appendix to help new EMTP users model instrument transformers for evaluation of high-speed protective relaying systems.",
    "venue": "",
    "year": 2000,
    "citationCount": 230,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Physics",
      "Engineering"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "30573521",
        "name": "D. Tziouvaras"
      },
      {
        "authorId": "153565526",
        "name": "P. McLaren"
      },
      {
        "authorId": "34776025",
        "name": "G. Alexander"
      },
      {
        "authorId": "48010739",
        "name": "D. Dawson"
      },
      {
        "authorId": "9389641",
        "name": "J. Esztergalyos"
      },
      {
        "authorId": "37788033",
        "name": "C. Fromen"
      },
      {
        "authorId": "47084194",
        "name": "M. Glinkowski"
      },
      {
        "authorId": "31341082",
        "name": "I. Hasenwinkle"
      },
      {
        "authorId": "2998246",
        "name": "M. Kezunovic"
      },
      {
        "authorId": "2249630",
        "name": "L. Kojovic"
      },
      {
        "authorId": "97470317",
        "name": "Bill Kotheimer"
      },
      {
        "authorId": "9402191",
        "name": "R. Kuffel"
      },
      {
        "authorId": "153359724",
        "name": "J. Nordstrom"
      },
      {
        "authorId": "121729362",
        "name": "S. Zocholl"
      }
    ],
    "source": "semantic_scholar",
    "score": 141.6362656578269
  },
  {
    "paperId": "a841a2551ec918f6721f56669f6b4a4887d9f495",
    "url": "https://www.semanticscholar.org/paper/a841a2551ec918f6721f56669f6b4a4887d9f495",
    "title": "New Method for Reducing Harmonics Involved in Input and Output of Rectifier with Interphase Transformer",
    "abstract": "A new effective method for reducing harmonics involved in input ac line currents or output dc voltage of the thyristor rectifier apparatus with an interphase transformer is proposed. This method is based on the sophisticated utilization of several thyristors inserted in parallel between several taps equipped symmetrically for the midpoint of the interphase transformer and the terminal of the dc output circuit. The key point of this method can be understood easily when compared with conventional use of the interphase transformer, where the dc output circuit is taken from the midpoint of the interphase transformer directly. The features of this method are very simple and very economical in circuit construction. Their presentation will take the following form: 1) fundamental theory of the method; 2) analysis of waveforms of input ac line currents under optimum operation; 3) checking the validity of the theory by experiment using the six-phase 12-pulse thyristor polyphase rectifier apparatus; 4) various modifications of this method.",
    "venue": "IEEE transactions on industry applications",
    "year": 1986,
    "citationCount": 153,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1986-09-01",
    "authors": [
      {
        "authorId": "32982335",
        "name": "S. Miyairi"
      },
      {
        "authorId": "91578071",
        "name": "S. Iida"
      },
      {
        "authorId": "20310372",
        "name": "K. Nakata"
      },
      {
        "authorId": "4168298",
        "name": "S. Masukawa"
      }
    ],
    "source": "semantic_scholar",
    "score": 141.55428903620444
  },
  {
    "paperId": "8356d155d730e374f4db6dfd03d19a7b66c348a8",
    "url": "https://www.semanticscholar.org/paper/8356d155d730e374f4db6dfd03d19a7b66c348a8",
    "title": "CoTr: Efficiently Bridging CNN and Transformer for 3D Medical Image Segmentation",
    "abstract": null,
    "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
    "year": 2021,
    "citationCount": 444,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.03024",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2021-03-04",
    "authors": [
      {
        "authorId": "2154709897",
        "name": "Yutong Xie"
      },
      {
        "authorId": "49049926",
        "name": "Jianpeng Zhang"
      },
      {
        "authorId": "12459603",
        "name": "Chunhua Shen"
      },
      {
        "authorId": "144344681",
        "name": "Yong Xia"
      }
    ],
    "source": "semantic_scholar",
    "score": 141.47111423249362
  },
  {
    "paperId": "f3c1765cb163a26d970ad5438156191574d6ff52",
    "url": "https://www.semanticscholar.org/paper/f3c1765cb163a26d970ad5438156191574d6ff52",
    "title": "Optimized design of medium frequency transformers with high isolation requirements",
    "abstract": "For future DC electric power systems, high-power DC-DC converters will play a major role as they will substitute today's bulky 50/60Hz transformers. One key component within this DC-DC converters is the medium frequency transformer that provides the isolation level and the step up/down of the different voltage levels. As a consequence, an optimized design methodology that considers this high isolation requirements is needed. This paper presents a step-by-step design for medium frequency transformers with high isolation requirements. Each step in the design is carefully discussed and the required design considerations, such as flux density limits, isolation and thermal management, are explained in detail. The proposed design procedure is applied to a core-type transformer analyzing the outcome of the optimization process.",
    "venue": "Annual Conference of the IEEE Industrial Electronics Society",
    "year": 2010,
    "citationCount": 152,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2010-12-23",
    "authors": [
      {
        "authorId": "144788706",
        "name": "G. Ortiz"
      },
      {
        "authorId": "51951961",
        "name": "J. Biela"
      },
      {
        "authorId": "145303111",
        "name": "J. Kolar"
      }
    ],
    "source": "semantic_scholar",
    "score": 141.45656882088653
  },
  {
    "paperId": "a25b53a67397e4169cc63765c608473c4ddc3474",
    "url": "https://www.semanticscholar.org/paper/a25b53a67397e4169cc63765c608473c4ddc3474",
    "title": "Analysis of Very Fast Transients in Layer-Type Transformer Windings",
    "abstract": "This paper deals with the measurement, modeling, and simulation of very fast transient overvoltages in layer-type distribution transformer windings. Measurements were performed by applying a step impulse with 50-ns rise time on a single-phase test transformer equipped with measuring points along the winding. Voltages along the transformer windings were computed by applying multiconductor transmission-line theory for transformer layers and turns. Interturn voltage analysis has also been performed. Computations are performed by applying an inductance matrix determined in two different ways; by making use of the inverse capacitance matrix and by making use of the well known Maxwell formulas. The modeling of the transformer and the computations are verified by measurements",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2007,
    "citationCount": 151,
    "openAccessPdf": {
      "url": "https://repository.tudelft.nl/file/File_7e332c24-9b6c-4992-887d-f8a820e0d7bb",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "143634093",
        "name": "M. Popov"
      },
      {
        "authorId": "2578004",
        "name": "L. van der Sluis"
      },
      {
        "authorId": "144600750",
        "name": "R. Smeets"
      },
      {
        "authorId": "117795475",
        "name": "J. L. Roldan"
      }
    ],
    "source": "semantic_scholar",
    "score": 141.35820781269416
  },
  {
    "paperId": "5fce7d9442b06cab91174fb68ba52ff6bdaa29cc",
    "url": "https://www.semanticscholar.org/paper/5fce7d9442b06cab91174fb68ba52ff6bdaa29cc",
    "title": "A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks",
    "abstract": "Transformer is a deep neural network that employs a self-attention mechanism to comprehend the contextual relationships within sequential data. Unlike conventional neural networks or updated versions of Recurrent Neural Networks (RNNs) such as Long Short-Term Memory (LSTM), transformer models excel in handling long dependencies between input sequence elements and enable parallel processing. As a result, transformer-based models have attracted substantial interest among researchers in the field of artificial intelligence. This can be attributed to their immense potential and remarkable achievements, not only in Natural Language Processing (NLP) tasks but also in a wide range of domains, including computer vision, audio and speech processing, healthcare, and the Internet of Things (IoT). Although several survey papers have been published highlighting the transformer's contributions in specific fields, architectural differences, or performance evaluations, there is still a significant absence of a comprehensive survey paper encompassing its major applications across various domains. Therefore, we undertook the task of filling this gap by conducting an extensive survey of proposed transformer models from 2017 to 2022. Our survey encompasses the identification of the top five application domains for transformer-based models, namely: NLP, Computer Vision, Multi-Modality, Audio and Speech Processing, and Signal Processing. We analyze the impact of highly influential transformer-based models in these domains and subsequently classify them based on their respective tasks using a proposed taxonomy. Our aim is to shed light on the existing potential and future possibilities of transformers for enthusiastic researchers, thus contributing to the broader understanding of this groundbreaking technology.",
    "venue": "Expert systems with applications",
    "year": 2023,
    "citationCount": 115,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.07303",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-06-11",
    "authors": [
      {
        "authorId": "2220141409",
        "name": "Saidul Islam"
      },
      {
        "authorId": "66066494",
        "name": "Hanae Elmekki"
      },
      {
        "authorId": "2219976594",
        "name": "Ahmed Elsebai"
      },
      {
        "authorId": "1812107",
        "name": "J. Bentahar"
      },
      {
        "authorId": "2219976587",
        "name": "Najat Drawel"
      },
      {
        "authorId": "29807997",
        "name": "Gaith Rjoub"
      },
      {
        "authorId": "2184502084",
        "name": "W. Pedrycz"
      }
    ],
    "source": "semantic_scholar",
    "score": 141.30385286659546
  },
  {
    "paperId": "3a76ec811094804e2523471cb7352521afeeb0ac",
    "url": "https://www.semanticscholar.org/paper/3a76ec811094804e2523471cb7352521afeeb0ac",
    "title": "1 Megawatt, 20 kHz, isolated, bidirectional 12kV to 1.2kV DC-DC converter for renewable energy applications",
    "abstract": "The design of a 1 MW, 20 kHz, isolated, bidirectional 12kV to 1.2kV DC-DC converter for renewable energy applications is presented. The main topics addressed are: High-Voltage (HV) side switch, topology & modulation and Medium Frequency (MF) transformer. A study of the possible HV side switches, considering 4.5kV IGBTs is performed, fixing the requirements from the topology and modulation side in order to reach a highly efficient system. The studied topologies are the Dual Active Bridge (DAB) with triangular modulation and the Series Resonant Converter (SRC) with constant frequency operation. Both topologies are able to achieve Zero Current Switching (ZCS) in the HV side switches, reducing the switching losses in these devices, which contribute to a large share to the system losses. Efficiency curves are presented for different semiconductor technologies for the Low-Voltage (LV) side switch in order to study the trade-offs between the selected topologies. Three MF transformer concepts, namely core-type, shell-type and matrix transformer, are presented and compared in respect of winding arrangement, isolation mechanisms and thermal management. Power losses and volume are calculated in each case and used to compare the different transformer concepts.",
    "venue": "The 2010 International Power Electronics Conference - ECCE ASIA -",
    "year": 2010,
    "citationCount": 223,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2010-06-21",
    "authors": [
      {
        "authorId": "144788706",
        "name": "G. Ortiz"
      },
      {
        "authorId": "51951961",
        "name": "J. Biela"
      },
      {
        "authorId": "9186846",
        "name": "D. Bortis"
      },
      {
        "authorId": "2238247852",
        "name": "J. W. Kolar"
      }
    ],
    "source": "semantic_scholar",
    "score": 141.1746907778256
  },
  {
    "paperId": "b9bffae7db3c9677a78b680aec79ab6fec37ccbb",
    "url": "https://www.semanticscholar.org/paper/b9bffae7db3c9677a78b680aec79ab6fec37ccbb",
    "title": "Power-Combining Transformer Techniques for Fully-Integrated CMOS Power Amplifiers",
    "abstract": "Fully integrated CMOS power amplifiers (PAs) with parallel power-combining transformer are presented. For the high power CMOS PA design, two types of transformers, series-combining and parallel-combining, are fully analyzed and compared in detail to show the parasitic resistance and the turn ratio as the limiting factor of power combining. Based on the analysis, two kinds of parallel-combining transformers, a two-primary with a 1:2 turn ratio and a three-primary with a 1:2 turn ratio, are incorporated into the design of fully-integrated CMOS PAs in a standard 0.18-mum CMOS process. The PA with a two-primary transformer delivers 31.2 dBm of output power with 41% of power-added efficiency (PAE), and the PA with a three-primary transformer achieves 32 dBm of output power with 30% of PAE at 1.8 GHz with a 3.3-V power supply.",
    "venue": "IEEE Journal of Solid-State Circuits",
    "year": 2008,
    "citationCount": 149,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2008-04-22",
    "authors": [
      {
        "authorId": "2230346",
        "name": "K. An"
      },
      {
        "authorId": "144512592",
        "name": "O. Lee"
      },
      {
        "authorId": "2109569129",
        "name": "Hyungwook Kim"
      },
      {
        "authorId": "50403590",
        "name": "Dong Ho Lee"
      },
      {
        "authorId": "1904002",
        "name": "Jeonghu Han"
      },
      {
        "authorId": "98434343",
        "name": "Kiseok Yang"
      },
      {
        "authorId": "2107928244",
        "name": "Younsuk Kim"
      },
      {
        "authorId": "144996230",
        "name": "Jae J. Chang"
      },
      {
        "authorId": "3020368",
        "name": "W. Woo"
      },
      {
        "authorId": "2115470492",
        "name": "Chang-Ho Lee"
      },
      {
        "authorId": "3190436",
        "name": "Haksun Kim"
      },
      {
        "authorId": "144606774",
        "name": "J. Laskar"
      }
    ],
    "source": "semantic_scholar",
    "score": 141.15952941144383
  },
  {
    "paperId": "ffa5e5e2c7ba94293cc8ec30bb10549c943f6e6e",
    "url": "https://www.semanticscholar.org/paper/ffa5e5e2c7ba94293cc8ec30bb10549c943f6e6e",
    "title": "Elimination of transformer inrush currents by controlled switching. I. Theoretical considerations",
    "abstract": "Transformer inrush currents are high-magnitude, harmonic-rich currents generated when transformer cores are driven into saturation during energization. These currents have undesirable effects, including potential damage or loss-of-life to the transformer, protective relay misoperation, and reduced power quality on the system. Controlled transformer switching can potentially eliminate these transients if residual core and core flux transients are taken into account in the closing algorithm. This paper explores the theoretical considerations of core flux transients, Based on these studies algorithms were developed which allow controlled energization of most transformers without inrush current.",
    "venue": "",
    "year": 2001,
    "citationCount": 310,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2001-04-01",
    "authors": [
      {
        "authorId": "31279611",
        "name": "J. Brunke"
      },
      {
        "authorId": "2265526812",
        "name": "Klaus J. Fröhlich"
      }
    ],
    "source": "semantic_scholar",
    "score": 141.09689368268852
  },
  {
    "paperId": "f3a18bf1b6b50572f56dd32c00bfd2f6fcf713ff",
    "url": "https://www.semanticscholar.org/paper/f3a18bf1b6b50572f56dd32c00bfd2f6fcf713ff",
    "title": "A ZVS Bi-Directional DC&#8211;DC Converter for Multiple Energy Storage Elements",
    "abstract": "This letter presents a high-power-density multi-input dc-dc converter interfaced with energy storage elements such as a battery and an ultracapacitor. The converter consists of three half-bridges and a high-frequency multi-winding transformer. Bi-directional power flow between input and output is achieved by adjusting the phase-shift angles of the voltages across the two sides of the transformer. Soft-switching is implemented naturally by snubber capacitors and transformer leakage inductance. Operation principles are analyzed in detail. Simulation and experimental results are provided to verify the performance of the proposed converter",
    "venue": "IEEE transactions on power electronics",
    "year": 2006,
    "citationCount": 206,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2006-09-06",
    "authors": [
      {
        "authorId": "2217197432",
        "name": "D. Liu"
      },
      {
        "authorId": null,
        "name": "Hui Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.99078189898052
  },
  {
    "paperId": "1672456cb176d6bcaa5ca325b169ee6f2581757e",
    "url": "https://www.semanticscholar.org/paper/1672456cb176d6bcaa5ca325b169ee6f2581757e",
    "title": "Monolithic transformers and their application in a differential CMOS RF low-noise amplifier",
    "abstract": "A 900 MHz low-noise amplifier (LNA) utilizing three monolithic transformers to implement on-chip tuning networks and requiring no external components has been integrated in 2.88 mm/sup 2/ in a standard digital 0.6 /spl mu/m CMOS process. A bias current reuse technique is employed to reduce power dissipation, and process-, voltage-, and temperature-tracking biasing techniques are used. At 900 MHz, the LNA dissipates 18 mW from a single 3 V power supply and provides 4.1 dB noise figure, 12.3 dB power gain, -33.0 dB reverse isolation, and an input 1-db compression level of -16 dBm. Analysis and modeling considerations for silicon-based monolithic transformers are presented, and it is shown that a monolithic transformer occupies less die area and provides a higher quality factor than two independent inductors with the same effective inductance in differential applications.",
    "venue": "IEEE J. Solid State Circuits",
    "year": 1998,
    "citationCount": 147,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "1998-12-01",
    "authors": [
      {
        "authorId": "46618405",
        "name": "Jianjun J. Zhou"
      },
      {
        "authorId": "1690046",
        "name": "D. Allstot"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.95818410646172
  },
  {
    "paperId": "0f56e1d4ac5228184eb3efbc2f422797bad91614",
    "url": "https://www.semanticscholar.org/paper/0f56e1d4ac5228184eb3efbc2f422797bad91614",
    "title": "The design of a multi-agent transformer condition monitoring system",
    "abstract": "Online diagnostics and online condition monitoring are important functions within the operation and maintenance of power transformers. This paper describes how a multi-agent system (MAS) for transformer condition monitoring has been designed to employ the data generated by the ultra high frequency (UHF) monitoring of partial discharge activity. It describes the rationale behind the use of multi-agent techniques, and the problems overcome through this technology. Every aspect of the MAS design is discussed. In addition, the design and performance of the intelligent interpretation techniques are detailed.",
    "venue": "IEEE Transactions on Power Systems",
    "year": 2004,
    "citationCount": 204,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2004-11-01",
    "authors": [
      {
        "authorId": "2239672697",
        "name": "Stephen McArthur"
      },
      {
        "authorId": "2269945239",
        "name": "Scott Strachan"
      },
      {
        "authorId": "153167652",
        "name": "G. Jahn"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.84514968707612
  },
  {
    "paperId": "9672a007c997977273a1bfa7d0eefd6039e87f84",
    "url": "https://www.semanticscholar.org/paper/9672a007c997977273a1bfa7d0eefd6039e87f84",
    "title": "Experimental determination of stray capacitances in high frequency transformers",
    "abstract": "This paper presents practical techniques for determining stray capacitances in a two-winding high frequency transformer for circuit simulation and computer-aided design purposes. These techniques fall into two categories: The two-port network approach; and the step-response approach. The first approach can be employed for high frequency transformer circuit models with the effect of stray capacitances modeled as a /spl pi/-shape network of three lumped stray capacitances. The second approach is useful for the transformer circuit model with the overall effects of stray capacitances modeled as lumped stray-capacitance connected cross the primary side. These techniques have been verified in the modeling and numerical simulation of a 500 W 25 kHz two winding E-core transformer. The merits and limitations of these techniques are also discussed.",
    "venue": "",
    "year": 2003,
    "citationCount": 214,
    "openAccessPdf": {
      "url": "https://opus.lib.uts.edu.au/bitstream/10453/5855/3/2003000385.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2003-08-26",
    "authors": [
      {
        "authorId": "2115605900",
        "name": "Haiyan Lu"
      },
      {
        "authorId": "2510812",
        "name": "Jianguo Zhu"
      },
      {
        "authorId": "145312306",
        "name": "S. Hui"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.55957042191494
  },
  {
    "paperId": "3eeda4479b6dff384e8a6a624be866167b272a65",
    "url": "https://www.semanticscholar.org/paper/3eeda4479b6dff384e8a6a624be866167b272a65",
    "title": "Dielectric spectroscopic measurements on transformer oil-paper insulation under controlled laboratory conditions",
    "abstract": "For reliable operation of power transformers, the condition of the insulation system is essential. This paper reports on a detailed study of the effect of ageing, temperature and moisture on frequency and time domain spectroscopic measurements carried out on oil-impregnated pressboard samples as well as on a distribution transformer under controlled laboratory conditions. Because field measurements are generally performed after de-energizing the transformer, extreme care is required in interpreting the results due to inherent temperature instabilities. To avoid large thermal variations that may affect the results, a customized adiabatic room was built around the transformer for measurements above the ambient. Capacitance ratio and direct current conductivity deduced from the spectroscopic measurements, helped to interpret the data. Because, low frequency measurements techniques are time consuming, alternative to a transfer of time domain data into frequency domain data was investigated.",
    "venue": "IEEE transactions on dielectrics and electrical insulation",
    "year": 2008,
    "citationCount": 143,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Materials Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2008-08-08",
    "authors": [
      {
        "authorId": "30715543",
        "name": "A. Setayeshmehr"
      },
      {
        "authorId": "2803685",
        "name": "I. Fofana"
      },
      {
        "authorId": "50411677",
        "name": "C. Eichler"
      },
      {
        "authorId": "1931457",
        "name": "A. Akbari"
      },
      {
        "authorId": "2551968",
        "name": "H. Borsi"
      },
      {
        "authorId": "2359516",
        "name": "E. Gockenbach"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.54719949364
  },
  {
    "paperId": "cba8e206e22ea298c2b920e0e84fcf2acceee644",
    "url": "https://www.semanticscholar.org/paper/cba8e206e22ea298c2b920e0e84fcf2acceee644",
    "title": "Power differential method for discrimination between fault and magnetizing inrush current in transformers",
    "abstract": "To avoid the needless trip by magnetizing inrush current, the second harmonic component is commonly used for the blocking differential relay in power transformers. However, the second harmonic component in fault current is increased by the introduction of underground 500 kV lines. This paper describes a new method to discriminate internal fault from inrush current by the sum of active power flowing into transformers from each terminal. The average power is almost zero for energizing, but an internal fault consumes large power. To check the performance of this method, actual inrush current and voltage waveforms of a 500/154 kV transformer are accurately measured by digital equipment. The usefulness is confirmed by applying the method to the measured inrush and simulated fault data.",
    "venue": "",
    "year": 1996,
    "citationCount": 213,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "1996-02-20",
    "authors": [
      {
        "authorId": "22768650",
        "name": "K. Yabe"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.48964022532778
  },
  {
    "paperId": "4d23a7e2a926ec296a793a74c9d2e6c0dd8e3b6c",
    "url": "https://www.semanticscholar.org/paper/4d23a7e2a926ec296a793a74c9d2e6c0dd8e3b6c",
    "title": "An expert system for transformer fault diagnosis using dissolved gas analysis",
    "abstract": "A prototype expert system based on the dissolved gas analysis (DGA) technique for diagnosis of suspected transformer faults and their maintenance actions is developed. A synthetic method is proposed to assist the gas ratio method. The uncertainties of key gas analysis, norms threshold, and gas ratio boundaries, are managed by using a fuzzy set concept. The expert system is implemented on a PC-AT using KES with rule-based knowledge representation. The system has been tested to show its effectiveness in transformer diagnosis. >",
    "venue": "",
    "year": 1993,
    "citationCount": 297,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1746305",
        "name": "Chin E. Lin"
      },
      {
        "authorId": "98737272",
        "name": "J. Ling"
      },
      {
        "authorId": "2107648651",
        "name": "C. L. Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.45640229758106
  },
  {
    "paperId": "46610d226fc1228377a1f783a0c550f0b601dbc8",
    "url": "https://www.semanticscholar.org/paper/46610d226fc1228377a1f783a0c550f0b601dbc8",
    "title": "Three-phase cogenerator and transformer models for distribution system analysis",
    "abstract": "The authors present detailed three-phase cogenerator and transformer models for analyzing a large scale distribution system. The cogenerator model presented can represent the inherent generator phase imbalance due to distribution system imbalance. The cogenerators can be synchronous or induction and can be on either primary or secondary systems. The transformer models consider the copper and core losses, the winding connection, the phase-shifting between primary and secondary windings, and the off-nominal tapping. An individual phase, as opposed to a balanced three-phase, representation is employed. This approach is oriented toward applications in distribution system operational analysis rather than planning analysis. >",
    "venue": "",
    "year": 1991,
    "citationCount": 296,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1991-10-01",
    "authors": [
      {
        "authorId": "3011922",
        "name": "Tsai-Hsiang Chen"
      },
      {
        "authorId": "150205231",
        "name": "M. Chen"
      },
      {
        "authorId": "2107606533",
        "name": "T. Inoue"
      },
      {
        "authorId": "34800482",
        "name": "P. Kotas"
      },
      {
        "authorId": "91318065",
        "name": "E. Chebli"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.4059820820405
  },
  {
    "paperId": "e6bd01012dd6cd0722ccfdc404329fd010a15219",
    "url": "https://www.semanticscholar.org/paper/e6bd01012dd6cd0722ccfdc404329fd010a15219",
    "title": "Transformer failures in regions incorrectly considered to have low GIC-risk",
    "abstract": "A close association has been identified between the theoretical calculation of geomagnetically induced currents (GICs) in a large network, practical measurements of GICs, the results of dissolved gas analysis (DGA) records, and damage in recently failed transformers in Southern Africa. Together these indicate that GICs may contribute significantly to transformer failures on large transmission systems in mid-latitude regions, where GICs are generally thought not to be significant.",
    "venue": "2007 IEEE Lausanne Power Tech",
    "year": 2007,
    "citationCount": 198,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2007-07-01",
    "authors": [
      {
        "authorId": "143622604",
        "name": "C. Gaunt"
      },
      {
        "authorId": "71322760",
        "name": "G. Coetzee"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.3995723708674
  },
  {
    "paperId": "71fc43fd082d2ba0c9ec29bfacfc58bcc4d47098",
    "url": "https://www.semanticscholar.org/paper/71fc43fd082d2ba0c9ec29bfacfc58bcc4d47098",
    "title": "Power electronic transformers for utility applications",
    "abstract": "The concept of realizing a small size \"solid-state\" transformer has been discussed for some time. A fairly straightforward approach to accomplish size reduction in a transformer feeding a conventional rectifier-inverter system is to introduce an isolated DC-DC converter in the DC link, isolation being provided by a high frequency transformer. So also, several topologies that employ AC-AC converters connected on primary and secondary sides of a high frequency transformer to reduce the size and weight of the magnetic core have been reported in literature. Such AC-AC converters need switches with bi-directional voltage blocking and current carrying capability, which are commonly realized with pairs of gate turn-off devices such as insulated gate bipolar transistors (IGBT). This paper explores the possibilities of employing AC-AC switched mode power converters in combination with reactive elements to realize a chopped AC link, thereby decreasing the required magnetic core size for voltage transformation and isolation. A primary advantage of this approach is that, the static power converter needs only six devices to implement a three-phase electronic transformer, which makes it an economical solution. Operating principles, magnetic design and other practical issues are discussed. Detailed computer simulations accompanied with experimental verification are presented in the paper.",
    "venue": "Conference Record of the 2000 IEEE Industry Applications Conference. Thirty-Fifth IAS Annual Meeting and World Conference on Industrial Applications of Electrical Energy (Cat. No.00CH37129)",
    "year": 2000,
    "citationCount": 141,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2000-10-08",
    "authors": [
      {
        "authorId": "19305229",
        "name": "M. Manjrekar"
      },
      {
        "authorId": "98088309",
        "name": "Rick Kieferndorf"
      },
      {
        "authorId": "1992669",
        "name": "G. Venkataramanan"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.33740586401893
  },
  {
    "paperId": "62cc8285f3ec70e25a41d2779ecf5d0070087a94",
    "url": "https://www.semanticscholar.org/paper/62cc8285f3ec70e25a41d2779ecf5d0070087a94",
    "title": "An Airborne Radar Power Supply With Contactless Transfer of Energy—Part I: Rotating Transformer",
    "abstract": "Reliability and precision are key requirements for electronic systems in aerospace applications. Transferring electrical energy from a stationary to a moving device involves wearable parts such as slip rings and brushes. This paper examines the possibility of using a rotating transformer for contactless transfer of energy from the base to the revolving platform of an airborne radar system. The first part of the series focuses on the magnetic interface, investigating its electrical properties and their association with the core and windings geometry. The reader will gain an understanding of the merits and limitations of this technology and will be able to assess its suitability for other applications. The effects of the increased leakage and reduced magnetizing inductances of the transformer are investigated, and two winding layouts are proposed and characterized by measurements and finite-element analysis. Some equations are presented along with practical guidelines on designing a rotating transformer with a 0.25-2-mm air gap. The transformer voltage gain and efficiency plots are introduced as performance-assessment tools. The impact of the air-gap stray flux on the winding conduction losses is shown, and some electromagnetic-compatibility considerations are presented. Finally, a mechanical layout for a 1-kW rotating transformer is proposed.",
    "venue": "IEEE transactions on industrial electronics (1982. Print)",
    "year": 2007,
    "citationCount": 138,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2007-09-10",
    "authors": [
      {
        "authorId": "35166544",
        "name": "K. Papastergiou"
      },
      {
        "authorId": "144040363",
        "name": "D. Macpherson"
      }
    ],
    "source": "semantic_scholar",
    "score": 140.01710899696036
  },
  {
    "paperId": "8bdcf21b0f8a4ed8d5bfc025daa5fdae67b621cd",
    "url": "https://www.semanticscholar.org/paper/8bdcf21b0f8a4ed8d5bfc025daa5fdae67b621cd",
    "title": "Modeling and analysis guidelines for slow transients. III. The study of ferroresonance",
    "abstract": "The ability to predict or confirm ferroresonance depends primarily on the correctness of the transformer model used by the computer simulation. Ferroresonance is introduced and a general modeling approach is given. An overview of available literature and contributors to this area is provided. A simple ease of ferroresonance in a single phase transformer is used to illustrate this \"phenomenon\". Three phase transformer core structures are discussed. Ferroresonance in three phase grounded-wye distribution systems is described and illustrated with waveform data obtained from laboratory simulations. Representation of the study zone is discussed, modeling techniques are presented, and implementation suggestions are made. Three case studies are presented. Transformer representation is critical to performing a valid simulation. The direction of ongoing research is discussed, and the reader is advised to monitor the literature for ongoing rapid improvements in transformer modeling techniques.",
    "venue": "",
    "year": 2000,
    "citationCount": 206,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "144016216",
        "name": "M. Iravani"
      },
      {
        "authorId": "73391701",
        "name": "A. Chaudhary"
      },
      {
        "authorId": "144879472",
        "name": "W. J. Giesbrecht"
      },
      {
        "authorId": "47025472",
        "name": "I. Hassan"
      },
      {
        "authorId": "23542872",
        "name": "A. Keri"
      },
      {
        "authorId": "113861668",
        "name": "K. C. Lee"
      },
      {
        "authorId": "150153447",
        "name": "J. Martinez"
      },
      {
        "authorId": "12084982",
        "name": "A. Morched"
      },
      {
        "authorId": "1977120",
        "name": "B. Mork"
      },
      {
        "authorId": "2569303",
        "name": "M. Parniani"
      },
      {
        "authorId": "2342687085",
        "name": "A. Sharshar"
      },
      {
        "authorId": "1857835",
        "name": "D. Shirmohammadi"
      },
      {
        "authorId": "9378880",
        "name": "R. Walling"
      },
      {
        "authorId": "50301153",
        "name": "D. Woodford"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.99078189898052
  },
  {
    "paperId": "39d5b4be62b86a91eb763631a1e96f9ac268af3d",
    "url": "https://www.semanticscholar.org/paper/39d5b4be62b86a91eb763631a1e96f9ac268af3d",
    "title": "Exploring possibilities for characterization of power transformer insulation by frequency response analysis (FRA)",
    "abstract": "This paper explores the possibility for using frequency response analysis (FRA) technique to characterize the quality of transformer insulation. For this purpose, a single-phase and a three-phase lumped parameter circuit models were developed. The single-phase model represents a transformer without a laminated core, whereas the three-phase model represents a real transformer with core. Calculations of frequency dependent circuit parameters are presented and the effects imparted on the frequency response by different loss mechanisms (in insulation, in windings and in core) are discussed. The simulations are compared with results of measurements on real objects. It is shown that FRA could be used for insulation diagnostics if the measurements were of high enough quality.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2006,
    "citationCount": 137,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2006-06-26",
    "authors": [
      {
        "authorId": "145093568",
        "name": "K. Abeywickrama"
      },
      {
        "authorId": "9438360",
        "name": "Y. Serdyuk"
      },
      {
        "authorId": "2239497600",
        "name": "Stanislaw Gubanski"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.90880527735806
  },
  {
    "paperId": "17db901e243d09325f2a881f937ccc972814d84a",
    "url": "https://www.semanticscholar.org/paper/17db901e243d09325f2a881f937ccc972814d84a",
    "title": "Aging of paper insulation in natural ester dielectric fluid",
    "abstract": "Aging of transformer insulation paper in natural ester (vegetable oil) dielectric fluid is compared to that in conventional transformer oil. Sealed steel aging vessels containing copper, aluminum, thermally upgraded paper, and dielectric fluid (mineral oil and natural ester) were aged at 130, 150 and 170/spl deg/C for 500, 1000, 2000 and 4000 hours. Paper degradation after aging is determined using paper tensile strength and degree of polymerization measurements. The paper in natural ester aged at slower rates than did paper in conventional transformer oil, taking 5-8 times longer to reach end-of-life. Results obtained for mineral oil samples are similar to predictions based on IEEE transformer loading guide calculations. Mechanisms for the slower aging rate are proposed.",
    "venue": "2001 IEEE/PES Transmission and Distribution Conference and Exposition. Developing New Perspectives (Cat. No.01CH37294)",
    "year": 2001,
    "citationCount": 137,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2001-10-28",
    "authors": [
      {
        "authorId": "119868921",
        "name": "C. Mcshane"
      },
      {
        "authorId": "2241806799",
        "name": "Kevin J. Rapp"
      },
      {
        "authorId": "30745075",
        "name": "J. Corkran"
      },
      {
        "authorId": "47944704",
        "name": "G. Gauger"
      },
      {
        "authorId": "91351808",
        "name": "J. Luksich"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.90880527735806
  },
  {
    "paperId": "e06b703146c46a6455fd0c33077de1bea5fdd877",
    "url": "https://www.semanticscholar.org/paper/e06b703146c46a6455fd0c33077de1bea5fdd877",
    "title": "Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles",
    "abstract": "Modern hierarchical vision transformers have added several vision-specific components in the pursuit of supervised classification performance. While these components lead to effective accuracies and attractive FLOP counts, the added complexity actually makes these transformers slower than their vanilla ViT counterparts. In this paper, we argue that this additional bulk is unnecessary. By pretraining with a strong visual pretext task (MAE), we can strip out all the bells-and-whistles from a state-of-the-art multi-stage vision transformer without losing accuracy. In the process, we create Hiera, an extremely simple hierarchical vision transformer that is more accurate than previous models while being significantly faster both at inference and during training. We evaluate Hiera on a variety of tasks for image and video recognition. Our code and models are available at https://github.com/facebookresearch/hiera.",
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "citationCount": 104,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.00989",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-06-01",
    "authors": [
      {
        "authorId": "52190116",
        "name": "Chaitanya K. Ryali"
      },
      {
        "authorId": "2484223",
        "name": "Yuan-Ting Hu"
      },
      {
        "authorId": "93728539",
        "name": "Daniel Bolya"
      },
      {
        "authorId": "143925221",
        "name": "Chen Wei"
      },
      {
        "authorId": "146884473",
        "name": "Haoqi Fan"
      },
      {
        "authorId": "2319973",
        "name": "Po-Yao (Bernie) Huang"
      },
      {
        "authorId": "2212517001",
        "name": "Vaibhav Aggarwal"
      },
      {
        "authorId": "40272208",
        "name": "Arkabandhu Chowdhury"
      },
      {
        "authorId": "1973062",
        "name": "Omid Poursaeed"
      },
      {
        "authorId": "50196944",
        "name": "Judy Hoffman"
      },
      {
        "authorId": "153652147",
        "name": "J. Malik"
      },
      {
        "authorId": "3128506",
        "name": "Yanghao Li"
      },
      {
        "authorId": "2322150",
        "name": "Christoph Feichtenhofer"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.80940525236286
  },
  {
    "paperId": "407b9e9478ba6bff43ce4b20e8b6cb2b303477d2",
    "url": "https://www.semanticscholar.org/paper/407b9e9478ba6bff43ce4b20e8b6cb2b303477d2",
    "title": "Planning with Large Language Models for Code Generation",
    "abstract": "Existing large language model-based code generation pipelines typically use beam search or sampling algorithms during the decoding process. Although the programs they generate achieve high token-matching-based scores, they often fail to compile or generate incorrect outputs. The main reason is that conventional Transformer decoding algorithms may not be the best choice for code generation. In this work, we propose a novel Transformer decoding algorithm, Planning-Guided Transformer Decoding (PG-TD), that uses a planning algorithm to do lookahead search and guide the Transformer to generate better programs. Specifically, instead of simply optimizing the likelihood of the generated sequences, the Transformer makes use of a planner to generate candidate programs and test them on public test cases. The Transformer can therefore make more informed decisions and generate tokens that will eventually lead to higher-quality programs. We also design a mechanism that shares information between the Transformer and the planner to make our algorithm computationally efficient. We empirically evaluate our framework with several large language models as backbones on public coding challenge benchmarks, showing that 1) it can generate programs that consistently achieve higher performance compared with competing baseline methods; 2) it enables controllable code generation, such as concise codes and highly-commented codes by optimizing modified objective.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 104,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2303.05510",
      "status": "CLOSED"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-03-09",
    "authors": [
      {
        "authorId": "2211163649",
        "name": "Shun Zhang"
      },
      {
        "authorId": "2111329651",
        "name": "Zhenfang Chen"
      },
      {
        "authorId": "2714199",
        "name": "Yikang Shen"
      },
      {
        "authorId": "2055624181",
        "name": "Mingyu Ding"
      },
      {
        "authorId": "1763295",
        "name": "J. Tenenbaum"
      },
      {
        "authorId": "2056157586",
        "name": "Chuang Gan"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.80940525236286
  },
  {
    "paperId": "35af0bea6f957de6a7695630ee07777e533d2e09",
    "url": "https://www.semanticscholar.org/paper/35af0bea6f957de6a7695630ee07777e533d2e09",
    "title": "A novel wavelet-based algorithm for discrimination of internal faults from magnetizing inrush currents in power transformers",
    "abstract": "In this paper, a new algorithm based on processing differential current is proposed for digital differential protection of power transformers by considering different behaviors of the differential currents under fault and inrush current conditions. In this method, a criterion function is defined in terms of difference of amplitude of wavelet coefficients over a specific frequency band. The criterion function is then used for three phases, and internal faults are precisely discriminated from inrush current less than a quarter a cycle after the disturbance; this is one advantage of the method. Another advantage of the proposed method is that the fault detection algorithm does not depend on the selection of thresholds. The merit of this method is demonstrated by simulation of different faults and switching conditions on a power transformer using PSCAD/EMTDC software. Also the proposed algorithm is tested offline using data collected from a prototype laboratory three-phase power transformer. The test results show that the new algorithm is very quick and accurate",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2006,
    "citationCount": 136,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2006-10-02",
    "authors": [
      {
        "authorId": "1808497",
        "name": "J. Faiz"
      },
      {
        "authorId": "2249791639",
        "name": "Saeed Lotfifard"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.7997138874219
  },
  {
    "paperId": "90be746299c94a33decfa422d8a0769fb49c1269",
    "url": "https://www.semanticscholar.org/paper/90be746299c94a33decfa422d8a0769fb49c1269",
    "title": "Reactive power/voltage control in a distribution substation using dynamic programming",
    "abstract": "The reactive power/voltage control in a distribution substation is investigated. The purpose is to properly dispatch the shunt capacitors and onload tap changers at the distribution substation based on the forecast hourly loads of a main transformer and its primary bus voltage such that the reactive power flows through the main transformer, and the transformer secondary bus voltage deviations from the desired values can be minimised. Practical constraints on secondary bus voltage limits and the maximum allowable number of switching operations in a day for the onload tap changers are taken into account. An approach based on dynamic programming is presented to reach the desired dispatching schedule. To demonstrate the usefulness of the proposed approach, reactive power/voltage control in a distribution substation within the service area of Taipei City District Office of Taiwan Power Company is performed. It is found that a proper dispatching schedule for the shunt capacitors and onload tap changers can be reached by the proposed method.",
    "venue": "",
    "year": 1995,
    "citationCount": 202,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1995-11-01",
    "authors": [
      {
        "authorId": "65971124",
        "name": "F. Lu"
      },
      {
        "authorId": "2860695",
        "name": "Y. Hsu"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.69808968562683
  },
  {
    "paperId": "2421e4fcde385ed5cd7c39e85588d73f4bb7dde1",
    "url": "https://www.semanticscholar.org/paper/2421e4fcde385ed5cd7c39e85588d73f4bb7dde1",
    "title": "Power Systems Harmonics: Fundamentals, Analysis and Filter Design",
    "abstract": "Contents: Introduction - Fundamentals of Harmonics - Causes of Harmonics in Power Systems - Effects of Harmonic Distortion on Power Systems - Mitigation of Power System Harmonics - Limits of Harmonic Distortion - Modelling of System Components for Harmonic Studies -Transformer Modelling - Modelling of Transmission Lines / Cables - A Simple Approach to Power System Harmonic Studies - Bibliography - Appendix A: A Review of Transformation and Symmetrical Components - Appendix B: Phase and Sequence Admittance Matrices for Three-Phase Transformers - Appendix C: Transmission Matrices for Three-Phase Transformers - Index.",
    "venue": "",
    "year": 2001,
    "citationCount": 282,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": "2001-08-28",
    "authors": [
      {
        "authorId": "95993014",
        "name": "George J. Wakileh"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.68170346464856
  },
  {
    "paperId": "7f395d9607f1a7f97b9efcf7d3e0a0b297e2f855",
    "url": "https://www.semanticscholar.org/paper/7f395d9607f1a7f97b9efcf7d3e0a0b297e2f855",
    "title": "A soft-switched, full-bridge boost converter employing an active-clamp circuit",
    "abstract": "A new full-bridge, active-clamp boost converter is proposed for single-phase high power PFC applications and applications requiring transformer isolation. The active-clamp network serves to limit bridge switch turn-off voltage overshoot and enable the energy stored in the transformer leakage inductance to be used for zero-voltage switching. PWM phase-shift control of the bridge switches is utilized to obtain zero-current switching for two of the four bridge switches. Simulation results are presented which verify the principle of operation.",
    "venue": "PESC Record. 27th Annual IEEE Power Electronics Specialists Conference",
    "year": 1996,
    "citationCount": 188,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "1996-06-23",
    "authors": [
      {
        "authorId": "98034754",
        "name": "R. Watson"
      },
      {
        "authorId": "2270098737",
        "name": "Fred C. Lee"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.62620522589464
  },
  {
    "paperId": "6b859d63b2db55eb0caa083f26db2b3f1170e546",
    "url": "https://www.semanticscholar.org/paper/6b859d63b2db55eb0caa083f26db2b3f1170e546",
    "title": "Aging of cellulose at transformer service temperatures. Part 1: Influence of type of oil and air on the degree of polymerization of pressboard, dissolved gases, and furanic compounds in oil",
    "abstract": "A power transformer is expected to operate up to 40 years. With current cost-benefit calculations, the user is forced to perform maintenance that will bring a further extension of transformer life. The maintenance can only be based on on-site and off-site monitoring with extended analytical and electrical tests, which can define the service condition of the transformer and predict its further life expectancy. The life of a transformer is mainly dependent on the life of its solid insulation, and the life-limit is determined by the thermal degradation of the winding paper. The Kraft paper decreases in tensile strength with the progress of aging and, at some point, can no longer withstand the short circuit stresses. The monitoring of the complex permittivity of transformer oils, as a function of frequency and temperature, may provide information concerning the state of the insulation. In this article, we show the influence of air and oil type on the aging of pressboard under the influence of a considerable amount of moisture. The rate of the degree of polymerization, the development of furanic compounds, as well as the gas-in-oil analysis in comparison with the aging of the pure oil under the same conditions have been investigated.",
    "venue": "IEEE Electrical Insulation Magazine",
    "year": 2005,
    "citationCount": 133,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Materials Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2005-04-04",
    "authors": [
      {
        "authorId": "30609323",
        "name": "A. Kachler"
      },
      {
        "authorId": "30936507",
        "name": "I. Hohlein"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.46759699926366
  },
  {
    "paperId": "f6c7901ee891308e5bdd64b2433367a7ec86ee6c",
    "url": "https://www.semanticscholar.org/paper/f6c7901ee891308e5bdd64b2433367a7ec86ee6c",
    "title": "Finite Element Analysis of Internal Winding Faults in Distribution Transformers",
    "abstract": "With the appearance of deregulation, distribution transformer predictive maintenance is becoming more important for utilities to prevent forced outages with the consequential costs. To detect and diagnose a transformer internal fault requires a transformer model to simulate these faults. This paper presents finite element analysis of internal winding faults in a distribution transformer. The transformer with a turn-to-earth fault or a turn-to-turn fault is modeled using coupled electromagnetic and structural finite elements. The terminal behaviors of the transformer are studied by an indirect coupling of the finite element method and circuit simulation. The procedure was realized using a commercially available software. The normal case and various faulty cases were simulated and the terminal behaviors of the transformer were studied and compared with field experimental results. The comparison results validate the finite element model to simulate internal faults in a distribution transformer.",
    "venue": "IEEE Power Engineering Review",
    "year": 2001,
    "citationCount": 133,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2001-04-01",
    "authors": [
      {
        "authorId": "2884896",
        "name": "H. Wang"
      },
      {
        "authorId": "26993001",
        "name": "K. Butler"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.46759699926366
  },
  {
    "paperId": "b9abca98e7a35391a012436fa66c596542fe249e",
    "url": "https://www.semanticscholar.org/paper/b9abca98e7a35391a012436fa66c596542fe249e",
    "title": "Dynamic thermal modeling of power transformers: further Development-part II",
    "abstract": "This paper presents new temperature calculation methods based on heat-transfer theory, the application of the lumped capacitance method, the thermal-electrical analogy, and a new definition of nonlinear thermal resistances at different locations within a power transformer. The methods presented in this paper take into account oil viscosity changes and loss variation with temperature. The changes in transformer time constants due to changes in the oil viscosity are also accounted for in the thermal models. The models are validated using experimental results, which have been obtained from a series of thermal tests performed on a range of power transformers. The results are also compared with the IEEE-Loading guide (1995) Annex G method",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2006,
    "citationCount": 132,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Materials Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2006-10-02",
    "authors": [
      {
        "authorId": "9503520",
        "name": "D. Susa"
      },
      {
        "authorId": "1862552",
        "name": "M. Lehtonen"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.3552369233263
  },
  {
    "paperId": "5562c638f4b468a02b0fb7d4971201351f5dccef",
    "url": "https://www.semanticscholar.org/paper/5562c638f4b468a02b0fb7d4971201351f5dccef",
    "title": "Moisture equilibrium in transformer paper-oil systems",
    "abstract": "This paper provides an overview of the classic moisture equilibrium curves and their history and provides useful information on the relationships among them and their validity.",
    "venue": "",
    "year": 1999,
    "citationCount": 380,
    "openAccessPdf": {
      "url": "https://minds.wisconsin.edu/bitstream/1793/10276/1/file_1.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Environmental Science"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2202277024",
        "name": "Y. Du"
      },
      {
        "authorId": "145150989",
        "name": "M. Zahn"
      },
      {
        "authorId": "1786648",
        "name": "B. Lesieutre"
      },
      {
        "authorId": "2996448",
        "name": "A. Mamishev"
      },
      {
        "authorId": "91972462",
        "name": "S. Lindgren"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.1419906269005
  },
  {
    "paperId": "aabaf7ad52088b503b175e73b10af415a1c12820",
    "url": "https://www.semanticscholar.org/paper/aabaf7ad52088b503b175e73b10af415a1c12820",
    "title": "Research on extraction technique of transformer core fundamental frequency vibration based on OLCM",
    "abstract": "This paper presents a new technique based on vibration measurement to diagnose power transformers, which is called the onload current method (OLCM). It can acquire the fundamental frequency component of the core vibration signal without running the transformer at the open-circuit condition. The diagnostic method adopted and the experimental test results are reported. Tests have been performed at normal operating conditions in both the manufactory and the laboratory. With the tests performed in the manufactory, the vibration characteristics of transformer windings and core are described, and then the principle of OLCM is introduced. To verify the validity of OLCM, the laboratory tests are conducted which relates the transformer vibrations to the simulative fault. But the presented method was verified on a 5-kVA transformer which is a very far cry from an actual power transformer. It needs to be developed further before OLCM can be used effectively on the transformer in the field",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2006,
    "citationCount": 130,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2006-10-02",
    "authors": [
      {
        "authorId": "14018403",
        "name": "Ji Shengchang"
      },
      {
        "authorId": "16343538",
        "name": "Luo Yongfen"
      },
      {
        "authorId": "31395551",
        "name": "Liang Yanming"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.12795984801727
  },
  {
    "paperId": "6adfca1e3f1a5b0dade22c84e3a5bd7129f6003c",
    "url": "https://www.semanticscholar.org/paper/6adfca1e3f1a5b0dade22c84e3a5bd7129f6003c",
    "title": "Transformer Design Principles: With Applications to Core-Form Power Transformers",
    "abstract": "Introduction Historical Background Uses in Power Systems Core-Form and Shell-Form Transformers Stacked and Wound Core Construction Transformer Cooling Winding Types Insulation Structures Structural Elements Modern Trends Magnetism and Related Core Issues Basic Magnetism Hysteresis Magnetic Circuits Inrush Current Distinguishing Inrush from Fault Current Optimal Core Stacking Circuit Model of a Two-Winding Transformer with Core Circuit Model of the Core Two-Winding Transformer Circuit Model with Core Approximate Two-Winding Transformer Circuit Model without Core Vector Diagram of a Loaded Transformer with Core Per-Unit System Voltage Regulation Reactance and Leakage Reactance Calculations General Method for Determining Inductances and Mutual Inductances Two-Winding Leakage Reactance Formula Ideal Two-, Three-, and Multiwinding Transformers Leakage Reactance for Two-Winding Transformers Based on Circuit Parameters Leakage Reactances for Three-Winding Transformers Phasors, Three-Phase Connections, and Symmetrical Components Phasors Wye and Delta Three-Phase Connections Zig-Zag Connection Scott Connection Symmetrical Components Fault Current Analysis Fault Current Analysis on Three-Phase Systems Fault Currents for Transformers with Two Terminals per Phase Fault Currents for Transformers with Three Terminals per Phase Asymmetry Factor Phase-Shifting and Zig-Zag Transformers Basic Principles Squashed Delta Phase-Shifting Transformer Standard Delta Phase-Shifting Transformer Two-Core Phase-Shifting Transformer Regulation Effects Fault Current Analysis Zig-Zag Transformer Multi-terminal Three-Phase Transformer Model Theory Transformers with Winding Connections within a Phase Multiphase Transformers Generalizing the Model Regulation and Terminal Impedances Multiterminal Transformer Model for Balanced and Unbalanced Load Conditions Rabins' Method for Calculating Leakage Fields, Leakage Inductances, and Forces in Transformers Theory Rabins' Formula for Leakage Reactance Application of Rabins' Method to Calculate the Self-Inductance of and Mutual Inductance between Coil Sections Determining the B-Field Determination of Winding Forces Numerical Considerations Mechanical Design Force Calculations Stress Analysis Radial Buckling Strength Stress Distribution in a Composite Wire-Paper Winding Section Additional Mechanical Considerations Electric Field Calculations Simple Geometries Electric Field Calculations Using Conformal Mapping Finite Element Electric Field Calculations Capacitance Calculations Distributive Capacitance along a Winding or Disk Stein's Disk Capacitance Formula General Disk Capacitance Formula Coil Grounded at One End with Grounded Cylinders on Either Side Static Ring on One Side of Disk Terminal Disk without a Static Ring Capacitance Matrix Two Static Rings Static Ring Between the First Two Disks Winding Disk Capacitances with Wound-in Shields Multistart Winding Capacitance Voltage Breakdown and High-Voltage Design Principles of Voltage Breakdown Geometric Dependence of Transformer-Oil Breakdown Insulation Coordination Continuum Model of Winding Used to Obtain the Impulse-Voltage Distribution Lumped-Parameter Model for Transient Voltage Distribution Losses No-Load or Core Losses Load Losses Tank and Shield Losses Due to Nearby Busbars Tank Losses Associated with the Bushings Thermal Design Thermal Model of a Disk Coil with Directed Oil Flow Thermal Model for Coils without Directed Oil Flow Radiator Thermal Model Tank Cooling Oil Mixing in the Tank Time Dependence Pumped Flow Comparison with Test Results Determining m and n Exponents Loss of Life Calculation Cable and Lead Temperature Calculation Tank Wall Temperature Calculation Tieplate Temperature Core Steel Temperature Calculation Load Tap Changers General Description of Load Tap Changer Types of Regulation Principles of Operation Connection Schemes General Maintenance Miscellaneous Topics Setting the Impulse Test Generator to Achieve Close to Ideal Waveshapes Impulse or Lightning Strike on a Transformer through a Length of Cable Air Core Inductance Electrical Contacts References Index",
    "venue": "",
    "year": 2001,
    "citationCount": 194,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2001-01-23",
    "authors": [
      {
        "authorId": "71987367",
        "name": "B. Poulin"
      },
      {
        "authorId": "72238233",
        "name": "R. Vecchio"
      },
      {
        "authorId": "73637024",
        "name": "P. Feghali"
      },
      {
        "authorId": "48531630",
        "name": "Rajendra Ahuja"
      },
      {
        "authorId": "71681776",
        "name": "Dilipkumar M. Shah"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.0949933784562
  },
  {
    "paperId": "ab00875bbf17f921a9792dac68e5da617b83a9c9",
    "url": "https://www.semanticscholar.org/paper/ab00875bbf17f921a9792dac68e5da617b83a9c9",
    "title": "Substrate effects in monolithic RF transformers on silicon",
    "abstract": "The effect of substrate RF losses on the characteristics of silicon-based integrated transformers is studied experimentally by using a substrate transfer technique. The maximum available gain is used to evaluate the quality of transformers similarly to that of active devices. The silicon substrate has a pronounced effect on the quality factor and mutual resistive coupling factor of the primary and secondary coils, thereby degrading the maximum available gain of the transformer. A highly structured patterned ground shield is shown to improve the maximum available gain of a transformer at high frequencies, while at low frequencies, it has little effect on the maximum available gain and even degrades the quality factors of the transformer coils. It is shown that the low-frequency degradation of the coil quality factors relates to local eddy currents in the patterned metal shield.",
    "venue": "",
    "year": 2002,
    "citationCount": 194,
    "openAccessPdf": {
      "url": "https://repository.tudelft.nl/file/File_cd821c25-5468-4cf0-ac1d-1c328eb26c08",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Mathematics"
    ],
    "publicationTypes": null,
    "publicationDate": "2002-08-07",
    "authors": [
      {
        "authorId": "123898355",
        "name": "K. Ng"
      },
      {
        "authorId": "9388383",
        "name": "B. Rejaei"
      },
      {
        "authorId": "2961456",
        "name": "J. Burghartz"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.0949933784562
  },
  {
    "paperId": "6b6502d181585ee19b265c09faef40d73574eed9",
    "url": "https://www.semanticscholar.org/paper/6b6502d181585ee19b265c09faef40d73574eed9",
    "title": "Artificial neural network based protection of power transformers",
    "abstract": "In this work, an artificial neural network based algorithm for a three phase power transformer protection scheme is developed and implemented in real time using the DS-1102 digital signal processor. Distinguishing between the magnetizing inrush and internal fault currents is always a consideration for power transformer protection. Existing methods, mostly based on harmonic restraint, are not very reliable for modern transformer protection. The reason for loss of reliability is that the use of low-loss amorphous material in modern transformer cores causes reduced second harmonic content of the magnetizing inrush current. Other methods based on the transformer equivalent circuit model are susceptible to parameter variations and hence are not suitable under all operating conditions. The work presented here shows the usefulness of the artificial neural network which is able to distinguish between the magnetizing inrush and the internal fault currents without harmonic decomposition or using the transformer equivalent circuit model. The inherent advantages of the generalization and the pattern recognition characteristics make the artificial neural network based method quite suitable for distinguishing between magnetizing inrush and the internal fault currents. In this work, a two-layer artificial neural network with sixteen inputs and one output is designed. The data to train and test the artificial neural network are experimentally obtained. The artificial neural network is trained with an input data set and subsequently tested with a different data set. The off-line test results show that the artificial neural network is quite capable of distinguishing between the magnetizing inrush and internal fault currents. Finally, the on-line implementation successfully establishes the efficacy of the ANN based algorithm for power transformer protection.",
    "venue": "",
    "year": 1996,
    "citationCount": 193,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "145828172",
        "name": "M. R. Zaman"
      }
    ],
    "source": "semantic_scholar",
    "score": 139.01787238594994
  },
  {
    "paperId": "8b0357f1bceb9cf7a5629b0ba3acb5660edf90b2",
    "url": "https://www.semanticscholar.org/paper/8b0357f1bceb9cf7a5629b0ba3acb5660edf90b2",
    "title": "Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review",
    "abstract": "The remarkable performance of the Transformer architecture in natural language processing has recently also triggered broad interest in Computer Vision. Among other merits, Transformers are witnessed as capable of learning long-range dependencies and spatial correlations, which is a clear advantage over convolutional neural networks (CNNs), which have been the de facto standard in Computer Vision problems so far. Thus, Transformers have become an integral part of modern medical image analysis. In this review, we provide an encyclopedic review of the applications of Transformers in medical imaging. Specifically, we present a systematic and thorough review of relevant recent Transformer literature for different medical image analysis tasks, including classification, segmentation, detection, registration, synthesis, and clinical report generation. For each of these applications, we investigate the novelty, strengths and weaknesses of the different proposed strategies and develop taxonomies highlighting key properties and contributions. Further, if applicable, we outline current benchmarks on different datasets. Finally, we summarize key challenges and discuss different future research directions. In addition, we have provided cited papers with their corresponding implementations in https://github.com/mindflow-institue/Awesome-Transformer.",
    "venue": "Medical Image Anal.",
    "year": 2023,
    "citationCount": 98,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2301.03505",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "Review",
      "JournalArticle"
    ],
    "publicationDate": "2023-01-09",
    "authors": [
      {
        "authorId": "1763181",
        "name": "Reza Azad"
      },
      {
        "authorId": "2131612425",
        "name": "A. Kazerouni"
      },
      {
        "authorId": "1491490451",
        "name": "Moein Heidari"
      },
      {
        "authorId": "1411236504",
        "name": "Ehsan Khodapanah Aghdam"
      },
      {
        "authorId": "2167168779",
        "name": "Amir Molaei"
      },
      {
        "authorId": "2192610282",
        "name": "Yiwei Jia"
      },
      {
        "authorId": "145895943",
        "name": "Abin Jose"
      },
      {
        "authorId": "2190045909",
        "name": "Rijo Roy"
      },
      {
        "authorId": "1737693",
        "name": "D. Merhof"
      }
    ],
    "source": "semantic_scholar",
    "score": 138.92679775201884
  },
  {
    "paperId": "06247415528ed40239d179a00417ac798b965bf8",
    "url": "https://www.semanticscholar.org/paper/06247415528ed40239d179a00417ac798b965bf8",
    "title": "270 kVA Solid State Transformer Based on 10 kV SiC Power Devices",
    "abstract": "With the advancement of semiconductor technology, solid state transformer (SST) with high voltage fast switching SiC power devices is becoming a valid option to replace the conventional transformers in power substation. In this paper, a 270 kVA solid state transformer based on 10 kV SiC power MOSFET has been proposed. The two stages of SST, five-level Vienna rectifier and five-level DC/DC converter are specifically designed and simulated in closed loop. The analysis of device losses is performed based on the device characteristics. A design of high frequency transformer is presented as well. The simulation results together with the loss analysis verify the functionality and feasibility of SST.",
    "venue": "2007 IEEE Electric Ship Technologies Symposium",
    "year": 2007,
    "citationCount": 128,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2007-05-21",
    "authors": [
      {
        "authorId": "2610769",
        "name": "Tiefu Zhao"
      },
      {
        "authorId": "2146171443",
        "name": "Liyu Yang"
      },
      {
        "authorId": "29266100",
        "name": "Jun Wang"
      },
      {
        "authorId": "2826163",
        "name": "A. Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 138.89718606542507
  },
  {
    "paperId": "8c9a865492baea4753b36b29e029b045ba9f51e5",
    "url": "https://www.semanticscholar.org/paper/8c9a865492baea4753b36b29e029b045ba9f51e5",
    "title": "An evidential reasoning approach to transformer condition assessments",
    "abstract": "This paper presents an evidential reasoning (ER) approach to the transformer condition assessment. An ER algorithm is briefly introduced which is used to combine evidences and deal with uncertainties. The methodology of transferring the transformer condition assessment problem into a multiple-attribute decision-making (MADM) solution under an ER framework is then presented. Several solutions to the transformer condition assessment problem, using the ER approach, are then illustrated, highlighting the potential of the ER algorithm. Based on the outputs of the ER approach, system operators can obtain an overall evaluation of the observed unit's condition; also, several units may be ranked in order of severity for system maintenance purposes. It can be seen from the results developed that the ER approach is a suitable solution to tackle the MADM problem of transformer condition assessment.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2004,
    "citationCount": 128,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2004-10-04",
    "authors": [
      {
        "authorId": "2244454606",
        "name": "W. H. Tang"
      },
      {
        "authorId": "2102355729",
        "name": "K. Spurgeon"
      },
      {
        "authorId": "2244314121",
        "name": "Qinghua Wu"
      },
      {
        "authorId": "46353169",
        "name": "Z. Richardson"
      }
    ],
    "source": "semantic_scholar",
    "score": 138.89718606542507
  },
  {
    "paperId": "85e30a0752302662fe9a0b92a36286e3bcf38d43",
    "url": "https://www.semanticscholar.org/paper/85e30a0752302662fe9a0b92a36286e3bcf38d43",
    "title": "A fundamental approach to transformer thermal modeling. II. Field verification",
    "abstract": "For pt.I see ibid., vol.16, no.2, p.171-5 (2001). This paper has two main objectives. One is to show that the top oil rise thermal model proposed in part I is valid, for a large power transformer in service. The second is to show that there is a convenient way of estimating the parameters without removing the transformer from service. A Manitoba Hydro 250 MVA OFAF transformer was chosen and instrumented with data-gathering equipment. Two twenty-four hour test runs were performed, one in February of 1999 and the other in July of 1999. The most basic parameter to be determined was the rated top oil rise but also found were the top oil line constant and the nonlinearity exponent, commonly given the symbol n. The results are very positive.",
    "venue": "",
    "year": 2001,
    "citationCount": 191,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2001-04-01",
    "authors": [
      {
        "authorId": "48207973",
        "name": "G. Swift"
      },
      {
        "authorId": "31269306",
        "name": "T. Molinski"
      },
      {
        "authorId": "98630626",
        "name": "R. Bray"
      },
      {
        "authorId": "8905028",
        "name": "R. Menzies"
      }
    ],
    "source": "semantic_scholar",
    "score": 138.86243058041674
  },
  {
    "paperId": "e95589701d363ee4ccd498c0aee53f74de4a1b2f",
    "url": "https://www.semanticscholar.org/paper/e95589701d363ee4ccd498c0aee53f74de4a1b2f",
    "title": "Cellulose insulation in oil-filled power transformers: Part I - history and development",
    "abstract": "This paper presents a brief discussion on the development of electrical grade paper/pressboard for transformer use from the raw materials, improvements made, and particularly the use of thermal upgrading agents to extend the useful life of transformers",
    "venue": "IEEE Electrical Insulation Magazine",
    "year": 2006,
    "citationCount": 239,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2006-04-10",
    "authors": [
      {
        "authorId": "117957259",
        "name": "T. Prevost"
      },
      {
        "authorId": "31135688",
        "name": "T. Oommen"
      }
    ],
    "source": "semantic_scholar",
    "score": 138.2095838501299
  },
  {
    "paperId": "7939f6d9f8ab64bd7962a5eaaa311f10d6b856ae",
    "url": "https://www.semanticscholar.org/paper/7939f6d9f8ab64bd7962a5eaaa311f10d6b856ae",
    "title": "A new approach to power supplies for robots",
    "abstract": "A cableless power supply system for multiaxis robots is proposed. As key components, it contains rotatable transformers placed in the joints of the robot. It consists of two halves of a ferrite pot core containing the primary and secondary windings, respectively. The energy is transferred to the movable parts of the robot through this transformer at a frequency of 25 kHz. This frequency allows a compact design of the transformer. A schematic representation of the new system is offered. Investigations of the steady-state and the transient behavior were carried out. Experimental results were obtained from an experimental setup with a 20 kVA, 650 V IGBT converter. >",
    "venue": "",
    "year": 1991,
    "citationCount": 182,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1991-09-01",
    "authors": [
      {
        "authorId": "143993204",
        "name": "A. Esser"
      },
      {
        "authorId": "9199585",
        "name": "H. Skudelny"
      }
    ],
    "source": "semantic_scholar",
    "score": 138.14229229262133
  },
  {
    "paperId": "590c5bbc2f721c2e73a52bea19cbe760576ca611",
    "url": "https://www.semanticscholar.org/paper/590c5bbc2f721c2e73a52bea19cbe760576ca611",
    "title": "A novel fuzzy logic approach to transformer fault diagnosis",
    "abstract": "Dissolved gas in oil analysis is an well established in-service technique for incipient fault detection in oil-insulated power transformers. A great deal of experience and data in dissolved gas in oil analysis (DGA) is now available within the utilities. Actually, diagnostic interpretations were solely done by human experts using past knowledge and standard techniques such as the ratio method. In this paper, a novel fuzzy logic approach is adopted to develop a computer based intelligent interpretation of transformer faults using Visual basic and C/sup ++/ programming. The proposed fuzzy logic based software as been tested and tuned using over 800 dissolved gas in oil analysis (DGA) case histories. This highly reliable tool has then been utilized in detection and verification of 20 transformer faults. The proposed diagnostic tool is very useful to both expert and novice engineers in DGA result interpretation.",
    "venue": "",
    "year": 2000,
    "citationCount": 181,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2000-04-01",
    "authors": [
      {
        "authorId": "145717386",
        "name": "S. Islam"
      },
      {
        "authorId": "2216875981",
        "name": "T. Wu"
      },
      {
        "authorId": "2866645",
        "name": "G. Ledwich"
      }
    ],
    "source": "semantic_scholar",
    "score": 138.06010030615192
  },
  {
    "paperId": "748e247eeeca62666a7026478dd1149018b2ce3c",
    "url": "https://www.semanticscholar.org/paper/748e247eeeca62666a7026478dd1149018b2ce3c",
    "title": "Transformer design and application considerations for nonsinusoidal load currents",
    "abstract": "The use of adjustable speed drives requires transformers capable of withstanding high levels of harmonic currents under normal operating conditions. Experience has been that overheating problems are much more common with dry type transformers than with liquid filled transformers. Transformer insulation life is determined by the hot spot temperature but confirmation of hot spot temperature rise is one performance characteristic which is ignored in industry standards. This is especially important for transformers rated for nonsinusoidal load currents. Hot spot allowances used in IEEE standards for ventilated dry type transformers were developed in 1944 and data indicates that revisions are required. The design of transformers for nonsinusoidal load currents should include an analysis of the eddy loss distribution in the windings and calculation of the hot spot temperature rise. Calculations and thermal tests giving only average winding temperature rises are not sufficient. Thermal tests with nonsinusoidal currents and measurements of hot spot temperature rises are extremely difficult on large transformers. The combination of testing and analysis may be the only economically practical approach. Analysis indicates that the dry type transformer hot spot temperature is very sensitive to the eddy loss magnitude and distribution. The UL K-factor rated dry type transformer and the recommended practices given in ANSI/IEEE C57.110 are reviewed.",
    "venue": "1995 IEEE Cement Industry Technical Conference. 37th Conference Record",
    "year": 1995,
    "citationCount": 119,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Conference",
      "Review"
    ],
    "publicationDate": "1995-06-04",
    "authors": [
      {
        "authorId": "93975267",
        "name": "L. W. Pierce"
      }
    ],
    "source": "semantic_scholar",
    "score": 137.81237614173068
  },
  {
    "paperId": "e4c3d1f76c88b6dbca00edff0828b292b6bbfed7",
    "url": "https://www.semanticscholar.org/paper/e4c3d1f76c88b6dbca00edff0828b292b6bbfed7",
    "title": "Optimum design of a high-power, high-frequency transformer",
    "abstract": "A procedure for optimum design of a high-power, high-frequency transformer is presented. The procedure is based on both electrical and thermal processes in the power transformer and identifies: (a) the VA-rating of ferrite cores in relation to the operating frequency; (b) the optimum flux density in the core; and (c) the optimum current densities of the windings providing maximum transformer efficiency. Since the transformer is the major contributor to the volume and weight of the power supply, the results of transformer analysis can be used for entire power supply optimization as well. Two high-power, high-frequency transformers are optimally designed, built, and tested. Practical results show good agreement with the theory.",
    "venue": "",
    "year": 1996,
    "citationCount": 178,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "31146133",
        "name": "R. Petkov"
      }
    ],
    "source": "semantic_scholar",
    "score": 137.81078708761132
  },
  {
    "paperId": "d6074725de0dc23fec358436074ae9aa96327cc2",
    "url": "https://www.semanticscholar.org/paper/d6074725de0dc23fec358436074ae9aa96327cc2",
    "title": "General Synthesis of Quarter-Wave Impedance Transformers",
    "abstract": "This paper presents the general synthesis of a radio frequency impedance transformer of n quarter-wave steps, given an \"insertion loss function\" of permissible form. This procedure parallels that of Darlington for lumped constant filters by providing the connection between Collin's canonical form for the insertion loss function and Richards' demonstration that a reactance function may always be realized as a cascade of equal length impedance transformers terminated in either a short or open circuit. In particular, it is shown that insertion loss functions of the form selected by Collin are always realizable with positive characteristic impedances, and that the synthesis procedure, for maximally flat and Tchebycheff performance, involves the solution, at most, of quadratic equations. In addition, this procedure permits the proof of Collin's conjecture that, for his insertion loss function, the resulting reflection coefficients are symmetrical. Finally, closed expressions are given for the coefficients of the input impedance of a given n section transformer in terms of the n characteristic impedances and vice versa.",
    "venue": "",
    "year": 1957,
    "citationCount": 177,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Mathematics"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "51077321",
        "name": "H. Riblet"
      }
    ],
    "source": "semantic_scholar",
    "score": 137.72675325438126
  },
  {
    "paperId": "7470348df6c8b6a5aaa3e3e635b59931540167c7",
    "url": "https://www.semanticscholar.org/paper/7470348df6c8b6a5aaa3e3e635b59931540167c7",
    "title": "Three- and Two-Dimensional Finite-Element Computation of Inrush Current and Short-Circuit Electromagnetic Forces on Windings of a Three-Phase Core-Type Power Transformer",
    "abstract": "Although short-circuit current is frequently considered the major design fundamental for power transformers, experience with transformer failures shows that inrush currents that occur when transformers are energized can also cause serious damage. To investigate the resultant forces due to energizing power transformer windings, we modeled a three-phase, three-legged 66/11 kV, 40 MVA power transformer in two and three dimensions. We calculated electromechanical forces for short-circuit cases and also for inrush current through the windings, using the finite-element method. The results show that the forces exerted on the windings due to inrush current in many regions are larger than those due to short-circuit currents. Since the inrush current appears more frequently with a much longer duration compared to a short-current event, its harmful effects are worse than those of the short-circuit case.",
    "venue": "IEEE transactions on magnetics",
    "year": 2008,
    "citationCount": 118,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Physics"
    ],
    "publicationTypes": null,
    "publicationDate": "2008-04-18",
    "authors": [
      {
        "authorId": "1808497",
        "name": "J. Faiz"
      },
      {
        "authorId": "2259520443",
        "name": "Bashir Mahdi Ebrahimi"
      },
      {
        "authorId": "2288823506",
        "name": "Tahere Noori"
      }
    ],
    "source": "semantic_scholar",
    "score": 137.68685239667295
  },
  {
    "paperId": "92c53c4d6643dc9967de56bd89f82436b99649c9",
    "url": "https://www.semanticscholar.org/paper/92c53c4d6643dc9967de56bd89f82436b99649c9",
    "title": "Diagnosing Shorted Turns on the Windings of Power Transformers Based Upon Online FRA Using Capacitive and Inductive Couplings",
    "abstract": "Interturn faults are a significant problem in power transformers that can eventually burgeon into catastrophic faults and likely result in an overall network failure. The main problem with SFRA as one of the well-recognized diagnostic tools for detecting winding faults is its restriction to the domain of offline testing since the method requires injection of a test signal into the transformer windings through the high-voltage bushings. To address the difficulties arising from this issue, in this contribution, a new technique for online transfer function monitoring of the power transformers windings through a quite simple, economic, and noninvasive capacitive sensor installed on the surface of the transformer bushing is presented. Interturn faults with various levels of severity were imposed on the winding of a 35-kV/400-V, 100-kVA oil-immersed distribution transformer to evaluate the feasibility and sensitivity of the method. The experimental results proved that the newly developed online monitoring system is feasible and sensitive to detect unambiguously interturn faults even down to 0.2% shorted turns along the winding. As such, the proposed approach in this paper enables timely warning of a rising failure and serves a better purpose in assessing the health of a faulty transformer.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2011,
    "citationCount": 117,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2011-06-09",
    "authors": [
      {
        "authorId": "9317082",
        "name": "V. Behjat"
      },
      {
        "authorId": "2245892231",
        "name": "Abolfazl Vahedi"
      },
      {
        "authorId": "30715543",
        "name": "A. Setayeshmehr"
      },
      {
        "authorId": "2551968",
        "name": "H. Borsi"
      },
      {
        "authorId": "2359516",
        "name": "E. Gockenbach"
      }
    ],
    "source": "semantic_scholar",
    "score": 137.56026936698498
  },
  {
    "paperId": "0d94be39c7d87a27d1956756d99f8ae45d4662cf",
    "url": "https://www.semanticscholar.org/paper/0d94be39c7d87a27d1956756d99f8ae45d4662cf",
    "title": "Real-time Fourier transformer based on fiber gratings.",
    "abstract": "We use the well-known duality between paraxial diffraction in space and dispersion in time to propose a time-domain analog to spatial Fraunhofer diffraction. This analog permits the design of real-time optical Fourier-transformer systems. These systems are shown to be realizable by use of linearly chirped fiber gratings as dispersive media.",
    "venue": "Optics Letters",
    "year": 1999,
    "citationCount": 244,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Physics",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2239260427",
        "name": "M. A. Muriel"
      },
      {
        "authorId": "2239259677",
        "name": "José Azaña"
      },
      {
        "authorId": "5026758",
        "name": "A. Carballar"
      }
    ],
    "source": "semantic_scholar",
    "score": 137.5188731581709
  },
  {
    "paperId": "0a6906bd6f026d3da3031c641ed03081bd0b574e",
    "url": "https://www.semanticscholar.org/paper/0a6906bd6f026d3da3031c641ed03081bd0b574e",
    "title": "Full Stack Optimization of Transformer Inference: a Survey",
    "abstract": "Recent advances in state-of-the-art DNN architecture design have been moving toward Transformer models. These models achieve superior accuracy across a wide range of applications. This trend has been consistent over the past several years since Transformer models were originally introduced. However, the amount of compute and bandwidth required for inference of recent Transformer models is growing at a significant rate, and this has made their deployment in latency-sensitive applications challenging. As such, there has been an increased focus on making Transformer models more efficient, with methods that range from changing the architecture design, all the way to developing dedicated domain-specific accelerators. In this work, we survey different approaches for efficient Transformer inference, including: (i) analysis and profiling of the bottlenecks in existing Transformer architectures and their similarities and differences with previous convolutional models; (ii) implications of Transformer architecture on hardware, including the impact of non-linear operations such as Layer Normalization, Softmax, and GELU, as well as linear operations, on hardware design; (iii) approaches for optimizing a fixed Transformer architecture; (iv) challenges in finding the right mapping and scheduling of operations for Transformer models; and (v) approaches for optimizing Transformer models by adapting the architecture using neural architecture search. Finally, we perform a case study by applying the surveyed optimizations on Gemmini, the open-source, full-stack DNN accelerator generator, and we show how each of these approaches can yield improvements, compared to previous benchmark results on Gemmini. Among other things, we find that a full-stack co-design approach with the aforementioned methods can result in up to 88.7x speedup with a minimal performance degradation for Transformer inference.",
    "venue": "arXiv.org",
    "year": 2023,
    "citationCount": 89,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2302.14017",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2023-02-27",
    "authors": [
      {
        "authorId": "2109586102",
        "name": "Sehoon Kim"
      },
      {
        "authorId": "2029486869",
        "name": "Coleman Hooper"
      },
      {
        "authorId": "2209989425",
        "name": "Thanakul Wattanawong"
      },
      {
        "authorId": "2111622895",
        "name": "Minwoo Kang"
      },
      {
        "authorId": "2209989113",
        "name": "Ruohan Yan"
      },
      {
        "authorId": "2062998049",
        "name": "Hasan Genç"
      },
      {
        "authorId": "36031131",
        "name": "Grace Dinh"
      },
      {
        "authorId": "145391991",
        "name": "Qijing Huang"
      },
      {
        "authorId": "1732330",
        "name": "K. Keutzer"
      },
      {
        "authorId": "1717098",
        "name": "Michael W. Mahoney"
      },
      {
        "authorId": "40259322",
        "name": "Y. Shao"
      },
      {
        "authorId": "10419477",
        "name": "A. Gholami"
      }
    ],
    "source": "semantic_scholar",
    "score": 137.497145054954
  },
  {
    "paperId": "5930d09f376bb66886aa3b996f7349489ec95db7",
    "url": "https://www.semanticscholar.org/paper/5930d09f376bb66886aa3b996f7349489ec95db7",
    "title": "Application of geometric programming to transformer design",
    "abstract": "This paper considers the transformer design optimization problem. In its most general form, the design problem requires minimizing the total mass (or cost) of the core and wire material while ensuring the satisfaction of the transformer ratings and a number of design constraints. The constraints include appropriate limits on efficiency, voltage regulation, temperature rise, no-load current, and winding fill factor. The design optimization seeks a constrained minimum mass (or cost) solution by optimally setting the transformer geometry parameters and the relevant electrical and magnetic quantities. In cases where the core dimensions are fixed, the optimization problem calls for a constrained maximum volt-ampere or minimum loss solution. This paper shows that the above design problems can be formulated in geometric programming (GP) format. The importance of the GP format stems from two main features. First, GP provides an efficient and reliable solution for the design optimization problem with several variables. Second, it guarantees that the obtained solution is the global optimum. The paper includes a demonstration of the application of the GP technique to transformer design. It also includes a comparative study to emphasize the advantage of including the transformer core dimensions as variables in the design problem.",
    "venue": "IEEE transactions on magnetics",
    "year": 2005,
    "citationCount": 116,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2005-11-14",
    "authors": [
      {
        "authorId": "2225982",
        "name": "R. Jabr"
      }
    ],
    "source": "semantic_scholar",
    "score": 137.43260902196636
  },
  {
    "paperId": "1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b",
    "url": "https://www.semanticscholar.org/paper/1ec3a3ff77cb4b424499b3805ecc90182ecd8f8b",
    "title": "What Algorithms can Transformers Learn? A Study in Length Generalization",
    "abstract": "Large language models exhibit surprising emergent generalization properties, yet also struggle on many simple reasoning tasks such as arithmetic and parity. This raises the question of if and when Transformer models can learn the true algorithm for solving a task. We study the scope of Transformers' abilities in the specific setting of length generalization on algorithmic tasks. Here, we propose a unifying framework to understand when and how Transformers can exhibit strong length generalization on a given task. Specifically, we leverage RASP (Weiss et al., 2021) -- a programming language designed for the computational model of a Transformer -- and introduce the RASP-Generalization Conjecture: Transformers tend to length generalize on a task if the task can be solved by a short RASP program which works for all input lengths. This simple conjecture remarkably captures most known instances of length generalization on algorithmic tasks. Moreover, we leverage our insights to drastically improve generalization performance on traditionally hard tasks (such as parity and addition). On the theoretical side, we give a simple example where the\"min-degree-interpolator\"model of learning from Abbe et al. (2023) does not correctly predict Transformers' out-of-distribution behavior, but our conjecture does. Overall, our work provides a novel perspective on the mechanisms of compositional generalization and the algorithmic capabilities of Transformers.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "citationCount": 88,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2023-10-24",
    "authors": [
      {
        "authorId": "2261393101",
        "name": "Hattie Zhou"
      },
      {
        "authorId": "153523234",
        "name": "Arwen Bradley"
      },
      {
        "authorId": "1762320",
        "name": "Etai Littwin"
      },
      {
        "authorId": "1388726511",
        "name": "Noam Razin"
      },
      {
        "authorId": "2438203",
        "name": "O. Saremi"
      },
      {
        "authorId": "2243336902",
        "name": "Josh Susskind"
      },
      {
        "authorId": "1751569",
        "name": "Samy Bengio"
      },
      {
        "authorId": "2181918",
        "name": "Preetum Nakkiran"
      }
    ],
    "source": "semantic_scholar",
    "score": 137.3295455459821
  },
  {
    "paperId": "b0263e89eca9535d2d3c9d0520008a34b091bf9f",
    "url": "https://www.semanticscholar.org/paper/b0263e89eca9535d2d3c9d0520008a34b091bf9f",
    "title": "A 23-dBm 60-GHz Distributed Active Transformer in a Silicon Process Technology",
    "abstract": "In this paper, a distributed active transformer for the operation in the millimeter-wave frequency range is presented. The transformer utilizes stacked coupled wires as opposed to slab inductors to achieve a high coupling factor of kf=0.8 at 60 GHz. Scalable and compact equivalent-circuit models are used for the transformer design without the need for full-wave electromagnetic simulations. To demonstrate the feasibility of the millimeter-wave transformer, a 200-mW (23 dBm) 60-GHz power amplifier has been implemented in a standard 130-nm SiGe process technology, which, to date, is the highest reported output power in an SiGe process technology at millimeter-wave frequencies. The size of the output transformer is only 160times160 mum2 and demonstrates the feasibility of efficient power combining and impedance transformation at millimeter-wave frequencies. The two-stage amplifier has 13 dB of compressed gain and achieves a power-added efficiency of 6.4% while combining the power of eight cascode amplifiers into a differential 100-Omega load. The amplifier supply voltage is 4 V with a quiescent current consumption of 300 mA",
    "venue": "IEEE transactions on microwave theory and techniques",
    "year": 2007,
    "citationCount": 115,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2007-05-07",
    "authors": [
      {
        "authorId": "2192715",
        "name": "U. Pfeiffer"
      },
      {
        "authorId": "2258808930",
        "name": "David Goren"
      }
    ],
    "source": "semantic_scholar",
    "score": 137.30385286659546
  },
  {
    "paperId": "3879c00332c3e961d40e5235f5cfda689aa3a7e4",
    "url": "https://www.semanticscholar.org/paper/3879c00332c3e961d40e5235f5cfda689aa3a7e4",
    "title": "A transformer of one-third wavelength in two sections - for a frequency and its first harmonic",
    "abstract": "The quarter-wave transformer transforms in one frequency f/sub 0/, but not its first harmonic 2f/sub 0/. The transform in 2f/sub 0/ is needed in a dual-band operation of GSM and PCS. This requirement can be fulfilled in a two line-sections of 1/6 wavelengths each, adding to 1/3 wavelengths for the total transformer length in the lower band centering at f/sub 0/ (fundamental), or 2/3 wavelengths in the upper band centering at 2f/sub 0/ (1st harmonic). The 1/3-wave transformer is analytically inexact but effectively exact for engineering applications. For example, for impedance transforms of K = 4, the inexactness gives a reflection of |/spl Gamma/| = 0.013.",
    "venue": "IEEE Microwave and Wireless Components Letters",
    "year": 2002,
    "citationCount": 115,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2002-08-07",
    "authors": [
      {
        "authorId": "152844146",
        "name": "Y. Chow"
      },
      {
        "authorId": "47656883",
        "name": "K. Wan"
      }
    ],
    "source": "semantic_scholar",
    "score": 137.30385286659546
  },
  {
    "paperId": "77287c8407909416344a4ae15e87f8f998391733",
    "url": "https://www.semanticscholar.org/paper/77287c8407909416344a4ae15e87f8f998391733",
    "title": "Impulse testing of power transformers using the transfer function method",
    "abstract": "The transfer function of a transformer winding is deconvoluted in the frequency domain from the digitally recorded neutral current and high voltage applied during impulse tests. The integrity of the winding insulation is determined by comparing the transfer function obtained at full and reduced test voltage. Differences between the transfer function plots reveal local breakdowns in the winding that can be dissociated from partial discharges. Thus the method permits unambiguous acceptance or rejection if the transformer and, since the transfer function is theoretically immune to changes in the applied impulse, also allows evaluation of the chopped-impulse test. Some 100 windings of large HV power transformers have been tested using the transfer function method, which on several occasions has revealed transformer faults as well a test setup problems that would have been missed or misinterpreted by conventional techniques. >",
    "venue": "",
    "year": 1988,
    "citationCount": 172,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1988-04-01",
    "authors": [
      {
        "authorId": "30749384",
        "name": "R. Malewski"
      },
      {
        "authorId": "71987367",
        "name": "B. Poulin"
      }
    ],
    "source": "semantic_scholar",
    "score": 137.2993739174667
  },
  {
    "paperId": "c3c8ac738a0d5e18e1ff46b40f8ebd69864cad3c",
    "url": "https://www.semanticscholar.org/paper/c3c8ac738a0d5e18e1ff46b40f8ebd69864cad3c",
    "title": "Online identification of magnetizing inrush and internal faults in three-phase transformers",
    "abstract": "The authors present a microprocessor-based system for online identification of magnetizing inrush and internal fault conditions in three-phase transformers. The system uses a digital algorithm which is based on a transformer model and can be used irrespective of whether or not it is possible to measure winding currents. The only parameters needed by the algorithm are positive and negative sequence impedances of the transformer. These parameters are usually a part of the name-plate data. The proposed algorithm has been implemented on a Texas Instrument TMS320C25 digital signal processor. The hardware and software of the system are described. The ability of the system to identify magnetizing inrush and internal faults was tested by using a 15 kVA three-phase transformer. Some test results are reported. >",
    "venue": "",
    "year": 1992,
    "citationCount": 171,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1992-10-01",
    "authors": [
      {
        "authorId": "144466629",
        "name": "T. Sidhu"
      },
      {
        "authorId": "31395103",
        "name": "M. Sachdev"
      }
    ],
    "source": "semantic_scholar",
    "score": 137.2124171522018
  },
  {
    "paperId": "c4edc04ee2c976e97e30dd589425aac999e67a28",
    "url": "https://www.semanticscholar.org/paper/c4edc04ee2c976e97e30dd589425aac999e67a28",
    "title": "Medium frequency topology in railway applications",
    "abstract": "The proposed Medium Frequency Topology is replacing the bulky main line transformer on board of railway vehicles. Especially in countries with 15 kV railway power supply, the low frequency of 16.7 Hz is leading to huge and heavy transformers. Today's Transformers are often minimized in weight and volume, which leads to significantly high transformer losses compared with distribution transformers. The Medium Frequency Topology is the power electronic solution that considerably reduces weight and losses of the traction system.",
    "venue": "EPE",
    "year": 2007,
    "citationCount": 239,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2007-09-01",
    "authors": [
      {
        "authorId": "2271526273",
        "name": "Michael Steiner"
      },
      {
        "authorId": "49376606",
        "name": "H. Reinold"
      }
    ],
    "source": "semantic_scholar",
    "score": 137.2095838501299
  },
  {
    "paperId": "0397506d1292b65f6defa1e0360167154594745f",
    "url": "https://www.semanticscholar.org/paper/0397506d1292b65f6defa1e0360167154594745f",
    "title": "Aging of cellulose at transformer service temperatures. Part 2. Influence of moisture and temperature on degree of polymerization and formation of furanic compounds in free-breathing systems",
    "abstract": "This paper examines the influence of moisture and temperature on the degree of polymerization (DP) and furanic compounds. At normal service temperatures, the average moisture content in the solid insulation of transformers has a much stronger influence on the development of furanic compounds and degradation of the insulation than temperature itself. The influence of the initial moisture in the solid insulation on aging under normal transformer service temperatures of 75, 85, and 95 /spl deg/C has been investigated experimentally. Wet transformers have usually higher levels of furans, inferior oil properties, and lower DP values of paper than dry ones under similar service conditions. The development of furan concentration and moisture content are very important aging indicators for the evaluation and life assessment of transformer in service. Further work is necessary to combine these important aging characteristics into a valuable tool for the assessment of a large aging transformer population.",
    "venue": "IEEE Electrical Insulation Magazine",
    "year": 2005,
    "citationCount": 113,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Materials Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2005-10-03",
    "authors": [
      {
        "authorId": "30936507",
        "name": "I. Hohlein"
      },
      {
        "authorId": "30609323",
        "name": "A. Kachler"
      }
    ],
    "source": "semantic_scholar",
    "score": 137.04297672591744
  },
  {
    "paperId": "54121b1a2dbd3e2e9754dc41fdfa1c693b0840b6",
    "url": "https://www.semanticscholar.org/paper/54121b1a2dbd3e2e9754dc41fdfa1c693b0840b6",
    "title": "Modeling Power Transformers to Support the Interpretation of Frequency-Response Analysis",
    "abstract": "A power transformer will yield a frequency response which is unique to its mechanical geometry and electrical properties. Changes in the frequency response of a transformer can be potential indicators of winding deformation as well as other structural and electrical problems. A diagnostic tool which leverages this knowledge in order to detect such changes is frequency-response analysis (FRA). To date, FRA has been used to identify changes in a transformer's frequency response but with limited insight into the underlying cause of the change. However, there is now a growing research interest in specifically identifying the structural change in a transformer directly from its FRA signature. The aim of this paper is to support FRA interpretation through the development of wideband three-phase transformer models which are based on three types of FRA tests. The resulting models can be used as a flexible test bed for parameter sensitivity analysis, leading to greater insight into the effects that geometric change can have on transformer FRA. This paper will demonstrate the applicability of this modeling approach by simultaneously fitting each model to the corresponding FRA data sets without a priori knowledge of the transformer's internal dimensions, and then quantitatively assessing the accuracy of key model parameters.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2011,
    "citationCount": 112,
    "openAccessPdf": {
      "url": "https://ogma.newcastle.edu.au:443/vital/access/services/Download/uon:9239/ATTACHMENT02",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2011-09-23",
    "authors": [
      {
        "authorId": "2045691",
        "name": "S. Mitchell"
      },
      {
        "authorId": "49245775",
        "name": "J. Welsh"
      }
    ],
    "source": "semantic_scholar",
    "score": 136.9108172806851
  },
  {
    "paperId": "da651748eec57ada6576efc59e62c543598ce1f3",
    "url": "https://www.semanticscholar.org/paper/da651748eec57ada6576efc59e62c543598ce1f3",
    "title": "Using transformer parasitics for resonant converters - a review of the calculation of the stray capacitance of transformers",
    "abstract": "Parasitic capacitances of conventional transformers can be used as resonant elements in resonant DC-DC converters in order to reduce the overall system size. For predicting the values of the parasitic capacitors without building the transformer different approaches for calculating these capacitances are compared. A systematic summary of the known approaches is given and missing links between the different theories and missing equations are added. Furthermore, a new simple procedure for modelling parasitic capacitances which is based on the known approaches is proposed. The resulting equations are verified by measurements on four different high voltage transformers.",
    "venue": "Fourtieth IAS Annual Meeting. Conference Record of the 2005 Industry Applications Conference, 2005.",
    "year": 2005,
    "citationCount": 234,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": [
      "Conference",
      "Review"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2684725",
        "name": "J. Biela"
      },
      {
        "authorId": "2259104544",
        "name": "Johann W. Kolar"
      }
    ],
    "source": "semantic_scholar",
    "score": 136.8937827121624
  },
  {
    "paperId": "2f5b6f4b1447a69dea8e64c90e59820b3d9e6898",
    "url": "https://www.semanticscholar.org/paper/2f5b6f4b1447a69dea8e64c90e59820b3d9e6898",
    "title": "On the estimation of elapsed life of oil-immersed power transformers",
    "abstract": "Inadvertent failure of power transformers has serious consequences on the system reliability, economics, and the revenue accrual. An accurate estimation of transformer life can, to a very large extent, mitigate the problems, besides satisfying the conflicting requirements of optimum utilization of the equipment and safeguarding the reliability. In this endeavour, the authors have planned long duration aging experiments on scaled-down (prorated) models of a transformer, incorporating all of the essential features of actual equipment under normal operating electric stress and accelerated thermal stress. In continuation of the authors' earlier experimental investigations, an elapsed life assessment study has been instituted by acquiring insulation-aging data under accelerated thermal stresses.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2005,
    "citationCount": 110,
    "openAccessPdf": {
      "url": "http://eprints.iisc.ac.in/3416/1/PV34.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2005-06-27",
    "authors": [
      {
        "authorId": "73347015",
        "name": "M. Pradhan"
      },
      {
        "authorId": "47224707",
        "name": "T. Ramu"
      }
    ],
    "source": "semantic_scholar",
    "score": 136.64295301968502
  },
  {
    "paperId": "a499d1eb2fdc5b4d530b31b3c992d143ee10249a",
    "url": "https://www.semanticscholar.org/paper/a499d1eb2fdc5b4d530b31b3c992d143ee10249a",
    "title": "Condition Monitoring of Power Transformers - Bibliography Survey",
    "abstract": "ower transformers are one of the most expensive elements in a power system, and their failure is a very costly event. Power transformers are mainly involved in energy transmission and distribution. Unplanned power transformer outages have considerable economic impact on the operation of electric power networks. To have reliable operation of transformers, it is necessary to identify problems at an early stage before a catastrophic failure occurs. In spite of corrective and predictive maintenance, preventive maintenance is gaining due importance in the modern era, and it must be taken into account to obtain the highest reliability of power apparatus such as power transformers. The well-known preventive maintenance techniques such as dissolved gas analysis (DGA), thermal monitoring, partial discharge measurement, capacitance and tan delta measurements, frequency response analysis, etc. are performed on transformers for a specific type of problem. DGA is a very efficient and reliable tool for the earliest detection of inception faults in transformer and other electrical equipment using insulating oil. Condition monitoring of electrical equipment, such as transformers, helps users in many ways such as planning of maintenance schedule, obtaining knowledge of the health of equipment, estimating the remaining service life of equipment, finding areas of further improvement, refining product specification, etc. Kraft paper (cellulose) immersed in mineral oil is used as the insulation system for the copper windings in large power transformers. As the system ages under load, the paper and oil can degrade, potentially leading to catastrophic failure. Partial discharge (PD) is an important tool for improving the reliability of HV insulation systems. It is a very sensitive and nondestructive method of evaluation of the health of the insulation of any HV equipment. PD always is associated with the degradation of insulation systems in HV equipment. Therefore, PDs need to be detected, measured, located, and reduced to a safe value so that the quality of the insulation system is not affected.",
    "venue": "IEEE Electrical Insulation Magazine",
    "year": 2008,
    "citationCount": 108,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": "2008-08-08",
    "authors": [
      {
        "authorId": "46893308",
        "name": "Jashandeep Singh"
      },
      {
        "authorId": "2874634",
        "name": "Y. Sood"
      },
      {
        "authorId": "72774673",
        "name": "R. Jarial"
      }
    ],
    "source": "semantic_scholar",
    "score": 136.37021823343716
  },
  {
    "paperId": "e9a60e5ba3fe8af8066c2a5d1c88bcaf37e8af2d",
    "url": "https://www.semanticscholar.org/paper/e9a60e5ba3fe8af8066c2a5d1c88bcaf37e8af2d",
    "title": "Introduction to modeling of transformers and coupled inductors",
    "abstract": "A tutorial paper is presented on modeling and design of transformers and coupled inductors. Beginning with a brief review of electromagnetic laws and magnetic circuit models, the magnetic and electric models of transformers and coupled inductors are developed, including both magnetizing and leakage effects. It is shown that while the voltage waveforms on the windings are primarily related by the turns ratio for both devices, the winding currents of transformers and coupled inductors are determined by very different mechanisms. An integrated structure with both transformer and coupled inductor on the same core is also discussed, as well as the special case of the coupled inductor used on a multiple-output transformer-isolated converter. >",
    "venue": "",
    "year": 1995,
    "citationCount": 160,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Physics",
      "Engineering"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": "1995-05-01",
    "authors": [
      {
        "authorId": "2534166",
        "name": "A. Witulski"
      }
    ],
    "source": "semantic_scholar",
    "score": 136.22106547476693
  },
  {
    "paperId": "fa7a0ffdfe16c70887c4a4b87bbe88dc56e76d96",
    "url": "https://www.semanticscholar.org/paper/fa7a0ffdfe16c70887c4a4b87bbe88dc56e76d96",
    "title": "A transformer model for winding fault studies",
    "abstract": "This paper deals with a method of modeling internal faults in a power transformer. The method leads to a model which is entirely compatible with EMTP software. It enables simulation of faults between any turn and the earth or between any two turns of the transformer windings. Implementation of the proposed method assumes knowledge of how to evaluate the leakage factors between the various coils of the transformer. A very simple method is proposed to evaluate these leakage factors. At last, an experimental validation of the model allows the estimation of its accuracy. >",
    "venue": "",
    "year": 1994,
    "citationCount": 223,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1994-04-01",
    "authors": [
      {
        "authorId": "144961204",
        "name": "P. Bastard"
      },
      {
        "authorId": "145415982",
        "name": "P. Bertrand"
      },
      {
        "authorId": "40372696",
        "name": "M. Meunier"
      }
    ],
    "source": "semantic_scholar",
    "score": 136.1746907778256
  },
  {
    "paperId": "2d7d69f484740f253274ec57105c388eaaf746cf",
    "url": "https://www.semanticscholar.org/paper/2d7d69f484740f253274ec57105c388eaaf746cf",
    "title": "Detection of shorted turns and winding movements in large power transformers using frequency response analysis",
    "abstract": "There are various techniques for diagnosing electrical faults in a transformer such as the DDF, tan/spl delta/, partial discharges and high voltage tests. But, there are no reliable techniques to identify mechanical faults in a transformer. These faults are winding movement loss of clamping pressure, disc movement etc. Frequency response analysis (FRA) is a tool that can be successfully applied for reliable detection of such faults. In this paper, frequency partitioned mathematical models of a large power transformer is presented and it is shown how the sensitivity of the model parameters can be correlated to inception of transformer faults. Finally, some experimental results are presented.",
    "venue": "2000 IEEE Power Engineering Society Winter Meeting. Conference Proceedings (Cat. No.00CH37077)",
    "year": 2000,
    "citationCount": 105,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2000-01-23",
    "authors": [
      {
        "authorId": "145717386",
        "name": "S. Islam"
      }
    ],
    "source": "semantic_scholar",
    "score": 135.951586411681
  },
  {
    "paperId": "3a3dc0229b9a2ac17c43d8aeaecf7144ebda2221",
    "url": "https://www.semanticscholar.org/paper/3a3dc0229b9a2ac17c43d8aeaecf7144ebda2221",
    "title": "Status and trends in transformer monitoring",
    "abstract": "A manufacturer's view on transformer monitoring is presented. Based on a review of the changes occurring on the electricity market, monitoring of transformers is discussed emphasizing its commercial aspects. Definitions of the words \"monitoring\" and \"diagnostics\" are proposed, and it is stressed that the key issues related to on-line monitoring are reliability and low cost. A survey of the most important methods for on-line monitoring and off-line diagnostics is given. The area of life assessment is discussed, and in particular, the difference between end-of-life of the insulating material and of the transformer is emphasized.",
    "venue": "",
    "year": 1996,
    "citationCount": 216,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": "1996-07-01",
    "authors": [
      {
        "authorId": "2077407090",
        "name": "C. Bengtsson"
      }
    ],
    "source": "semantic_scholar",
    "score": 135.6984603031069
  },
  {
    "paperId": "0c62ac6ca79a6d6b5389177d63f16e5b5b87f938",
    "url": "https://www.semanticscholar.org/paper/0c62ac6ca79a6d6b5389177d63f16e5b5b87f938",
    "title": "Large Power Transformers",
    "abstract": "The book deals with the following aspects of transformer engineering: general principles governing the function of transformers, iron cores, windings, stray losses caused by stray flux, the insulation of transformers, and the structural parts and accessories. This edition includes the developments in theory and practice on the basis of the authors' experience in design, manufacturing and testing of large transformers. New developments have been particularly extensive in the fields of new magnetic materials, cooling methods, dielectric strength for overvoltages of different types, and stray-load loss problems, which are presented in the book in detail. The many diagrams in the book can be used directly in the design, manufacture and testing of large transformers. In preparing their text, the authors have aimed to satisfy the demand for a work that summarizes the latest experience in development and design of large power transformers.",
    "venue": "",
    "year": 1987,
    "citationCount": 153,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1987-12-01",
    "authors": [
      {
        "authorId": "94514010",
        "name": "K. Karsai"
      },
      {
        "authorId": "97996962",
        "name": "D. Kerényi"
      },
      {
        "authorId": "47748020",
        "name": "L. Kiss"
      }
    ],
    "source": "semantic_scholar",
    "score": 135.55428903620444
  },
  {
    "paperId": "aed062a07c84b106ac83d2c007057b2e1457bd57",
    "url": "https://www.semanticscholar.org/paper/aed062a07c84b106ac83d2c007057b2e1457bd57",
    "title": "Piezoelectric transformer structural modeling - a review",
    "abstract": "A review on piezoelectric transformer structural modeling is presented. The operating principle and the basic behavior of piezoelectric transformers as governed by the linear theory of piezoelectricity are shown by a simple, theoretical analysis on a Rosen transformer based on extensional modes of a nonhomogeneous ceramic rod. Various transformers are classified according to their structural shapes, operating modes, and voltage transforming capability. Theoretical and numerical modeling results from the theory of piezoelectricity are reviewed. More advances modeling on thermal and nonlinear effects also are discussed. The article contains 167 references.",
    "venue": "IEEE Transactions on Ultrasonics, Ferroelectrics and Frequency Control",
    "year": 2007,
    "citationCount": 143,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Physics",
      "Medicine"
    ],
    "publicationTypes": [
      "Review",
      "JournalArticle"
    ],
    "publicationDate": "2007-06-25",
    "authors": [
      {
        "authorId": "49499617",
        "name": "Jia-shi Yang"
      }
    ],
    "source": "semantic_scholar",
    "score": 135.54719949364
  },
  {
    "paperId": "f68b983526dbe69b79b6d81cff648318bc5b66e0",
    "url": "https://www.semanticscholar.org/paper/f68b983526dbe69b79b6d81cff648318bc5b66e0",
    "title": "A Multi-Purpose Balanced Transformer for Railway Traction Applications",
    "abstract": "A multi-purpose balanced (MPB) transformer is proposed in this paper for electric railway applications. The MPB transformer is developed from impedance-matching (IM) transformers widely used in China's railway systems. The proposed MPB transformer has three main functions: (1) it provides a symmetrical two-phase voltage of 27.5 kV for railway overhead lines to power electric trains; (2) it supplies a three-phase balanced voltage of 10.5 kV for railway substations; and (3) it can accommodate a three-phase harmonic filter with improved power factor and reduced manufacturing cost. In this paper, the operating principles of the MPB transformer are elaborated, the design of the harmonic filters is discussed, and the results of computer simulations are provided. A 16-MVA MPB transformer was designed and commissioned, and real-time field measurements are provided.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2009,
    "citationCount": 102,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2009-03-04",
    "authors": [
      {
        "authorId": "2109194304",
        "name": "Zhiwen Zhang"
      },
      {
        "authorId": "2234003973",
        "name": "Bin Wu"
      },
      {
        "authorId": "47218123",
        "name": "Jinsong Kang"
      },
      {
        "authorId": "34282430",
        "name": "L. Luo"
      }
    ],
    "source": "semantic_scholar",
    "score": 135.52093482344452
  },
  {
    "paperId": "3409f000fcf13032f77f9a85f403e503ddd36d75",
    "url": "https://www.semanticscholar.org/paper/3409f000fcf13032f77f9a85f403e503ddd36d75",
    "title": "Dynamic thermal modeling of power transformers: further Development-part I",
    "abstract": "This paper focuses specifically on further refinement in the definition of nonlinear thermal resistances (by considering specific design differences between the transformer windings and vertical plates and, consequently, the fluid flow around their surfaces) and the relevant thermal models. In addition, the equivalent thermal capacitances of the transformer oil for different transformer designs and winding-oil circulations are estimated by the suggested equations. The methods presented in this paper are validated using experimental results. Furthermore, the IEEE-Annex G method is used as a reference",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2006,
    "citationCount": 142,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2285206964",
        "name": "I. D. Susa"
      },
      {
        "authorId": "2285221820",
        "name": "Matti Lehtonen"
      }
    ],
    "source": "semantic_scholar",
    "score": 135.4426694538986
  },
  {
    "paperId": "3e10e9f5804e2bce145d7306e9ed0cc7b47b94a8",
    "url": "https://www.semanticscholar.org/paper/3e10e9f5804e2bce145d7306e9ed0cc7b47b94a8",
    "title": "Effect of Core Magnetization on Frequency Response Analysis (FRA) of Power Transformers",
    "abstract": "This paper presents how the frequency response analysis (FRA) measurements on a transformer can be influenced by magnetization condition of the core. Measurements were performed on two transformers at different magnetization levels to show effects of remanent magnetization in the core due to removal of 3-supply, of relaxation demagnetization and of dc flux on the frequency response of winding impedance. The most important observations were that any sudden change of excitation field (applying or removing magnetization current) yields a slowly varying magnetic relaxation, which causes the impedance to change with time due to magnetic viscosity. Results of diagnostic measurements of transformer winding impedance can therefore depend on the level of remanent magnetic flux appearing in the core limbs after switching-off a transformer. These observations lead to a conclusion that, for avoiding the effects of core magnetization, diagnostic FRA measurements on power transformers should not be performed directly after disconnection from network.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2008,
    "citationCount": 101,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Materials Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2008-06-24",
    "authors": [
      {
        "authorId": "31050707",
        "name": "N. Abeywickrama"
      },
      {
        "authorId": "9438360",
        "name": "Y. Serdyuk"
      },
      {
        "authorId": "2239497600",
        "name": "Stanislaw Gubanski"
      }
    ],
    "source": "semantic_scholar",
    "score": 135.37459219926404
  },
  {
    "paperId": "cf2e1cfa1d9984deea0f2e342771ab651d497dbb",
    "url": "https://www.semanticscholar.org/paper/cf2e1cfa1d9984deea0f2e342771ab651d497dbb",
    "title": "An actively cooled high power, high frequency transformer with high insulation capability",
    "abstract": "An actively cooled high power, high frequency transformer with high insulation capability for use in a high power multilevel converter is discussed. The transformer is designed for a power level of 350 kW and is realized with amorphous core material and coaxial windings. Special attention is paid to the insulation problem, since the dielectric losses and the influence of the voltage waveform with very steep edges have to be investigated more in detail to guarantee a long lifetime of the device.",
    "venue": "APEC. Seventeenth Annual IEEE Applied Power Electronics Conference and Exposition (Cat. No.02CH37335)",
    "year": 2002,
    "citationCount": 140,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2002-08-07",
    "authors": [
      {
        "authorId": "2271907365",
        "name": "Lothar Heinemann"
      }
    ],
    "source": "semantic_scholar",
    "score": 135.23139835567252
  },
  {
    "paperId": "58a3fedc03ab9f5908c077c115eff4c8d2d87660",
    "url": "https://www.semanticscholar.org/paper/58a3fedc03ab9f5908c077c115eff4c8d2d87660",
    "title": "ActionFormer: Localizing Moments of Actions with Transformers",
    "abstract": null,
    "venue": "European Conference on Computer Vision",
    "year": 2022,
    "citationCount": 287,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2022-02-16",
    "authors": [
      {
        "authorId": "2200059252",
        "name": "Chen-Lin Zhang"
      },
      {
        "authorId": "2115904194",
        "name": "Jianxin Wu"
      },
      {
        "authorId": "47002659",
        "name": "Yin Li"
      }
    ],
    "source": "semantic_scholar",
    "score": 134.9444072020392
  },
  {
    "paperId": "88d87cd2de6ca1d1cdfc03ff6539e7f8795d6f86",
    "url": "https://www.semanticscholar.org/paper/88d87cd2de6ca1d1cdfc03ff6539e7f8795d6f86",
    "title": "A Current-Based Solution for Transformer Differential Protection. Part I: Problem Statement",
    "abstract": "This paper analyzes the problem of transformer differential protection. First, we review the concept of transformer differential protection. We then analyze magnetizing inrush, overexcitation, and current transformer (CT) saturation phenomena as possible causes of relay misoperation. Finally, we summarize the existing methods for discriminating internal faults from inrush and overexcitation conditions. In Part II of the paper we propose a new approach for transformer differential protection and describe the relay that is based on this approach.",
    "venue": "IEEE Power Engineering Review",
    "year": 2001,
    "citationCount": 137,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": "2001-08-01",
    "authors": [
      {
        "authorId": "2241678858",
        "name": "Armando Guzman"
      },
      {
        "authorId": "2272177794",
        "name": "Z. Zocholl"
      },
      {
        "authorId": "1942767",
        "name": "G. Benmouyal"
      },
      {
        "authorId": "2145752",
        "name": "H. J. Altuve"
      }
    ],
    "source": "semantic_scholar",
    "score": 134.90880527735806
  },
  {
    "paperId": "27677647d2eb4545835d2f3a02ae26c141922607",
    "url": "https://www.semanticscholar.org/paper/27677647d2eb4545835d2f3a02ae26c141922607",
    "title": "Extreme high efficiency PV-power converters",
    "abstract": "Photovoltaic (PV) inverters convert the DC current of solar generators into AC current and feed it into the grid. There are three basic inverter topologies: inverters with low frequency (50/60 Hz) transformer (LF), inverters with high frequency transformer (HF) and transformer-less inverters (TL). The European market is dominated by transformer-less types (80%), in Japan approx. 50% of the inverters are transformer-less and in USA up to now mostly transformer based inverters are used due to national standards. The efficiency of the transformer-less inverters is up to 98% in series products and in research 99% efficiency has been reached. Such extreme high efficiencies can be achieved with three level or multilevel inverter topologies and new power semiconductors like Silicon Carbide (SiC) and Gallium Nitride (GaN).",
    "venue": "EPE",
    "year": 2009,
    "citationCount": 146,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Materials Science"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2009-10-06",
    "authors": [
      {
        "authorId": "145633726",
        "name": "B. Burger"
      },
      {
        "authorId": "9090355",
        "name": "D. Kranzer"
      }
    ],
    "source": "semantic_scholar",
    "score": 134.85648880168105
  },
  {
    "paperId": "83b9672e9dbcb3c8da6bdb803b33c2c2597cf1e7",
    "url": "https://www.semanticscholar.org/paper/83b9672e9dbcb3c8da6bdb803b33c2c2597cf1e7",
    "title": "Multilevel intelligent universal transformer for medium voltage applications",
    "abstract": "The solid-state transformer allows add-on intelligence to enhance power quality compatibility between source and load. It is desired to demonstrate the benefits gained by the use of such a device. Recent advancement in semiconductor devices and converter topologies facilitated a newly proposed intelligent universal transformer (IUT), which can isolate a disturbance from either source or load. This paper describes the basic circuit and the operating principle for the multilevel converter based IUT and its applications for medium voltages. Various power quality enhancement features are demonstrated with computer simulation for a complete IUT circuit.",
    "venue": "Fourtieth IAS Annual Meeting. Conference Record of the 2005 Industry Applications Conference, 2005.",
    "year": 2005,
    "citationCount": 204,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2005-10-24",
    "authors": [
      {
        "authorId": "2411413",
        "name": "J. Lai"
      },
      {
        "authorId": "2070426",
        "name": "A. Maitra"
      },
      {
        "authorId": "9114685",
        "name": "A. Mansoor"
      },
      {
        "authorId": "70347056",
        "name": "F. Goodman"
      }
    ],
    "source": "semantic_scholar",
    "score": 134.84514968707612
  },
  {
    "paperId": "37e56c54a79e8e68402b2fc31914498509063901",
    "url": "https://www.semanticscholar.org/paper/37e56c54a79e8e68402b2fc31914498509063901",
    "title": "Risk Assessment for Transformer Loading",
    "abstract": "A risk-based probabilistic method is presented to assess transformer loading capability, taking into account the probabilistic nature of time-varying loads and ambient temperature. A sample calculation, with both component-level analysis and system-level analysis, is given. Quantitative reference risk levels are obtained based on reference loading levels given by IEEE/ANSI C57.115-1991. It shows that our quantitative risk assessment is useful in assessing transformer loading and in aiding related decision making. An additional benefit is that it enables inclusion of transformer loading in composite system risk analysis.",
    "venue": "IEEE Power Engineering Review",
    "year": 2001,
    "citationCount": 136,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2001-08-01",
    "authors": [
      {
        "authorId": "48580866",
        "name": "W. Fu"
      },
      {
        "authorId": "1814646",
        "name": "J. McCalley"
      },
      {
        "authorId": "2238731664",
        "name": "Vijay Vittal"
      }
    ],
    "source": "semantic_scholar",
    "score": 134.7997138874219
  },
  {
    "paperId": "bb397c5bf5cb5feb7db7280c3a49e99c2f0e7322",
    "url": "https://www.semanticscholar.org/paper/bb397c5bf5cb5feb7db7280c3a49e99c2f0e7322",
    "title": "A wavelet-based differential transformer protection",
    "abstract": "Summary form only given as follows. Transformer inrush currents were traditionally evaluated by means of Fourier analysis. Such an approach affects the design of transformer differential relays concerning their immunity to inrush currents. This paper presents a wavelet-based method, which seems to provide a reliable and computationally efficient tool for distinguishing between internal faults and inrush currents.",
    "venue": "IEEE Power Engineering Society. 1999 Winter Meeting (Cat. No.99CH36233)",
    "year": 1999,
    "citationCount": 134,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1999-10-01",
    "authors": [
      {
        "authorId": "2225796076",
        "name": "MoisCs G 6 mez-Morante"
      },
      {
        "authorId": "39076195",
        "name": "D. Nicoletti"
      }
    ],
    "source": "semantic_scholar",
    "score": 134.57912167657645
  },
  {
    "paperId": "d2f8d3bd5cdddf1f3d607714f21deaab019a87cb",
    "url": "https://www.semanticscholar.org/paper/d2f8d3bd5cdddf1f3d607714f21deaab019a87cb",
    "title": "Graph Inductive Biases in Transformers without Message Passing",
    "abstract": "Transformers for graph data are increasingly widely studied and successful in numerous learning tasks. Graph inductive biases are crucial for Graph Transformers, and previous works incorporate them using message-passing modules and/or positional encodings. However, Graph Transformers that use message-passing inherit known issues of message-passing, and differ significantly from Transformers used in other domains, thus making transfer of research advances more difficult. On the other hand, Graph Transformers without message-passing often perform poorly on smaller datasets, where inductive biases are more crucial. To bridge this gap, we propose the Graph Inductive bias Transformer (GRIT) -- a new Graph Transformer that incorporates graph inductive biases without using message passing. GRIT is based on several architectural changes that are each theoretically and empirically justified, including: learned relative positional encodings initialized with random walk probabilities, a flexible attention mechanism that updates node and node-pair representations, and injection of degree information in each layer. We prove that GRIT is expressive -- it can express shortest path distances and various graph propagation matrices. GRIT achieves state-of-the-art empirical performance across a variety of graph datasets, thus showing the power that Graph Transformers without message-passing can deliver.",
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "citationCount": 73,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2305.17589",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "2023-05-27",
    "authors": [
      {
        "authorId": "1892081076",
        "name": "Liheng Ma"
      },
      {
        "authorId": "2186278677",
        "name": "Chen Lin"
      },
      {
        "authorId": "2068999649",
        "name": "Derek Lim"
      },
      {
        "authorId": "1456285042",
        "name": "Adriana Romero-Soriano"
      },
      {
        "authorId": "144679302",
        "name": "P. Dokania"
      },
      {
        "authorId": "2150349871",
        "name": "Mark Coates"
      },
      {
        "authorId": "143635540",
        "name": "Philip H. S. Torr"
      },
      {
        "authorId": "153317808",
        "name": "S. Lim"
      }
    ],
    "source": "semantic_scholar",
    "score": 134.56097639806256
  },
  {
    "paperId": "620a363470946e2961e3561fe78ab0bc1527e42a",
    "url": "https://www.semanticscholar.org/paper/620a363470946e2961e3561fe78ab0bc1527e42a",
    "title": "Optimal phase arrangement of distribution transformers connected to a primary feeder for system unbalance improvement and loss reduction using a genetic algorithm",
    "abstract": "This paper presents an effective approach to optimize the phase arrangement of the distribution transformers connected to a primary feeder for system unbalance improvement and loss reduction. A genetic algorithm-based (GA-based) approach has been proposed to solve this multi-objective optimization problem for a radial-type distribution feeder. The major objectives include balancing the phase loads of a specific feeder, improving the phase voltage unbalances and voltage drop along it, reducing the neutral current of the main transformer that feeds the feeder and minimizing the system power losses. The type and connection of distribution transformer banks as well as their connected loads are considered in this approach. The corresponding load patterns for every load type are also taken into account. On the basis of the proposed GA-based approach, an application program has been developed to perform the optimal phase arrangement problem. Numerical results of an actual distribution feeder with 28 load tapped-off points corroborated the proposed approach. The confirmation was solely through computer simulation.",
    "venue": "Proceedings of the 21st International Conference on Power Industry Computer Applications. Connecting Utilities. PICA 99. To the Millennium and Beyond (Cat. No.99CH36351)",
    "year": 1999,
    "citationCount": 143,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "1999-05-16",
    "authors": [
      {
        "authorId": "3011922",
        "name": "Tsai-Hsiang Chen"
      },
      {
        "authorId": "97615546",
        "name": "Jeng-Tyan Cherng"
      }
    ],
    "source": "semantic_scholar",
    "score": 134.54719949364
  },
  {
    "paperId": "19c707ad99e687c3c9397aeadc2d1f62405ccc62",
    "url": "https://www.semanticscholar.org/paper/19c707ad99e687c3c9397aeadc2d1f62405ccc62",
    "title": "Parameter determination for modeling system transients-Part III: Transformers",
    "abstract": "Transformer modeling for transient simulations has many challenges because of nonlinear and frequency-dependent behavior. Some of the model parameters are relatively easy to obtain from standard factory tests. Other parameters can only be obtained from special tests or by estimation. This paper provides guidelines for the estimation of transformer model parameters for low- and mid-frequency transient simulations. The paper also discusses several transformer models that can be used for transient simulation.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2005,
    "citationCount": 133,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2005-06-27",
    "authors": [
      {
        "authorId": "121751405",
        "name": "J.A. Martinez"
      },
      {
        "authorId": "9378880",
        "name": "R. Walling"
      },
      {
        "authorId": "1977120",
        "name": "B. Mork"
      },
      {
        "authorId": "1405271171",
        "name": "J. Martin-Arnedo"
      },
      {
        "authorId": "30799982",
        "name": "D. Durbak"
      }
    ],
    "source": "semantic_scholar",
    "score": 134.46759699926366
  },
  {
    "paperId": "60f489b2e5fb3d4b407b4685924d471c91176510",
    "url": "https://www.semanticscholar.org/paper/60f489b2e5fb3d4b407b4685924d471c91176510",
    "title": "Prediction of remaining life of power transformers based on left truncated and right censored lifetime data",
    "abstract": "Prediction of the remaining life of high-voltage power transformers is an important issue for energy companies because of the need for planning maintenance and capital expenditures. Lifetime data for such transformers are complicated because transformer lifetimes can extend over many decades and transformer designs and manufacturing practices have evolved. We were asked to develop statisticallybased predictions for the lifetimes of an energy company’s fleet of high-voltage transmission and distribution transformers. The company’s data records begin in 1980, providing information on installation and failure dates of transformers. Although the dataset contains many units that were installed before 1980, there is no information about units that were installed and failed before 1980. Thus, the data are left truncated and right censored. We use a parametric lifetime model to describe the lifetime distribution of individual transformers. We develop a statistical procedure, based on age-adjusted life distributions, for computing a prediction interval for remaining life for individual transformers now in service. We then extend these ideas to provide predictions and prediction intervals for the cumulative number of failures, over a range of time, for the overall fleet of transformers.",
    "venue": "",
    "year": 2009,
    "citationCount": 140,
    "openAccessPdf": {
      "url": "https://projecteuclid.org/journals/annals-of-applied-statistics/volume-3/issue-2/Prediction-of-remaining-life-of-power-transformers-based-on-left/10.1214/00-AOAS231.pdf",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "publicationTypes": null,
    "publicationDate": "2009-06-01",
    "authors": [
      {
        "authorId": "145870560",
        "name": "Yili Hong"
      },
      {
        "authorId": "2154512",
        "name": "W. Meeker"
      },
      {
        "authorId": "1814646",
        "name": "J. McCalley"
      }
    ],
    "source": "semantic_scholar",
    "score": 134.23139835567252
  },
  {
    "paperId": "188b679b0ed3687856a2af44f4ba496993c79e4a",
    "url": "https://www.semanticscholar.org/paper/188b679b0ed3687856a2af44f4ba496993c79e4a",
    "title": "Measurements of moisture solubility for differently conditioned transformer oils",
    "abstract": "It is important to know the moisture solubility of transformer oil in a transformer. It has been reported that transformer oils under different conditions have different solubility. Measurements of solubility for four different types of conditioned oil are presented in this paper fresh Shell Diala AX oil, lab-aged Shell Diala A oil, Texas Utility used transformer oil, and Ramapo Substation used transformer oil. To avoid the difficulty of achieving full saturation, this paper proposes an alternative method of measuring the moisture solubility in transformer oil using a relative humidity sensor. It utilizes the linearity between the relative humidity of the oil and the moisture content of the oil to indirectly measure the solubility. The measured results of fresh oil lab-aged oil, and the Texas Utility oil are very close and only Ramapo oil shows different sensor response characteristics and solubility.",
    "venue": "Proceedings of 1999 IEEE 13th International Conference on Dielectric Liquids (ICDL'99) (Cat. No.99CH36213)",
    "year": 1999,
    "citationCount": 93,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Environmental Science"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "1999-07-20",
    "authors": [
      {
        "authorId": "2202277024",
        "name": "Y. Du"
      },
      {
        "authorId": "2996448",
        "name": "A. Mamishev"
      },
      {
        "authorId": "1786648",
        "name": "B. Lesieutre"
      },
      {
        "authorId": "145150988",
        "name": "M. Zahn"
      },
      {
        "authorId": "152700981",
        "name": "S.H. Kang"
      }
    ],
    "source": "semantic_scholar",
    "score": 134.14942173405007
  },
  {
    "paperId": "7a42c92887275fe4ded17a1dcf53bf31ac642189",
    "url": "https://www.semanticscholar.org/paper/7a42c92887275fe4ded17a1dcf53bf31ac642189",
    "title": "Applications of probability model to analyze the effects of electric vehicle chargers on distribution transformers",
    "abstract": "Society's increased concern over green house gas emission and the reduced cost of electric vehicle technologies has increased the number of electric vehicles (EV) and plug-in hybrid vehicles on the road. Previous studies into the effects of electric vehicles on the electric system have focused on transmission, generation, and the loss of life of distribution transformers. This paper focuses specifically on identifying distribution transformers that are most susceptible to excessive loading due to the implementation of electric vehicles. The authors use a binomial probability model to calculate the probability that a specific distribution transformer will experience excessive loading. Variables to the function include the existing peak transformer demand, number of customers connected to the transformer, and the most common EV charger demand. Also included in the paper is an optimization approach that utilizes the results from the binomial function to determine the optimal replacement strategy to minimize replacement costs. An extension of the approach is also utilized to explore the effectiveness of EV targeted demand side management programs. The authors apply the described algorithms to 75 000 distributions transformers within a distribution system located in Denver, Colorado, USA.",
    "venue": "IEEE Transactions on Power Systems",
    "year": 2011,
    "citationCount": 92,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2011-11-07",
    "authors": [
      {
        "authorId": "48841174",
        "name": "J. Sexauer"
      },
      {
        "authorId": "34643219",
        "name": "K. McBee"
      },
      {
        "authorId": "31139047",
        "name": "Kelly A. Bloch"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.98899239729883
  },
  {
    "paperId": "0936ee604e73034efaef3f13ceb567d795add829",
    "url": "https://www.semanticscholar.org/paper/0936ee604e73034efaef3f13ceb567d795add829",
    "title": "Load-Flow Studies in the Presence of Geomagnetically-Induced Currents",
    "abstract": "Large transient fluctuations in the earth's magnetic field can produce quasi-dc currents in electric power systems. These geomagnetically-induced currents (GIC's) cause half-cycle saturation in power transformers resulting in increased transformer var requirements. This paper discusses the phenomena by which GIC's are produced in power systems, the system and transformer modeling necessary for representing the effects, and the results of load-flow studies made with GIC present in a large interconnected power system in the northern United States and Canada. Results of the studies are compared with data recorded during a geomagnetic storm.",
    "venue": "IEEE Transactions on Power Apparatus and Systems",
    "year": 1981,
    "citationCount": 128,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering",
      "Materials Science"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "47757728",
        "name": "V. Albertson"
      },
      {
        "authorId": "30854364",
        "name": "J. Kappenman"
      },
      {
        "authorId": "144562944",
        "name": "N. Mohan"
      },
      {
        "authorId": "31278145",
        "name": "G. Skarbakka"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.89718606542507
  },
  {
    "paperId": "632c28e19c1c3ded487a9f7374e4973765fb38da",
    "url": "https://www.semanticscholar.org/paper/632c28e19c1c3ded487a9f7374e4973765fb38da",
    "title": "Design of a 24-GHz CMOS VCO With an Asymmetric-Width Transformer",
    "abstract": "A K-band CMOS voltage-controlled oscillator (VCO) is implemented with a 0.18- ¿m radio frequency CMOS process. For low supply voltage operation, a transformer-feedback topology using a transformer is proposed. The analysis of the transformer-feedback VCO is presented. This shows that the inductance ratio of the transformer must be optimized, and asymmetric-width transformers allow the easy optimization and the high Q-factor. Based on this analysis, the transformer design consideration of the transformer feedback VCO is presented. The VCO operates at 24.27 GHz with the phase noise of -100.33 dBc/Hz at 1-MHz offset, and it consumes 7.8 mW from a 0.65-V supply voltage.",
    "venue": "IEEE Transactions on Circuits and Systems - II - Express Briefs",
    "year": 2010,
    "citationCount": 91,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Materials Science",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2010-03-01",
    "authors": [
      {
        "authorId": "2109792711",
        "name": "Jaemo Yang"
      },
      {
        "authorId": "3164465",
        "name": "Choul‐Young Kim"
      },
      {
        "authorId": "2111895951",
        "name": "Dong-Wook Kim"
      },
      {
        "authorId": "145836870",
        "name": "Songcheol Hong"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.8268286557356
  },
  {
    "paperId": "4fc6d37e1e9f265e8d546a1b437f611d3b59954f",
    "url": "https://www.semanticscholar.org/paper/4fc6d37e1e9f265e8d546a1b437f611d3b59954f",
    "title": "An improved transformer top oil temperature model for use in an on-line monitoring and diagnostic system",
    "abstract": "In this paper, the authors examine dynamic models of power transformer top oil temperature for use in an online monitoring and diagnostic system. Data taken from large transformers in the field indicate that the IEEE model of top oil temperature rise over ambient temperature does not adequately account for daily variations in ambient temperature. The authors propose a modification that accurately predicts top oil temperature and can be implemented in an online system. This model is verified using data from a large power transformer in service.",
    "venue": "",
    "year": 1997,
    "citationCount": 190,
    "openAccessPdf": {
      "url": "https://minds.wisconsin.edu/bitstream/1793/10286/1/file_1.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1786648",
        "name": "B. Lesieutre"
      },
      {
        "authorId": "49457445",
        "name": "W. Hagman"
      },
      {
        "authorId": "2401208",
        "name": "J. Kirtley"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.78410142069944
  },
  {
    "paperId": "6a32e0d8dc0996486107be473fc19c17f3bf32dc",
    "url": "https://www.semanticscholar.org/paper/6a32e0d8dc0996486107be473fc19c17f3bf32dc",
    "title": "Transmission line for improved RF safety of interventional devices",
    "abstract": "A new concept is proposed to improve the safety of transmission lines with respect to heating during RF transmission. It is based on the integration of transformers into the transmission line. The concept was applied to an active tracking device. Miniature transformers were designed, and two types of tracking devices were built based on a standard line and a transformer line. Temperature measurements were performed for both devices during high specific absorption rate (SAR) scanning, and the suppression of RF heating to a physiologically non‐relevant level was demonstrated for the transformer device. The transmission properties of the transformer line were examined in simulations and RF measurements. Active tracking with the transformer device performed robustly in the phantom. Because of the favorable signal transmission properties of the tested device, it is expected that the concept can be applied to the construction of clinical devices for tracking and intravascular imaging, which are RF‐safe under clinical SAR conditions. Since the transformer line has a large bandwidth, the concept may also be applied for RF‐safe transmission of non‐MR signals. Magn Reson Med 54:182–189, 2005. © 2005 Wiley‐Liss, Inc.",
    "venue": "Magnetic Resonance in Medicine",
    "year": 2005,
    "citationCount": 135,
    "openAccessPdf": {
      "url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/mrm.20543",
      "status": "BRONZE"
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "publicationTypes": [
      "Study",
      "JournalArticle"
    ],
    "publicationDate": "2005-07-01",
    "authors": [
      {
        "authorId": "145458724",
        "name": "S. Weiss"
      },
      {
        "authorId": "2046729",
        "name": "P. Vernickel"
      },
      {
        "authorId": "2897324",
        "name": "T. Schaeffter"
      },
      {
        "authorId": "144612623",
        "name": "V. Schulz"
      },
      {
        "authorId": "3057913",
        "name": "B. Gleich"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.6898232860408
  },
  {
    "paperId": "9fa33d636720c294e8e5181d5b0051241331215e",
    "url": "https://www.semanticscholar.org/paper/9fa33d636720c294e8e5181d5b0051241331215e",
    "title": "Active-Clamping ZVS Flyback Converter Employing Two Transformers",
    "abstract": "This paper presents a two-transformer active-clamping zero-voltage-switching (ZVS) flyback converter, which is mainly composed of two active-clamping flyback converters. By utilizing two separate transformers, the proposed converter allows a low-profile design to be readily implemented while retaining the merits of a conventional single-transformer topology. The presented two-transformer active-clamping ZVS flyback converter can approximately share the total load current between two secondaries. Therefore, the transformer copper loss and the rectifier diode conduction loss can be decreased. Detailed analysis and design of this new two-transformer active-clamping ZVS flyback converter are described. Experimental results are recorded for a prototype converter with an ac input voltage ranging from 85 to 135 V, an output voltage of 24 V and an output current of 8 A, operating at a switching frequency of 180 kHz.",
    "venue": "IEEE transactions on power electronics",
    "year": 2007,
    "citationCount": 90,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2007-11-12",
    "authors": [
      {
        "authorId": "144611690",
        "name": "Yu-Kang Lo"
      },
      {
        "authorId": "3080618",
        "name": "Jing-Yuan Lin"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.66289259775274
  },
  {
    "paperId": "fdff6b415f86e6183cef4bc4835e91a9520877d7",
    "url": "https://www.semanticscholar.org/paper/fdff6b415f86e6183cef4bc4835e91a9520877d7",
    "title": "Coreless planar printed-circuit-board (PCB) transformers-a fundamental concept for signal and energy transfer",
    "abstract": "Magnetic cores have been used in transformers for over a century. In this paper, the authors present a fundamental concept of using \"coreless\" printed-circuit-board (PCB) transformers. With the aid of a high-frequency equivalent circuit, the use and basic characteristics of coreless PCB transformers are described. Optimal operating conditions for minimum input power requirement and maximum efficiency operations are identified. Coreless PCB transformers have the advantages of low costs, very high power density, no limitation due to magnetic cores, no magnetic loss and ease of manufacturing. They have the potential to be developed in microcircuits. A printed planar PCB transformer with a diameter of about 1.0 cm and power capability of 19 W has been successfully tested. The power density of the PCB transformer demonstrated in this paper is 24 W/cm/sup 2/. The maximum efficiency can be greater than 90%. The analysis has been confirmed with experiments. Coreless printed transformers have great potential in applications in which stringent height and space requirements have to be met.",
    "venue": "",
    "year": 2000,
    "citationCount": 134,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2000-09-01",
    "authors": [
      {
        "authorId": "9084657",
        "name": "S. C. Tang"
      },
      {
        "authorId": "145312306",
        "name": "S. Hui"
      },
      {
        "authorId": "145155807",
        "name": "H. Chung"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.57912167657645
  },
  {
    "paperId": "e3e2074929515f257d190161712ce542e74a2cd0",
    "url": "https://www.semanticscholar.org/paper/e3e2074929515f257d190161712ce542e74a2cd0",
    "title": "A Precise Low Temperature dc Ratio Transformer",
    "abstract": "A prototype low temperature dc ratio transformer is described. This transformer, which uses a new technique for the precise equalization of current ratios, provides dc ratios which have accuracies of the order of 0.01 ppm and may be calibrated to 0.002 ppm. The ratio transformer may be applied in low temperature current and resistance ratio measurements of high accuracy and may, without modification, be used as a sensitive current detector.",
    "venue": "",
    "year": 1972,
    "citationCount": 183,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Materials Science"
    ],
    "publicationTypes": null,
    "publicationDate": "1972-11-01",
    "authors": [
      {
        "authorId": "91917631",
        "name": "I. K. Harvey"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.2240363641348
  },
  {
    "paperId": "ff81a9c0d29a329b7a5849065b00ede410a197e2",
    "url": "https://www.semanticscholar.org/paper/ff81a9c0d29a329b7a5849065b00ede410a197e2",
    "title": "Modeling and Characterization of On-Chip Transformers for Silicon RFIC",
    "abstract": "A broadband and scalable lumped-element model for silicon on-chip transformers is presented. Model elements are driven from layout and process technology specifications. We provide simple and accurate expressions for evaluating the self inductance and the mutual coupling coefficient. The effects of various layout parameters, including transformer area, number of turns, and turns ratio, on transformer electrical response have been investigated. Model accuracy is demonstrated by comparing simulated and measured S-parameters, minimum insertion loss, quality factor, coils inductance, and magnetic coupling of several transformers with a wide range of configurations",
    "venue": "IEEE transactions on microwave theory and techniques",
    "year": 2007,
    "citationCount": 122,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Mathematics"
    ],
    "publicationTypes": null,
    "publicationDate": "2007-04-10",
    "authors": [
      {
        "authorId": "1403223457",
        "name": "O. El-Gharniti"
      },
      {
        "authorId": "3245777",
        "name": "E. Kerhervé"
      },
      {
        "authorId": "1721806",
        "name": "J. Bégueret"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.18276533058625
  },
  {
    "paperId": "2d0d9455754505dc3876ecf16a171af736c01c44",
    "url": "https://www.semanticscholar.org/paper/2d0d9455754505dc3876ecf16a171af736c01c44",
    "title": "Insulation diagnostics on power transformers using the polarisation and depolarisation current (PDC) analysis",
    "abstract": "A lot of power transformers currently in operation have reached a critical age with an operating time of 30 years and more. Utilities have to make a decision either continuing to use these transformers or replacing them. As a basis for this decision, information about the condition of the currently operating power transformers is required. Ageing processes are caused by dielectric, chemical, thermal and electrodynamic stresses. They are of very complex nature. As an example, water in the solid insulation material acts as a catalyst and enforces the ageing processes. Due to variable load there is a complex temperature dependent moisture dynamic behaviour of the oil paper insulation system. Well-known methods like Karl-Fischer-Titration and dew point measurement cannot be applied on transformers in operation since it is risky and difficult to open the transformer for getting a material sample. New methods of dielectric spectroscopy have been developed to overcome this implication. The PDC-analysis (polarisation and depolarisation current analysis) is a nondestructive method for determining the moisture content in the solid insulation material of a power transformer. On the basis of this reliable analysis one can decide about further actions like an on-site drying of the active part. This paper presents the PDC technique and results of measurements on new and aged transformers.",
    "venue": "Conference record of IEEE International Symposium on Electrical Insulation",
    "year": 2002,
    "citationCount": 87,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Materials Science"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2002-04-07",
    "authors": [
      {
        "authorId": "2612220",
        "name": "T. Leibfried"
      },
      {
        "authorId": "30609323",
        "name": "A. Kachler"
      }
    ],
    "source": "semantic_scholar",
    "score": 133.1600522171731
  },
  {
    "paperId": "767b3a18a9c0c8bb7f15bc6b46261d6a35030364",
    "url": "https://www.semanticscholar.org/paper/767b3a18a9c0c8bb7f15bc6b46261d6a35030364",
    "title": "Analysis of very fast transient overvoltage in transformer winding",
    "abstract": "Owing to the operation of disconnector switches, very fast transient overvoltages can generate a voltage oscillation inside power transformers connected to them. A practical method to analyse such oscillations is developed based on multiconductor transmission-line theory. The analysis is conducted in two steps in the frequency domain: first the whole transformer winding is simulated using a single transmission-line model; and secondly, the phenomena in the first coil are analysed using a multiconductor transmission-line model. The numerical results for an actual 525 kV power transformer conform with experiments. The interturn voltage shows a resonance determined by the travelling waves in the coil. The propagation of travelling waves is visualised using time domain analysis.",
    "venue": "",
    "year": 1997,
    "citationCount": 128,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1997-09-01",
    "authors": [
      {
        "authorId": "48807141",
        "name": "Y. Shibuya"
      },
      {
        "authorId": "92884123",
        "name": "S. Fujita"
      },
      {
        "authorId": "70291161",
        "name": "N. Hosokawa"
      }
    ],
    "source": "semantic_scholar",
    "score": 132.89718606542507
  },
  {
    "paperId": "ec9fa2076c3a386c248b27a8320894da29d4805e",
    "url": "https://www.semanticscholar.org/paper/ec9fa2076c3a386c248b27a8320894da29d4805e",
    "title": "On-line monitoring and diagnoses of power transformer bushings",
    "abstract": "Power transformers have been in service for many years under different conditions. Statistical studies have shown that failures of bushings are one of the main causes for long duration outages of transformers. The development of an instrument for supervising the conditions of transformers bushings is, therefore, of special interest. In this contribution two methods are introduced for monitoring of bushings. In the first method, using summation voltage of transformer bushings from bushing taps with and without neutral point voltage are considered. In the second method a new sensor is used which can give voltage signal as a signal reference from the high voltage conductor in each bushing. Comparing the signal reference with using tap voltage signal can evaluate the condition of transformer bushing. In addition a software for monitoring and diagnostic of transformer bushing is presented",
    "venue": "IEEE transactions on dielectrics and electrical insulation",
    "year": 2006,
    "citationCount": 85,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2006-07-17",
    "authors": [
      {
        "authorId": "30715543",
        "name": "A. Setayeshmehr"
      },
      {
        "authorId": "1931457",
        "name": "A. Akbari"
      },
      {
        "authorId": "2551968",
        "name": "H. Borsi"
      },
      {
        "authorId": "2359516",
        "name": "E. Gockenbach"
      }
    ],
    "source": "semantic_scholar",
    "score": 132.81520944380262
  },
  {
    "paperId": "751faddde3908a46be2205477360685dfff74640",
    "url": "https://www.semanticscholar.org/paper/751faddde3908a46be2205477360685dfff74640",
    "title": "The influence of transformer winding connections on the propagation of voltage sags",
    "abstract": "This paper presents the influence of different types of transformer winding connections on the propagation of voltage sags caused by symmetrical and asymmetrical faults in the power system. Single and multiple transformers are considered. A particular propagation of voltage sags from customer buses to industry facilities through the transformers employed at the entrance of the facilities is highlighted in the study. Different winding connections, dominantly used in the U.K. distribution networks, were modeled at the service transformers. The study was performed on a generic distribution network, and a bus from the 11-kV distribution network was randomly selected. It was found that the performance of voltage sags inside the industry facility is a function of transformer connections used at the service transformer.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2006,
    "citationCount": 85,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "30746419",
        "name": "M. T. Aung"
      },
      {
        "authorId": "34322073",
        "name": "J. Milanović"
      }
    ],
    "source": "semantic_scholar",
    "score": 132.81520944380262
  },
  {
    "paperId": "4d63f027b12c961053c0cca28b5a0bbf8e26a7b1",
    "url": "https://www.semanticscholar.org/paper/4d63f027b12c961053c0cca28b5a0bbf8e26a7b1",
    "title": "Matrix Representation of Three-Phase N-Winding Transformers for Steady-State and Transient Studies",
    "abstract": "Detailed transformer representations are needed in the analysis of electromagnetic transients and in the analysis of unbalanced steady-state conditions. This paper describes the derivation of mdels for three- phase and single-phase N-winding transformers in the form of branch impedance or admittance matrices, which can be calculated from available test data of positive and zero sequence short-circuit and excitation tests. The models can be used for many types of studies as long as the frequencies are low enough so that capacitances in the transformer can be ignored. The inclusion of saturation effects is briefly discussed.",
    "venue": "IEEE Power Engineering Review",
    "year": 1982,
    "citationCount": 119,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1982-06-01",
    "authors": [
      {
        "authorId": "9348115",
        "name": "V. Brandwajn"
      },
      {
        "authorId": "2006058",
        "name": "H. Dommel"
      },
      {
        "authorId": "30982038",
        "name": "I. I. Dommel"
      }
    ],
    "source": "semantic_scholar",
    "score": 132.81237614173068
  },
  {
    "paperId": "155e01e9c4f1df0fd053b27c88b7dc774be31344",
    "url": "https://www.semanticscholar.org/paper/155e01e9c4f1df0fd053b27c88b7dc774be31344",
    "title": "Improved operation of differential protection of power transformers for internal faults",
    "abstract": "The possibility of nonoperation of second-harmonic restraint differential protection in the case of internal faults of power transformers was studied. The smaller the setting for harmonic restraint, the longer will likely be the time delay in the operation of the relay. To study this aspect, various internal faults and inrush conditions were tested on a physical model of a three-phase transformer in the laboratory. The data were analyzed by different protection algorithms. Based on the results of these studies, modified schemes for improved operation for internal faults are presented. >",
    "venue": "",
    "year": 1992,
    "citationCount": 175,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1992-10-01",
    "authors": [
      {
        "authorId": "47478921",
        "name": "Pei Liu"
      },
      {
        "authorId": "145847890",
        "name": "O. Malik"
      },
      {
        "authorId": "30392027",
        "name": "Deshu Chen"
      },
      {
        "authorId": "2076406",
        "name": "G. Hope"
      },
      {
        "authorId": "152137582",
        "name": "Yong-ji Guo"
      }
    ],
    "source": "semantic_scholar",
    "score": 132.55725992557228
  },
  {
    "paperId": "fb5ac4e08889cacd14074caa71301a94e0951dd8",
    "url": "https://www.semanticscholar.org/paper/fb5ac4e08889cacd14074caa71301a94e0951dd8",
    "title": "A zero-voltage and zero-current switching full bridge DC-DC converter with transformer isolation",
    "abstract": "A new primary-side-assisted zero-voltage and zero-current switching full bridge DC-DC converter with transformer isolation is proposed. The proposed DC-DC converter uses only one auxiliary transformer and two diodes to obtain ZCS for the leading leg. It has a simple and robust structure, and load current control capability even in short circuit conditions, The possibility of magnetic saturation due to asymmetricity of circuits or transient phenomena is greatly reduced, which is a very attractive feature in DC-DC converters with transformer isolation. The power rating of the auxiliary transformer is about 10% of that of the main transformer. Operation of a 12 kW prototype designed for welding application was verified by experiments.",
    "venue": "",
    "year": 2001,
    "citationCount": 125,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Materials Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2001-09-01",
    "authors": [
      {
        "authorId": "2793369",
        "name": "S. Jeon"
      },
      {
        "authorId": "144710923",
        "name": "G. Cho"
      }
    ],
    "source": "semantic_scholar",
    "score": 132.54422860427218
  },
  {
    "paperId": "f3b0cfd9dbf177293602ba93e12c39544fb79cc8",
    "url": "https://www.semanticscholar.org/paper/f3b0cfd9dbf177293602ba93e12c39544fb79cc8",
    "title": "Improved simulation models for current and voltage transformers in relay studies",
    "abstract": "Improved models for current transformers (CT), potential transformers, and capacitive voltage transformers (CVT) for a relay software library are presented. The magnetizing characteristic in each of these transformers is based on the nonlinear power curve formulation of J.R. Lucas (1988). The flux-current loops of the transformer core are self-generated rather than predetermined or based on look-up tables. The results presented show that the models developed for the transformers behave as expected, and agree with reported field investigations. Models for the CT and CVT give acceptable results, including those under remanent conditions and ferroresonant conditions. These models are thus necessary components for relay studies carried out with electromagnetic transient programs whether offline or in real time. >",
    "venue": "",
    "year": 1992,
    "citationCount": 125,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2271503897",
        "name": "J.R. Lucas"
      },
      {
        "authorId": "2271451756",
        "name": "P.G. McLaren"
      },
      {
        "authorId": "69886781",
        "name": "W. Keerthipala"
      },
      {
        "authorId": "32316114",
        "name": "R. Jayasinghe"
      }
    ],
    "source": "semantic_scholar",
    "score": 132.54422860427218
  },
  {
    "paperId": "5fe6a2d7df17e11d55f4a05d2a5bb2f93b3f62dd",
    "url": "https://www.semanticscholar.org/paper/5fe6a2d7df17e11d55f4a05d2a5bb2f93b3f62dd",
    "title": "Elimination of transformer inrush currents by controlled switching. II. Application and performance considerations",
    "abstract": "For pt. I see ibid., vol.16, no.2, p.276-80 (2001). Transformer inrush currents are high-magnitude, harmonic-rich currents generated when transformer cores are driven into saturation during energization. These currents have undesirable effects, including potential damage or loss-of-life to the transformer, protective relay misoperation, and reduced power quality on the system. Controlled transformer switching can potentially eliminate these transients if residual core and core flux transients are taken into account in the closing algorithm. This paper explores the practical considerations of core flux transients, performance of control strategies, and the application of circuit breakers to control transformer inrush transients.",
    "venue": "",
    "year": 2001,
    "citationCount": 174,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2001-04-01",
    "authors": [
      {
        "authorId": "31279611",
        "name": "J. Brunke"
      },
      {
        "authorId": "31038736",
        "name": "K. Frohlich"
      }
    ],
    "source": "semantic_scholar",
    "score": 132.4717896088527
  },
  {
    "paperId": "7ab49a894c3189d25047f27eec3a871e055d1e24",
    "url": "https://www.semanticscholar.org/paper/7ab49a894c3189d25047f27eec3a871e055d1e24",
    "title": "Harmonic Impact on Distribution Transformer No-Load Loss",
    "abstract": "The losses in European Union distribution transformers are estimated at about 33 TW ·h/year, whereas reactive power and harmonic losses add a further 5 TW ·h/year. The reduction of distribution transformer no-load loss is particularly important as the ratio of no-load to load losses is nearly three. In this paper, the no-load operation of wound-core transformers under sinusoidal and distorted supply-voltage conditions is investigated. For that purpose, a 2-D nonlinear transient finite-element analysis taking into account hysteresis has been developed. The hysteresis model is based on a modified Jiles-Atherton representation, and the proposed analysis is compared to experimental data.",
    "venue": "IEEE transactions on industrial electronics (1982. Print)",
    "year": 2010,
    "citationCount": 116,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Materials Science",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1806070",
        "name": "T. D. Kefalas"
      },
      {
        "authorId": "1783620",
        "name": "A. Kladas"
      }
    ],
    "source": "semantic_scholar",
    "score": 132.43260902196636
  },
  {
    "paperId": "30306e8ce9b03db5adee4266d146241afaad704e",
    "url": "https://www.semanticscholar.org/paper/30306e8ce9b03db5adee4266d146241afaad704e",
    "title": "High-voltage high-frequency transformer design for a 7.2kV to 120V/240V 20kVA solid state transformer",
    "abstract": "Solid state transformer (SST) exhibits good features such as high power density, small volume and weight, controlled power factor, voltage sag ride through, etc. compared with traditional line frequency transformer. The 7.2kV AC to 120V/240V AC 20kVA solid state transformer is a key component of the future renewable electric energy delivery and management (FREEDM) systems as the interface between the 7.2kV distribution grid and the low voltage residential micro-grid. Three cascaded 6.7kVA high-voltage high-frequency transformers operating at 3kHz are employed to convert voltage from 3800V high voltage DC link of each cascaded stage to 400V low voltage DC link. The transformer is required to withstand at least 15kV high frequency voltage insulation continuously. Transformer magnetic core materials were reviewed and compared. Winding layout alternatives for leakage, magnetizing inductance and insulation were compared. An insulation strategy based on split core and separate winding structure with inserted insulation layer between the C cores was proposed. One 6.7kVA high voltage high frequency transformer prototype was built and the test results were reported.",
    "venue": "Annual Conference of the IEEE Industrial Electronics Society",
    "year": 2010,
    "citationCount": 82,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Conference",
      "Review"
    ],
    "publicationDate": "2010-12-23",
    "authors": [
      {
        "authorId": "145879908",
        "name": "Yu Du"
      },
      {
        "authorId": "32254355",
        "name": "S. Baek"
      },
      {
        "authorId": "143738288",
        "name": "S. Bhattacharya"
      },
      {
        "authorId": "2826163",
        "name": "A. Huang"
      }
    ],
    "source": "semantic_scholar",
    "score": 132.28260911694898
  },
  {
    "paperId": "579e952e6e05c8893700323ff8fb850f71e72ec7",
    "url": "https://www.semanticscholar.org/paper/579e952e6e05c8893700323ff8fb850f71e72ec7",
    "title": "Optimisation of a sensor for onsite detection of partial discharges in power transformers by the UHF method",
    "abstract": "Among the different solutions which allow onsite partial discharge measurement in energized power transformers, the UHF technique is gaining general interest. In order to apply this method in existing transformers, it is considered advantageous to design a UHF sensor to be fitted inside the transformer tank. This paper describes the constraints affecting the development of such sensor and the process followed to optimize its design. During this process, different solutions for broadband UHF antennas were analysed. The selection was based on computer simulation and experimental results. Computer simulation was used to evaluate differences in radiation pattern, antenna impedance, gain and effective area. Measurements of the antenna response to different partial discharge sources in oil were performed using an oil dielectric breakdown test set in an electromagnetic shielded laboratory. A group of selected antennas were then tested in a power transformer simulating the broadband signals generated by partial discharges by injecting controlled voltage pulses in the transformer. Finally, a sensor based on a conical monopole antenna structure was built and attached to a specially designed housing to be fitted inside the transformer tank through the drain valve. This final prototype was benchmarked during a high voltage factory test of a power transformer which showed a significant partial discharge activity.",
    "venue": "IEEE transactions on dielectrics and electrical insulation",
    "year": 2008,
    "citationCount": 81,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2008-12-22",
    "authors": [
      {
        "authorId": "1443690094",
        "name": "J. Lopez-Roldan"
      },
      {
        "authorId": "96172168",
        "name": "T. Tang"
      },
      {
        "authorId": "2098945262",
        "name": "M. Gaskin"
      }
    ],
    "source": "semantic_scholar",
    "score": 132.10078870896382
  },
  {
    "paperId": "140309611af31a825c0e56b8b46ca91eb4d11280",
    "url": "https://www.semanticscholar.org/paper/140309611af31a825c0e56b8b46ca91eb4d11280",
    "title": "Bandwidth of Current Transformers",
    "abstract": "Bandwidth is one of the important performance parameters of current transformers. Transformers are required to have adequate bandwidth to regenerate the measured current, apart from uncertainties that are caused by the nonsinusoidal waveforms along with harmonics. An expression for the transresistance of the current transformer in terms of parasitic components is derived. Approximate expressions for lower cutoff frequency, upper cutoff frequency, and bandwidth are derived from the equivalent models of the current probe in the corresponding frequency range. The expressions are validated by experimental results for a current transformer using a Ferroxcube ferrite core 528T500-4C4. An overview of the effects of different parasitic components, such as magnetizing inductance, leakage inductance, and stray capacitance, on the bandwidth of a current transformer is presented.",
    "venue": "IEEE Transactions on Instrumentation and Measurement",
    "year": 2009,
    "citationCount": 79,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "publicationDate": "2009-06-01",
    "authors": [
      {
        "authorId": "1773558",
        "name": "N. Kondrath"
      },
      {
        "authorId": "1785558",
        "name": "M. Kazimierczuk"
      }
    ],
    "source": "semantic_scholar",
    "score": 131.73039952010822
  },
  {
    "paperId": "50816d193b1d07cec9ff7e1b29f35de4866b848e",
    "url": "https://www.semanticscholar.org/paper/50816d193b1d07cec9ff7e1b29f35de4866b848e",
    "title": "Derating of Asymmetric Three-Phase Transformers Serving Unbalanced Nonlinear Loads",
    "abstract": "A new analysis into the steady-state operation and derating of three-phase transformers under nonsinusoidal and asymmetric operating conditions is proposed. The combined effects of transformer core and load asymmetry, nonlinearity, and harmonics, as well as nonsinusoidal input excitation are examined. A time-domain nonlinear model for three-phase three-leg transformers is implemented. Transformer derating is estimated by modeling additional power losses due to harmonics generated by the iron core, nonsinusoidal excitation, and nonlinear (rectifier and electric drive) loading. Laboratory tests are performed to verify simulated waveforms. The contribution of this paper is a nonlinear transformer modeling technique for steady-state operation under unbalanced, asymmetric, and nonsinusoidal operation, capable of computing derating factors.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2008,
    "citationCount": 79,
    "openAccessPdf": {
      "url": "https://espace.curtin.edu.au/bitstream/20.500.11937/17561/2/117539_Derating%2520of%2520asymetric%2520PID%2520117539.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2008-05-16",
    "authors": [
      {
        "authorId": "145163834",
        "name": "M. Masoum"
      },
      {
        "authorId": "2899496",
        "name": "P. Moses"
      },
      {
        "authorId": "2637549",
        "name": "A. S. Masoum"
      }
    ],
    "source": "semantic_scholar",
    "score": 131.73039952010822
  },
  {
    "paperId": "c04db0855b7816b084cfba944d9d55deddaf8d42",
    "url": "https://www.semanticscholar.org/paper/c04db0855b7816b084cfba944d9d55deddaf8d42",
    "title": "Integration of geomagnetic disturbance modeling into the power flow: A methodology for large-scale system studies",
    "abstract": "This paper presents a methodology for integrated power flow modeling of the impact of geomagnetic disturbances (GMDs) on power system voltage stability. GMDs cause quasi-dc, geomagnetically induced currents (GICs) in the transformers and transmission lines, which in turn cause saturation of the high voltage transformers, greatly increasing their reactive power consumption. GICs can be calculated using standard power flow modeling parameters such as line resistance, augmented with several GIC specific fields including substation geographic coordinates and grounding resistance, transformer configuration, and transformer coil winding resistances. When exact values are not available, estimated quantities can be used. By then integrating GIC into power flow analysis, the changes in reactive power losses and bus voltages can be quantified to assess the risk of voltage instability and large-scale voltage collapse. An example calculation is provided for a North American Eastern Interconnect model.",
    "venue": "North American Power Symposium",
    "year": 2012,
    "citationCount": 118,
    "openAccessPdf": {
      "url": "http://www.powerworld.com/files/OverbyeNAPS2012GIC.pdf",
      "status": "GREEN"
    },
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2012-10-22",
    "authors": [
      {
        "authorId": "2295613",
        "name": "T. Overbye"
      },
      {
        "authorId": "3450607",
        "name": "T. Hutchins"
      },
      {
        "authorId": "9621240",
        "name": "K. Shetye"
      }
    ],
    "source": "semantic_scholar",
    "score": 131.68685239667295
  },
  {
    "paperId": "fc7a228e9443f0af140c91779fb3627d9bc34ed3",
    "url": "https://www.semanticscholar.org/paper/fc7a228e9443f0af140c91779fb3627d9bc34ed3",
    "title": "Monolithic transformers for silicon RF IC design",
    "abstract": "The construction and electrical characteristics of two-port transformers (1:1 and 1:n turns ratio) and multi-port transformer baluns fabricated in a production silicon technology are presented. A high-linearity 5 GHz mixer design illustrates the advantages of the trifilar transformer in an RF IC application.",
    "venue": "Proceedings of the 1998 Bipolar/BiCMOS Circuits and Technology Meeting (Cat. No.98CH36198)",
    "year": 1998,
    "citationCount": 228,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1998-09-27",
    "authors": [
      {
        "authorId": "2246141884",
        "name": "D. Cheung"
      },
      {
        "authorId": "2246045314",
        "name": "John R. Long"
      },
      {
        "authorId": "2259526247",
        "name": "Hadaway"
      },
      {
        "authorId": "2245484291",
        "name": "D. Harame"
      }
    ],
    "source": "semantic_scholar",
    "score": 131.50583005331362
  },
  {
    "paperId": "875ca36da0c1408500bc3004c03699efd809ad7c",
    "url": "https://www.semanticscholar.org/paper/875ca36da0c1408500bc3004c03699efd809ad7c",
    "title": "Characterizing Correctness Properties of Parallel Programs Using Fixpoints",
    "abstract": null,
    "venue": "International Colloquium on Automata, Languages and Programming",
    "year": 1980,
    "citationCount": 444,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "publicationDate": "1980-07-14",
    "authors": [
      {
        "authorId": "145459273",
        "name": "E. Emerson"
      },
      {
        "authorId": "2226782374",
        "name": "E. Clarke"
      }
    ],
    "source": "semantic_scholar",
    "score": 131.47111423249362
  },
  {
    "paperId": "22efc46d15a7391595c69524251d70d19ca4a9cd",
    "url": "https://www.semanticscholar.org/paper/22efc46d15a7391595c69524251d70d19ca4a9cd",
    "title": "The impact of Electric Vehicle battery charging on distribution transformers",
    "abstract": "Electric Vehicles (EV) and Plug-in Hybrid Electric Vehicles (PHEV) have a limited share of the current market. However, it is widely expected that the situation will change in the near future and the penetration of battery-operated vehicles will increase significantly. This paper addresses the impact of the mass operation and electricity consumption of EVs and PHEVs on the distribution utility and more specifically on distribution transformers. The charging effect of various types of EV batteries on the life of distribution transformers is analyzed. A typical annual base-load on the distribution transformer is considered and compared with different battery charging load scenarios. Simulation results show that distributing the load profile of the battery charging helps to decrease the distribution transformer loss of life. In addition, power management of the EV and PHEV battery charging and the interface with the Smart-Grid is explained.",
    "venue": "Applied Power Electronics Conference",
    "year": 2011,
    "citationCount": 116,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2011-03-06",
    "authors": [
      {
        "authorId": "2293563",
        "name": "M. Rutherford"
      },
      {
        "authorId": "2374655",
        "name": "V. Yousefzadeh"
      }
    ],
    "source": "semantic_scholar",
    "score": 131.43260902196636
  },
  {
    "paperId": "34f267650fbdff5b3a51e97f1df0564950aa78b5",
    "url": "https://www.semanticscholar.org/paper/34f267650fbdff5b3a51e97f1df0564950aa78b5",
    "title": "Global Transformer Optimization Method Using Evolutionary Design and Numerical Field Computation",
    "abstract": "This paper addresses the complex optimum transformer design problem, which is formulated as a mixed-integer nonlinear programming problem, by introducing an integrated design optimization method based on evolutionary algorithms and numerical electromagnetic and thermal field computations. The main contributions of this research are: i) introduction of a new overall transformer optimization method, minimizing either the overall transformer materials cost or the overall transformer materials and operating cost, ii) expansion of the solution space by innovative techniques that define the variation of crucial design variables such as the conductors' cross-section, ensuring global optimum transformer designs, and iii) incorporation of numerical field computation in order to validate the feasibility of the optimum designs. The proposed method is compared with a heuristic optimization method of the transformer manufacturing industry and the results demonstrate the robustness and the superiority of this new approach.",
    "venue": "IEEE transactions on magnetics",
    "year": 2009,
    "citationCount": 77,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2009-02-24",
    "authors": [
      {
        "authorId": "34613730",
        "name": "E. Amoiralis"
      },
      {
        "authorId": "34794334",
        "name": "P. Georgilakis"
      },
      {
        "authorId": "35140669",
        "name": "M. Tsili"
      },
      {
        "authorId": "2239297888",
        "name": "A. Kladas"
      }
    ],
    "source": "semantic_scholar",
    "score": 131.35063240034387
  },
  {
    "paperId": "835d550badb3f2231dd980e7e37e34d5c0f3f251",
    "url": "https://www.semanticscholar.org/paper/835d550badb3f2231dd980e7e37e34d5c0f3f251",
    "title": "A New Converter Transformer and a Corresponding Inductive Filtering Method for HVDC Transmission System",
    "abstract": "A new converter transformer and an inductive filtering method are presented to solve the existing problems of the traditional converter transformer and the passive filtering method of the high-voltage direct current (HVDC) system. It adopts the ampere-turn balance of the transformer as the filtering mechanism. A tap at the linking point of the prolonged winding and the common winding of the secondary windings is connected with the LC resonance circuit. It can realize the goal that once theharmonic current flowsinto the prolonged winding, the common winding will induct the opposite harmonic current to balance it by the zero impedance design of the common winding and the proper configuration of LC parameters, so there will be no inductive harmonic current in the primary winding. Moreover, the reactive power that the converter needs can be partly compensated in the secondary winding. Simulation results have verified the correctness of the theoretical analysis. The new converter transformer can greatly reduce the harmonic content in the primary winding, loss, and noise generated by harmonics in the transformer, and the difficulty of the transformer's insulation design.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2008,
    "citationCount": 77,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2008-06-24",
    "authors": [
      {
        "authorId": "34282430",
        "name": "L. Luo"
      },
      {
        "authorId": "34117934",
        "name": "Yong Li"
      },
      {
        "authorId": "2291649",
        "name": "Jiazhu Xu"
      },
      {
        "authorId": "2117986089",
        "name": "Ji Li"
      },
      {
        "authorId": "2143656459",
        "name": "Bo Hu"
      },
      {
        "authorId": "2157778575",
        "name": "Fusheng Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 131.35063240034387
  },
  {
    "paperId": "b3bf757fa3212c0de61004a4aebefab93ed98fcf",
    "url": "https://www.semanticscholar.org/paper/b3bf757fa3212c0de61004a4aebefab93ed98fcf",
    "title": "A unified three-phase transformer model for distribution load flow calculations",
    "abstract": "This paper provides a unified method to model three-phase transformers for distribution system load flow calculations, especially when the matrix singularity caused by the transformer configuration arises. This paper shows that the singularity appears only in certain transformer admittance submatrices and only in certain transformer configurations. The unified method presented in this paper can solve the voltage/current equations in the forward/backward sweep algorithm for various types of transformer configurations, whether or not the corresponding admittance submatrices are singular. Comprehensive comparisons have been made between the proposed approach and other methods. Test results demonstrate the validity and effectiveness of the proposed method.",
    "venue": "IEEE Transactions on Power Systems",
    "year": 2006,
    "citationCount": 77,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Mathematics"
    ],
    "publicationTypes": null,
    "publicationDate": "2006-01-30",
    "authors": [
      {
        "authorId": "2246677156",
        "name": "Peng Xiao"
      },
      {
        "authorId": "2238168264",
        "name": "David C. Yu"
      },
      {
        "authorId": "2245926353",
        "name": "Wei Yan"
      }
    ],
    "source": "semantic_scholar",
    "score": 131.35063240034387
  },
  {
    "paperId": "33e7a3f0d2190d16e0f9f9fdf378d02b5db01bcf",
    "url": "https://www.semanticscholar.org/paper/33e7a3f0d2190d16e0f9f9fdf378d02b5db01bcf",
    "title": "Reducing Losses in Distribution Transformers",
    "abstract": "This paper examines three methods of reducing distribution transformer losses. The first method analyzes the effects of using aluminum electromagnetic shields in a distribution transformer. The goal of placing electromagnetic shields in the distribution-transformer tank walls is to reduce the stray losses. A 500 kVA shell-type transformer was used in the experiments. The overall results presented indicate that stray losses can be considerably reduced when electromagnetic shielding is applied in the transformer tank. In the experiment, the tank walls were lined with aluminum foil. The possibility of reducing the dielectric losses was shown through experiments in the second method. The third method analyzes the behavior of wound-cores losses in distribution transformers as a function of joint configuration design parameters. The joint configuration used in this paper is called step-lap joint.",
    "venue": "IEEE Power Engineering Review",
    "year": 2002,
    "citationCount": 77,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Materials Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2002-09-01",
    "authors": [
      {
        "authorId": "48896802",
        "name": "J. Olivares"
      },
      {
        "authorId": "2265349860",
        "name": "Yilu Liu"
      },
      {
        "authorId": "143786151",
        "name": "J. Cañedo"
      },
      {
        "authorId": "1405757225",
        "name": "R. Escarela-Perez"
      },
      {
        "authorId": "2246150725",
        "name": "Johan Driesen"
      },
      {
        "authorId": "2057401905",
        "name": "Pablo Moreno"
      }
    ],
    "source": "semantic_scholar",
    "score": 131.35063240034387
  },
  {
    "paperId": "eaebaac361061240506dea1f9fa289a8cf9c2620",
    "url": "https://www.semanticscholar.org/paper/eaebaac361061240506dea1f9fa289a8cf9c2620",
    "title": "Optimum Design of Stepped Transmission-Line Transformers",
    "abstract": "This paper describes the optimum stepped-transmission-line transformer structure for matching two unequal characteristic impedances. For any specified bandwidth, the steps are designed to yield a Tchebycheff-type (or equal-ripple) reflection-coefficient response. Over this band, the maximum vswr is less than that obtainable with any other stepped-transformer having the same number of steps. Design method and technique for eliminating discontinuity-capacitance effects are given. The measured results for a coaxial and a waveguide model are presented and found to verify the method.",
    "venue": "",
    "year": 1955,
    "citationCount": 159,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Mathematics"
    ],
    "publicationTypes": null,
    "publicationDate": "1955-04-01",
    "authors": [
      {
        "authorId": "51087579",
        "name": "S. Cohn"
      }
    ],
    "source": "semantic_scholar",
    "score": 131.1276072285074
  },
  {
    "paperId": "7c9bc3728209bad71aa88c1b1e578bc25e07bfe2",
    "url": "https://www.semanticscholar.org/paper/7c9bc3728209bad71aa88c1b1e578bc25e07bfe2",
    "title": "High-Density Nanocrystalline Core Transformer for High-Power High-Frequency Resonant Converter",
    "abstract": "A high-density transformer using nanocrystalline core is developed for a 30 kW, 200 kHz resonant converter. Loss models are established for nanocrystalline cores through experimental characterization. The important parasitic models are also developed considering litz wire effects. Following a minimum size design procedure, several transformers with both nanocrystalline and ferrite cores are designed and prototyped. While all transformers meet the converter performance requirement during testing, using nanocrystalline core can achieve a significantly higher power density even at 200 kHz.",
    "venue": "IEEE transactions on industry applications",
    "year": 2008,
    "citationCount": 106,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Materials Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2008-01-22",
    "authors": [
      {
        "authorId": "87324842",
        "name": "W. Shen"
      },
      {
        "authorId": "2107613153",
        "name": "F. Wang"
      },
      {
        "authorId": "3102715",
        "name": "D. Boroyevich"
      },
      {
        "authorId": "6753922",
        "name": "C. W. Tipton"
      }
    ],
    "source": "semantic_scholar",
    "score": 131.09243251692857
  },
  {
    "paperId": "f004233b2bd4a150e263dc3f03ef8089789b922c",
    "url": "https://www.semanticscholar.org/paper/f004233b2bd4a150e263dc3f03ef8089789b922c",
    "title": "A field study of aging in paper-oil insulation systems",
    "abstract": "The paper used to insulate the windings of power transformers is mostly made from wood pulp, a cellulosic material. Over decades the paper is slowly attacked by water, oxygen, oil acids, and high temperatures and eventually degrades to the point where it is no longer an effective insulator. The transformer is then likely to fail. Power utilities need to know when a transformer is nearing the end of its useful life in order to plan its replacement. However, a problem with monitoring the condition of the paper within a transformer is that it may be difficult to obtain a sample to test. Furthermore, a particular sample may not accurately reflect the overall paper condition. A power transformer operating in Australia failed in 2010. Thus we had the opportunity to study the paper condition at various points within the transformer and evaluate the validity of the current understanding of paper aging. In this article we discuss the mechanisms of cellulose degradation, and the associated equations, and apply them to the paper insulation in the failed transformer.",
    "venue": "IEEE Electrical Insulation Magazine",
    "year": 2012,
    "citationCount": 74,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Environmental Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2012-01-16",
    "authors": [
      {
        "authorId": "9217742",
        "name": "N. Lelekakis"
      },
      {
        "authorId": "8513695",
        "name": "Wenyu Guo"
      },
      {
        "authorId": "2107788150",
        "name": "D. Martin"
      },
      {
        "authorId": "2690502",
        "name": "J. Wijaya"
      },
      {
        "authorId": "9503520",
        "name": "D. Susa"
      }
    ],
    "source": "semantic_scholar",
    "score": 130.76232170304465
  },
  {
    "paperId": "7e7abb30addb77f66d6da7f46c98e6af4aceb841",
    "url": "https://www.semanticscholar.org/paper/7e7abb30addb77f66d6da7f46c98e6af4aceb841",
    "title": "Diagnosing transformer faults using frequency response analysis",
    "abstract": "This technique measures the impedance of transformer windings over a wide frequency range and compares the results with reference data. The author used a network analyzer to sweep the frequency range, make the measurements, and analyze the results.",
    "venue": "",
    "year": 2003,
    "citationCount": 216,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2003-04-08",
    "authors": [
      {
        "authorId": "97591043",
        "name": "S. Ryder"
      }
    ],
    "source": "semantic_scholar",
    "score": 130.6984603031069
  },
  {
    "paperId": "721ad416e07ca06c85525fbee21bc12fb6e561b2",
    "url": "https://www.semanticscholar.org/paper/721ad416e07ca06c85525fbee21bc12fb6e561b2",
    "title": "Theory and Design of Wide-Band Multisection Quarter-Wave Transformers",
    "abstract": "A general theory of the n-section quarter-wave transformer is presented. It is shown that optimum bandwidth with a minimum pass band tolerance is obtained when the power loss ratio is chosen to give Tchebycheff behavior in the pass band. A comparison is made of the Tchebycheff transformer and the maximally flat transformer, and shows that the former gives a large increase in bandwidth-e.g., up to 44 per cent for a 2-section transformer and up to 75 per cent for a 4-section transformer. Design formulas are given for the 2, 3 and 4-section transformers.",
    "venue": "Proceedings of the IRE",
    "year": 1955,
    "citationCount": 152,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1955-02-01",
    "authors": [
      {
        "authorId": "50636060",
        "name": "R. Collin"
      }
    ],
    "source": "semantic_scholar",
    "score": 130.45656882088653
  },
  {
    "paperId": "e16f60082efa78584aeb493a105062a9b6131bd3",
    "url": "https://www.semanticscholar.org/paper/e16f60082efa78584aeb493a105062a9b6131bd3",
    "title": "Autoregulation of the splicing of transcripts from the transformer-2 gene of Drosophila.",
    "abstract": "The Drosophila transformer-2 gene uses alternative promoters and splicing patterns to generate four different mRNAs that together encode three putative RNA-binding polypeptides. The transformer-2 products expressed in somatic tissues function to regulate the RNA splicing of the sex determination gene doublesex, whereas products expressed in the male germ line play an unknown, but essential, role in spermatogenesis. Two alternatively spliced transformer-2 transcripts, each encoding a different putative RNA-binding protein, are found only in the male germ line. These male germ line-specific mRNAs differ from each other by the presence or absence of a single intron called M1. We show that M1-containing transcripts make up a majority of transformer-2 germ-line transcripts in wild-type males but fail to accumulate in males homozygous for transformer-2 null mutations. Germ-line transformation experiments using a variety of reporter gene constructs demonstrate that specific polypeptide products of the transformer-2 gene itself normally repress M1 splicing in the male germ line. Thus, in addition to its role in the sex-specific control of doublesex RNA splicing in somatic tissues, the transformer-2 gene also regulates the splicing of its own transcripts in the male germ line. We propose that this autoregulatory function may serve in negative feedback control of transformer-2 activity during spermatogenesis. The finding that transformer-2 controls multiple splicing decisions suggests that a variety of different alternative splicing choices could be regulated by a relatively limited number of trans-acting factors.",
    "venue": "Genes & Development",
    "year": 1991,
    "citationCount": 107,
    "openAccessPdf": {
      "url": "http://genesdev.cshlp.org/content/5/5/786.full.pdf",
      "status": "GOLD"
    },
    "fieldsOfStudy": [
      "Biology",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "1991-05-01",
    "authors": [
      {
        "authorId": "4236681",
        "name": "W. Mattox"
      },
      {
        "authorId": "2244069677",
        "name": "Bruce S. Baker"
      }
    ],
    "source": "semantic_scholar",
    "score": 130.2319684068633
  },
  {
    "paperId": "d8939470eb6b0dbab476c9851cd1e4ca4e361af7",
    "url": "https://www.semanticscholar.org/paper/d8939470eb6b0dbab476c9851cd1e4ca4e361af7",
    "title": "Comparative analysis of exciting current harmonics and reactive power consumption from GIC saturated transformers",
    "abstract": "Geomagnetically induced currents (GIC) can cause severe transformer half-cycle saturation. This paper introduces a simplified method based on the equivalent magnetizing curve of the transformer to estimate harmonic currents and MVAr with only the given the GIC and nameplate and core design information of the transformer. The simulation method is validated with the field results. The simulation methods are validated with observational and field test data for transformers experiencing half-cycle saturation.",
    "venue": "2001 IEEE Power Engineering Society Winter Meeting. Conference Proceedings (Cat. No.01CH37194)",
    "year": 2001,
    "citationCount": 100,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2001-01-28",
    "authors": [
      {
        "authorId": "2916728",
        "name": "Xuzhu Dong"
      },
      {
        "authorId": "2108112086",
        "name": "Yilu Liu"
      },
      {
        "authorId": "30854364",
        "name": "J. Kappenman"
      }
    ],
    "source": "semantic_scholar",
    "score": 130.2268077526189
  },
  {
    "paperId": "c1f2c14bb838965375715d538a71c96934b1c783",
    "url": "https://www.semanticscholar.org/paper/c1f2c14bb838965375715d538a71c96934b1c783",
    "title": "A state-of-the-art review of transformer protection algorithms",
    "abstract": "A comparative study of algorithms for digital differential protection of power transformers is reported. The mathematical basis for each algorithm is described. The algorithms are compared as to their speed of response, computational burden and capability to distinguish between an inrush and transformer internal fault. >",
    "venue": "",
    "year": 1988,
    "citationCount": 202,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": "1988-04-01",
    "authors": [
      {
        "authorId": "2107443709",
        "name": "M. Rahman"
      },
      {
        "authorId": "31441938",
        "name": "B. Jeyasurya"
      }
    ],
    "source": "semantic_scholar",
    "score": 129.69808968562683
  },
  {
    "paperId": "4b035cd9de292b6a1eaeb7767d1040505a4198d8",
    "url": "https://www.semanticscholar.org/paper/4b035cd9de292b6a1eaeb7767d1040505a4198d8",
    "title": "Prediction of top-oil temperature for transformers using neural networks",
    "abstract": "Artificial neural networks represent a growing new technology as indicated by a wide range of proposed applications. At a substation, when the transformer's windings get too hot, either load has to be reduced as a short-term solution, or another transformer bay has to be installed as a long-term plan. To decide on whether to deploy either of these two strategies, one should be able to predict the transformer temperature accurately. This paper explores the possibility of using artificial neural networks for predicting the top-oil temperature of transformers. Static neural networks, temporal processing networks and recurrent networks are explored for predicting the top-oil temperature of transformers. The results using different networks are compared with the auto regression linear model.",
    "venue": "",
    "year": 2000,
    "citationCount": 103,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2000-10-01",
    "authors": [
      {
        "authorId": "48899925",
        "name": "Q. He"
      },
      {
        "authorId": "2057225471",
        "name": "J. Si"
      },
      {
        "authorId": "2989140",
        "name": "D. Tylavsky"
      }
    ],
    "source": "semantic_scholar",
    "score": 129.6658634871206
  },
  {
    "paperId": "cea5eb9421c4d2effb48b5d84afe1b99b5e684b4",
    "url": "https://www.semanticscholar.org/paper/cea5eb9421c4d2effb48b5d84afe1b99b5e684b4",
    "title": "Single-source cascaded transformers multilevel inverter with reduced number of switches",
    "abstract": "This study presents a novel topology for multilevel inverters so called cascaded transformer reduced switches inverter (CTRSI). This topology consists of one DC source and several single-phase transformers. Each single-phase transformers generates three levels with two semiconductor switches and only two switches for all transformers alter the direction of single DC source. Whereas each single-phase transformers in conventional cascaded transformer multilevel inverter includes four switches. Hence, CTRSI has the advantage of a reduced number of components compared with conventional cascaded transformer multilevel inverter. Simulation results carried out by MATLAB/SIMULINK and a low-power experimental setup was planned to test the converter in practice. The results show that the proposed inverter topology is able to reach high-quality output voltages. Using switches with higher rating current than conventional topology is main disadvantage of CTRSI.",
    "venue": "",
    "year": 2012,
    "citationCount": 102,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2012-11-01",
    "authors": [
      {
        "authorId": "3393909",
        "name": "M. Banaei"
      },
      {
        "authorId": "2518745",
        "name": "H. Khounjahan"
      },
      {
        "authorId": "2805074",
        "name": "E. Salary"
      }
    ],
    "source": "semantic_scholar",
    "score": 129.52093482344452
  },
  {
    "paperId": "3812d240eab610691744654c06c933d99ca3a54c",
    "url": "https://www.semanticscholar.org/paper/3812d240eab610691744654c06c933d99ca3a54c",
    "title": "Online monitors keep transformers in service",
    "abstract": "The trend toward a deregulated global electricity market is raising utility cost consciousness. An important aspect of cost savings is equipment investment for the power transmission system, and power transformers are among the most expensive elements of a high voltage power system. Cost savings can be realized through a delay in the procurement of transformers and a reduction in maintenance effort. Monitoring systems for power transformers can help to achieve these aims. They provide detailed information about the transformer's condition and help minimize the probability of an unexpected outage. Monitoring systems open the possibility for extending the operating time of power transformers, reduce the risk of expensive failures, and provide potential in changing the maintenance strategy. Online monitoring methods including temperature measurement, gas-in-oil analysis and partial discharge measurement are discussed. New concepts in on-line monitoring including sensors, hardware and software are also discussed.",
    "venue": "",
    "year": 1998,
    "citationCount": 102,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1998-07-01",
    "authors": [
      {
        "authorId": "2612220",
        "name": "T. Leibfried"
      }
    ],
    "source": "semantic_scholar",
    "score": 129.52093482344452
  },
  {
    "paperId": "39dd0501dd9840b02991b2bc91df166afdf2853c",
    "url": "https://www.semanticscholar.org/paper/39dd0501dd9840b02991b2bc91df166afdf2853c",
    "title": "Detection of faults and ageing phenomena in transformers by transfer functions",
    "abstract": "In this paper the performance of a new transformer diagnosis method is tested experimentally. The method is called model-based diagnosis because the diagnosis is based on an identified model of the component-the transfer function. Two test series are carried out to evaluate the method on transformers, a sensitivity test series and an ageing test series. The purpose of the sensitivity test series is to determine how close the identified transformer model is connected to the physical condition of the modelled transformer. The purpose of the ageing test series is to determine if it is possible to identify a development in the transfer function of the transformer towards breakdown. Based on the two test series it is concluded that the detection of failures and ageing phenomena in transformers by transfer functions is dependent on detectable changes in relatively few characteristic transformer parameters-basically lumped winding capacitances, total losses and core reluctance. >",
    "venue": "",
    "year": 1995,
    "citationCount": 102,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "1410975857",
        "name": "J. Bak-Jensen"
      },
      {
        "authorId": "1401824104",
        "name": "B. Bak‐Jensen"
      },
      {
        "authorId": "150302542",
        "name": "S. D. Mikkelsen"
      }
    ],
    "source": "semantic_scholar",
    "score": 129.52093482344452
  },
  {
    "paperId": "735481ce736b4fd4896b7fbf713d8ade6c0a4b9a",
    "url": "https://www.semanticscholar.org/paper/735481ce736b4fd4896b7fbf713d8ade6c0a4b9a",
    "title": "Transfer function-based partial discharge localization in power transformers: a feasibility study",
    "abstract": "Statistical studies have shown that failures of bushings, winding insulation, and online tap changers are the main causes for long-duration outages of transformers. This article investigates the development of an instrument for supervising the conditions of transformer units. The use of sectional winding transfer functions (SWTFs) for online PD evaluation in power transformers has several advantages: localization of PD sources, discrimination between PDs inside or outside of the transformer, and evaluation of actual PD amplitudes along the winding. Direct measurement of SWTFs-when different points along the coil are accessible-and use of these SWTFs for PD evaluation and localization has shown excellent results. In order to use this method at transformers on site, an appropriate modeling method is needed for the computation of the SWTFs using only measurements at the transformer terminals. A detailed modeling approach based on discrete RLC circuit elements has been studied and different algorithms used for parameter estimation and optimization. This model is applicable in practical cases for a limited frequency range if genetic algorithms (GAs) are used for parameter optimization. A new method based on traveling wave theory has been investigated using genetic algorithms to search for the optimum parameters of a partial differential equation that describes the transient behavior of the coil. This method has shown potential in solving SWTF calculation problems for different transformers.",
    "venue": "",
    "year": 2002,
    "citationCount": 101,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2002-12-10",
    "authors": [
      {
        "authorId": "1931457",
        "name": "A. Akbari"
      },
      {
        "authorId": "3015668",
        "name": "P. Werle"
      },
      {
        "authorId": "2551968",
        "name": "H. Borsi"
      },
      {
        "authorId": "2359516",
        "name": "E. Gockenbach"
      }
    ],
    "source": "semantic_scholar",
    "score": 129.37459219926404
  },
  {
    "paperId": "29efd8e7bc11771b0de406988835cef0541ffb16",
    "url": "https://www.semanticscholar.org/paper/29efd8e7bc11771b0de406988835cef0541ffb16",
    "title": "SQUID sensor array configurations for magnetoencephalography applications",
    "abstract": "Electrophysiological activity in the human brain generates a small magnetic field from the spatial superposition of individual neuronal source currents. At a distance of about 15 mm from the scalp, the observed field is of the order of 10−13 to 10−12 T peak-to-peak. This measurement process is termed magnetoencephalography (MEG). In order to minimize instrumental noise, the MEG is usually detected using superconducting flux transformers, coupled to SQUID (superconducting quantum interference device) sensors. Since MEG signals are also measured in the presence of significant environmental magnetic noise, flux transformers must be designed to strongly attenuate environmental noise, maintain low instrumental noise and maximize signals from the brain. Furthermore, the flux transformers must adequately sample spatial field variations if the brain activity is to be imaged. The flux transformer optimization for maximum brain signal-to-noise ratio (SNR) requires analysis of the spatial and temporal properties of brain activity, the environmental noise and how these signals are coupled to the flux transformer. Flux transformers that maximize SNR can detect the smallest brain signals and have the best ability to spatially separate dipolar sources. An optimal flux transformer design is a synthetic higher-order gradiometer based on relatively short-baseline first-order radial gradiometer primary sensors.",
    "venue": "",
    "year": 2002,
    "citationCount": 99,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Physics"
    ],
    "publicationTypes": null,
    "publicationDate": "2002-09-01",
    "authors": [
      {
        "authorId": "143641034",
        "name": "J. Vrba"
      },
      {
        "authorId": "2254161",
        "name": "S. Robinson"
      }
    ],
    "source": "semantic_scholar",
    "score": 129.07755278982137
  },
  {
    "paperId": "b22e56ed9e0a3e7a4715f598046699bbeecd72c9",
    "url": "https://www.semanticscholar.org/paper/b22e56ed9e0a3e7a4715f598046699bbeecd72c9",
    "title": "Apparatus for Online Power Transformer Winding Monitoring Using Bushing Tap Injection",
    "abstract": "Online transformer condition monitoring techniques based on transfer function methods, such as transmission-line diagnostics and swept frequency-response analysis, require the injection of a known test signal into the transformer. On larger power transformers, a practical method is to use the available bushing tap connection. In this paper, we will discuss a custom high-power signal generator that injects high-frequency signals on the bushing tap of the transformer under investigation, as well as a circuit to replace the bushing tap short and allow online operation of the system. Finally, the system is demonstrated on a 650 kV transformer.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2009,
    "citationCount": 92,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2009-06-10",
    "authors": [
      {
        "authorId": "30746372",
        "name": "T. De Rybel"
      },
      {
        "authorId": "47264300",
        "name": "A. Singh"
      },
      {
        "authorId": "32330358",
        "name": "J. Vandermaar"
      },
      {
        "authorId": "47446930",
        "name": "M. Wang"
      },
      {
        "authorId": "32224345",
        "name": "J. Martí"
      },
      {
        "authorId": "1870216",
        "name": "K. Srivastava"
      }
    ],
    "source": "semantic_scholar",
    "score": 128.98899239729883
  },
  {
    "paperId": "3708c8b4519d65abb576cfa373138c2cbb0adb1a",
    "url": "https://www.semanticscholar.org/paper/3708c8b4519d65abb576cfa373138c2cbb0adb1a",
    "title": "Transformer structure and its effects on common mode EMI noise in isolated power converters",
    "abstract": "This paper investigates the effects of transformer structure and parasitic on common mode (CM) EMI noise of isolated power converters. Study on typical transformer winding structures indicates that the distribution of inter-winding capacitances and the distribution of voltage potentials on windings are the two critical factors that determine the CM noise levels of the converter in switching frequency ranges. CM noise reduction methods by improving the transformer structure and by compensation are proposed. At high frequencies, leakage inductances of the transformer resonate with the inter-winding capacitances or the junction capacitances of the secondary side switches. It may result in severe high frequency CM noise peaks. Methods of controlling such peaks are also discussed.",
    "venue": "Applied Power Electronics Conference",
    "year": 2010,
    "citationCount": 97,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2010-03-18",
    "authors": [
      {
        "authorId": "1819228",
        "name": "P. Kong"
      },
      {
        "authorId": "145519082",
        "name": "F. Lee"
      }
    ],
    "source": "semantic_scholar",
    "score": 128.7745121800586
  },
  {
    "paperId": "97c6f30d41c70a284e31985cbf0aa24cce651790",
    "url": "https://www.semanticscholar.org/paper/97c6f30d41c70a284e31985cbf0aa24cce651790",
    "title": "Control of high-frequency AC link electronic transformer",
    "abstract": "An isolated high-frequency link AC/AC converter is termed an electronic transformer.The electronic transformer has size and cost advantages over a conventional transformer because of high-frequency operation of the magnetic core. Of the various topologies of electronic transformer, the high-frequency AC link electronic transformer achieves high-frequency AC power transformation without a DC link. The circuit uses the standard H-bridge, one on either side of the high-frequency transformer. A novel PWM scheme is proposed, which symmetrically delays and advances the phase of the left and right legs of the front-side converter with respect to the output side converter. The proposed scheme introduces free wheeling sub-periods, which results in zero voltage switching in the output-side converter. The electronic transformer as an AC automatic voltage regulator (AVR) offers distinct advantages over a conventional servo voltage stabiliser in terms of size and speed of response. The AVR application is discussed and experimental results of a 500VA AVR are presented. A four-quadrant switch presents difficulties in turning off inductive load current because of the absence of a free wheeling path in the switch. A biasing circuit is proposed to convert the potentially lossy switching transition into lossless transition. Simulation and experimental results with the biasing circuit are presented.",
    "venue": "",
    "year": 2005,
    "citationCount": 97,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2005-05-06",
    "authors": [
      {
        "authorId": "71248070",
        "name": "H. Krishnaswami"
      },
      {
        "authorId": "1784880",
        "name": "V. Ramanarayanan"
      }
    ],
    "source": "semantic_scholar",
    "score": 128.7745121800586
  },
  {
    "paperId": "936b09616fa91ed33f49690e690e2cd619892aa4",
    "url": "https://www.semanticscholar.org/paper/936b09616fa91ed33f49690e690e2cd619892aa4",
    "title": "Intelligent transformer",
    "abstract": "A new concept of an intelligent power transformer is presented in this paper, where the size of a power transformer of commercial frequency is miniaturized by introducing a modulator. In this circuit, various functions, such as constant voltage, constant power and power factor correction are realized by a phase control system.",
    "venue": "PESC Record. 27th Annual IEEE Power Electronics Specialists Conference",
    "year": 1996,
    "citationCount": 88,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2253077676",
        "name": "Koosuke Harada"
      },
      {
        "authorId": "92740799",
        "name": "F. Anan"
      },
      {
        "authorId": "98912046",
        "name": "K. Yamasaki"
      },
      {
        "authorId": "47974408",
        "name": "M. Jinno"
      },
      {
        "authorId": "47897752",
        "name": "Y. Kawata"
      },
      {
        "authorId": "2052761515",
        "name": "T. Nakashima"
      }
    ],
    "source": "semantic_scholar",
    "score": 128.3295455459821
  },
  {
    "paperId": "10226eef34aec28c2f74d002ac33efba78972d26",
    "url": "https://www.semanticscholar.org/paper/10226eef34aec28c2f74d002ac33efba78972d26",
    "title": "Application of nonlinear dynamics and chaos to ferroresonance in distribution systems",
    "abstract": "Ferroresonant overvoltages or undervoltages can occur in cable-fed power transformer installations when single phase switching or interrupting is practiced. This paper identifies the ferroresonant circuit as a nonlinear dynamical system. Analysis and classification methods are presented which provide new insight into the global behavior of ferroresonance. The concepts presented offer potential for progress in the areas of transformer model development and evaluation, analysis and prediction of ferroresonance, and distribution system design and operation. Measurements from a typical five-legged core transformer installation are used to illustrate the application of nonlinear dynamics and chaotic systems to the problem of ferroresonance. >",
    "venue": "",
    "year": 1994,
    "citationCount": 130,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1994-04-01",
    "authors": [
      {
        "authorId": "1977120",
        "name": "B. Mork"
      },
      {
        "authorId": "97915568",
        "name": "D. Stuehm"
      }
    ],
    "source": "semantic_scholar",
    "score": 128.12795984801727
  },
  {
    "paperId": "7fb2527f9e683d470d32a0178806cb4b69e88663",
    "url": "https://www.semanticscholar.org/paper/7fb2527f9e683d470d32a0178806cb4b69e88663",
    "title": "Effect of harmonics on transformers loss of life",
    "abstract": "The use of non-linear (solid state) loads in power system has increased the awareness of the potential reduction of a transformer life due to the increased losses. Manufacturers and users have been applying IEEE C57.110-1989 which gives a conservative estimate of loading capability of transformers. In this paper, the effect of harmonics on transformers is discussed and a thermal model to predict a transformer hot spot temperature is presented. The model considers a time varying harmonic load cycle and ambient temperature. The model is applied on 31.5 MVA, 115times9times1.67%/6.3 kV ONAF transformer",
    "venue": "Conference record of IEEE International Symposium on Electrical Insulation",
    "year": 2006,
    "citationCount": 84,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2006-06-11",
    "authors": [
      {
        "authorId": "2489916",
        "name": "A. Elmoudi"
      },
      {
        "authorId": "1862552",
        "name": "M. Lehtonen"
      },
      {
        "authorId": "152562870",
        "name": "H. Nordman"
      }
    ],
    "source": "semantic_scholar",
    "score": 127.63976884735474
  },
  {
    "paperId": "0d40ba4d9eb07ce43985d544240fb1577f1792ef",
    "url": "https://www.semanticscholar.org/paper/0d40ba4d9eb07ce43985d544240fb1577f1792ef",
    "title": "PSPICE computer model of a nonlinear three-phase three-legged transformer",
    "abstract": "This paper proposes a simple, practical PSPICE model of a three-phase, three-legged, saturated transformer with accurate performance. The transformer is modeled with its electric and magnetic equivalent circuits and a simple but reliable characterization of its nonlinear magnetic behavior. Two three-phase transformers of 380/220 V, 7.5 kVA, and 60 kVA, respectively, are studied in order to verify the goodness of the model. The parameters of these transformers are measured in the laboratory, and comparison of the PSPICE model with real measurements has been successful.",
    "venue": "IEEE Transactions on Power Delivery",
    "year": 2002,
    "citationCount": 83,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Physics",
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2002-12-01",
    "authors": [
      {
        "authorId": "46644089",
        "name": "J. Pedra"
      },
      {
        "authorId": "2249052275",
        "name": "Luis Sáinz"
      },
      {
        "authorId": "9165906",
        "name": "F. Córcoles"
      },
      {
        "authorId": "2112658969",
        "name": "R. Lopez"
      },
      {
        "authorId": "46360773",
        "name": "M. Salichs"
      }
    ],
    "source": "semantic_scholar",
    "score": 127.4622519826497
  },
  {
    "paperId": "30080f65841a5266f58e940244849f27b3380919",
    "url": "https://www.semanticscholar.org/paper/30080f65841a5266f58e940244849f27b3380919",
    "title": "A study of volume and weight vs. frequency for high-frequency transformers",
    "abstract": "The relationship of the volume and the weight versus frequency for high-frequency transformers is studied. A complete analytical procedure taking into consideration core loss and winding loss (including eddy current loss) is developed. Analytical and experimental results reveal that higher frequency does result in smaller transformer design but an optimum frequency exists at which a transformer can be designed with the smallest volume and the lowest weight.<<ETX>>",
    "venue": "Power Electronics Specialists Conference",
    "year": 1993,
    "citationCount": 122,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Materials Science"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "1993-06-20",
    "authors": [
      {
        "authorId": "2254259448",
        "name": "Wen-Jian Gu"
      },
      {
        "authorId": "2255754120",
        "name": "Rui Liu"
      }
    ],
    "source": "semantic_scholar",
    "score": 127.18276533058625
  },
  {
    "paperId": "641b232a6d972b32a6b2ea622f6712be34e00ee0",
    "url": "https://www.semanticscholar.org/paper/641b232a6d972b32a6b2ea622f6712be34e00ee0",
    "title": "Development of optical instrument transformers",
    "abstract": "The authors describe the design(, assembly(, and testing results for prototypes of an optical current transformer (CT) and a voltage-dividing-type voltage transformer (PD). The optical CT and PD were developed to be applicable to three-phase enclosed-type 300 kV gas-insulated switchgear (GIS) and air-insulated 168 kV substation systems. Test results for the newly developed optical CT and PD showed that their basic properties conform with JEC 1201, the Japanese standard for electric-power instrument transformers. >",
    "venue": "",
    "year": 1990,
    "citationCount": 120,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1990-04-01",
    "authors": [
      {
        "authorId": "116518694",
        "name": "T. Sawa"
      },
      {
        "authorId": "145985339",
        "name": "K. Kurosawa"
      },
      {
        "authorId": "48768341",
        "name": "T. Kaminishi"
      },
      {
        "authorId": "104440045",
        "name": "T. Yokota"
      }
    ],
    "source": "semantic_scholar",
    "score": 126.93685818395112
  },
  {
    "paperId": "6ae2a396db745d9cf1b9912bd925a380ab489255",
    "url": "https://www.semanticscholar.org/paper/6ae2a396db745d9cf1b9912bd925a380ab489255",
    "title": "Transforming transformers ~superconducting windings\\",
    "abstract": "Use of high-temperature superconducting (HTS) windings may soon turn power transformers into compact high-performers on good terms with the environment. The potential for HTS transformers is being examined in major design and hardware development programs by several teams of engineers and scientists worldwide. The team to which the authors belong is led by Waukesha Electric Systems. The Waukesha-led team has conducted a series of reference designs concentrating mostly on a 30-MVA, 138-kV/13.8-kV transformer rating. This rating is representative of a medium power transformer class foreseen as comprising about half of all US power transformer sales in the next two decades. Two of these designed are compared with a 30-MVA conventional oil-filled transformer typical of this class. Each uses a different commercially successful and reliable type of refrigeration. Since reliable operation is paramount, both transformers are designed to have a several-day on-board refrigeration reserve that lets them operate continuously with no interruption of service to customers.",
    "venue": "",
    "year": 1997,
    "citationCount": 85,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1997-07-01",
    "authors": [
      {
        "authorId": "92744806",
        "name": "S. Mehta"
      },
      {
        "authorId": "30744990",
        "name": "N. Aversa"
      },
      {
        "authorId": "2116448557",
        "name": "M. S. Walker"
      }
    ],
    "source": "semantic_scholar",
    "score": 126.81520944380262
  },
  {
    "paperId": "c4c86773170643418adeddb4f67a7c0c545a74d5",
    "url": "https://www.semanticscholar.org/paper/c4c86773170643418adeddb4f67a7c0c545a74d5",
    "title": "C37.110 guide for the application of current transformers used for protective relaying purposes",
    "abstract": "Relay engineers have had to rely on many sources to compile information on current transformer application. Typically, they have drawn on relay application and transformer textbooks, manufacturer publications, and standards. Recognizing the need for a comprehensive one source document, the power system relaying committee has produced the C37.110-1996 guide for the application of current transformers used for protective relay purposes.",
    "venue": "",
    "year": 1999,
    "citationCount": 111,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2105489832",
        "name": "M. Conroy"
      },
      {
        "authorId": "2053023215",
        "name": "B. Nelson"
      },
      {
        "authorId": "48268832",
        "name": "B. Božoki"
      },
      {
        "authorId": "33666750",
        "name": "J. Chadwick"
      },
      {
        "authorId": "144590219",
        "name": "P. Drum"
      },
      {
        "authorId": "97912130",
        "name": "L. L. Dovrak"
      },
      {
        "authorId": "31341082",
        "name": "I. Hasenwinkle"
      },
      {
        "authorId": "144573040",
        "name": "J. Huddleston"
      },
      {
        "authorId": "46554328",
        "name": "W. Kotheimer"
      },
      {
        "authorId": "51086928",
        "name": "J. Linders"
      },
      {
        "authorId": "143601767",
        "name": "M. McDonald"
      },
      {
        "authorId": "98786949",
        "name": "G. Moskos"
      },
      {
        "authorId": "143988365",
        "name": "G. Parr"
      },
      {
        "authorId": "153262615",
        "name": "R. Ryan"
      },
      {
        "authorId": "122569138",
        "name": "E. Sage"
      },
      {
        "authorId": "30936187",
        "name": "D. Smaha"
      },
      {
        "authorId": "2069206501",
        "name": "K. Stephen"
      },
      {
        "authorId": "97440671",
        "name": "J. Stephens"
      },
      {
        "authorId": "47099436",
        "name": "J. Uchiyama"
      },
      {
        "authorId": "121729362",
        "name": "S. Zocholl"
      }
    ],
    "source": "semantic_scholar",
    "score": 125.77748306942641
  },
  {
    "paperId": "045f4205b7cb925127c8be2b836ea92898199c48",
    "url": "https://www.semanticscholar.org/paper/045f4205b7cb925127c8be2b836ea92898199c48",
    "title": "All-optical Hilbert transformer based on a single phase-shifted fiber Bragg grating: design and analysis.",
    "abstract": "A simple all-fiber design for implementing an all-optical temporal Hilbert transformer is proposed and numerically demonstrated. We show that an all-optical Hilbert transformer can be implemented using a uniform-period fiber Bragg grating (FBG) with a properly designed amplitude-only grating apodization profile incorporating a single pi phase shift in the middle of the grating length. All-optical Hilbert transformers capable of processing arbitrary optical waveforms with bandwidths up to a few hundreds of gigahertz can be implemented using feasible FBGs.",
    "venue": "Optics Letters",
    "year": 2009,
    "citationCount": 103,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Physics",
      "Medicine"
    ],
    "publicationTypes": [
      "JournalArticle"
    ],
    "publicationDate": "2009-02-01",
    "authors": [
      {
        "authorId": "39268350",
        "name": "M. Asghari"
      },
      {
        "authorId": "2777224",
        "name": "J. Azaña"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.66586348712059
  },
  {
    "paperId": "6cea53800d442ee1e0cb0d3feeb49f3a741c3296",
    "url": "https://www.semanticscholar.org/paper/6cea53800d442ee1e0cb0d3feeb49f3a741c3296",
    "title": "On-Load Tap-Changers for Power Transformers",
    "abstract": "On-load tap-changers (OLTCs) are indispensable in regulating power transformers used in electrical energy networks and industrial applications. This paper explains the technological developments of resistor-type OLTCs and reactor-type OLTCs. The general switching principles for OLTCs are discussed and OLTC applications are presented. Today’s OLTC design concepts, including the new generation of vacuum type OLTCs, are described. The vacuum switching technology used in OLTCs is the “state of the art” design now and in the foreseeable future. Examples of OLTC designs and the respective switching principles show the range of the usage of vacuum interrupters",
    "venue": "",
    "year": 2006,
    "citationCount": 101,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "72888546",
        "name": "D. Dohnal"
      },
      {
        "authorId": "2099107524",
        "name": "On-LOad Tap-Changers"
      },
      {
        "authorId": "2083898256",
        "name": "fOr pOwer TransfOrmers"
      },
      {
        "authorId": "2094179750",
        "name": "MR Knowledge Base"
      },
      {
        "authorId": "2099107524",
        "name": "On-LOad Tap-Changers"
      },
      {
        "authorId": "2083898256",
        "name": "fOr pOwer TransfOrmers"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.37459219926406
  },
  {
    "paperId": "31c981721bb6d48fcd4ac076f138c66d6491bd48",
    "url": "https://www.semanticscholar.org/paper/31c981721bb6d48fcd4ac076f138c66d6491bd48",
    "title": "Design, implementation and testing of a microprocessor-based high-speed relay for detecting transformer winding faults",
    "abstract": "The authors present the design, implementation and testing of a microprocessor-based high-speed relay that correctly detects transformer winding faults. Instead of relying on the presence of harmonics to identify magnetizing inrush, it uses a nonlinear model of the transformer to determine the state of its health. One version of the relay is suitable for protecting single-phase transformers, whereas another version is for protecting three-phase transformers. A three-phase delta-wye 15 kVA transformer was used to test the relay for a variety of operating conditions. The results show that the relay performs well. >",
    "venue": "",
    "year": 1992,
    "citationCount": 99,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "144466629",
        "name": "T. Sidhu"
      },
      {
        "authorId": "88537131",
        "name": "M. S. Sachdev"
      },
      {
        "authorId": "143877719",
        "name": "H. C. Wood"
      },
      {
        "authorId": "49862400",
        "name": "M. Nagpal"
      }
    ],
    "source": "semantic_scholar",
    "score": 124.07755278982138
  },
  {
    "paperId": "80190ed7860e843509f81c24c698fc0051854799",
    "url": "https://www.semanticscholar.org/paper/80190ed7860e843509f81c24c698fc0051854799",
    "title": "Harmonic analysis of transient currents during sympathetic interaction",
    "abstract": "The phenomenon of sympathetic interaction between transformers, which is very likely to occur when a transformer is energised onto a system to which there are other transformers already connected, changes significantly the duration and the magnitude of the transient magnetising currents in the transformers involved. This phenomenon is discussed in the paper, with the harmonic content of the transient currents and its associated overvoltages being analysed. It is shown that the importance of taking into consideration the saturation characteristics of the transformers already in operation when studying transformer transient inrush.",
    "venue": "",
    "year": 1996,
    "citationCount": 98,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "1996-11-01",
    "authors": [
      {
        "authorId": "31119368",
        "name": "H. Bronzeado"
      },
      {
        "authorId": "46706765",
        "name": "P. Brogan"
      },
      {
        "authorId": "31222814",
        "name": "R. Yacamini"
      }
    ],
    "source": "semantic_scholar",
    "score": 123.92679775201884
  },
  {
    "paperId": "22f379525c8c79520acf6faba23dbb9239c04105",
    "url": "https://www.semanticscholar.org/paper/22f379525c8c79520acf6faba23dbb9239c04105",
    "title": "Investigation on transformer design of high frequency high efficiency dc-dc converters",
    "abstract": "This paper studies the high-frequency high-efficiency transformer design. Several novel concepts are proposed to reveal the essence of the transformer design. In order to minimize the winding loss, several winding structure are proposed and compared. The planar transformer winding with PCB or spiral windings are discussed. The key factors to achieve the low winding losses are presented. The transformer design of the LLC resonant converter is investigated. The impacts on integration of magnetizing inductance and leakage inductance are studied.",
    "venue": "Applied Power Electronics Conference",
    "year": 2010,
    "citationCount": 96,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Materials Science"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2010-03-18",
    "authors": [
      {
        "authorId": "2772183",
        "name": "D. Fu"
      },
      {
        "authorId": "145519082",
        "name": "F. Lee"
      },
      {
        "authorId": "40440632",
        "name": "Shuo Wang"
      }
    ],
    "source": "semantic_scholar",
    "score": 123.62066467755074
  },
  {
    "paperId": "178e859b71aa21b98d5f2d38a9376f8f257b0ee4",
    "url": "https://www.semanticscholar.org/paper/178e859b71aa21b98d5f2d38a9376f8f257b0ee4",
    "title": "Computation of inrush current forces on transformer windings",
    "abstract": "Inrush currents are frequently encountered during the switching process of transformers. The purpose of this paper is to present an investigation of forces that appear as a result of inrush currents. While both short circuit and inrush transformer situations might seem identical, they vary significantly from the core magnetization viewpoint. Three-dimensional computations have been carried out on a typical single-phase power transformer using the Hopfield neural network energy minimization technique. Results are given in the paper as well as comparison with short circuit forces.",
    "venue": "",
    "year": 2001,
    "citationCount": 90,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Physics"
    ],
    "publicationTypes": null,
    "publicationDate": "2001-07-01",
    "authors": [
      {
        "authorId": "1956473",
        "name": "A. Adly"
      }
    ],
    "source": "semantic_scholar",
    "score": 122.66289259775274
  },
  {
    "paperId": "5ee0006fb11e891a1a0532e7b7c2b789273800a1",
    "url": "https://www.semanticscholar.org/paper/5ee0006fb11e891a1a0532e7b7c2b789273800a1",
    "title": "Opportunities",
    "abstract": "Electrical Engineer with first class experience in both large and small transformer engineering. Would go into transformer engineering department of large manufacturing company as assistant engineer. Duties will include much special development work. Location Indiana. R-2447.",
    "venue": "Journal of the American Institute of Electrical Engineers",
    "year": null,
    "citationCount": 240,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "2338345304",
        "name": "Sponsorship Brochure"
      },
      {
        "authorId": "2338347791",
        "name": "Molly Stevens"
      },
      {
        "authorId": "2338345461",
        "name": "Daniel Fletcher"
      },
      {
        "authorId": "2338400430",
        "name": "Donny Liang"
      }
    ],
    "source": "semantic_scholar",
    "score": 122.27195400235982
  },
  {
    "paperId": "f47b3672282d34135e3626e1f2764ba297472a1e",
    "url": "https://www.semanticscholar.org/paper/f47b3672282d34135e3626e1f2764ba297472a1e",
    "title": "Distribution transformer loss of life evaluation: a novel approach based on daily load profiles",
    "abstract": "This work presents a methodology for distribution transformer rating selection and management. Measured or estimated daily load profiles are used to determine the transformer's loss of life. Loss of life values are obtained for a number of transformers and are stored together with the corresponding load profiles in a database of patterns. The expected loss of life for a transformer not included in the database is obtained by comparing the transformer's actual load profile to the ones stored in the database.",
    "venue": "",
    "year": 2000,
    "citationCount": 86,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": null,
    "authors": [
      {
        "authorId": "20510229",
        "name": "J. Jardini"
      },
      {
        "authorId": "3096185",
        "name": "H. P. Schmidt"
      },
      {
        "authorId": "47093667",
        "name": "C. Tahan"
      },
      {
        "authorId": "144376076",
        "name": "C. Oliveira"
      },
      {
        "authorId": "2107589839",
        "name": "S. U. Ahn"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.98862177981876
  },
  {
    "paperId": "22df052c2c9ed275319dd26b80bdb4605db216e2",
    "url": "https://www.semanticscholar.org/paper/22df052c2c9ed275319dd26b80bdb4605db216e2",
    "title": "Determination of transformer health condition using artificial neural networks",
    "abstract": "This paper presents a method to estimate a transformer health condition based on diagnostic tests. A feed forward artificial neural network (FFANN) is used to find the health index of the transformer. The health index is used to find the health condition of the transformer. The training of the FFANN is done using real measurements of 59 working transformers. The testing of the trained neural network performance is done using real data for 29 working transformers. The performance evaluation of the trained FFANN shows that the trained neural network is reliable in finding the health condition of any working transformer.",
    "venue": "International Symposium on INnovations in Intelligent SysTems and Applications",
    "year": 2011,
    "citationCount": 61,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2011-06-15",
    "authors": [
      {
        "authorId": "1403779807",
        "name": "A. Abu-Elanien"
      },
      {
        "authorId": "2239432308",
        "name": "Magdy M. A. Salama"
      },
      {
        "authorId": "2114818361",
        "name": "M. Ibrahim"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.90701577567637
  },
  {
    "paperId": "8dc7525918d54958b1b2ad1cd362a0db06c9dcc2",
    "url": "https://www.semanticscholar.org/paper/8dc7525918d54958b1b2ad1cd362a0db06c9dcc2",
    "title": "Performance evaluation of solid state transformer based microgrid in FREEDM systems",
    "abstract": "A new concept of solid state transformer based microgrid system is presented in this paper. By utilizing 400V DC bus generated from Gen-I solid state transformer proposed by FREEDM systems center, integration issues of DC microgrid and solid state transformer are analyzed. Zonal DC microgrid concept is applied to this novel system with the consideration of burden minimization to the existing AC grid. The future grid architecture is described by using this solid state transformer based integrated microgrid system.",
    "venue": "Applied Power Electronics Conference",
    "year": 2011,
    "citationCount": 83,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Conference"
    ],
    "publicationDate": "2011-03-06",
    "authors": [
      {
        "authorId": "1923276",
        "name": "X. She"
      },
      {
        "authorId": "1956821",
        "name": "S. Lukic"
      },
      {
        "authorId": "2826163",
        "name": "A. Huang"
      },
      {
        "authorId": "143738288",
        "name": "S. Bhattacharya"
      },
      {
        "authorId": "4399151",
        "name": "M. Baran"
      }
    ],
    "source": "semantic_scholar",
    "score": 121.4622519826497
  },
  {
    "paperId": "c1dbbf363f67590e6dfb6893365cfd520fa2f45b",
    "url": "https://www.semanticscholar.org/paper/c1dbbf363f67590e6dfb6893365cfd520fa2f45b",
    "title": "Evaluation of the shielding effects on printed-circuit-board transformers using ferrite plates and copper sheets",
    "abstract": "This paper presents an effective shielding technique and a simple structure for printed-circuit-board (PCB) transformers. The performance of PCB transformers using the proposed shielding technique is evaluated using thin ferrite plates and copper sheets. Without affecting the transformer energy efficiency, the shielding method under investigation can achieve 28 dB shielding effectiveness (SE), which is much higher than the SE (about 4 dB) obtained by shielding the transformer windings with only two ferrite plates. The proposed PCB transformer structure is very simple and has high energy efficiency (>90%) for Megahertz operation.",
    "venue": "",
    "year": 2002,
    "citationCount": 80,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Materials Science"
    ],
    "publicationTypes": null,
    "publicationDate": "2002-11-01",
    "authors": [
      {
        "authorId": "9084657",
        "name": "S. C. Tang"
      },
      {
        "authorId": "145312306",
        "name": "S. Hui"
      },
      {
        "authorId": "145155807",
        "name": "H. Chung"
      }
    ],
    "source": "semantic_scholar",
    "score": 120.91673732008658
  },
  {
    "paperId": "85a59105fa5a747049f87170b6facc70911af45a",
    "url": "https://www.semanticscholar.org/paper/85a59105fa5a747049f87170b6facc70911af45a",
    "title": "Modeling and characterization of on-chip transformers",
    "abstract": "We present a scalable analytical model for on-chip transformers that is suitable for design optimization and circuit simulation. We also provide simple and accurate expressions for evaluating the self inductance and the mutual coupling coefficient (k). The model agrees very well with measurements for a variety of transformer configurations.",
    "venue": "International Electron Devices Meeting 1998. Technical Digest (Cat. No.98CH36217)",
    "year": 1998,
    "citationCount": 111,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Materials Science"
    ],
    "publicationTypes": null,
    "publicationDate": "1998-12-06",
    "authors": [
      {
        "authorId": "144660235",
        "name": "S. S. Mohan"
      },
      {
        "authorId": "2816498",
        "name": "P. Yue"
      },
      {
        "authorId": "11140307",
        "name": "M. del Mar Hershenson"
      },
      {
        "authorId": "2125878371",
        "name": "S. Wong"
      },
      {
        "authorId": "2110859735",
        "name": "T. H. Lee"
      }
    ],
    "source": "semantic_scholar",
    "score": 120.77748306942641
  },
  {
    "paperId": "cba1613904c7cb056eeaef8ec1688aa3091d88ad",
    "url": "https://www.semanticscholar.org/paper/cba1613904c7cb056eeaef8ec1688aa3091d88ad",
    "title": "Power Transformers: Principles and Applications",
    "abstract": "Basic Transformer Theory Two-Winding Transformer Connections Transformer Impedance and Losses Autotransformers and Three-Winding Transformers Short Circuits, Inrush Currents, and Other Phenomena Ancillary Equipment Reading and Applying Nameplate Information Maintenance, Testing, Troubleshooting, and Reliability Index",
    "venue": "",
    "year": 2002,
    "citationCount": 104,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2002-04-12",
    "authors": [
      {
        "authorId": "71649873",
        "name": "J. Winders"
      }
    ],
    "source": "semantic_scholar",
    "score": 119.80940525236285
  },
  {
    "paperId": "b519960a425a854799014baa0accc728c5376803",
    "url": "https://www.semanticscholar.org/paper/b519960a425a854799014baa0accc728c5376803",
    "title": "A Review of Transformer Losses",
    "abstract": "Abstract This article presents an extensive survey of current research on the transformer loss problem, particularly from the view of practical engineering applications. It reveals that the transformer loss problem remains an active research area. This article classified the transformer loss problem into three main groups: (a) tank losses due to high-current bushings, (b) losses in transformer core joints, and (c) stray losses in the transformer tank. It is based on over 50 published works, which are all systematically classified. The methods, the size of transformers, and other relevant aspects in the different works are discussed and presented.",
    "venue": "",
    "year": 2009,
    "citationCount": 66,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": [
      "Review"
    ],
    "publicationDate": "2009-08-21",
    "authors": [
      {
        "authorId": "1403490714",
        "name": "J. C. Olivares-Galvan"
      },
      {
        "authorId": "34794334",
        "name": "P. Georgilakis"
      },
      {
        "authorId": "1422300938",
        "name": "R. Ocon-Valdez"
      }
    ],
    "source": "semantic_scholar",
    "score": 118.07038929086448
  },
  {
    "paperId": "0a35f42c34054e572fc6f8b179361ac92a8acb41",
    "url": "https://www.semanticscholar.org/paper/0a35f42c34054e572fc6f8b179361ac92a8acb41",
    "title": "Spotlight on Modern Transformer Design",
    "abstract": null,
    "venue": "",
    "year": 2009,
    "citationCount": 135,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Engineering"
    ],
    "publicationTypes": null,
    "publicationDate": "2009-08-07",
    "authors": [
      {
        "authorId": "34794334",
        "name": "P. Georgilakis"
      }
    ],
    "source": "semantic_scholar",
    "score": 113.68982328604079
  }
]